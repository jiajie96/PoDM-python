{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import os\n",
    "import torchvision.utils as tvu\n",
    "import cv2\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(P,Q):\n",
    "    var_p = P.var()\n",
    "    var_q = Q.var()\n",
    "    mean_p = P.mean()\n",
    "    mean_q = Q.mean()\n",
    "\n",
    "    divergence = np.log(np.sqrt(var_p)/np.sqrt(var_p)) + (var_p + (mean_p - mean_q)**2)/(2 * var_q) - 0.5\n",
    "     \n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x20000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = 'FFHQ'\n",
    "dataset_files_list = os.listdir(dataset_path)\n",
    "print(len(dataset_files_list))\n",
    "\n",
    "row_images=[]\n",
    "for dataset_file_name in dataset_files_list[:10]:\n",
    "        image=cv2.imread(os.path.join(dataset_path, dataset_file_name), cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (64,64), interpolation = cv2.INTER_AREA)\n",
    "        row_images.append(image)\n",
    "        \n",
    "row_images = np.array(row_images)\n",
    "row_images = row_images.astype('int')\n",
    "\n",
    "row_images = row_images/127.5-1\n",
    "# row_images = np.double(row_images)\n",
    "\n",
    "X_train_r = row_images[:,:,:,2]\n",
    "X_train_g = row_images[:,:,:,1]\n",
    "X_train_b = row_images[:,:,:,0]\n",
    "\n",
    "row_images = np.stack([X_train_r,X_train_g,X_train_b],3)\n",
    "\n",
    "plt.figure(figsize=(40, 200))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow((row_images[i, :, :, :]+1)/2)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_images = []\n",
    "masks = []\n",
    "bbs = [[17,55,12,44], [14,53,18,46], [17,55,15,40], [20,52,14,47], [20,55,18,48], [20,55,10,43]]\n",
    "for i, bb in enumerate(bbs):\n",
    "    masked_image = row_images[i, :, :, :].copy()\n",
    "    mask = np.zeros_like(masked_image)\n",
    "    masked_image[bb[0]:bb[1], bb[2]:bb[3], :] = (masked_image[bb[0]:bb[1], bb[2]:bb[3], :]+1)/2\n",
    "    mask[bb[0]:bb[1], bb[2]:bb[3], :] = 1\n",
    "    masked_images.append(masked_image)\n",
    "    masks.append(mask)\n",
    "plt.figure(figsize=(30, 5))\n",
    "for i in range(len(bbs)):\n",
    "    plt.subplot(1, len(bbs), i+1)\n",
    "    plt.imshow((masked_images[i]+1)/2)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout(h_pad=0, w_pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 28 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_steps = [0, 0.2, 0.3, 0.4, 0.6, 1,3]\n",
    "# fig, axs = plt.figure(figsize=(25, 8))\n",
    "fig, axs = plt.subplots(4, len(t_steps), figsize=(16, 6))\n",
    "plt.ioff()\n",
    "colors = ['red', 'green', 'blue', 'olive']\n",
    "color_b = 'gray'\n",
    "colors_b = [color_b,color_b,color_b, 'gray']\n",
    "masks = []\n",
    "bbs = [[17,55,12,44], [14,53,18,46], [17,55,15,40], [20,52,14,47], [20,55,18,48], [20,55,10,43]]\n",
    "for i, bb in enumerate(bbs):\n",
    "    mask = np.zeros_like(row_images[i, :, :, 0])\n",
    "    mask[bb[0]:bb[1], bb[2]:bb[3]] = 1\n",
    "    masks.append(mask)\n",
    "\n",
    "for color_channel in range(4):\n",
    "    if color_channel ==3:\n",
    "        r,g,b = row_images[:6,:,:,0], row_images[:6,:,:,1], row_images[:6,:,:,2]\n",
    "        checking_image_values = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "        checking_image_values = checking_image_values.flatten()\n",
    "    else:\n",
    "        checking_image_values = row_images[:6,:,:,color_channel].flatten()\n",
    "    mask_flatten = np.asarray(masks).flatten()\n",
    "\n",
    "    feature_map = []\n",
    "    background_map = []\n",
    "    for i in range(mask_flatten.shape[0]):\n",
    "        if mask_flatten[i]==1:\n",
    "            feature_map.append(i)\n",
    "        elif mask_flatten[i]==0:\n",
    "            background_map.append(i)\n",
    "            \n",
    "    list_of_flatten = [checking_image_values]\n",
    "    list_of_feature = [checking_image_values[feature_map]]\n",
    "    list_of_background = [checking_image_values[background_map]]\n",
    "\n",
    "    for i, t_cur in enumerate(t_steps[1:]):\n",
    "        checking_images = checking_image_values + (t_cur) * np.random.randn(*checking_image_values.shape)\n",
    "        list_of_flatten.append(checking_images)\n",
    "        list_of_feature.append(checking_images[feature_map])\n",
    "        list_of_background.append(checking_images[background_map])\n",
    "    num_bins = 100\n",
    "    alpha = 0.4\n",
    "    for t in range(len(t_steps)):\n",
    "        plt.subplot(4, len(t_steps), t+1+color_channel*len(t_steps))\n",
    "        title = r'$\\sigma=$'+ f'%.2f'%t_steps[t]\n",
    "        if color_channel == 3:\n",
    "            plt.xlabel(title, loc= 'center',fontsize = 30)\n",
    "            # plt.title(title, fontsize = 30)\n",
    "        if t ==0:\n",
    "            plt.hist(list_of_feature[t], num_bins, density = False , color= colors[color_channel],label = 'object', alpha=alpha)\n",
    "            plt.hist(list_of_background[t], num_bins, density = False, color= colors_b[color_channel],label = 'background', alpha=alpha)\n",
    "        elif t ==1:\n",
    "            plt.hist(list_of_feature[t], num_bins, density = False , color= colors[color_channel],label = 'object', alpha=alpha)\n",
    "            plt.hist(list_of_background[t], num_bins, density = False, color= colors_b[color_channel],label = 'background', alpha=alpha)\n",
    "        else:\n",
    "            plt.hist(list_of_feature[t], num_bins, density = True , color= colors[color_channel],label = 'object', alpha=alpha)\n",
    "\n",
    "            plt.hist(list_of_background[t], num_bins, density = True, color= colors_b[color_channel],label = 'background', alpha=alpha)\n",
    "        \n",
    "        # plt.ylabel('density') \n",
    "        plt.yticks([]) \n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['left'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "        if t <=3:\n",
    "            plt.xticks([-1,1],['-1','1']) \n",
    "\n",
    "        if t ==0:\n",
    "            bb = (fig.subplotpars.left, fig.subplotpars.top-0.1, \n",
    "            fig.subplotpars.right-fig.subplotpars.left+1.4,.1)\n",
    "            axs[color_channel,0].legend(bbox_to_anchor=bb, mode=\"expand\", loc=\"lower left\",\n",
    "               ncol=10, borderaxespad=0, prop = { \"size\": 20 }, frameon=False)\n",
    "        \n",
    "        axs[color_channel,1].set_zorder(-1)\n",
    "        if t <=1:\n",
    "            # plt.ylim(0, 3)\n",
    "            plt.ylim(0, 800)\n",
    "\n",
    "# bb = (fig.subplotpars.left+0.1, fig.subplotpars.top+0.02, \n",
    "#       fig.subplotpars.right-fig.subplotpars.left-0.2,.1)\n",
    "# axs[3,2].legend(bbox_to_anchor=bb, mode=\"expand\", loc=\"lower left\",\n",
    "#                ncol=10, borderaxespad=0, bbox_transform=fig.transFigure)\n",
    "\n",
    "plt.tight_layout(h_pad=-0.2, w_pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.62554654e-36 6.46491983e-18 2.12816350e-13 8.15201862e-09\n",
      " 1.05877174e-03 1.33948357e-04 2.19627749e-02 6.35014996e-02\n",
      " 9.76336062e-01 7.95782954e-02 4.99427408e-01 2.04318136e-01\n",
      " 4.84739810e-01 4.18209970e-01 1.35240659e-01 4.13322449e-01\n",
      " 1.35151548e-02 5.66475987e-01 2.77308285e-01 1.52138233e-01\n",
      " 9.32229817e-01 4.33562696e-01 4.63010401e-01 9.83252704e-01]\n",
      "(24,)\n",
      "[-0.12617054660361476, -0.11489261339872159, -0.10982494139213206, -0.10679132670442149, -0.08584709098338311, -0.09078131048233073, -0.060453770948048435, -0.05561518356626288, -0.03527267098036946, -0.02607453690495648, -0.0072866889646991395, -0.017377687942269182, -0.014829647354112185, -0.019276476184563407, -0.0032751303524711783, -0.004312551266763831, -0.0071533718047405714, 0.02114344090277287, -0.003617971530288866, 0.005318339567415897, 0.004642558768946814, -0.0039561011464108176, 0.00401642511284428, -0.009861338429832445]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/scipy/stats/_morestats.py:1800: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    }
   ],
   "source": [
    "color_channel = 2\n",
    "t_steps = [0, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 1, 1.2, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0, 6, 7, 8, 9, 10, 11, 12]\n",
    "# t_steps = [0, 0.02, 0.2, 0.4, 0.6, 0.8, 1, 3.0, 6.0, 10]\n",
    "\n",
    "# checking_image_values = row_images.flatten()\n",
    "row_images_normalized = (row_images+1)/2\n",
    "r,g,b = row_images_normalized[:6,:,:,0], row_images_normalized[:6,:,:,1], row_images_normalized[:6,:,:,2]\n",
    "row_images_normalized = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "row_images_normalized = row_images_normalized*2-1\n",
    "checking_image_values = checking_image_values.flatten()\n",
    "\n",
    "mask_flatten = np.asarray(masks).flatten()\n",
    "\n",
    "feature_map = []\n",
    "background_map = []\n",
    "for i in range(mask_flatten.shape[0]):\n",
    "    if mask_flatten[i]==1:\n",
    "        feature_map.append(i)\n",
    "    elif mask_flatten[i]==0:\n",
    "        background_map.append(i)\n",
    "        \n",
    "list_of_flatten = [checking_image_values.flatten()]\n",
    "list_of_feature = [checking_image_values.flatten()[feature_map]]\n",
    "list_of_background = [checking_image_values.flatten()[background_map]]\n",
    "\n",
    "list_of_kld = [KL(checking_image_values.flatten()[feature_map], checking_image_values.flatten()[background_map])]\n",
    "list_of_shapiro = [shapiro(checking_image_values.flatten()[feature_map]).pvalue]\n",
    "\n",
    "for i, t_cur in enumerate(t_steps[1:]):\n",
    "    checking_images = checking_image_values + (t_cur) * np.random.randn(*checking_image_values.shape)\n",
    "    list_of_flatten.append(checking_images.flatten())\n",
    "    list_of_feature.append(checking_images.flatten()[feature_map])\n",
    "    list_of_background.append(checking_images.flatten()[background_map])\n",
    "    list_of_kld.append(KL(checking_images.flatten()[feature_map], checking_images.flatten()[background_map]))      \n",
    "    list_of_shapiro.append(shapiro(checking_images.flatten()[feature_map]).pvalue) \n",
    "\n",
    "list_of_shapiro = np.asarray(list_of_shapiro)\n",
    "\n",
    "print(list_of_shapiro)\n",
    "print(list_of_shapiro.shape)\n",
    "print(list_of_kld)\n",
    "num_bins = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011962774899999999"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.19627749e-02 -0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAHelper(object):\n",
    "    \"\"\"read about EMA\n",
    "    exponential moving average (EMA) of model weights\n",
    "    \"\"\"\n",
    "    def __init__(self, mu=0.999):\n",
    "        self.mu = mu\n",
    "        self.shadow = {}\n",
    "\n",
    "    def register(self, module):\n",
    "        if isinstance(module,torch.nn.DataParallel):\n",
    "            module = module.module\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, module):\n",
    "        if isinstance(module,torch.nn.DataParallel):\n",
    "            module = module.module\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name].data = (\n",
    "                    1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
    "\n",
    "    def ema(self, module):\n",
    "        if isinstance(module,torch.nn.DataParallel):\n",
    "            module = module.module\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    def ema_copy(self, module):\n",
    "        if isinstance(module,torch.nn.DataParallel):\n",
    "            inner_module = module.module\n",
    "            module_copy = type(inner_module)(\n",
    "                inner_module.config).to(inner_module.config.device)\n",
    "            module_copy.load_state_dict(inner_module.state_dict())\n",
    "            module_copy =torch.nn.DataParallel(module_copy, inner_module.args.dataparallel)\n",
    "        else:\n",
    "            module_copy = type(module)(module.config).to(module.config.device)\n",
    "            module_copy.load_state_dict(module.state_dict())\n",
    "        self.ema(module_copy)\n",
    "        return module_copy\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.shadow\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.shadow = state_dict\n",
    "\n",
    "def get_optimizer(config, parameters):\n",
    "    if config.optim.optimizer == 'Adam':\n",
    "        return optim.Adam(parameters, lr=config.optim.lr, weight_decay=config.optim.weight_decay,\n",
    "                          betas=(config.optim.beta1, 0.999), amsgrad=config.optim.amsgrad,\n",
    "                          eps=config.optim.eps)\n",
    "    elif config.optim.optimizer == 'RMSProp':\n",
    "        return optim.RMSprop(parameters, lr=config.optim.lr, weight_decay=config.optim.weight_decay)\n",
    "    elif config.optim.optimizer == 'SGD':\n",
    "        return optim.SGD(parameters, lr=config.optim.lr, momentum=0.9)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'Optimizer {} not understood.'.format(config.optim.optimizer))\n",
    "    \n",
    "def noise_estimation_loss(net, images, labels=None, augment_pipe=None):\n",
    "    rnd_normal = torch.randn([images.shape[0],], device=images.device)\n",
    "    P_mean = np.log(args.sigma_start*args.sigma_end)/2\n",
    "    P_std = np.log(args.sigma_start/args.sigma_end)/2\n",
    "    sigma = (rnd_normal * P_std + P_mean).exp()\n",
    "    reshaped_sigma = sigma.reshape(images.shape[0], 1, 1, 1)\n",
    "    y = images\n",
    "    n = torch.randn_like(y) * reshaped_sigma\n",
    "    D_yn = net(y + n, sigma)\n",
    "    loss = ((D_yn - y) ** 2)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(object):\n",
    "    def __init__(self, args, config, device=None\n",
    "                ):\n",
    "        self.args = args\n",
    "        self.config = config\n",
    "        self.sigma_min = args.sigma_end **2 /args.sigma_start\n",
    "        self.sigma_max = args.sigma_start **2 /args.sigma_end\n",
    "        self.rho = 5\n",
    "        self.loss_his = []\n",
    "        self.val_loss_his = []\n",
    "        self.val_loss_steps = []\n",
    "        self.P_mean = np.log(args.sigma_start*args.sigma_end)/2\n",
    "        self.P_std = np.log(args.sigma_start/args.sigma_end)/2\n",
    "        \n",
    "        if device is None:\n",
    "            device = (\n",
    "                self.config.device\n",
    "                if torch.cuda.is_available()\n",
    "                else torch.device(\"cpu\")\n",
    "            )\n",
    "        self.device = device\n",
    "        self.num_timesteps = config.diffusion.num_diffusion_timesteps\n",
    "            \n",
    "    def round_sigma(self, sigma):\n",
    "        return torch.as_tensor(sigma)\n",
    "    \n",
    "    def train(self):\n",
    "        print(self.sigma_max)\n",
    "        print(self.sigma_min)\n",
    "        args, config = self.args, self.config\n",
    "        tb_logger = self.config.tb_logger\n",
    "        dataset = train_images_tensor\n",
    "        val_dataset = val_images_tensor[:config.training.batch_size]\n",
    "#         val_dataset = val_images_tensor\n",
    "        train_loader = data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.training.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=config.data.num_workers,\n",
    "        )\n",
    "        model = Model(config)\n",
    "\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Number of parameters in the model: {total_params}\")\n",
    "        \n",
    "        model = model.to(self.device)\n",
    "        model = torch.nn.DataParallel(model,self.args.dataparallel)\n",
    "\n",
    "        optimizer = get_optimizer(self.config, model.parameters())\n",
    "\n",
    "        if self.config.model.ema:\n",
    "            ema_helper = EMAHelper(mu=self.config.model.ema_rate)\n",
    "            ema_helper.register(model)\n",
    "        else:\n",
    "            ema_helper = None\n",
    "\n",
    "        start_epoch, step = 0, 0\n",
    "        if self.args.resume_training:\n",
    "            states = torch.load(os.path.join(self.args.log_path, \"ckpt.pth\"))\n",
    "            model.load_state_dict(states[0])\n",
    "\n",
    "            states[1][\"param_groups\"][0][\"eps\"] = self.config.optim.eps\n",
    "            states[1][\"param_groups\"][0][\"lr\"] = self.config.optim.lr\n",
    "            optimizer.load_state_dict(states[1])\n",
    "            start_epoch = states[2]\n",
    "            step = states[3]\n",
    "            if self.config.model.ema:\n",
    "                ema_helper.load_state_dict(states[4])\n",
    "\n",
    "        for epoch in range(start_epoch, self.config.training.n_epochs):\n",
    "            data_start = time.time()\n",
    "            data_time = 0\n",
    "            for i, x in enumerate(train_loader):\n",
    "                data_time += time.time() - data_start\n",
    "                model.train()\n",
    "                step += 1\n",
    "                x = x.to(self.device)\n",
    "                loss_registry = {'simple': noise_estimation_loss,}\n",
    "\n",
    "                loss = loss_registry[config.model.type](net = model, images = x)\n",
    "\n",
    "                print(\n",
    "                    f\"step: {step}, loss: {loss.item()}, data time: {data_time / (i+1)}\"\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                try:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), config.optim.grad_clip\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                if self.config.model.ema:\n",
    "                    ema_helper.update(model)\n",
    "                \n",
    "                data_start = time.time()\n",
    "            \n",
    "            if epoch % self.config.training.snapshot_freq == 0 or epoch == 0:\n",
    "                states = [\n",
    "                    model.state_dict(),\n",
    "                    optimizer.state_dict(),\n",
    "                    epoch,\n",
    "                    step,\n",
    "                ]\n",
    "                if self.config.model.ema:\n",
    "                    states.append(ema_helper.state_dict())\n",
    "\n",
    "                torch.save(\n",
    "                    states,\n",
    "                    os.path.join(self.args.log_path, \"ckpt_{}.pth\".format(step)),\n",
    "                )\n",
    "                torch.save(states, os.path.join(self.args.log_path, \"ckpt.pth\"))\n",
    "                \n",
    "            if epoch % self.config.training.validation_freq == 0 :   \n",
    "                model.eval()\n",
    "                self.sample_sequence(model, training = True, epoch = epoch)\n",
    "            \n",
    "            val_dataset = val_dataset.to(self.device)\n",
    "            val_loss = loss_registry[config.model.type](net = model, images = val_dataset)\n",
    "            self.val_loss_his = np.append(self.val_loss_his, val_loss.item())\n",
    "            self.val_loss_steps = np.append(self.val_loss_steps, epoch)\n",
    "            \n",
    "            self.loss_his = np.append(self.loss_his,loss.item())\n",
    "            his_len = [ _ for _ in range(0, len(self.loss_his))]\n",
    "            plt.figure()\n",
    "            plt.plot(his_len, self.loss_his, label = \"loss\")\n",
    "            plt.plot(self.val_loss_steps, self.val_loss_his, label = \"val_loss\")\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(self.args.log_path, 'loss_his.png'))\n",
    "            np.save(os.path.join(self.args.log_path, \"loss_his.npy\"), self.loss_his)\n",
    "\n",
    "    def sample(self):\n",
    "        model = Model(self.config)\n",
    "        if self.args.ckpt_id is None:\n",
    "            states = torch.load(\n",
    "                os.path.join(self.args.log_path, \"ckpt.pth\"),\n",
    "                map_location=self.config.device,\n",
    "            )\n",
    "        else:\n",
    "            states = torch.load(\n",
    "                os.path.join(\n",
    "                    self.args.log_path, f\"ckpt_{self.args.ckpt_id}.pth\"\n",
    "                ),\n",
    "                map_location=self.config.device,\n",
    "            )\n",
    "            print('load ckpt: ', self.args.ckpt_id)\n",
    "            \n",
    "        model = model.to(self.device)\n",
    "        model = torch.nn.DataParallel(model, self.args.dataparallel)\n",
    "#         model = model.to(self.device)\n",
    "        model.load_state_dict(states[0], strict=True)\n",
    "        print('load epoch: ', states[3])\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if self.args.fid:\n",
    "            self.sample_fid(model)\n",
    "        elif self.args.interpolation:\n",
    "            self.sample_interpolation(model)\n",
    "        elif self.args.sequence:\n",
    "            self.sample_sequence(model)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Sample procedeure not defined\")\n",
    "\n",
    "    def sample_fid(self, model):\n",
    "        config = self.config\n",
    "        img_id = len(glob.glob(f\"{self.args.image_folder}/*\"))\n",
    "        print(f\"starting from image {img_id}\")\n",
    "        total_n_samples = 300\n",
    "        n_rounds = (total_n_samples - img_id) // config.sampling.batch_size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm.tqdm(\n",
    "                range(n_rounds), desc=\"Generating image samples for FID evaluation.\"\n",
    "            ):\n",
    "                n = config.sampling.batch_size\n",
    "                x = torch.randn(\n",
    "                    n,\n",
    "                    config.data.channels,\n",
    "                    config.data.image_size,\n",
    "                    config.data.image_size,\n",
    "                    device=self.device,\n",
    "                )\n",
    "\n",
    "                x = self.sample_image(x, model)\n",
    "#                 x = inverse_data_transform(config, x)\n",
    "                x = [(y + 1.0) / 2.0 for y in x]\n",
    "                for i in range(n):\n",
    "                    tvu.save_image(\n",
    "                        x[i], os.path.join(self.args.image_folder, f\"{img_id}.png\")\n",
    "                    )\n",
    "                    img_id += 1\n",
    "\n",
    "    def sample_sequence(self, model, training = False, epoch = None):\n",
    "        config = self.config\n",
    "\n",
    "        x = torch.randn(\n",
    "            config.sampling.batch_size,\n",
    "            config.data.channels,\n",
    "            config.data.image_size,\n",
    "            config.data.image_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "        data_start = time.time()\n",
    "        data_time = 0\n",
    "        # NOTE: This means that we are producing each predicted x0, not x_{t-1} at timestep t.\n",
    "        with torch.no_grad():\n",
    "            x = self.sample_image(x, model)\n",
    "        data_time += time.time() - data_start\n",
    "        print(f\"the sample time of {self.config.sampling.batch_size} images takes {data_time}\")\n",
    "\n",
    "#         x = [inverse_data_transform(config, y) for y in x]\n",
    "        x = [(y + 1.0) / 2.0 for y in x]\n",
    "#         if not training:\n",
    "#             for i in range(len(x)):\n",
    "#                 for j in range(x[i].size(0)):\n",
    "#                     tvu.save_image(\n",
    "#                         x[i][j], os.path.join(self.args.image_folder, f\"{step}_{j}_{i}.png\")\n",
    "#                     )\n",
    "        for i in range(20):\n",
    "            tvu.save_image(x[i], os.path.join(self.args.image_folder, f\"generated_{i}.png\"))\n",
    "        if not training:\n",
    "            plt.figure(figsize=(40, 8))\n",
    "            \n",
    "            for i in range(20):\n",
    "                plt.ioff()\n",
    "                plt.subplot(2, 10, i+1)\n",
    "                img = x[i]\n",
    "                img = img.permute((1, 2, 0)).numpy()\n",
    "                # img = (img+1)*2\n",
    "                plt.imshow(np.asarray(img))\n",
    "                plt.axis('off')\n",
    "            plt.savefig(os.path.join(self.args.image_folder, f\"generated_image_{self.args.sample_type}_{self.args.skip_type}_{self.args.timesteps}_{data_time}s.png\"))  \n",
    "        \n",
    "        else:\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            \n",
    "            for i in range(8):\n",
    "                plt.ioff()\n",
    "                plt.subplot(2, 4, i+1)\n",
    "                img = x[i]\n",
    "                img = img.permute((1, 2, 0)).numpy()\n",
    "                # img = (img+1)*2\n",
    "                plt.imshow(np.asarray(img))\n",
    "                plt.axis('off')\n",
    "            plt.savefig(os.path.join(self.args.image_folder, f\"{epoch}.png\"))\n",
    "            \n",
    "#             tvu.save_image(\n",
    "#                         x[999,:,:,:,:], os.path.join(self.args.image_folder, f\"{step}.png\")\n",
    "#                     )\n",
    "\n",
    "    def sample_interpolation(self, model):\n",
    "        config = self.config\n",
    "\n",
    "        test_dataset = test_images_tensor \n",
    "        test_dataset = test_dataset.to(self.device)\n",
    "        \n",
    "        def slerp(z1, z2, alpha):\n",
    "            theta = torch.acos(torch.sum(z1 * z2) / (torch.norm(z1) * torch.norm(z2)))\n",
    "            return (\n",
    "                torch.sin((1 - alpha) * theta) / torch.sin(theta) * z1\n",
    "                + torch.sin(alpha * theta) / torch.sin(theta) * z2\n",
    "            )\n",
    "        x1 = test_dataset[:1]\n",
    "        x2 = test_dataset[7:8]\n",
    "\n",
    "        tvu.save_image(x1, os.path.join(self.args.generated_image_folder, f\"source_1.png\"))\n",
    "        tvu.save_image(x2, os.path.join(self.args.generated_image_folder, f\"source_2.png\"))\n",
    "        \n",
    "         # Time step discretization.\n",
    "        step_indices = torch.arange(self.args.timesteps, device=self.device)\n",
    "        t_steps = (self.sigma_max ** (1 / self.rho) + step_indices / (self.args.timesteps - 1) * (self.sigma_min ** (1 / self.rho) - self.sigma_max ** (1 / self.rho))) ** self.rho\n",
    "        t_steps = torch.cat([torch.as_tensor(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
    "        t_steps = t_steps.flip(0)\n",
    "        \n",
    "        z1 = x1\n",
    "        z2 = x2\n",
    "        \n",
    "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):\n",
    "            z1 = z1 + (t_next-t_cur) * torch.randn_like(z1)\n",
    "            z2 = z2 + (t_next-t_cur) * torch.randn_like(z2)\n",
    "        \n",
    "        z1 = z1/80\n",
    "        z2 = z2/80\n",
    "        \n",
    "        alpha = torch.arange(0.0, 1.01, 0.1).to(z1.device)\n",
    "        z_ = []\n",
    "        for i in range(alpha.size(0)):\n",
    "            z_.append(slerp(z1, z2, alpha[i]))\n",
    "\n",
    "        x = torch.cat(z_, dim=0)\n",
    "        xs = []\n",
    "\n",
    "        # Hard coded here, modify to your preferences\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, x.size(0), 8):\n",
    "                xs.append(self.sample_image(x[i : i + 8], model))\n",
    "#         x = inverse_data_transform(config, torch.cat(xs, dim=0))\n",
    "        x = [(y + 1.0) / 2.0 for y in torch.cat(xs, dim=0)]\n",
    "        for i in range(len(x)):\n",
    "            tvu.save_image(x[i], os.path.join(self.args.generated_image_folder, f\"{i}.png\"))\n",
    "\n",
    "    def sample_image(self, x, model):\n",
    "        \n",
    "        \n",
    "        data_start = time.time()\n",
    "        data_time = 0\n",
    "        if self.args.sample_type == \"deterministic\":\n",
    "             x = edm_sampler(latents = x, num_steps = self.args.timesteps, net = model, randn_like=torch.randn_like, sigma_min = self.sigma_min, sigma_max = self.sigma_max )\n",
    "        elif self.args.sample_type == \"stochastic\":\n",
    "             x = edm_sampler(latents = x, num_steps = self.args.timesteps, net = model, randn_like=torch.randn_like, sigma_min = self.sigma_min, sigma_max = self.sigma_max, S_churn=5, S_min= 0, S_max=1000, S_noise=1.003)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "        data_time += time.time() - data_start\n",
    "        print(data_time)\n",
    "        \n",
    "    def test(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edm_sampler(\n",
    "    latents, num_steps, net,  class_labels=None, randn_like=torch.randn_like, sigma_min=0.002, sigma_max=80, rho=5,\n",
    "    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1\n",
    "):\n",
    "    \n",
    "    # Time step discretization.\n",
    "    step_indices = torch.arange(num_steps, device=latents.device)\n",
    "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([torch.as_tensor(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
    "\n",
    "    # Main sampling loop.\n",
    "    x_next = [latents * t_steps[0]]\n",
    "#     x_next = [latents]\n",
    "    print(t_steps)\n",
    "    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n",
    "        \n",
    "        t_cur = torch.ones(x_next[0].shape[0], device=t_cur.device) * t_cur\n",
    "        t_next = torch.ones(x_next[0].shape[0], device=t_next.device) * t_next\n",
    "        \n",
    "        x_cur = x_next[-1]\n",
    "\n",
    "        # Increase noise temporarily.\n",
    "        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur[0] <= S_max else 0\n",
    "        t_hat = torch.as_tensor(t_cur + gamma * t_cur)\n",
    "        x_hat = (x_cur + (t_hat[0] ** 2 - t_cur[0] ** 2).sqrt() * S_noise * randn_like(x_cur))\n",
    "        \n",
    "        # Euler step.\n",
    "#         denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n",
    "        denoised = net(x_hat, t_hat)\n",
    "        d_cur = (x_hat - denoised) / t_hat[0]\n",
    "        x_next_ = x_hat + (t_next[0] - t_hat[0]) * d_cur\n",
    "\n",
    "        # Apply 2nd order correction.\n",
    "        if i < num_steps - 1:\n",
    "            denoised = net(x_next_, t_next)\n",
    "#             denoised = net(x_next_, t_next, class_labels)\n",
    "            d_prime = (x_next_ - denoised) / t_next[0]\n",
    "            x_next_ = x_hat + (t_next[0] - t_hat[0]) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "        \n",
    "        x_next.append(x_next_)\n",
    "        \n",
    "#         for i, _x_next in enumerate(x_next):\n",
    "#             tvu.save_image(_x_next, os.path.join(\"18steps_generation_process\", f\"generated_step_{i}.png\"))\n",
    "        \n",
    "    return x_next[-1].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "args = {\n",
    "    'config': 'FFHQ_64.yml',\n",
    "    'seed': 1234,\n",
    "    'exp': 'result_diffusion_model',\n",
    "    'doc': 'log_folder',\n",
    "    'comment': \"\",\n",
    "    'verbose': \"info\",\n",
    "    'sequence': False,\n",
    "    'test': False,\n",
    "    'sample': False,\n",
    "    'fid': False,\n",
    "    'interpolation': False,\n",
    "    'resume_training': True,\n",
    "    'image_folder': \"result_diffusion_model/images\",\n",
    "    # 'generated_image_folder': \"SDM_iterative_noise_best\",\n",
    "    'ni': False,\n",
    "    'use_pretrained': False,\n",
    "    'sample_type': \"stochastic\",\n",
    "#     deterministic, stochastic\n",
    "    'timesteps': 18,\n",
    "    'eta': 0.0,\n",
    "    'dataparallel': [0],\n",
    "    # 'ckpt_id': '196000',\n",
    "    'rho' : 7,\n",
    "#     'skip': 500,\n",
    "    'sigma_start' : 8,\n",
    "    'sigma_end' : 0.8,\n",
    "    \n",
    "}\n",
    "\n",
    "args['log_path']=os.path.join(args['exp'], \"logs\", args['doc'])\n",
    "args = dict2namespace(args)\n",
    "with open(args.config, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config['tb_logger'] = None\n",
    "config = dict2namespace(config)\n",
    "tb_path = os.path.join(args.exp, \"tensorboard\", args.doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 64, 64, 3)\n",
      "train image data range: { -1.0 ~ 1.0 }\n",
      "(1000, 64, 64, 3)\n",
      "train image data range: { -1.0 ~ 1.0 }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda99d2add0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x20000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not args.test and not args.sample:\n",
    "    train_images = np.load('FFHQ_train_64_10k.npy', mmap_mode='r')\n",
    "    val_images = np.load('FFHQ_val_64.npy', mmap_mode='r')\n",
    "    train_images = train_images/0.5-1\n",
    "    val_images = val_images/0.5-1\n",
    "    print(train_images.shape)\n",
    "    print(\"train image data range: {\", train_images[:100].min(),\"~\",train_images[:100].max(),\"}\")\n",
    "    print(val_images.shape)\n",
    "    print(\"train image data range: {\", val_images.min(),\"~\",val_images.max(),\"}\")\n",
    "    train_images_tensor = torch.tensor(train_images).float()\n",
    "    train_images_tensor = train_images_tensor.permute((0, 3, 1, 2))\n",
    "    val_images_tensor = torch.tensor(val_images).float()\n",
    "    val_images_tensor = val_images_tensor.permute((0,3, 1, 2))\n",
    "    plotting_images = (train_images+1)*0.5\n",
    "    del train_images\n",
    "\n",
    "else:\n",
    "    test_images = np.load('../PoDM_chair_128/chair_train_128_s.npy', mmap_mode='r')\n",
    "    test_images = test_images/0.5-1\n",
    "    test_images_tensor = torch.tensor(test_images)\n",
    "    test_images_tensor = test_images_tensor.permute((0,3, 1, 2))\n",
    "    plotting_images = test_images\n",
    "    del test_images\n",
    "\n",
    "plt.figure(figsize=(40, 200))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(plotting_images[0, :, :])\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(plotting_images[5, :, :])\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.imshow(plotting_images[6, :, :])\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.imshow(plotting_images[7, :, :])\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.imshow(plotting_images[8, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in [0,5,6,7,8]:\n",
    "    tvu.save_image(torch.tensor(plotting_images[i]).permute((2, 0, 1)), f\"test_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch_device = \"cuda:\" + str(args.dataparallel[0])\n",
    "device = torch.device(torch_device) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device: {}\".format(device))\n",
    "config.device = device\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if not args.test and not args.sample:\n",
    "    if not args.resume_training:\n",
    "        if os.path.exists(args.log_path):\n",
    "            overwrite = False\n",
    "            if args.ni:\n",
    "                overwrite = True\n",
    "            if overwrite:\n",
    "                shutil.rmtree(args.log_path)\n",
    "                os.makedirs(args.log_path)\n",
    "                if os.path.exists(tb_path):\n",
    "                    shutil.rmtree(tb_path)\n",
    "            else:\n",
    "                print(\"Folder exists. Program halted.\")\n",
    "        else:\n",
    "            os.makedirs(args.log_path)\n",
    "        \n",
    "        if os.path.exists(args.image_folder):\n",
    "            if args.ni:\n",
    "                overwrite = True\n",
    "            if overwrite:\n",
    "                shutil.rmtree(args.image_folder)\n",
    "                os.makedirs(args.image_folder)\n",
    "            else:\n",
    "                print(\"Folder exists. Program halted.\")\n",
    "        else:\n",
    "            os.makedirs(args.image_folder)\n",
    "\n",
    "        with open(os.path.join(args.log_path, \"config.yml\"), \"w\") as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n",
      "0.08000000000000002\n",
      "Number of parameters in the model: 1525411\n",
      "step: 675706, loss: 0.0629381388425827, data time: 0.13767337799072266\n",
      "step: 675707, loss: 0.06726555526256561, data time: 0.0696946382522583\n",
      "step: 675708, loss: 0.06706247478723526, data time: 0.04705047607421875\n",
      "step: 675709, loss: 0.06343020498752594, data time: 0.03606313467025757\n",
      "step: 675710, loss: 0.06904904544353485, data time: 0.029201793670654296\n",
      "step: 675711, loss: 0.05826302617788315, data time: 0.024611234664916992\n",
      "step: 675712, loss: 0.06725192070007324, data time: 0.021351235253470286\n",
      "step: 675713, loss: 0.06534653156995773, data time: 0.01898530125617981\n",
      "step: 675714, loss: 0.05909223482012749, data time: 0.01713903745015462\n",
      "step: 675715, loss: 0.0664842277765274, data time: 0.015662074089050293\n",
      "step: 675716, loss: 0.06267265230417252, data time: 0.01446357640353116\n",
      "step: 675717, loss: 0.06819504499435425, data time: 0.013463775316874186\n",
      "step: 675718, loss: 0.06079743430018425, data time: 0.012625987713153545\n",
      "step: 675719, loss: 0.0635753870010376, data time: 0.011892352785382952\n",
      "step: 675720, loss: 0.06475453823804855, data time: 0.01123652458190918\n",
      "step: 675721, loss: 0.06297348439693451, data time: 0.010661512613296509\n",
      "step: 675722, loss: 0.05735054612159729, data time: 0.010156631469726562\n",
      "step: 675723, loss: 0.0642414391040802, data time: 0.009709305233425565\n",
      "step: 675724, loss: 0.06275393813848495, data time: 0.009302841989617599\n",
      "step: 675725, loss: 0.07108508050441742, data time: 0.008940804004669189\n",
      "step: 675726, loss: 0.06440430134534836, data time: 0.008618911107381185\n",
      "step: 675727, loss: 0.06231185793876648, data time: 0.008331428874622692\n",
      "step: 675728, loss: 0.06895478069782257, data time: 0.008058879686438519\n",
      "step: 675729, loss: 0.05737151950597763, data time: 0.007808874050776164\n",
      "step: 675730, loss: 0.06317228078842163, data time: 0.007582807540893554\n",
      "step: 675731, loss: 0.06418294459581375, data time: 0.007370086816641001\n",
      "step: 675732, loss: 0.061032429337501526, data time: 0.007175728126808449\n",
      "step: 675733, loss: 0.059266574680805206, data time: 0.006992161273956299\n",
      "step: 675734, loss: 0.0628647431731224, data time: 0.00682089246552566\n",
      "step: 675735, loss: 0.06433941423892975, data time: 0.006667629877726237\n",
      "step: 675736, loss: 0.06123217195272446, data time: 0.006520632774599137\n",
      "step: 675737, loss: 0.06585925817489624, data time: 0.006387636065483093\n",
      "step: 675738, loss: 0.059786491096019745, data time: 0.006252433314467921\n",
      "step: 675739, loss: 0.059865936636924744, data time: 0.006127378519843607\n",
      "step: 675740, loss: 0.054741572588682175, data time: 0.006007160459245954\n",
      "step: 675741, loss: 0.06677886098623276, data time: 0.005894826518164741\n",
      "step: 675742, loss: 0.06420581042766571, data time: 0.005787984744922535\n",
      "step: 675743, loss: 0.0663231760263443, data time: 0.00568938882727372\n",
      "step: 675744, loss: 0.05965757369995117, data time: 0.005595500652606671\n",
      "step: 675745, loss: 0.07229810953140259, data time: 0.005505913496017456\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.6760437488555908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 675746, loss: 0.06002487987279892, data time: 0.12861919403076172\n",
      "step: 675747, loss: 0.056587398052215576, data time: 0.06640315055847168\n",
      "step: 675748, loss: 0.06120461970567703, data time: 0.044711033503214516\n",
      "step: 675749, loss: 0.06128303334116936, data time: 0.03414982557296753\n",
      "step: 675750, loss: 0.06205202266573906, data time: 0.027571439743041992\n",
      "step: 675751, loss: 0.06686960160732269, data time: 0.02320249875386556\n",
      "step: 675752, loss: 0.06331866979598999, data time: 0.020088604518345425\n",
      "step: 675753, loss: 0.0646083876490593, data time: 0.017826944589614868\n",
      "step: 675754, loss: 0.06647607684135437, data time: 0.015984747144911025\n",
      "step: 675755, loss: 0.06272748857736588, data time: 0.014580345153808594\n",
      "step: 675756, loss: 0.06426536291837692, data time: 0.013438658280806108\n",
      "step: 675757, loss: 0.05611374229192734, data time: 0.012484471003214518\n",
      "step: 675758, loss: 0.06452152132987976, data time: 0.011684894561767578\n",
      "step: 675759, loss: 0.06308083981275558, data time: 0.010989563805716378\n",
      "step: 675760, loss: 0.05637272819876671, data time: 0.01039590835571289\n",
      "step: 675761, loss: 0.06429815292358398, data time: 0.0099019855260849\n",
      "step: 675762, loss: 0.0595608651638031, data time: 0.00945889248567469\n",
      "step: 675763, loss: 0.06226975470781326, data time: 0.009060541788736979\n",
      "step: 675764, loss: 0.0646725445985794, data time: 0.008705779125815943\n",
      "step: 675765, loss: 0.06620858609676361, data time: 0.008388650417327882\n",
      "step: 675766, loss: 0.062307536602020264, data time: 0.008101974214826311\n",
      "step: 675767, loss: 0.060343317687511444, data time: 0.007839527997103605\n",
      "step: 675768, loss: 0.06241755932569504, data time: 0.0075971354608950405\n",
      "step: 675769, loss: 0.06338316202163696, data time: 0.00738180677096049\n",
      "step: 675770, loss: 0.06102543696761131, data time: 0.007179174423217773\n",
      "step: 675771, loss: 0.06580519676208496, data time: 0.006992835264939528\n",
      "step: 675772, loss: 0.06237298250198364, data time: 0.006817685233222114\n",
      "step: 675773, loss: 0.060080334544181824, data time: 0.006657123565673828\n",
      "step: 675774, loss: 0.06428208202123642, data time: 0.006511301829897124\n",
      "step: 675775, loss: 0.0629974752664566, data time: 0.006375646591186524\n",
      "step: 675776, loss: 0.05805989354848862, data time: 0.0062461668445218\n",
      "step: 675777, loss: 0.0613800585269928, data time: 0.006132058799266815\n",
      "step: 675778, loss: 0.05794551223516464, data time: 0.006006486488111092\n",
      "step: 675779, loss: 0.061439719051122665, data time: 0.005887929131002987\n",
      "step: 675780, loss: 0.062270499765872955, data time: 0.0057759148733956475\n",
      "step: 675781, loss: 0.05845467373728752, data time: 0.0056684547000461155\n",
      "step: 675782, loss: 0.055946897715330124, data time: 0.005568111265027845\n",
      "step: 675783, loss: 0.060655683279037476, data time: 0.005473412965473376\n",
      "step: 675784, loss: 0.06163623556494713, data time: 0.005384628589336689\n",
      "step: 675785, loss: 0.06634610146284103, data time: 0.005300760269165039\n",
      "step: 675786, loss: 0.061334289610385895, data time: 0.15366864204406738\n",
      "step: 675787, loss: 0.05590645596385002, data time: 0.07765865325927734\n",
      "step: 675788, loss: 0.0688558891415596, data time: 0.05226492881774902\n",
      "step: 675789, loss: 0.06568529456853867, data time: 0.03992640972137451\n",
      "step: 675790, loss: 0.0639776811003685, data time: 0.032206153869628905\n",
      "step: 675791, loss: 0.057810425758361816, data time: 0.027059396107991535\n",
      "step: 675792, loss: 0.05903234705328941, data time: 0.023388113294328963\n",
      "step: 675793, loss: 0.06606520712375641, data time: 0.02070927619934082\n",
      "step: 675794, loss: 0.06067327782511711, data time: 0.018556170993381076\n",
      "step: 675795, loss: 0.06097238510847092, data time: 0.016893672943115234\n",
      "step: 675796, loss: 0.05951256304979324, data time: 0.015541250055486506\n",
      "step: 675797, loss: 0.06302198767662048, data time: 0.014407634735107422\n",
      "step: 675798, loss: 0.06298552453517914, data time: 0.013447303038377028\n",
      "step: 675799, loss: 0.06213744729757309, data time: 0.012623310089111328\n",
      "step: 675800, loss: 0.061862021684646606, data time: 0.011908912658691406\n",
      "step: 675801, loss: 0.05527442321181297, data time: 0.011284247040748596\n",
      "step: 675802, loss: 0.06972794234752655, data time: 0.010742692386402804\n",
      "step: 675803, loss: 0.06635650992393494, data time: 0.01025182671017117\n",
      "step: 675804, loss: 0.05998971313238144, data time: 0.00981525370949193\n",
      "step: 675805, loss: 0.06502800434827805, data time: 0.009423696994781494\n",
      "step: 675806, loss: 0.058274902403354645, data time: 0.009070884613763718\n",
      "step: 675807, loss: 0.05889221280813217, data time: 0.008746894923123446\n",
      "step: 675808, loss: 0.06076029688119888, data time: 0.008450414823449177\n",
      "step: 675809, loss: 0.0686817318201065, data time: 0.0081806480884552\n",
      "step: 675810, loss: 0.06490027159452438, data time: 0.007933740615844726\n",
      "step: 675811, loss: 0.06676433980464935, data time: 0.007704578913175142\n",
      "step: 675812, loss: 0.05723459646105766, data time: 0.007493469450208876\n",
      "step: 675813, loss: 0.06792253255844116, data time: 0.007297635078430176\n",
      "step: 675814, loss: 0.06743644177913666, data time: 0.00711704944742137\n",
      "step: 675815, loss: 0.0580352358520031, data time: 0.006946849822998047\n",
      "step: 675816, loss: 0.056741438806056976, data time: 0.006786777127173639\n",
      "step: 675817, loss: 0.06290018558502197, data time: 0.006642766296863556\n",
      "step: 675818, loss: 0.06307759881019592, data time: 0.0064960248542554454\n",
      "step: 675819, loss: 0.07063862681388855, data time: 0.006358637529260972\n",
      "step: 675820, loss: 0.05781173333525658, data time: 0.006228896549769811\n",
      "step: 675821, loss: 0.06564228236675262, data time: 0.0061092376708984375\n",
      "step: 675822, loss: 0.05899925157427788, data time: 0.005993572441307274\n",
      "step: 675823, loss: 0.05523791164159775, data time: 0.005885607317874306\n",
      "step: 675824, loss: 0.06623954325914383, data time: 0.0057848967038668115\n",
      "step: 675825, loss: 0.05864948034286499, data time: 0.005686968564987183\n",
      "step: 675826, loss: 0.061961885541677475, data time: 0.16469335556030273\n",
      "step: 675827, loss: 0.059724777936935425, data time: 0.08306097984313965\n",
      "step: 675828, loss: 0.05990932509303093, data time: 0.055862824122111\n",
      "step: 675829, loss: 0.061772048473358154, data time: 0.042638957500457764\n",
      "step: 675830, loss: 0.06579680740833282, data time: 0.03437318801879883\n",
      "step: 675831, loss: 0.06351499259471893, data time: 0.028878251711527508\n",
      "step: 675832, loss: 0.060330282896757126, data time: 0.024945838110787526\n",
      "step: 675833, loss: 0.06067824363708496, data time: 0.02210572361946106\n",
      "step: 675834, loss: 0.06047498807311058, data time: 0.01979631847805447\n",
      "step: 675835, loss: 0.06336921453475952, data time: 0.018010997772216798\n",
      "step: 675836, loss: 0.06601131707429886, data time: 0.01655359701676802\n",
      "step: 675837, loss: 0.07070519775152206, data time: 0.015338480472564697\n",
      "step: 675838, loss: 0.06389760226011276, data time: 0.014309039482703576\n",
      "step: 675839, loss: 0.06063630431890488, data time: 0.013424873352050781\n",
      "step: 675840, loss: 0.06201136112213135, data time: 0.012661091486612956\n",
      "step: 675841, loss: 0.062150489538908005, data time: 0.011995717883110046\n",
      "step: 675842, loss: 0.0669776052236557, data time: 0.011412788839901196\n",
      "step: 675843, loss: 0.07201094925403595, data time: 0.010887093014187284\n",
      "step: 675844, loss: 0.06254187226295471, data time: 0.010416419882523386\n",
      "step: 675845, loss: 0.06654608249664307, data time: 0.009994745254516602\n",
      "step: 675846, loss: 0.0681237131357193, data time: 0.009612946283249628\n",
      "step: 675847, loss: 0.06782063841819763, data time: 0.009266929192976519\n",
      "step: 675848, loss: 0.06680987030267715, data time: 0.008954442065695057\n",
      "step: 675849, loss: 0.06033411622047424, data time: 0.008670667807261149\n",
      "step: 675850, loss: 0.06116405874490738, data time: 0.008403100967407227\n",
      "step: 675851, loss: 0.06059489771723747, data time: 0.008154190503633939\n",
      "step: 675852, loss: 0.06253508478403091, data time: 0.007925792976661964\n",
      "step: 675853, loss: 0.06527241319417953, data time: 0.00771273033959525\n",
      "step: 675854, loss: 0.06501330435276031, data time: 0.007517387127054149\n",
      "step: 675855, loss: 0.06149978190660477, data time: 0.007333827018737793\n",
      "step: 675856, loss: 0.0643041655421257, data time: 0.007163001644995904\n",
      "step: 675857, loss: 0.06414633989334106, data time: 0.007010452449321747\n",
      "step: 675858, loss: 0.06577469408512115, data time: 0.006856889435739228\n",
      "step: 675859, loss: 0.0678931176662445, data time: 0.0067121281343347884\n",
      "step: 675860, loss: 0.06147555634379387, data time: 0.006578520366123744\n",
      "step: 675861, loss: 0.05797189474105835, data time: 0.00644651386472914\n",
      "step: 675862, loss: 0.059740252792835236, data time: 0.0063215590812064505\n",
      "step: 675863, loss: 0.0625789538025856, data time: 0.006204969004580849\n",
      "step: 675864, loss: 0.06354882568120956, data time: 0.006094192847227439\n",
      "step: 675865, loss: 0.05458969995379448, data time: 0.005987560749053955\n",
      "step: 675866, loss: 0.06389634311199188, data time: 0.16230225563049316\n",
      "step: 675867, loss: 0.06000479310750961, data time: 0.08247184753417969\n",
      "step: 675868, loss: 0.06190807744860649, data time: 0.05584096908569336\n",
      "step: 675869, loss: 0.06267739832401276, data time: 0.04276013374328613\n",
      "step: 675870, loss: 0.0648341178894043, data time: 0.034529256820678714\n",
      "step: 675871, loss: 0.060048919171094894, data time: 0.029044111569722492\n",
      "step: 675872, loss: 0.0656844824552536, data time: 0.025133064814976284\n",
      "step: 675873, loss: 0.05863552913069725, data time: 0.02227756381034851\n",
      "step: 675874, loss: 0.05599444359540939, data time: 0.019986338085598417\n",
      "step: 675875, loss: 0.062286101281642914, data time: 0.018224120140075684\n",
      "step: 675876, loss: 0.059159766882658005, data time: 0.016794811595569958\n",
      "step: 675877, loss: 0.062155913561582565, data time: 0.01559011141459147\n",
      "step: 675878, loss: 0.060950867831707, data time: 0.01456590799184946\n",
      "step: 675879, loss: 0.06158621236681938, data time: 0.013696840831211634\n",
      "step: 675880, loss: 0.05651364475488663, data time: 0.012937053044637045\n",
      "step: 675881, loss: 0.060300420969724655, data time: 0.012273088097572327\n",
      "step: 675882, loss: 0.058203428983688354, data time: 0.011689214145436007\n",
      "step: 675883, loss: 0.06487170606851578, data time: 0.011171976725260416\n",
      "step: 675884, loss: 0.06082407757639885, data time: 0.010709185349313836\n",
      "step: 675885, loss: 0.06251025944948196, data time: 0.010294640064239502\n",
      "step: 675886, loss: 0.059841327369213104, data time: 0.009916157949538458\n",
      "step: 675887, loss: 0.0626492127776146, data time: 0.009569948369806463\n",
      "step: 675888, loss: 0.06427624821662903, data time: 0.009257731230362602\n",
      "step: 675889, loss: 0.05880320072174072, data time: 0.00897298256556193\n",
      "step: 675890, loss: 0.0693468302488327, data time: 0.008706655502319336\n",
      "step: 675891, loss: 0.05777475982904434, data time: 0.008460411658653846\n",
      "step: 675892, loss: 0.0670032650232315, data time: 0.008234571527551722\n",
      "step: 675893, loss: 0.06540179252624512, data time: 0.008022078445979528\n",
      "step: 675894, loss: 0.05514021962881088, data time: 0.007827298394564924\n",
      "step: 675895, loss: 0.06208088994026184, data time: 0.0076463619867960615\n",
      "step: 675896, loss: 0.06105833873152733, data time: 0.0074754684202132686\n",
      "step: 675897, loss: 0.06265046447515488, data time: 0.007321998476982117\n",
      "step: 675898, loss: 0.05863002687692642, data time: 0.007158770705714371\n",
      "step: 675899, loss: 0.0632912814617157, data time: 0.007007998578688677\n",
      "step: 675900, loss: 0.06460990011692047, data time: 0.006864139011928013\n",
      "step: 675901, loss: 0.06092660129070282, data time: 0.006727503405676948\n",
      "step: 675902, loss: 0.0641481876373291, data time: 0.006598595026377085\n",
      "step: 675903, loss: 0.06055912375450134, data time: 0.006479426434165553\n",
      "step: 675904, loss: 0.05793784186244011, data time: 0.006366729736328125\n",
      "step: 675905, loss: 0.08643342554569244, data time: 0.006258350610733032\n",
      "step: 675906, loss: 0.06207317113876343, data time: 0.15612030029296875\n",
      "step: 675907, loss: 0.06257190555334091, data time: 0.07938492298126221\n",
      "step: 675908, loss: 0.05980634316802025, data time: 0.05377117792765299\n",
      "step: 675909, loss: 0.0552322082221508, data time: 0.04106956720352173\n",
      "step: 675910, loss: 0.06277737021446228, data time: 0.03313136100769043\n",
      "step: 675911, loss: 0.06657978892326355, data time: 0.027831395467122395\n",
      "step: 675912, loss: 0.06023406609892845, data time: 0.02404771532331194\n",
      "step: 675913, loss: 0.06512924283742905, data time: 0.021283745765686035\n",
      "step: 675914, loss: 0.05900871381163597, data time: 0.019069698121812608\n",
      "step: 675915, loss: 0.06709510087966919, data time: 0.017358899116516113\n",
      "step: 675916, loss: 0.058881234377622604, data time: 0.0159691030328924\n",
      "step: 675917, loss: 0.060597360134124756, data time: 0.014809052149454752\n",
      "step: 675918, loss: 0.06467267125844955, data time: 0.013837906030508189\n",
      "step: 675919, loss: 0.05990731343626976, data time: 0.012987102781023298\n",
      "step: 675920, loss: 0.06774456053972244, data time: 0.012248881657918294\n",
      "step: 675921, loss: 0.05826819688081741, data time: 0.011603400111198425\n",
      "step: 675922, loss: 0.05685366317629814, data time: 0.01103371732375201\n",
      "step: 675923, loss: 0.05693614482879639, data time: 0.010526418685913086\n",
      "step: 675924, loss: 0.06366413831710815, data time: 0.010075092315673828\n",
      "step: 675925, loss: 0.05923280864953995, data time: 0.009675979614257812\n",
      "step: 675926, loss: 0.06009285897016525, data time: 0.009310654231480189\n",
      "step: 675927, loss: 0.05940090864896774, data time: 0.008979125456376509\n",
      "step: 675928, loss: 0.05356933921575546, data time: 0.008674538653829823\n",
      "step: 675929, loss: 0.07391451299190521, data time: 0.008392026027043661\n",
      "step: 675930, loss: 0.05643690750002861, data time: 0.008132896423339843\n",
      "step: 675931, loss: 0.05717366188764572, data time: 0.007894460971538838\n",
      "step: 675932, loss: 0.059052374213933945, data time: 0.007672203911675347\n",
      "step: 675933, loss: 0.0598490945994854, data time: 0.007469534873962402\n",
      "step: 675934, loss: 0.05948420614004135, data time: 0.007285899129407159\n",
      "step: 675935, loss: 0.060221634805202484, data time: 0.007109387715657552\n",
      "step: 675936, loss: 0.05896996706724167, data time: 0.006942579823155557\n",
      "step: 675937, loss: 0.06572617590427399, data time: 0.006793797016143799\n",
      "step: 675938, loss: 0.06650898605585098, data time: 0.006643027970285127\n",
      "step: 675939, loss: 0.06288062781095505, data time: 0.006500791100894704\n",
      "step: 675940, loss: 0.061028800904750824, data time: 0.006367410932268415\n",
      "step: 675941, loss: 0.06268490850925446, data time: 0.006239480442470974\n",
      "step: 675942, loss: 0.05966971442103386, data time: 0.006120469119097735\n",
      "step: 675943, loss: 0.05613983795046806, data time: 0.006010595120881733\n",
      "step: 675944, loss: 0.05368955433368683, data time: 0.0059052247267503\n",
      "step: 675945, loss: 0.050483010709285736, data time: 0.005803316831588745\n",
      "step: 675946, loss: 0.061110418289899826, data time: 0.15946507453918457\n",
      "step: 675947, loss: 0.06209314987063408, data time: 0.08124077320098877\n",
      "step: 675948, loss: 0.06234687566757202, data time: 0.05460619926452637\n",
      "step: 675949, loss: 0.060805946588516235, data time: 0.04159080982208252\n",
      "step: 675950, loss: 0.05997006595134735, data time: 0.033539342880249026\n",
      "step: 675951, loss: 0.06035574898123741, data time: 0.028173049290974934\n",
      "step: 675952, loss: 0.05686312913894653, data time: 0.024354934692382812\n",
      "step: 675953, loss: 0.062125347554683685, data time: 0.021568477153778076\n",
      "step: 675954, loss: 0.05965788662433624, data time: 0.019318951500786677\n",
      "step: 675955, loss: 0.06282399594783783, data time: 0.0176008939743042\n",
      "step: 675956, loss: 0.06352499127388, data time: 0.01619191603227095\n",
      "step: 675957, loss: 0.06229695305228233, data time: 0.015005528926849365\n",
      "step: 675958, loss: 0.06378885358572006, data time: 0.014010429382324219\n",
      "step: 675959, loss: 0.06278303265571594, data time: 0.013146213122776576\n",
      "step: 675960, loss: 0.0652707889676094, data time: 0.012422831853230794\n",
      "step: 675961, loss: 0.06199287623167038, data time: 0.011768966913223267\n",
      "step: 675962, loss: 0.059484921395778656, data time: 0.011197188321281882\n",
      "step: 675963, loss: 0.05949416011571884, data time: 0.010692675908406576\n",
      "step: 675964, loss: 0.06034402176737785, data time: 0.01024007797241211\n",
      "step: 675965, loss: 0.06364704668521881, data time: 0.009833836555480957\n",
      "step: 675966, loss: 0.06288175284862518, data time: 0.009465853373209635\n",
      "step: 675967, loss: 0.06346974521875381, data time: 0.009128451347351074\n",
      "step: 675968, loss: 0.06250973045825958, data time: 0.008816180021866508\n",
      "step: 675969, loss: 0.06943342089653015, data time: 0.008529881636301676\n",
      "step: 675970, loss: 0.06484828889369965, data time: 0.008268060684204102\n",
      "step: 675971, loss: 0.05776084214448929, data time: 0.008028543912447415\n",
      "step: 675972, loss: 0.06210215017199516, data time: 0.007805824279785156\n",
      "step: 675973, loss: 0.06419694423675537, data time: 0.007597684860229492\n",
      "step: 675974, loss: 0.06105443090200424, data time: 0.007408865566911368\n",
      "step: 675975, loss: 0.06785120069980621, data time: 0.007230067253112793\n",
      "step: 675976, loss: 0.05790635943412781, data time: 0.007067949541153446\n",
      "step: 675977, loss: 0.06560447812080383, data time: 0.006920710206031799\n",
      "step: 675978, loss: 0.05927348881959915, data time: 0.0067691947474624176\n",
      "step: 675979, loss: 0.06466396152973175, data time: 0.0066226370194379025\n",
      "step: 675980, loss: 0.06004458665847778, data time: 0.006486497606549944\n",
      "step: 675981, loss: 0.05918003246188164, data time: 0.006355279021792942\n",
      "step: 675982, loss: 0.06268741190433502, data time: 0.006232802932326858\n",
      "step: 675983, loss: 0.06080929934978485, data time: 0.006121045664737099\n",
      "step: 675984, loss: 0.06492407619953156, data time: 0.006011852851280799\n",
      "step: 675985, loss: 0.08422909677028656, data time: 0.005909663438796997\n",
      "step: 675986, loss: 0.060197751969099045, data time: 0.14811921119689941\n",
      "step: 675987, loss: 0.06684291362762451, data time: 0.07474863529205322\n",
      "step: 675988, loss: 0.0602458119392395, data time: 0.05115191141764323\n",
      "step: 675989, loss: 0.06660555303096771, data time: 0.03872436285018921\n",
      "step: 675990, loss: 0.060978930443525314, data time: 0.031252813339233396\n",
      "step: 675991, loss: 0.06427525728940964, data time: 0.026265978813171387\n",
      "step: 675992, loss: 0.058712564408779144, data time: 0.0228100163596017\n",
      "step: 675993, loss: 0.05748362094163895, data time: 0.02014005184173584\n",
      "step: 675994, loss: 0.05855437368154526, data time: 0.018048233456081815\n",
      "step: 675995, loss: 0.059209927916526794, data time: 0.016434621810913087\n",
      "step: 675996, loss: 0.06102412939071655, data time: 0.015122652053833008\n",
      "step: 675997, loss: 0.05906824395060539, data time: 0.014035522937774658\n",
      "step: 675998, loss: 0.06383741647005081, data time: 0.0131074098440317\n",
      "step: 675999, loss: 0.06070278584957123, data time: 0.012322374752589635\n",
      "step: 676000, loss: 0.07004854083061218, data time: 0.011629978815714518\n",
      "step: 676001, loss: 0.05968741700053215, data time: 0.011026665568351746\n",
      "step: 676002, loss: 0.06464295089244843, data time: 0.010494035833022174\n",
      "step: 676003, loss: 0.0666557252407074, data time: 0.010022070672776964\n",
      "step: 676004, loss: 0.0593443289399147, data time: 0.00960624845404374\n",
      "step: 676005, loss: 0.0649714469909668, data time: 0.009228754043579101\n",
      "step: 676006, loss: 0.060375064611434937, data time: 0.008883953094482422\n",
      "step: 676007, loss: 0.06773575395345688, data time: 0.008570183407176624\n",
      "step: 676008, loss: 0.05778614431619644, data time: 0.008282816928365955\n",
      "step: 676009, loss: 0.06095653772354126, data time: 0.00801694393157959\n",
      "step: 676010, loss: 0.05711846798658371, data time: 0.007775869369506836\n",
      "step: 676011, loss: 0.060310423374176025, data time: 0.00755013869358943\n",
      "step: 676012, loss: 0.062269654124975204, data time: 0.007342197276927807\n",
      "step: 676013, loss: 0.059254251420497894, data time: 0.007149764469691685\n",
      "step: 676014, loss: 0.06148311495780945, data time: 0.006974828654322131\n",
      "step: 676015, loss: 0.06350699067115784, data time: 0.0068130890528361005\n",
      "step: 676016, loss: 0.06199698895215988, data time: 0.0066572081658148\n",
      "step: 676017, loss: 0.06324778497219086, data time: 0.006518334150314331\n",
      "step: 676018, loss: 0.06170900911092758, data time: 0.006378484494758375\n",
      "step: 676019, loss: 0.06589555740356445, data time: 0.006247253978953642\n",
      "step: 676020, loss: 0.07016396522521973, data time: 0.0061216422489711216\n",
      "step: 676021, loss: 0.06118202209472656, data time: 0.0060028500027126735\n",
      "step: 676022, loss: 0.06007426604628563, data time: 0.0058900858904864335\n",
      "step: 676023, loss: 0.06792492419481277, data time: 0.005785201725206877\n",
      "step: 676024, loss: 0.06520058959722519, data time: 0.005686460397182367\n",
      "step: 676025, loss: 0.04734452813863754, data time: 0.0055921971797943115\n",
      "step: 676026, loss: 0.062015898525714874, data time: 0.1524186134338379\n",
      "step: 676027, loss: 0.057857804000377655, data time: 0.07695257663726807\n",
      "step: 676028, loss: 0.06150760501623154, data time: 0.05271665255228678\n",
      "step: 676029, loss: 0.05803236365318298, data time: 0.03986889123916626\n",
      "step: 676030, loss: 0.06101956218481064, data time: 0.03217763900756836\n",
      "step: 676031, loss: 0.06170349940657616, data time: 0.027048110961914062\n",
      "step: 676032, loss: 0.05993677303195, data time: 0.023470027106148855\n",
      "step: 676033, loss: 0.06376206129789352, data time: 0.020703434944152832\n",
      "step: 676034, loss: 0.06336680799722672, data time: 0.018551614549424913\n",
      "step: 676035, loss: 0.06427767872810364, data time: 0.016891336441040038\n",
      "step: 676036, loss: 0.06386624276638031, data time: 0.015548402612859552\n",
      "step: 676037, loss: 0.06212310120463371, data time: 0.014430344104766846\n",
      "step: 676038, loss: 0.06912390887737274, data time: 0.013471695093008189\n",
      "step: 676039, loss: 0.055747222155332565, data time: 0.012651886258806502\n",
      "step: 676040, loss: 0.06460810452699661, data time: 0.011945772171020507\n",
      "step: 676041, loss: 0.062436990439891815, data time: 0.01132175326347351\n",
      "step: 676042, loss: 0.06085030734539032, data time: 0.010771428837495692\n",
      "step: 676043, loss: 0.059939637780189514, data time: 0.010280754831102159\n",
      "step: 676044, loss: 0.05734170600771904, data time: 0.009841969138697573\n",
      "step: 676045, loss: 0.05639399215579033, data time: 0.009460377693176269\n",
      "step: 676046, loss: 0.06008606404066086, data time: 0.009106567927769251\n",
      "step: 676047, loss: 0.07321274280548096, data time: 0.008783589709888805\n",
      "step: 676048, loss: 0.061602409929037094, data time: 0.008488903874936312\n",
      "step: 676049, loss: 0.06492354720830917, data time: 0.008219440778096518\n",
      "step: 676050, loss: 0.05752227455377579, data time: 0.007970714569091797\n",
      "step: 676051, loss: 0.052275385707616806, data time: 0.007740699327909029\n",
      "step: 676052, loss: 0.06125079095363617, data time: 0.007526750917787905\n",
      "step: 676053, loss: 0.05999484658241272, data time: 0.007328595433916364\n",
      "step: 676054, loss: 0.06661966443061829, data time: 0.007145865210171403\n",
      "step: 676055, loss: 0.06246626377105713, data time: 0.006977303822835287\n",
      "step: 676056, loss: 0.06171019375324249, data time: 0.006816210285309822\n",
      "step: 676057, loss: 0.06306703388690948, data time: 0.006673343479633331\n",
      "step: 676058, loss: 0.058578163385391235, data time: 0.006528630401148941\n",
      "step: 676059, loss: 0.05849842354655266, data time: 0.006395269842708812\n",
      "step: 676060, loss: 0.06348954141139984, data time: 0.006267452239990234\n",
      "step: 676061, loss: 0.06791072338819504, data time: 0.006145530276828342\n",
      "step: 676062, loss: 0.06262615323066711, data time: 0.006029412553117082\n",
      "step: 676063, loss: 0.05845293402671814, data time: 0.00592384840312757\n",
      "step: 676064, loss: 0.06570667028427124, data time: 0.005823110922788963\n",
      "step: 676065, loss: 0.052488356828689575, data time: 0.005723321437835693\n",
      "step: 676066, loss: 0.061675168573856354, data time: 0.1571788787841797\n",
      "step: 676067, loss: 0.06177709251642227, data time: 0.07932531833648682\n",
      "step: 676068, loss: 0.06327690929174423, data time: 0.05339519182840983\n",
      "step: 676069, loss: 0.061896324157714844, data time: 0.04079395532608032\n",
      "step: 676070, loss: 0.06298021972179413, data time: 0.03290824890136719\n",
      "step: 676071, loss: 0.06271778047084808, data time: 0.027660210927327473\n",
      "step: 676072, loss: 0.06300139427185059, data time: 0.02390922818865095\n",
      "step: 676073, loss: 0.054866403341293335, data time: 0.021169275045394897\n",
      "step: 676074, loss: 0.059317272156476974, data time: 0.018966197967529297\n",
      "step: 676075, loss: 0.06434750556945801, data time: 0.017262530326843262\n",
      "step: 676076, loss: 0.06634335964918137, data time: 0.015883510762994938\n",
      "step: 676077, loss: 0.05895579606294632, data time: 0.014728665351867676\n",
      "step: 676078, loss: 0.06138932704925537, data time: 0.013760805130004883\n",
      "step: 676079, loss: 0.06639400124549866, data time: 0.012926748820713587\n",
      "step: 676080, loss: 0.06057377904653549, data time: 0.01219924290974935\n",
      "step: 676081, loss: 0.06863312423229218, data time: 0.011563003063201904\n",
      "step: 676082, loss: 0.058972738683223724, data time: 0.011003858902875115\n",
      "step: 676083, loss: 0.06634225696325302, data time: 0.01050722599029541\n",
      "step: 676084, loss: 0.06070626527070999, data time: 0.010056319989656148\n",
      "step: 676085, loss: 0.060943882912397385, data time: 0.009661710262298584\n",
      "step: 676086, loss: 0.06471453607082367, data time: 0.009298812775384812\n",
      "step: 676087, loss: 0.06157992035150528, data time: 0.00898632136258212\n",
      "step: 676088, loss: 0.056747786700725555, data time: 0.008685485176418139\n",
      "step: 676089, loss: 0.06600244343280792, data time: 0.00840693712234497\n",
      "step: 676090, loss: 0.0641823559999466, data time: 0.008151283264160156\n",
      "step: 676091, loss: 0.06698828935623169, data time: 0.007918770496661846\n",
      "step: 676092, loss: 0.06228777766227722, data time: 0.00769992227907534\n",
      "step: 676093, loss: 0.06425648927688599, data time: 0.007501023156302316\n",
      "step: 676094, loss: 0.06964851915836334, data time: 0.00731654824881718\n",
      "step: 676095, loss: 0.06615657359361649, data time: 0.007140636444091797\n",
      "step: 676096, loss: 0.060947567224502563, data time: 0.006978611792287519\n",
      "step: 676097, loss: 0.057235173881053925, data time: 0.006828270852565765\n",
      "step: 676098, loss: 0.06256722658872604, data time: 0.006677584214643998\n",
      "step: 676099, loss: 0.06279231607913971, data time: 0.006535901742822984\n",
      "step: 676100, loss: 0.06105339527130127, data time: 0.006401824951171875\n",
      "step: 676101, loss: 0.06317570805549622, data time: 0.0062753624386257595\n",
      "step: 676102, loss: 0.06229227036237717, data time: 0.00615692138671875\n",
      "step: 676103, loss: 0.061663441359996796, data time: 0.006047186098600689\n",
      "step: 676104, loss: 0.06616504490375519, data time: 0.005942778709607246\n",
      "step: 676105, loss: 0.07221272587776184, data time: 0.005844610929489136\n",
      "step: 676106, loss: 0.06309879571199417, data time: 0.15503954887390137\n",
      "step: 676107, loss: 0.06515225768089294, data time: 0.07887852191925049\n",
      "step: 676108, loss: 0.059862565249204636, data time: 0.05357964833577474\n",
      "step: 676109, loss: 0.06577552109956741, data time: 0.040849149227142334\n",
      "step: 676110, loss: 0.060556210577487946, data time: 0.032947635650634764\n",
      "step: 676111, loss: 0.06547766178846359, data time: 0.027713497479756672\n",
      "step: 676112, loss: 0.06310777366161346, data time: 0.023940324783325195\n",
      "step: 676113, loss: 0.0688931941986084, data time: 0.0212155282497406\n",
      "step: 676114, loss: 0.07105094194412231, data time: 0.01900172233581543\n",
      "step: 676115, loss: 0.06456628441810608, data time: 0.01730823516845703\n",
      "step: 676116, loss: 0.05757318064570427, data time: 0.015932343222878197\n",
      "step: 676117, loss: 0.07174867391586304, data time: 0.01477829615275065\n",
      "step: 676118, loss: 0.06270404160022736, data time: 0.013792954958402194\n",
      "step: 676119, loss: 0.06751704961061478, data time: 0.012952957834516252\n",
      "step: 676120, loss: 0.06141676753759384, data time: 0.012224467595418294\n",
      "step: 676121, loss: 0.0655234307050705, data time: 0.011585578322410583\n",
      "step: 676122, loss: 0.06164806708693504, data time: 0.011019173790426814\n",
      "step: 676123, loss: 0.05865815281867981, data time: 0.01052464379204644\n",
      "step: 676124, loss: 0.05855831503868103, data time: 0.010081090425190172\n",
      "step: 676125, loss: 0.06029894948005676, data time: 0.009683191776275635\n",
      "step: 676126, loss: 0.06596100330352783, data time: 0.009317897614978608\n",
      "step: 676127, loss: 0.06310129165649414, data time: 0.008985042572021484\n",
      "step: 676128, loss: 0.06521982699632645, data time: 0.008679960084998089\n",
      "step: 676129, loss: 0.05552353709936142, data time: 0.00840449333190918\n",
      "step: 676130, loss: 0.05536838620901108, data time: 0.008150405883789062\n",
      "step: 676131, loss: 0.06641837954521179, data time: 0.007912470744206356\n",
      "step: 676132, loss: 0.0635971873998642, data time: 0.0076916659319842305\n",
      "step: 676133, loss: 0.06554724276065826, data time: 0.00748945985521589\n",
      "step: 676134, loss: 0.05989428982138634, data time: 0.007305194591653758\n",
      "step: 676135, loss: 0.06302215158939362, data time: 0.007128103574117025\n",
      "step: 676136, loss: 0.056026093661785126, data time: 0.00696207631018854\n",
      "step: 676137, loss: 0.06252303719520569, data time: 0.006813712418079376\n",
      "step: 676138, loss: 0.05437813326716423, data time: 0.006664463967987986\n",
      "step: 676139, loss: 0.06295891106128693, data time: 0.006525965297923368\n",
      "step: 676140, loss: 0.06554664671421051, data time: 0.006396491186959403\n",
      "step: 676141, loss: 0.06544380635023117, data time: 0.00627299149831136\n",
      "step: 676142, loss: 0.06070592626929283, data time: 0.006160355903006889\n",
      "step: 676143, loss: 0.06567178666591644, data time: 0.006055154298481189\n",
      "step: 676144, loss: 0.06618858873844147, data time: 0.005951826389019306\n",
      "step: 676145, loss: 0.055894844233989716, data time: 0.0058549225330352785\n",
      "step: 676146, loss: 0.06245856732130051, data time: 0.15694904327392578\n",
      "step: 676147, loss: 0.05929801985621452, data time: 0.07927286624908447\n",
      "step: 676148, loss: 0.0615784227848053, data time: 0.053561131159464516\n",
      "step: 676149, loss: 0.059341542422771454, data time: 0.04101574420928955\n",
      "step: 676150, loss: 0.061368003487586975, data time: 0.03308358192443848\n",
      "step: 676151, loss: 0.058017656207084656, data time: 0.027805407842000324\n",
      "step: 676152, loss: 0.05884578078985214, data time: 0.024033444268362864\n",
      "step: 676153, loss: 0.06591423600912094, data time: 0.021280765533447266\n",
      "step: 676154, loss: 0.057363756000995636, data time: 0.019063393274943035\n",
      "step: 676155, loss: 0.062340687960386276, data time: 0.017351531982421876\n",
      "step: 676156, loss: 0.06463353335857391, data time: 0.015959392894398083\n",
      "step: 676157, loss: 0.0666617676615715, data time: 0.01480245590209961\n",
      "step: 676158, loss: 0.05798006057739258, data time: 0.01381512788625864\n",
      "step: 676159, loss: 0.06042283773422241, data time: 0.012965662138802665\n",
      "step: 676160, loss: 0.06254222989082336, data time: 0.01222979227701823\n",
      "step: 676161, loss: 0.06288759410381317, data time: 0.011590525507926941\n",
      "step: 676162, loss: 0.06218821555376053, data time: 0.011027869056252873\n",
      "step: 676163, loss: 0.062188416719436646, data time: 0.010521888732910156\n",
      "step: 676164, loss: 0.06053536385297775, data time: 0.010071879939029091\n",
      "step: 676165, loss: 0.0730963721871376, data time: 0.009673857688903808\n",
      "step: 676166, loss: 0.059147875756025314, data time: 0.009313254129318964\n",
      "step: 676167, loss: 0.061443865299224854, data time: 0.008979819037697533\n",
      "step: 676168, loss: 0.06631308794021606, data time: 0.00867610392363175\n",
      "step: 676169, loss: 0.05710921436548233, data time: 0.008394648631413778\n",
      "step: 676170, loss: 0.06119404733181, data time: 0.008138771057128907\n",
      "step: 676171, loss: 0.061083000153303146, data time: 0.007901668548583984\n",
      "step: 676172, loss: 0.06591165065765381, data time: 0.0076804690890842015\n",
      "step: 676173, loss: 0.05637495964765549, data time: 0.0074762191091265\n",
      "step: 676174, loss: 0.05612415075302124, data time: 0.007289697384012157\n",
      "step: 676175, loss: 0.06259578466415405, data time: 0.007115697860717774\n",
      "step: 676176, loss: 0.06031080707907677, data time: 0.006951816620365266\n",
      "step: 676177, loss: 0.0603686198592186, data time: 0.006801865994930267\n",
      "step: 676178, loss: 0.06350451707839966, data time: 0.006650635690400095\n",
      "step: 676179, loss: 0.06355737149715424, data time: 0.006510692484238569\n",
      "step: 676180, loss: 0.06418485939502716, data time: 0.006376498086111886\n",
      "step: 676181, loss: 0.061090137809515, data time: 0.006250354978773329\n",
      "step: 676182, loss: 0.06406533718109131, data time: 0.0061308757678882496\n",
      "step: 676183, loss: 0.05751714110374451, data time: 0.006024191254063656\n",
      "step: 676184, loss: 0.06219196319580078, data time: 0.00592191402728741\n",
      "step: 676185, loss: 0.049374163150787354, data time: 0.005822980403900146\n",
      "step: 676186, loss: 0.06319345533847809, data time: 0.134521484375\n",
      "step: 676187, loss: 0.06096905469894409, data time: 0.06925106048583984\n",
      "step: 676188, loss: 0.06617580354213715, data time: 0.04666010538736979\n",
      "step: 676189, loss: 0.06421707570552826, data time: 0.035638511180877686\n",
      "step: 676190, loss: 0.06471811234951019, data time: 0.02877774238586426\n",
      "step: 676191, loss: 0.060087524354457855, data time: 0.02422308921813965\n",
      "step: 676192, loss: 0.058635514229536057, data time: 0.020951100758143833\n",
      "step: 676193, loss: 0.06481754779815674, data time: 0.018599361181259155\n",
      "step: 676194, loss: 0.05870150029659271, data time: 0.016684638129340276\n",
      "step: 676195, loss: 0.059933580458164215, data time: 0.015221977233886718\n",
      "step: 676196, loss: 0.06348718702793121, data time: 0.014027508822354403\n",
      "step: 676197, loss: 0.05921371281147003, data time: 0.013027230898539225\n",
      "step: 676198, loss: 0.05440882593393326, data time: 0.012186582271869365\n",
      "step: 676199, loss: 0.06330686807632446, data time: 0.011453475270952498\n",
      "step: 676200, loss: 0.06474736332893372, data time: 0.010823790232340496\n",
      "step: 676201, loss: 0.0673782229423523, data time: 0.010278627276420593\n",
      "step: 676202, loss: 0.05505317077040672, data time: 0.009791374206542969\n",
      "step: 676203, loss: 0.06586717069149017, data time: 0.009355783462524414\n",
      "step: 676204, loss: 0.05903568118810654, data time: 0.008965755763806794\n",
      "step: 676205, loss: 0.0597873255610466, data time: 0.008619320392608643\n",
      "step: 676206, loss: 0.06299106776714325, data time: 0.008304981958298456\n",
      "step: 676207, loss: 0.06380647420883179, data time: 0.008022232489152388\n",
      "step: 676208, loss: 0.057483091950416565, data time: 0.007756575294162916\n",
      "step: 676209, loss: 0.06093926355242729, data time: 0.00752304991086324\n",
      "step: 676210, loss: 0.06690628826618195, data time: 0.00730290412902832\n",
      "step: 676211, loss: 0.06340378522872925, data time: 0.0070978311391977165\n",
      "step: 676212, loss: 0.06805649399757385, data time: 0.006909785447297273\n",
      "step: 676213, loss: 0.06471855938434601, data time: 0.006734226431165423\n",
      "step: 676214, loss: 0.06170932203531265, data time: 0.006584989613500135\n",
      "step: 676215, loss: 0.06148633360862732, data time: 0.006433892250061035\n",
      "step: 676216, loss: 0.05663762614130974, data time: 0.006299034241707095\n",
      "step: 676217, loss: 0.0647459551692009, data time: 0.006175726652145386\n",
      "step: 676218, loss: 0.0584409162402153, data time: 0.006047870173598781\n",
      "step: 676219, loss: 0.06514530628919601, data time: 0.00592509438009823\n",
      "step: 676220, loss: 0.05626966804265976, data time: 0.005807672228131976\n",
      "step: 676221, loss: 0.061089109629392624, data time: 0.005696203973558214\n",
      "step: 676222, loss: 0.058364152908325195, data time: 0.005591605160687421\n",
      "step: 676223, loss: 0.06185776740312576, data time: 0.005496834453783538\n",
      "step: 676224, loss: 0.06299097836017609, data time: 0.005405481045062725\n",
      "step: 676225, loss: 0.050446610897779465, data time: 0.005318999290466309\n",
      "step: 676226, loss: 0.05887669324874878, data time: 0.15439987182617188\n",
      "step: 676227, loss: 0.0628029927611351, data time: 0.0779653787612915\n",
      "step: 676228, loss: 0.06016287952661514, data time: 0.05302778879801432\n",
      "step: 676229, loss: 0.062102656811475754, data time: 0.040120065212249756\n",
      "step: 676230, loss: 0.06461455672979355, data time: 0.03236641883850098\n",
      "step: 676231, loss: 0.0634453222155571, data time: 0.027190645535786945\n",
      "step: 676232, loss: 0.06461993604898453, data time: 0.0236551080431257\n",
      "step: 676233, loss: 0.06232577562332153, data time: 0.02088165283203125\n",
      "step: 676234, loss: 0.05900757759809494, data time: 0.01877662870619032\n",
      "step: 676235, loss: 0.06567087024450302, data time: 0.017113041877746583\n",
      "step: 676236, loss: 0.06152042746543884, data time: 0.015745119615034622\n",
      "step: 676237, loss: 0.06542655825614929, data time: 0.014604449272155762\n",
      "step: 676238, loss: 0.06211592257022858, data time: 0.013635782095102163\n",
      "step: 676239, loss: 0.061377283185720444, data time: 0.012807232992989677\n",
      "step: 676240, loss: 0.06407234072685242, data time: 0.012083148956298828\n",
      "step: 676241, loss: 0.057121992111206055, data time: 0.011454716324806213\n",
      "step: 676242, loss: 0.06066695973277092, data time: 0.01089784678290872\n",
      "step: 676243, loss: 0.058705560863018036, data time: 0.010402838389078775\n",
      "step: 676244, loss: 0.06362615525722504, data time: 0.009982159263209292\n",
      "step: 676245, loss: 0.06123550981283188, data time: 0.009607934951782226\n",
      "step: 676246, loss: 0.05530131980776787, data time: 0.00926729610988072\n",
      "step: 676247, loss: 0.0632016584277153, data time: 0.008956562389026989\n",
      "step: 676248, loss: 0.056140750646591187, data time: 0.00866773854131284\n",
      "step: 676249, loss: 0.06467461585998535, data time: 0.008402814467748007\n",
      "step: 676250, loss: 0.05757368355989456, data time: 0.00816502571105957\n",
      "step: 676251, loss: 0.06879270076751709, data time: 0.007942566504845252\n",
      "step: 676252, loss: 0.0653647780418396, data time: 0.007735402495772751\n",
      "step: 676253, loss: 0.060052044689655304, data time: 0.007544100284576416\n",
      "step: 676254, loss: 0.0659608393907547, data time: 0.00737020887177566\n",
      "step: 676255, loss: 0.061288006603717804, data time: 0.0071944713592529295\n",
      "step: 676256, loss: 0.0564698800444603, data time: 0.007031033116002236\n",
      "step: 676257, loss: 0.06382638216018677, data time: 0.006879232823848724\n",
      "step: 676258, loss: 0.06192048639059067, data time: 0.006726004860617898\n",
      "step: 676259, loss: 0.05756676197052002, data time: 0.006583361064686495\n",
      "step: 676260, loss: 0.06131179258227348, data time: 0.006448636736188616\n",
      "step: 676261, loss: 0.06390697509050369, data time: 0.006319973203870986\n",
      "step: 676262, loss: 0.06385579705238342, data time: 0.0062007775177826754\n",
      "step: 676263, loss: 0.05782884359359741, data time: 0.006089223058600175\n",
      "step: 676264, loss: 0.06216367706656456, data time: 0.005984049576979417\n",
      "step: 676265, loss: 0.05742203816771507, data time: 0.005882006883621216\n",
      "step: 676266, loss: 0.06360946595668793, data time: 0.15383696556091309\n",
      "step: 676267, loss: 0.05946812033653259, data time: 0.07851672172546387\n",
      "step: 676268, loss: 0.05773011967539787, data time: 0.05285207430521647\n",
      "step: 676269, loss: 0.06435444205999374, data time: 0.040474653244018555\n",
      "step: 676270, loss: 0.06957926601171494, data time: 0.03264927864074707\n",
      "step: 676271, loss: 0.060992270708084106, data time: 0.027437011400858562\n",
      "step: 676272, loss: 0.05883243680000305, data time: 0.023720196315220425\n",
      "step: 676273, loss: 0.06693011522293091, data time: 0.0210016667842865\n",
      "step: 676274, loss: 0.06050039082765579, data time: 0.018818828794691298\n",
      "step: 676275, loss: 0.058722250163555145, data time: 0.01714034080505371\n",
      "step: 676276, loss: 0.061074282974004745, data time: 0.015766772356900303\n",
      "step: 676277, loss: 0.06362898647785187, data time: 0.014634132385253906\n",
      "step: 676278, loss: 0.0635051280260086, data time: 0.013661512961754432\n",
      "step: 676279, loss: 0.057618699967861176, data time: 0.01282451833997454\n",
      "step: 676280, loss: 0.05942689627408981, data time: 0.012103128433227538\n",
      "step: 676281, loss: 0.057766884565353394, data time: 0.011469617486000061\n",
      "step: 676282, loss: 0.060431502759456635, data time: 0.010910987854003906\n",
      "step: 676283, loss: 0.060662172734737396, data time: 0.010412957933213975\n",
      "step: 676284, loss: 0.061392903327941895, data time: 0.009972948777048211\n",
      "step: 676285, loss: 0.06643043458461761, data time: 0.009577000141143798\n",
      "step: 676286, loss: 0.0655600056052208, data time: 0.009225709097726005\n",
      "step: 676287, loss: 0.06332007795572281, data time: 0.008900783278725365\n",
      "step: 676288, loss: 0.06131356209516525, data time: 0.008598825205927309\n",
      "step: 676289, loss: 0.06460849940776825, data time: 0.008325189352035522\n",
      "step: 676290, loss: 0.06257660686969757, data time: 0.008073625564575195\n",
      "step: 676291, loss: 0.06389355659484863, data time: 0.007838991972116323\n",
      "step: 676292, loss: 0.06344951689243317, data time: 0.0076205553831877534\n",
      "step: 676293, loss: 0.058396246284246445, data time: 0.007418675082070487\n",
      "step: 676294, loss: 0.05992850288748741, data time: 0.007233603247280778\n",
      "step: 676295, loss: 0.06287471950054169, data time: 0.00706628163655599\n",
      "step: 676296, loss: 0.060885049402713776, data time: 0.006905201942689957\n",
      "step: 676297, loss: 0.06130338832736015, data time: 0.006760045886039734\n",
      "step: 676298, loss: 0.061118774116039276, data time: 0.006613998702078155\n",
      "step: 676299, loss: 0.06442846357822418, data time: 0.006474529995637781\n",
      "step: 676300, loss: 0.06713145226240158, data time: 0.006345258440290179\n",
      "step: 676301, loss: 0.06536318361759186, data time: 0.006220122178395589\n",
      "step: 676302, loss: 0.06353349983692169, data time: 0.00610133763906118\n",
      "step: 676303, loss: 0.05336514115333557, data time: 0.005992036116750617\n",
      "step: 676304, loss: 0.06174861639738083, data time: 0.0058898436717497995\n",
      "step: 676305, loss: 0.08452733606100082, data time: 0.0057910561561584474\n",
      "step: 676306, loss: 0.05454420670866966, data time: 0.14831137657165527\n",
      "step: 676307, loss: 0.05813171714544296, data time: 0.07550680637359619\n",
      "step: 676308, loss: 0.06127350777387619, data time: 0.05139613151550293\n",
      "step: 676309, loss: 0.060682326555252075, data time: 0.03918546438217163\n",
      "step: 676310, loss: 0.07036205381155014, data time: 0.0316159725189209\n",
      "step: 676311, loss: 0.06592555344104767, data time: 0.026584744453430176\n",
      "step: 676312, loss: 0.060608815401792526, data time: 0.022989068712506975\n",
      "step: 676313, loss: 0.06008809804916382, data time: 0.02035805583000183\n",
      "step: 676314, loss: 0.06074514240026474, data time: 0.01824408107333713\n",
      "step: 676315, loss: 0.06293082237243652, data time: 0.016621899604797364\n",
      "step: 676316, loss: 0.061015259474515915, data time: 0.01530599594116211\n",
      "step: 676317, loss: 0.06015083193778992, data time: 0.014211058616638184\n",
      "step: 676318, loss: 0.060834094882011414, data time: 0.01328418805049016\n",
      "step: 676319, loss: 0.06331394612789154, data time: 0.012476239885602678\n",
      "step: 676320, loss: 0.066979318857193, data time: 0.011785030364990234\n",
      "step: 676321, loss: 0.0641159638762474, data time: 0.011178016662597656\n",
      "step: 676322, loss: 0.06757259368896484, data time: 0.010640312643612133\n",
      "step: 676323, loss: 0.0644775927066803, data time: 0.01015717453426785\n",
      "step: 676324, loss: 0.056467264890670776, data time: 0.009728921087164628\n",
      "step: 676325, loss: 0.06338636577129364, data time: 0.009347617626190186\n",
      "step: 676326, loss: 0.061943233013153076, data time: 0.009005433037167504\n",
      "step: 676327, loss: 0.06470486521720886, data time: 0.008694800463589754\n",
      "step: 676328, loss: 0.05459605157375336, data time: 0.008403166480686354\n",
      "step: 676329, loss: 0.06597896665334702, data time: 0.008137822151184082\n",
      "step: 676330, loss: 0.06345352530479431, data time: 0.007891416549682617\n",
      "step: 676331, loss: 0.058577775955200195, data time: 0.007666624509371244\n",
      "step: 676332, loss: 0.0543459877371788, data time: 0.007454315821329753\n",
      "step: 676333, loss: 0.060672514140605927, data time: 0.007259241172245571\n",
      "step: 676334, loss: 0.06300413608551025, data time: 0.0070806536181219695\n",
      "step: 676335, loss: 0.060518912971019745, data time: 0.006915108362833659\n",
      "step: 676336, loss: 0.061742495745420456, data time: 0.006759682009297032\n",
      "step: 676337, loss: 0.0608525276184082, data time: 0.006617046892642975\n",
      "step: 676338, loss: 0.06091798469424248, data time: 0.0064736640814578895\n",
      "step: 676339, loss: 0.05820726603269577, data time: 0.006339304587420295\n",
      "step: 676340, loss: 0.06223772093653679, data time: 0.006211253574916295\n",
      "step: 676341, loss: 0.06331238895654678, data time: 0.00609976053237915\n",
      "step: 676342, loss: 0.05918453261256218, data time: 0.0059910142743909685\n",
      "step: 676343, loss: 0.06418315321207047, data time: 0.0058871319419459296\n",
      "step: 676344, loss: 0.0654701516032219, data time: 0.0057897812280899436\n",
      "step: 676345, loss: 0.03416929394006729, data time: 0.005693548917770385\n",
      "step: 676346, loss: 0.060640111565589905, data time: 0.155195951461792\n",
      "step: 676347, loss: 0.06310167908668518, data time: 0.07871115207672119\n",
      "step: 676348, loss: 0.06009428948163986, data time: 0.05388434727986654\n",
      "step: 676349, loss: 0.057162001729011536, data time: 0.04076164960861206\n",
      "step: 676350, loss: 0.0580446682870388, data time: 0.03291482925415039\n",
      "step: 676351, loss: 0.05851922929286957, data time: 0.027667919794718426\n",
      "step: 676352, loss: 0.06421203911304474, data time: 0.023996693747384206\n",
      "step: 676353, loss: 0.06642341613769531, data time: 0.02116250991821289\n",
      "step: 676354, loss: 0.05969247221946716, data time: 0.018957402971055772\n",
      "step: 676355, loss: 0.06060240790247917, data time: 0.017257046699523926\n",
      "step: 676356, loss: 0.05945539474487305, data time: 0.01588481122797186\n",
      "step: 676357, loss: 0.056991949677467346, data time: 0.01473905642827352\n",
      "step: 676358, loss: 0.061772316694259644, data time: 0.013759136199951172\n",
      "step: 676359, loss: 0.052683256566524506, data time: 0.01291755267551967\n",
      "step: 676360, loss: 0.06252690404653549, data time: 0.012189785639444986\n",
      "step: 676361, loss: 0.061017315834760666, data time: 0.011556148529052734\n",
      "step: 676362, loss: 0.06279540061950684, data time: 0.01099550022798426\n",
      "step: 676363, loss: 0.061921995133161545, data time: 0.010498523712158203\n",
      "step: 676364, loss: 0.06581204384565353, data time: 0.010049644269441304\n",
      "step: 676365, loss: 0.06386730074882507, data time: 0.009652090072631837\n",
      "step: 676366, loss: 0.06281273066997528, data time: 0.009291716984340124\n",
      "step: 676367, loss: 0.06105046719312668, data time: 0.00898377461866899\n",
      "step: 676368, loss: 0.06550990790128708, data time: 0.008698152459186056\n",
      "step: 676369, loss: 0.060885004699230194, data time: 0.008433908224105835\n",
      "step: 676370, loss: 0.05934658646583557, data time: 0.008190174102783204\n",
      "step: 676371, loss: 0.06642740219831467, data time: 0.007963611529423641\n",
      "step: 676372, loss: 0.05819026753306389, data time: 0.007754714400679977\n",
      "step: 676373, loss: 0.06112673878669739, data time: 0.007560355322701591\n",
      "step: 676374, loss: 0.061658069491386414, data time: 0.007385656751435378\n",
      "step: 676375, loss: 0.06923016905784607, data time: 0.007226300239562988\n",
      "step: 676376, loss: 0.06697385758161545, data time: 0.007070456781694966\n",
      "step: 676377, loss: 0.05927066132426262, data time: 0.006929926574230194\n",
      "step: 676378, loss: 0.06278516352176666, data time: 0.0067814335678562975\n",
      "step: 676379, loss: 0.060411639511585236, data time: 0.0066408550038057216\n",
      "step: 676380, loss: 0.06622090935707092, data time: 0.006506143297467913\n",
      "step: 676381, loss: 0.058157335966825485, data time: 0.006378736760881212\n",
      "step: 676382, loss: 0.06717447936534882, data time: 0.006259673350566142\n",
      "step: 676383, loss: 0.06289231777191162, data time: 0.006150101360521819\n",
      "step: 676384, loss: 0.055550139397382736, data time: 0.006047530051989433\n",
      "step: 676385, loss: 0.05460914969444275, data time: 0.005947822332382202\n",
      "step: 676386, loss: 0.058201275765895844, data time: 0.14907193183898926\n",
      "step: 676387, loss: 0.06163623183965683, data time: 0.07559621334075928\n",
      "step: 676388, loss: 0.06289559602737427, data time: 0.0508726437886556\n",
      "step: 676389, loss: 0.054956115782260895, data time: 0.039013683795928955\n",
      "step: 676390, loss: 0.06343236565589905, data time: 0.031485891342163085\n",
      "step: 676391, loss: 0.060651034116744995, data time: 0.026484966278076172\n",
      "step: 676392, loss: 0.05751156806945801, data time: 0.022914443697248186\n",
      "step: 676393, loss: 0.06195975840091705, data time: 0.02029469609260559\n",
      "step: 676394, loss: 0.061315909028053284, data time: 0.018185642030504014\n",
      "step: 676395, loss: 0.06641671061515808, data time: 0.016559481620788574\n",
      "step: 676396, loss: 0.06261582672595978, data time: 0.01523765650662509\n",
      "step: 676397, loss: 0.05940714478492737, data time: 0.014138619105021158\n",
      "step: 676398, loss: 0.06395542621612549, data time: 0.013221538983858548\n",
      "step: 676399, loss: 0.06221108138561249, data time: 0.012432490076337541\n",
      "step: 676400, loss: 0.06370711326599121, data time: 0.011747233072916667\n",
      "step: 676401, loss: 0.05492442473769188, data time: 0.011142328381538391\n",
      "step: 676402, loss: 0.05660234019160271, data time: 0.010603540083941291\n",
      "step: 676403, loss: 0.06692124903202057, data time: 0.01012126604715983\n",
      "step: 676404, loss: 0.05745641514658928, data time: 0.009689732601768091\n",
      "step: 676405, loss: 0.06409184634685516, data time: 0.00930553674697876\n",
      "step: 676406, loss: 0.06767626851797104, data time: 0.008963074002947127\n",
      "step: 676407, loss: 0.058527544140815735, data time: 0.00865317474712025\n",
      "step: 676408, loss: 0.062383007258176804, data time: 0.008368782375169836\n",
      "step: 676409, loss: 0.048581093549728394, data time: 0.008103956778844198\n",
      "step: 676410, loss: 0.05644410103559494, data time: 0.007858457565307618\n",
      "step: 676411, loss: 0.05726879462599754, data time: 0.007633227568406325\n",
      "step: 676412, loss: 0.06429272145032883, data time: 0.007421979197749385\n",
      "step: 676413, loss: 0.06012212857604027, data time: 0.007225581577845982\n",
      "step: 676414, loss: 0.06688307970762253, data time: 0.007055759429931641\n",
      "step: 676415, loss: 0.05664963647723198, data time: 0.006889478365580241\n",
      "step: 676416, loss: 0.06185632199048996, data time: 0.006734255821474137\n",
      "step: 676417, loss: 0.06597970426082611, data time: 0.006594359874725342\n",
      "step: 676418, loss: 0.06598197668790817, data time: 0.006452936114686908\n",
      "step: 676419, loss: 0.057415928691625595, data time: 0.0063190460205078125\n",
      "step: 676420, loss: 0.05900152772665024, data time: 0.006193535668509347\n",
      "step: 676421, loss: 0.05911283940076828, data time: 0.006072156959109836\n",
      "step: 676422, loss: 0.06710919737815857, data time: 0.005957886979386613\n",
      "step: 676423, loss: 0.06828676164150238, data time: 0.005852448312859786\n",
      "step: 676424, loss: 0.06282743811607361, data time: 0.005752704082391201\n",
      "step: 676425, loss: 0.05064091086387634, data time: 0.0056588053703308105\n",
      "step: 676426, loss: 0.06434520334005356, data time: 0.14863967895507812\n",
      "step: 676427, loss: 0.062109917402267456, data time: 0.07568764686584473\n",
      "step: 676428, loss: 0.0623631626367569, data time: 0.051205714543660484\n",
      "step: 676429, loss: 0.06183598190546036, data time: 0.03924441337585449\n",
      "step: 676430, loss: 0.05549570918083191, data time: 0.031658267974853514\n",
      "step: 676431, loss: 0.0630083978176117, data time: 0.02662368615468343\n",
      "step: 676432, loss: 0.06139936298131943, data time: 0.02302265167236328\n",
      "step: 676433, loss: 0.062435589730739594, data time: 0.020392119884490967\n",
      "step: 676434, loss: 0.05480275675654411, data time: 0.018275207943386503\n",
      "step: 676435, loss: 0.06197085231542587, data time: 0.01664109230041504\n",
      "step: 676436, loss: 0.06045445799827576, data time: 0.01533454114740545\n",
      "step: 676437, loss: 0.058352187275886536, data time: 0.01423583428064982\n",
      "step: 676438, loss: 0.0664747953414917, data time: 0.013311807925884541\n",
      "step: 676439, loss: 0.061626628041267395, data time: 0.012499655996050154\n",
      "step: 676440, loss: 0.06599684804677963, data time: 0.011805518468221029\n",
      "step: 676441, loss: 0.0630183070898056, data time: 0.01119367778301239\n",
      "step: 676442, loss: 0.060724031180143356, data time: 0.010654211044311523\n",
      "step: 676443, loss: 0.06817098706960678, data time: 0.01017259226904975\n",
      "step: 676444, loss: 0.06724722683429718, data time: 0.009745936644704719\n",
      "step: 676445, loss: 0.05854486674070358, data time: 0.009368109703063964\n",
      "step: 676446, loss: 0.06309047341346741, data time: 0.009022417522612073\n",
      "step: 676447, loss: 0.05820600315928459, data time: 0.008705084974115545\n",
      "step: 676448, loss: 0.05979564040899277, data time: 0.008413532505864683\n",
      "step: 676449, loss: 0.05886710807681084, data time: 0.00814687212308248\n",
      "step: 676450, loss: 0.06678508967161179, data time: 0.007901611328125\n",
      "step: 676451, loss: 0.05787933990359306, data time: 0.007675537696251502\n",
      "step: 676452, loss: 0.05805911868810654, data time: 0.00746590119821054\n",
      "step: 676453, loss: 0.05880250781774521, data time: 0.007284947804042271\n",
      "step: 676454, loss: 0.059970006346702576, data time: 0.007109082978347252\n",
      "step: 676455, loss: 0.06827066838741302, data time: 0.006947779655456543\n",
      "step: 676456, loss: 0.055195197463035583, data time: 0.006790753333799301\n",
      "step: 676457, loss: 0.06425751000642776, data time: 0.006646342575550079\n",
      "step: 676458, loss: 0.0628112405538559, data time: 0.006501934745094993\n",
      "step: 676459, loss: 0.06608894467353821, data time: 0.0063649906831629134\n",
      "step: 676460, loss: 0.05840428173542023, data time: 0.0062366962432861325\n",
      "step: 676461, loss: 0.06348877400159836, data time: 0.0061139994197421605\n",
      "step: 676462, loss: 0.06326937675476074, data time: 0.006002174841391074\n",
      "step: 676463, loss: 0.05829432234168053, data time: 0.005899579901444285\n",
      "step: 676464, loss: 0.06322922557592392, data time: 0.005802368506407127\n",
      "step: 676465, loss: 0.0573134571313858, data time: 0.0057049810886383055\n",
      "step: 676466, loss: 0.058375947177410126, data time: 0.13778090476989746\n",
      "step: 676467, loss: 0.06145346164703369, data time: 0.07085716724395752\n",
      "step: 676468, loss: 0.06683099269866943, data time: 0.04775881767272949\n",
      "step: 676469, loss: 0.061215609312057495, data time: 0.036672353744506836\n",
      "step: 676470, loss: 0.06545498222112656, data time: 0.02961597442626953\n",
      "step: 676471, loss: 0.0609971359372139, data time: 0.02491581439971924\n",
      "step: 676472, loss: 0.06517713516950607, data time: 0.021546500069754466\n",
      "step: 676473, loss: 0.061797868460416794, data time: 0.01909920573234558\n",
      "step: 676474, loss: 0.061215102672576904, data time: 0.017126295301649306\n",
      "step: 676475, loss: 0.05460767075419426, data time: 0.01561131477355957\n",
      "step: 676476, loss: 0.06580120325088501, data time: 0.014383641156283293\n",
      "step: 676477, loss: 0.05862104892730713, data time: 0.01336216926574707\n",
      "step: 676478, loss: 0.06781473755836487, data time: 0.012492638367872972\n",
      "step: 676479, loss: 0.06389304250478745, data time: 0.011739611625671387\n",
      "step: 676480, loss: 0.061256736516952515, data time: 0.011092344919840494\n",
      "step: 676481, loss: 0.062240879982709885, data time: 0.010524839162826538\n",
      "step: 676482, loss: 0.06050657853484154, data time: 0.010030465967514935\n",
      "step: 676483, loss: 0.06594262272119522, data time: 0.00958467854393853\n",
      "step: 676484, loss: 0.0651756227016449, data time: 0.009187911686144377\n",
      "step: 676485, loss: 0.06832420080900192, data time: 0.00883573293685913\n",
      "step: 676486, loss: 0.06144590675830841, data time: 0.008516731716337659\n",
      "step: 676487, loss: 0.05974600836634636, data time: 0.008225343444130638\n",
      "step: 676488, loss: 0.06610303372144699, data time: 0.007952348045680834\n",
      "step: 676489, loss: 0.06091656535863876, data time: 0.007705251375834147\n",
      "step: 676490, loss: 0.05664847046136856, data time: 0.00747441291809082\n",
      "step: 676491, loss: 0.055142324417829514, data time: 0.007261505493750939\n",
      "step: 676492, loss: 0.06234562024474144, data time: 0.00706509307578758\n",
      "step: 676493, loss: 0.06870456039905548, data time: 0.006884728159223284\n",
      "step: 676494, loss: 0.0591687373816967, data time: 0.006722557133641736\n",
      "step: 676495, loss: 0.05994655191898346, data time: 0.0065683682759602865\n",
      "step: 676496, loss: 0.057355545461177826, data time: 0.00642177366441296\n",
      "step: 676497, loss: 0.05802144855260849, data time: 0.006289444863796234\n",
      "step: 676498, loss: 0.057563167065382004, data time: 0.006159370595758612\n",
      "step: 676499, loss: 0.0627128928899765, data time: 0.006031085463131175\n",
      "step: 676500, loss: 0.06571557372808456, data time: 0.005911350250244141\n",
      "step: 676501, loss: 0.06124716252088547, data time: 0.005798757076263428\n",
      "step: 676502, loss: 0.06277741491794586, data time: 0.005695781192264041\n",
      "step: 676503, loss: 0.06159591302275658, data time: 0.005599147395083779\n",
      "step: 676504, loss: 0.06243617460131645, data time: 0.005512139736077724\n",
      "step: 676505, loss: 0.06491754949092865, data time: 0.0054241657257080075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1585/3775808921.py:130: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 676506, loss: 0.06542713940143585, data time: 0.1530437469482422\n",
      "step: 676507, loss: 0.06089073419570923, data time: 0.07727503776550293\n",
      "step: 676508, loss: 0.06147067993879318, data time: 0.0520174503326416\n",
      "step: 676509, loss: 0.06498103588819504, data time: 0.039941489696502686\n",
      "step: 676510, loss: 0.0638437420129776, data time: 0.032282686233520506\n",
      "step: 676511, loss: 0.06380265951156616, data time: 0.02716680367787679\n",
      "step: 676512, loss: 0.06284940242767334, data time: 0.023547989981515065\n",
      "step: 676513, loss: 0.06526368856430054, data time: 0.020901262760162354\n",
      "step: 676514, loss: 0.05701621621847153, data time: 0.018757237328423396\n",
      "step: 676515, loss: 0.06151486933231354, data time: 0.01711721420288086\n",
      "step: 676516, loss: 0.06503038853406906, data time: 0.01579217477278276\n",
      "step: 676517, loss: 0.06047624349594116, data time: 0.014681736628214518\n",
      "step: 676518, loss: 0.0578623041510582, data time: 0.013734065569364108\n",
      "step: 676519, loss: 0.06431950628757477, data time: 0.012922508375985282\n",
      "step: 676520, loss: 0.07283206284046173, data time: 0.012219460805257161\n",
      "step: 676521, loss: 0.057578377425670624, data time: 0.01160508394241333\n",
      "step: 676522, loss: 0.05832894146442413, data time: 0.011062327553244197\n",
      "step: 676523, loss: 0.055051904171705246, data time: 0.010573188463846842\n",
      "step: 676524, loss: 0.06005819886922836, data time: 0.010146856307983398\n",
      "step: 676525, loss: 0.05800161883234978, data time: 0.009764909744262695\n",
      "step: 676526, loss: 0.05844628065824509, data time: 0.009405102048601423\n",
      "step: 676527, loss: 0.05541914328932762, data time: 0.009071425958113237\n",
      "step: 676528, loss: 0.06100325286388397, data time: 0.008763209633205248\n",
      "step: 676529, loss: 0.06134913116693497, data time: 0.008481999238332113\n",
      "step: 676530, loss: 0.057067327201366425, data time: 0.008222732543945312\n",
      "step: 676531, loss: 0.05609623342752457, data time: 0.007982712525587816\n",
      "step: 676532, loss: 0.05729357898235321, data time: 0.007757345835367839\n",
      "step: 676533, loss: 0.061514802277088165, data time: 0.007552010672433036\n",
      "step: 676534, loss: 0.058293264359235764, data time: 0.0073657282467546135\n",
      "step: 676535, loss: 0.06257930397987366, data time: 0.007193168004353841\n",
      "step: 676536, loss: 0.06529632955789566, data time: 0.007027218418736611\n",
      "step: 676537, loss: 0.061822764575481415, data time: 0.006876230239868164\n",
      "step: 676538, loss: 0.05758296698331833, data time: 0.006724307031342478\n",
      "step: 676539, loss: 0.058673471212387085, data time: 0.006590850212994744\n",
      "step: 676540, loss: 0.07242754101753235, data time: 0.006461402348109654\n",
      "step: 676541, loss: 0.0648355782032013, data time: 0.006336139308081733\n",
      "step: 676542, loss: 0.06311408430337906, data time: 0.006219316173244167\n",
      "step: 676543, loss: 0.06130286678671837, data time: 0.006112657095256604\n",
      "step: 676544, loss: 0.05999041721224785, data time: 0.006011858964577699\n",
      "step: 676545, loss: 0.05167410895228386, data time: 0.0059119582176208494\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.410891056060791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 676546, loss: 0.06609033048152924, data time: 0.1676795482635498\n",
      "step: 676547, loss: 0.06420034170150757, data time: 0.08519017696380615\n",
      "step: 676548, loss: 0.0588192492723465, data time: 0.057657082875569664\n",
      "step: 676549, loss: 0.06332613527774811, data time: 0.04398345947265625\n",
      "step: 676550, loss: 0.06606119871139526, data time: 0.03547005653381348\n",
      "step: 676551, loss: 0.06259283423423767, data time: 0.029778162638346355\n",
      "step: 676552, loss: 0.0567590594291687, data time: 0.0257155214037214\n",
      "step: 676553, loss: 0.06814844906330109, data time: 0.022751539945602417\n",
      "step: 676554, loss: 0.061830248683691025, data time: 0.020367225011189777\n",
      "step: 676555, loss: 0.06697158515453339, data time: 0.018520641326904296\n",
      "step: 676556, loss: 0.05413498729467392, data time: 0.01701886003667658\n",
      "step: 676557, loss: 0.05915239453315735, data time: 0.015768508116404217\n",
      "step: 676558, loss: 0.06513440608978271, data time: 0.014719009399414062\n",
      "step: 676559, loss: 0.06367479264736176, data time: 0.013806564467293876\n",
      "step: 676560, loss: 0.0602460615336895, data time: 0.013014745712280274\n",
      "step: 676561, loss: 0.06603934615850449, data time: 0.012327581644058228\n",
      "step: 676562, loss: 0.06705759465694427, data time: 0.011716113371007583\n",
      "step: 676563, loss: 0.06350287050008774, data time: 0.011173672146267362\n",
      "step: 676564, loss: 0.05773168429732323, data time: 0.010693035627666273\n",
      "step: 676565, loss: 0.06330259144306183, data time: 0.010267925262451173\n",
      "step: 676566, loss: 0.05897042900323868, data time: 0.009875059127807617\n",
      "step: 676567, loss: 0.06212065741419792, data time: 0.009520899165760387\n",
      "step: 676568, loss: 0.06446674466133118, data time: 0.009194405182548191\n",
      "step: 676569, loss: 0.06386119872331619, data time: 0.008893787860870361\n",
      "step: 676570, loss: 0.05839161202311516, data time: 0.008616161346435548\n",
      "step: 676571, loss: 0.0588165745139122, data time: 0.008359872377835788\n",
      "step: 676572, loss: 0.06309955567121506, data time: 0.0081210489626284\n",
      "step: 676573, loss: 0.06183459609746933, data time: 0.007905857903616769\n",
      "step: 676574, loss: 0.05250973626971245, data time: 0.0077016518033784015\n",
      "step: 676575, loss: 0.06080371141433716, data time: 0.00751193364461263\n",
      "step: 676576, loss: 0.05802008509635925, data time: 0.007335570550733997\n",
      "step: 676577, loss: 0.0638640820980072, data time: 0.007177874445915222\n",
      "step: 676578, loss: 0.06398563832044601, data time: 0.007016088023330226\n",
      "step: 676579, loss: 0.062470804899930954, data time: 0.006867233444662655\n",
      "step: 676580, loss: 0.06808827817440033, data time: 0.00672354016985212\n",
      "step: 676581, loss: 0.0669921338558197, data time: 0.006587154335445828\n",
      "step: 676582, loss: 0.058493487536907196, data time: 0.006458868851532807\n",
      "step: 676583, loss: 0.06831762194633484, data time: 0.006340001758776213\n",
      "step: 676584, loss: 0.06929399818181992, data time: 0.006225964961907802\n",
      "step: 676585, loss: 0.051758334040641785, data time: 0.006118404865264893\n",
      "step: 676586, loss: 0.06790117919445038, data time: 0.15250778198242188\n",
      "step: 676587, loss: 0.067628875374794, data time: 0.07698595523834229\n",
      "step: 676588, loss: 0.061758853495121, data time: 0.052298943201700844\n",
      "step: 676589, loss: 0.06553006172180176, data time: 0.039845407009124756\n",
      "step: 676590, loss: 0.06651614606380463, data time: 0.03217182159423828\n",
      "step: 676591, loss: 0.058456480503082275, data time: 0.027040719985961914\n",
      "step: 676592, loss: 0.058938514441251755, data time: 0.023402282169886997\n",
      "step: 676593, loss: 0.06499627232551575, data time: 0.0207785964012146\n",
      "step: 676594, loss: 0.06434573233127594, data time: 0.018612967597113714\n",
      "step: 676595, loss: 0.057067058980464935, data time: 0.016946005821228027\n",
      "step: 676596, loss: 0.06293438374996185, data time: 0.015601851723410866\n",
      "step: 676597, loss: 0.05656849965453148, data time: 0.014469544092814127\n",
      "step: 676598, loss: 0.06392256915569305, data time: 0.013512904827411357\n",
      "step: 676599, loss: 0.06524071097373962, data time: 0.012696470533098494\n",
      "step: 676600, loss: 0.062254637479782104, data time: 0.011986716588338216\n",
      "step: 676601, loss: 0.06260533630847931, data time: 0.011408299207687378\n",
      "step: 676602, loss: 0.05898481607437134, data time: 0.010854426552267635\n",
      "step: 676603, loss: 0.06018549203872681, data time: 0.010357724295722114\n",
      "step: 676604, loss: 0.05961510166525841, data time: 0.009916481218839946\n",
      "step: 676605, loss: 0.06523041427135468, data time: 0.009526920318603516\n",
      "step: 676606, loss: 0.05550684779882431, data time: 0.009176458631243025\n",
      "step: 676607, loss: 0.056383006274700165, data time: 0.008848504586653276\n",
      "step: 676608, loss: 0.06230076402425766, data time: 0.00854938963185186\n",
      "step: 676609, loss: 0.06522305309772491, data time: 0.008279999097188314\n",
      "step: 676610, loss: 0.06026199087500572, data time: 0.008031091690063476\n",
      "step: 676611, loss: 0.06407587230205536, data time: 0.007797791407658503\n",
      "step: 676612, loss: 0.059730470180511475, data time: 0.007584598329332139\n",
      "step: 676613, loss: 0.05800767242908478, data time: 0.007385381630488804\n",
      "step: 676614, loss: 0.06537368893623352, data time: 0.007207615622158708\n",
      "step: 676615, loss: 0.058761753141880035, data time: 0.007034476598103841\n",
      "step: 676616, loss: 0.06088167428970337, data time: 0.00687193101452243\n",
      "step: 676617, loss: 0.06035872548818588, data time: 0.006725966930389404\n",
      "step: 676618, loss: 0.06896033138036728, data time: 0.006579673651492957\n",
      "step: 676619, loss: 0.06044209003448486, data time: 0.006441333714653464\n",
      "step: 676620, loss: 0.05527200549840927, data time: 0.006311348506382534\n",
      "step: 676621, loss: 0.05708659440279007, data time: 0.006187948915693495\n",
      "step: 676622, loss: 0.05764411389827728, data time: 0.0060706718547924145\n",
      "step: 676623, loss: 0.0653282031416893, data time: 0.005962547503019634\n",
      "step: 676624, loss: 0.05594513937830925, data time: 0.005859093788342598\n",
      "step: 676625, loss: 0.05238252878189087, data time: 0.0057605385780334474\n",
      "step: 676626, loss: 0.06650979816913605, data time: 0.14623451232910156\n",
      "step: 676627, loss: 0.06152994558215141, data time: 0.07464897632598877\n",
      "step: 676628, loss: 0.06185931712388992, data time: 0.05073237419128418\n",
      "step: 676629, loss: 0.0637577697634697, data time: 0.038688719272613525\n",
      "step: 676630, loss: 0.06478595733642578, data time: 0.031224536895751952\n",
      "step: 676631, loss: 0.06440868973731995, data time: 0.026261369387308758\n",
      "step: 676632, loss: 0.06040842458605766, data time: 0.022699764796665738\n",
      "step: 676633, loss: 0.06373604387044907, data time: 0.02010849118232727\n",
      "step: 676634, loss: 0.056098323315382004, data time: 0.018014483981662326\n",
      "step: 676635, loss: 0.06078556180000305, data time: 0.016422271728515625\n",
      "step: 676636, loss: 0.06085191294550896, data time: 0.01512135158885609\n",
      "step: 676637, loss: 0.06042979285120964, data time: 0.01403448979059855\n",
      "step: 676638, loss: 0.055241577327251434, data time: 0.013104915618896484\n",
      "step: 676639, loss: 0.07015998661518097, data time: 0.012308393205915178\n",
      "step: 676640, loss: 0.06568506360054016, data time: 0.011619853973388671\n",
      "step: 676641, loss: 0.05724061280488968, data time: 0.011020392179489136\n",
      "step: 676642, loss: 0.059701334685087204, data time: 0.01048887477201574\n",
      "step: 676643, loss: 0.06288889050483704, data time: 0.010016428099738227\n",
      "step: 676644, loss: 0.0583261214196682, data time: 0.00959577058490954\n",
      "step: 676645, loss: 0.06244915723800659, data time: 0.009221410751342774\n",
      "step: 676646, loss: 0.06648720800876617, data time: 0.008878480820428757\n",
      "step: 676647, loss: 0.06771434843540192, data time: 0.008566997267983177\n",
      "step: 676648, loss: 0.056426405906677246, data time: 0.008281199828438137\n",
      "step: 676649, loss: 0.06475977599620819, data time: 0.008024175961812338\n",
      "step: 676650, loss: 0.05996774882078171, data time: 0.007784442901611328\n",
      "step: 676651, loss: 0.05892617627978325, data time: 0.007559638756972093\n",
      "step: 676652, loss: 0.062487225979566574, data time: 0.007350621400056062\n",
      "step: 676653, loss: 0.06364372372627258, data time: 0.007162868976593018\n",
      "step: 676654, loss: 0.05920477956533432, data time: 0.006988689817231277\n",
      "step: 676655, loss: 0.057978495955467224, data time: 0.006822331746419271\n",
      "step: 676656, loss: 0.0608096569776535, data time: 0.006668075438468687\n",
      "step: 676657, loss: 0.06649802625179291, data time: 0.006530687212944031\n",
      "step: 676658, loss: 0.06208978220820427, data time: 0.006394212896173651\n",
      "step: 676659, loss: 0.06050185486674309, data time: 0.0062622953863704905\n",
      "step: 676660, loss: 0.06610588729381561, data time: 0.006137078148978097\n",
      "step: 676661, loss: 0.06168067455291748, data time: 0.0060162411795722116\n",
      "step: 676662, loss: 0.057197123765945435, data time: 0.005903811068148227\n",
      "step: 676663, loss: 0.06582778692245483, data time: 0.005800579723558928\n",
      "step: 676664, loss: 0.06494288146495819, data time: 0.005702452781872871\n",
      "step: 676665, loss: 0.07692946493625641, data time: 0.005607831478118897\n",
      "step: 676666, loss: 0.06766475737094879, data time: 0.16300582885742188\n",
      "step: 676667, loss: 0.06130465865135193, data time: 0.08223247528076172\n",
      "step: 676668, loss: 0.06593386083841324, data time: 0.05569108327229818\n",
      "step: 676669, loss: 0.06304578483104706, data time: 0.04252636432647705\n",
      "step: 676670, loss: 0.06482254713773727, data time: 0.03428778648376465\n",
      "step: 676671, loss: 0.06009026616811752, data time: 0.028803547223409016\n",
      "step: 676672, loss: 0.06424306333065033, data time: 0.024895633969988142\n",
      "step: 676673, loss: 0.06312805414199829, data time: 0.022036105394363403\n",
      "step: 676674, loss: 0.06689998507499695, data time: 0.01973613103230794\n",
      "step: 676675, loss: 0.06766389310359955, data time: 0.017963242530822755\n",
      "step: 676676, loss: 0.0615844652056694, data time: 0.016541827808726917\n",
      "step: 676677, loss: 0.05913769453763962, data time: 0.015336930751800537\n",
      "step: 676678, loss: 0.05910622701048851, data time: 0.014316998995267428\n",
      "step: 676679, loss: 0.05636065453290939, data time: 0.013438854898725237\n",
      "step: 676680, loss: 0.06082800775766373, data time: 0.012707090377807618\n",
      "step: 676681, loss: 0.06551273167133331, data time: 0.012038454413414001\n",
      "step: 676682, loss: 0.06456410884857178, data time: 0.011445676579194911\n",
      "step: 676683, loss: 0.05976789444684982, data time: 0.010916007889641656\n",
      "step: 676684, loss: 0.06306681036949158, data time: 0.010445017563669305\n",
      "step: 676685, loss: 0.06335028260946274, data time: 0.010027825832366943\n",
      "step: 676686, loss: 0.059646740555763245, data time: 0.009648516064598447\n",
      "step: 676687, loss: 0.054253362119197845, data time: 0.009300632910294966\n",
      "step: 676688, loss: 0.06354621052742004, data time: 0.008980813233748726\n",
      "step: 676689, loss: 0.05824228376150131, data time: 0.008689641952514648\n",
      "step: 676690, loss: 0.05908643454313278, data time: 0.008421831130981446\n",
      "step: 676691, loss: 0.06943506002426147, data time: 0.008175400587228628\n",
      "step: 676692, loss: 0.062010373920202255, data time: 0.007947930583247432\n",
      "step: 676693, loss: 0.06279915571212769, data time: 0.007733915533338275\n",
      "step: 676694, loss: 0.06379717588424683, data time: 0.0075408836890911235\n",
      "step: 676695, loss: 0.062043529003858566, data time: 0.007356468836466471\n",
      "step: 676696, loss: 0.0707261860370636, data time: 0.007186082101637317\n",
      "step: 676697, loss: 0.05839543044567108, data time: 0.007033675909042358\n",
      "step: 676698, loss: 0.06002521514892578, data time: 0.006878542177604906\n",
      "step: 676699, loss: 0.06320902705192566, data time: 0.006733094944673426\n",
      "step: 676700, loss: 0.06527930498123169, data time: 0.006594596590314593\n",
      "step: 676701, loss: 0.06377515196800232, data time: 0.006461792522006565\n",
      "step: 676702, loss: 0.05844463035464287, data time: 0.006336631001652898\n",
      "step: 676703, loss: 0.05962364003062248, data time: 0.006222542963529888\n",
      "step: 676704, loss: 0.061213888227939606, data time: 0.006114971943390675\n",
      "step: 676705, loss: 0.08607172220945358, data time: 0.006009989976882934\n",
      "step: 676706, loss: 0.06353642791509628, data time: 0.15785908699035645\n",
      "step: 676707, loss: 0.06425553560256958, data time: 0.08022522926330566\n",
      "step: 676708, loss: 0.05862247198820114, data time: 0.05453332265218099\n",
      "step: 676709, loss: 0.06323084235191345, data time: 0.04153251647949219\n",
      "step: 676710, loss: 0.05872080847620964, data time: 0.033502960205078126\n",
      "step: 676711, loss: 0.06096836179494858, data time: 0.028151790301005047\n",
      "step: 676712, loss: 0.061285700649023056, data time: 0.024330922535487583\n",
      "step: 676713, loss: 0.06224776804447174, data time: 0.021534234285354614\n",
      "step: 676714, loss: 0.06607857346534729, data time: 0.019283506605360243\n",
      "step: 676715, loss: 0.06255973875522614, data time: 0.0175537109375\n",
      "step: 676716, loss: 0.0652453824877739, data time: 0.01614314859563654\n",
      "step: 676717, loss: 0.06310912221670151, data time: 0.014970242977142334\n",
      "step: 676718, loss: 0.06329572945833206, data time: 0.013976830702561598\n",
      "step: 676719, loss: 0.06171954423189163, data time: 0.013126049722943987\n",
      "step: 676720, loss: 0.05945359915494919, data time: 0.012414201100667318\n",
      "step: 676721, loss: 0.05977952480316162, data time: 0.01176154613494873\n",
      "step: 676722, loss: 0.06477817893028259, data time: 0.0111846783581902\n",
      "step: 676723, loss: 0.059484705328941345, data time: 0.01067132420010037\n",
      "step: 676724, loss: 0.05881576985120773, data time: 0.01021693882189299\n",
      "step: 676725, loss: 0.060601942241191864, data time: 0.009810435771942138\n",
      "step: 676726, loss: 0.06289435178041458, data time: 0.009443362553914389\n",
      "step: 676727, loss: 0.06418845057487488, data time: 0.009108239954168146\n",
      "step: 676728, loss: 0.05770459398627281, data time: 0.00879985353221064\n",
      "step: 676729, loss: 0.06340678781270981, data time: 0.008516828219095865\n",
      "step: 676730, loss: 0.06210806965827942, data time: 0.008256969451904296\n",
      "step: 676731, loss: 0.06111406534910202, data time: 0.008014477216280423\n",
      "step: 676732, loss: 0.062109723687171936, data time: 0.007788057680483218\n",
      "step: 676733, loss: 0.051491763442754745, data time: 0.00758237498147147\n",
      "step: 676734, loss: 0.05675389617681503, data time: 0.007392274922338025\n",
      "step: 676735, loss: 0.059688519686460495, data time: 0.007213735580444336\n",
      "step: 676736, loss: 0.06445125490427017, data time: 0.007043284754599294\n",
      "step: 676737, loss: 0.06581464409828186, data time: 0.00689205527305603\n",
      "step: 676738, loss: 0.05984427034854889, data time: 0.006741292548902107\n",
      "step: 676739, loss: 0.0596538744866848, data time: 0.0065994192572201\n",
      "step: 676740, loss: 0.06408213078975677, data time: 0.006463541303362165\n",
      "step: 676741, loss: 0.060523372143507004, data time: 0.006334165732065837\n",
      "step: 676742, loss: 0.05851129814982414, data time: 0.006211963859764305\n",
      "step: 676743, loss: 0.059069227427244186, data time: 0.00610087419811048\n",
      "step: 676744, loss: 0.06768528372049332, data time: 0.0059953775161351915\n",
      "step: 676745, loss: 0.08252027630805969, data time: 0.005893045663833618\n",
      "step: 676746, loss: 0.06647404283285141, data time: 0.16147923469543457\n",
      "step: 676747, loss: 0.06758535653352737, data time: 0.08180701732635498\n",
      "step: 676748, loss: 0.06429123133420944, data time: 0.055551767349243164\n",
      "step: 676749, loss: 0.05943107604980469, data time: 0.04242122173309326\n",
      "step: 676750, loss: 0.05916959047317505, data time: 0.034212589263916016\n",
      "step: 676751, loss: 0.06306922435760498, data time: 0.028754552205403645\n",
      "step: 676752, loss: 0.06342505663633347, data time: 0.024860245840890065\n",
      "step: 676753, loss: 0.06520602107048035, data time: 0.022005289793014526\n",
      "step: 676754, loss: 0.0640808492898941, data time: 0.019702619976467557\n",
      "step: 676755, loss: 0.06369605660438538, data time: 0.01793637275695801\n",
      "step: 676756, loss: 0.06859629601240158, data time: 0.01649993116205389\n",
      "step: 676757, loss: 0.0655750185251236, data time: 0.015306333700815836\n",
      "step: 676758, loss: 0.056856028735637665, data time: 0.014295082825880785\n",
      "step: 676759, loss: 0.05886392295360565, data time: 0.013413940157209123\n",
      "step: 676760, loss: 0.06312575936317444, data time: 0.012652095158894856\n",
      "step: 676761, loss: 0.05437759682536125, data time: 0.0119895339012146\n",
      "step: 676762, loss: 0.06471167504787445, data time: 0.011401190477259019\n",
      "step: 676763, loss: 0.06125981733202934, data time: 0.010881622632344564\n",
      "step: 676764, loss: 0.06646649539470673, data time: 0.010414650565699526\n",
      "step: 676765, loss: 0.06359094381332397, data time: 0.009999525547027589\n",
      "step: 676766, loss: 0.06341652572154999, data time: 0.009625820886521112\n",
      "step: 676767, loss: 0.06380078196525574, data time: 0.009278167377818714\n",
      "step: 676768, loss: 0.06416469067335129, data time: 0.008958308593086575\n",
      "step: 676769, loss: 0.05915243551135063, data time: 0.008671323458353678\n",
      "step: 676770, loss: 0.058911245316267014, data time: 0.00840616226196289\n",
      "step: 676771, loss: 0.0586022287607193, data time: 0.008160554445706882\n",
      "step: 676772, loss: 0.06153785437345505, data time: 0.007928115350228769\n",
      "step: 676773, loss: 0.06148805841803551, data time: 0.007715020860944476\n",
      "step: 676774, loss: 0.05965890735387802, data time: 0.007523520239468278\n",
      "step: 676775, loss: 0.06136731430888176, data time: 0.007341734568277995\n",
      "step: 676776, loss: 0.06079968810081482, data time: 0.007170592584917622\n",
      "step: 676777, loss: 0.0605500265955925, data time: 0.007015340030193329\n",
      "step: 676778, loss: 0.06263928860425949, data time: 0.006861484411991004\n",
      "step: 676779, loss: 0.06001390144228935, data time: 0.006714989157284007\n",
      "step: 676780, loss: 0.06222428381443024, data time: 0.0065786702292306085\n",
      "step: 676781, loss: 0.06396365165710449, data time: 0.006449262301127116\n",
      "step: 676782, loss: 0.06857360154390335, data time: 0.0063271264772157415\n",
      "step: 676783, loss: 0.0616980642080307, data time: 0.0062144618285329715\n",
      "step: 676784, loss: 0.06266282498836517, data time: 0.006109500542665139\n",
      "step: 676785, loss: 0.04589657485485077, data time: 0.0060075104236602785\n",
      "step: 676786, loss: 0.06046869978308678, data time: 0.1575765609741211\n",
      "step: 676787, loss: 0.06928097456693649, data time: 0.08011019229888916\n",
      "step: 676788, loss: 0.05957949161529541, data time: 0.05438375473022461\n",
      "step: 676789, loss: 0.057594798505306244, data time: 0.04141765832901001\n",
      "step: 676790, loss: 0.06098586320877075, data time: 0.033413028717041014\n",
      "step: 676791, loss: 0.06542341411113739, data time: 0.02808698018391927\n",
      "step: 676792, loss: 0.06629502773284912, data time: 0.024274587631225586\n",
      "step: 676793, loss: 0.06049984693527222, data time: 0.021498948335647583\n",
      "step: 676794, loss: 0.058291636407375336, data time: 0.0192566712697347\n",
      "step: 676795, loss: 0.06053822487592697, data time: 0.017526578903198243\n",
      "step: 676796, loss: 0.07026064395904541, data time: 0.016124725341796875\n",
      "step: 676797, loss: 0.06213490664958954, data time: 0.014956891536712646\n",
      "step: 676798, loss: 0.06222138926386833, data time: 0.013958325752845177\n",
      "step: 676799, loss: 0.06249150633811951, data time: 0.013103570256914412\n",
      "step: 676800, loss: 0.06721726059913635, data time: 0.012366088231404622\n",
      "step: 676801, loss: 0.0596449077129364, data time: 0.011721059679985046\n",
      "step: 676802, loss: 0.059435974806547165, data time: 0.011150163762709674\n",
      "step: 676803, loss: 0.059166472405195236, data time: 0.010636263423495822\n",
      "step: 676804, loss: 0.05512803792953491, data time: 0.010179143202932258\n",
      "step: 676805, loss: 0.05714116618037224, data time: 0.009776639938354491\n",
      "step: 676806, loss: 0.06193683668971062, data time: 0.009408746446881975\n",
      "step: 676807, loss: 0.06834392994642258, data time: 0.009071826934814453\n",
      "step: 676808, loss: 0.06438033282756805, data time: 0.008764723072881285\n",
      "step: 676809, loss: 0.06623070687055588, data time: 0.008482366800308228\n",
      "step: 676810, loss: 0.05789625644683838, data time: 0.008222455978393555\n",
      "step: 676811, loss: 0.06192106008529663, data time: 0.007981676321763258\n",
      "step: 676812, loss: 0.0646531879901886, data time: 0.007759041256374783\n",
      "step: 676813, loss: 0.06160939857363701, data time: 0.007553475243704659\n",
      "step: 676814, loss: 0.05706919729709625, data time: 0.007366936782310749\n",
      "step: 676815, loss: 0.057447582483291626, data time: 0.007189528147379557\n",
      "step: 676816, loss: 0.0561465322971344, data time: 0.007023857485863471\n",
      "step: 676817, loss: 0.06393402069807053, data time: 0.006874792277812958\n",
      "step: 676818, loss: 0.057077448815107346, data time: 0.0067233533570260715\n",
      "step: 676819, loss: 0.058256156742572784, data time: 0.006581734208499684\n",
      "step: 676820, loss: 0.06759048998355865, data time: 0.006446933746337891\n",
      "step: 676821, loss: 0.05691567063331604, data time: 0.006318012873331706\n",
      "step: 676822, loss: 0.0615045540034771, data time: 0.006196582639539564\n",
      "step: 676823, loss: 0.06297817081212997, data time: 0.006086725937692742\n",
      "step: 676824, loss: 0.062208980321884155, data time: 0.005984324675339919\n",
      "step: 676825, loss: 0.05571606755256653, data time: 0.005886101722717285\n",
      "step: 676826, loss: 0.05815758928656578, data time: 0.16513609886169434\n",
      "step: 676827, loss: 0.06388713419437408, data time: 0.08408761024475098\n",
      "step: 676828, loss: 0.06353598833084106, data time: 0.05654144287109375\n",
      "step: 676829, loss: 0.061153437942266464, data time: 0.04318380355834961\n",
      "step: 676830, loss: 0.06316633522510529, data time: 0.03480706214904785\n",
      "step: 676831, loss: 0.062813900411129, data time: 0.029239177703857422\n",
      "step: 676832, loss: 0.0656471997499466, data time: 0.025265114648001536\n",
      "step: 676833, loss: 0.06465759873390198, data time: 0.02235814929008484\n",
      "step: 676834, loss: 0.06386490911245346, data time: 0.020030524995591905\n",
      "step: 676835, loss: 0.06907011568546295, data time: 0.018224334716796874\n",
      "step: 676836, loss: 0.06396417319774628, data time: 0.016775759783658115\n",
      "step: 676837, loss: 0.05813618004322052, data time: 0.01555025577545166\n",
      "step: 676838, loss: 0.06396505236625671, data time: 0.014507257021390475\n",
      "step: 676839, loss: 0.061187680810689926, data time: 0.013618298939296178\n",
      "step: 676840, loss: 0.06303940713405609, data time: 0.012845309575398763\n",
      "step: 676841, loss: 0.06613516807556152, data time: 0.012168154120445251\n",
      "step: 676842, loss: 0.06317725032567978, data time: 0.011572066475363338\n",
      "step: 676843, loss: 0.05886983871459961, data time: 0.011045985751681857\n",
      "step: 676844, loss: 0.06113439053297043, data time: 0.01056855603268272\n",
      "step: 676845, loss: 0.06336838006973267, data time: 0.010143935680389404\n",
      "step: 676846, loss: 0.06323538720607758, data time: 0.009759687242053804\n",
      "step: 676847, loss: 0.06110939383506775, data time: 0.00942285494370894\n",
      "step: 676848, loss: 0.05658932775259018, data time: 0.009101038393766983\n",
      "step: 676849, loss: 0.05821404978632927, data time: 0.008805652459462484\n",
      "step: 676850, loss: 0.06731060892343521, data time: 0.008534927368164063\n",
      "step: 676851, loss: 0.05622602254152298, data time: 0.008283899380610539\n",
      "step: 676852, loss: 0.06202992424368858, data time: 0.008050353438765914\n",
      "step: 676853, loss: 0.0568290650844574, data time: 0.007835337093898229\n",
      "step: 676854, loss: 0.06332874298095703, data time: 0.007638100920052364\n",
      "step: 676855, loss: 0.05753125995397568, data time: 0.007456660270690918\n",
      "step: 676856, loss: 0.06306123733520508, data time: 0.007280134385631931\n",
      "step: 676857, loss: 0.06345387548208237, data time: 0.007120221853256226\n",
      "step: 676858, loss: 0.06025974079966545, data time: 0.006967183315392696\n",
      "step: 676859, loss: 0.058738913387060165, data time: 0.006821099449606503\n",
      "step: 676860, loss: 0.06367499381303787, data time: 0.0066849981035505025\n",
      "step: 676861, loss: 0.06388410925865173, data time: 0.006554451253679063\n",
      "step: 676862, loss: 0.06705953180789948, data time: 0.00643065813425425\n",
      "step: 676863, loss: 0.062258802354335785, data time: 0.0063166932055824685\n",
      "step: 676864, loss: 0.06678969413042068, data time: 0.00620722159361228\n",
      "step: 676865, loss: 0.06885416805744171, data time: 0.006102144718170166\n",
      "step: 676866, loss: 0.06723832339048386, data time: 0.15858936309814453\n",
      "step: 676867, loss: 0.06236138939857483, data time: 0.08089983463287354\n",
      "step: 676868, loss: 0.062317654490470886, data time: 0.05515829722086588\n",
      "step: 676869, loss: 0.0571008026599884, data time: 0.042123258113861084\n",
      "step: 676870, loss: 0.05704405903816223, data time: 0.034012794494628906\n",
      "step: 676871, loss: 0.05952849239110947, data time: 0.02861674626668294\n",
      "step: 676872, loss: 0.058916084468364716, data time: 0.024775232587541853\n",
      "step: 676873, loss: 0.06524922698736191, data time: 0.021987497806549072\n",
      "step: 676874, loss: 0.061920102685689926, data time: 0.019730541441175673\n",
      "step: 676875, loss: 0.06301860511302948, data time: 0.017984986305236816\n",
      "step: 676876, loss: 0.06380369514226913, data time: 0.016573970968073063\n",
      "step: 676877, loss: 0.06618796288967133, data time: 0.015396058559417725\n",
      "step: 676878, loss: 0.06587472558021545, data time: 0.014393347960252028\n",
      "step: 676879, loss: 0.059183113276958466, data time: 0.01353503976549421\n",
      "step: 676880, loss: 0.06234326213598251, data time: 0.012795480092366536\n",
      "step: 676881, loss: 0.06016796827316284, data time: 0.012144789099693298\n",
      "step: 676882, loss: 0.05711384862661362, data time: 0.011566470651065601\n",
      "step: 676883, loss: 0.059427499771118164, data time: 0.011052581999037001\n",
      "step: 676884, loss: 0.059981487691402435, data time: 0.010595208720157021\n",
      "step: 676885, loss: 0.06577055156230927, data time: 0.010187363624572754\n",
      "step: 676886, loss: 0.0636611357331276, data time: 0.0098181338537307\n",
      "step: 676887, loss: 0.06030512973666191, data time: 0.009482578797773882\n",
      "step: 676888, loss: 0.06612028181552887, data time: 0.009176026219907015\n",
      "step: 676889, loss: 0.06348523497581482, data time: 0.008895903825759888\n",
      "step: 676890, loss: 0.06366923451423645, data time: 0.008634958267211914\n",
      "step: 676891, loss: 0.06658148020505905, data time: 0.00839681808765118\n",
      "step: 676892, loss: 0.06111457943916321, data time: 0.008169280158148872\n",
      "step: 676893, loss: 0.06114647909998894, data time: 0.007961741515568324\n",
      "step: 676894, loss: 0.06627536565065384, data time: 0.007773588443624562\n",
      "step: 676895, loss: 0.05776387080550194, data time: 0.0076012770334879555\n",
      "step: 676896, loss: 0.05640615150332451, data time: 0.007434083569434381\n",
      "step: 676897, loss: 0.06065203994512558, data time: 0.007279537618160248\n",
      "step: 676898, loss: 0.05973726511001587, data time: 0.007120291392008464\n",
      "step: 676899, loss: 0.0614389032125473, data time: 0.006969739409054027\n",
      "step: 676900, loss: 0.058339573442935944, data time: 0.0068291255405970985\n",
      "step: 676901, loss: 0.051873594522476196, data time: 0.006692634688483344\n",
      "step: 676902, loss: 0.05569303035736084, data time: 0.006565441956391206\n",
      "step: 676903, loss: 0.059431761503219604, data time: 0.00644818105195698\n",
      "step: 676904, loss: 0.062442898750305176, data time: 0.006336603409204728\n",
      "step: 676905, loss: 0.053843557834625244, data time: 0.0062289297580719\n",
      "step: 676906, loss: 0.059415798634290695, data time: 0.1539168357849121\n",
      "step: 676907, loss: 0.058769144117832184, data time: 0.07808506488800049\n",
      "step: 676908, loss: 0.06320361793041229, data time: 0.0532235304514567\n",
      "step: 676909, loss: 0.06065251678228378, data time: 0.04057002067565918\n",
      "step: 676910, loss: 0.06486082077026367, data time: 0.03272695541381836\n",
      "step: 676911, loss: 0.060592226684093475, data time: 0.02751012643178304\n",
      "step: 676912, loss: 0.061332862824201584, data time: 0.023781912667410716\n",
      "step: 676913, loss: 0.059507280588150024, data time: 0.02106994390487671\n",
      "step: 676914, loss: 0.0611586794257164, data time: 0.01887324121263292\n",
      "step: 676915, loss: 0.06418769806623459, data time: 0.017190051078796387\n",
      "step: 676916, loss: 0.05866384133696556, data time: 0.015816796909679066\n",
      "step: 676917, loss: 0.062376491725444794, data time: 0.014677882194519043\n",
      "step: 676918, loss: 0.059380918741226196, data time: 0.013721044246967021\n",
      "step: 676919, loss: 0.0619787722826004, data time: 0.012886098452976771\n",
      "step: 676920, loss: 0.06421709060668945, data time: 0.012161540985107421\n",
      "step: 676921, loss: 0.06026449799537659, data time: 0.011526823043823242\n",
      "step: 676922, loss: 0.06058846041560173, data time: 0.010963075301226448\n",
      "step: 676923, loss: 0.07145025581121445, data time: 0.01046042972140842\n",
      "step: 676924, loss: 0.06336314976215363, data time: 0.01002099639491031\n",
      "step: 676925, loss: 0.06096860393881798, data time: 0.00962902307510376\n",
      "step: 676926, loss: 0.056020431220531464, data time: 0.009272166660853795\n",
      "step: 676927, loss: 0.06644435226917267, data time: 0.0089423114603216\n",
      "step: 676928, loss: 0.06607604026794434, data time: 0.008640216744464376\n",
      "step: 676929, loss: 0.06174466013908386, data time: 0.00836340586344401\n",
      "step: 676930, loss: 0.06118176877498627, data time: 0.00811121940612793\n",
      "step: 676931, loss: 0.06158937141299248, data time: 0.007880064157339243\n",
      "step: 676932, loss: 0.06225454807281494, data time: 0.007661545718157733\n",
      "step: 676933, loss: 0.05875900387763977, data time: 0.007462288652147565\n",
      "step: 676934, loss: 0.05810301750898361, data time: 0.0072775462578082905\n",
      "step: 676935, loss: 0.06486975401639938, data time: 0.007103141148885091\n",
      "step: 676936, loss: 0.06419175863265991, data time: 0.006939795709425403\n",
      "step: 676937, loss: 0.06514458358287811, data time: 0.006792739033699036\n",
      "step: 676938, loss: 0.07278091460466385, data time: 0.006646380280003403\n",
      "step: 676939, loss: 0.06147824227809906, data time: 0.0065092198988970585\n",
      "step: 676940, loss: 0.06116964668035507, data time: 0.006378235135759626\n",
      "step: 676941, loss: 0.0652410238981247, data time: 0.006252937846713596\n",
      "step: 676942, loss: 0.06047099083662033, data time: 0.006134194296759528\n",
      "step: 676943, loss: 0.060486145317554474, data time: 0.006024737107126336\n",
      "step: 676944, loss: 0.06105152517557144, data time: 0.005921217111440806\n",
      "step: 676945, loss: 0.08524435013532639, data time: 0.005824333429336548\n",
      "step: 676946, loss: 0.05682307109236717, data time: 0.16580605506896973\n",
      "step: 676947, loss: 0.06647498905658722, data time: 0.08404684066772461\n",
      "step: 676948, loss: 0.06332173198461533, data time: 0.056591431299845375\n",
      "step: 676949, loss: 0.06717544794082642, data time: 0.04347634315490723\n",
      "step: 676950, loss: 0.05485740303993225, data time: 0.035078239440917966\n",
      "step: 676951, loss: 0.05540942773222923, data time: 0.02948164939880371\n",
      "step: 676952, loss: 0.06073383614420891, data time: 0.025488785334995816\n",
      "step: 676953, loss: 0.06398665904998779, data time: 0.022614240646362305\n",
      "step: 676954, loss: 0.0631781592965126, data time: 0.020286719004313152\n",
      "step: 676955, loss: 0.06386443972587585, data time: 0.018498158454895018\n",
      "step: 676956, loss: 0.06153502315282822, data time: 0.017053430730646305\n",
      "step: 676957, loss: 0.05691688507795334, data time: 0.015859683354695637\n",
      "step: 676958, loss: 0.06975115090608597, data time: 0.014844234173114482\n",
      "step: 676959, loss: 0.06338785588741302, data time: 0.013956546783447266\n",
      "step: 676960, loss: 0.0620918795466423, data time: 0.01319265365600586\n",
      "step: 676961, loss: 0.06472982466220856, data time: 0.012509375810623169\n",
      "step: 676962, loss: 0.06839410960674286, data time: 0.011903033537023208\n",
      "step: 676963, loss: 0.05716552585363388, data time: 0.011353267563713921\n",
      "step: 676964, loss: 0.06268623471260071, data time: 0.010862463398983604\n",
      "step: 676965, loss: 0.06590928137302399, data time: 0.010432028770446777\n",
      "step: 676966, loss: 0.054575152695178986, data time: 0.010041770480927966\n",
      "step: 676967, loss: 0.05844854563474655, data time: 0.009687250310724432\n",
      "step: 676968, loss: 0.05728289484977722, data time: 0.009355348089466925\n",
      "step: 676969, loss: 0.06471335142850876, data time: 0.009056289990743002\n",
      "step: 676970, loss: 0.06585735082626343, data time: 0.008784704208374024\n",
      "step: 676971, loss: 0.059883009642362595, data time: 0.008529250438396748\n",
      "step: 676972, loss: 0.06317468732595444, data time: 0.008294979731241861\n",
      "step: 676973, loss: 0.05948235094547272, data time: 0.008071660995483398\n",
      "step: 676974, loss: 0.06324143707752228, data time: 0.00788244707831021\n",
      "step: 676975, loss: 0.06317172199487686, data time: 0.007694911956787109\n",
      "step: 676976, loss: 0.06587293744087219, data time: 0.007519083638345042\n",
      "step: 676977, loss: 0.06127649545669556, data time: 0.007360473275184631\n",
      "step: 676978, loss: 0.060071952641010284, data time: 0.007198955073501124\n",
      "step: 676979, loss: 0.06081266328692436, data time: 0.007046390982235179\n",
      "step: 676980, loss: 0.06325468420982361, data time: 0.00690370968409947\n",
      "step: 676981, loss: 0.060411497950553894, data time: 0.006766021251678467\n",
      "step: 676982, loss: 0.058334797620773315, data time: 0.006635143950178816\n",
      "step: 676983, loss: 0.056877266615629196, data time: 0.006515942121806897\n",
      "step: 676984, loss: 0.059383757412433624, data time: 0.0064042409261067705\n",
      "step: 676985, loss: 0.06908499449491501, data time: 0.006298744678497314\n",
      "step: 676986, loss: 0.05794484540820122, data time: 0.16273832321166992\n",
      "step: 676987, loss: 0.06021475791931152, data time: 0.08219265937805176\n",
      "step: 676988, loss: 0.06288083642721176, data time: 0.05537192026774088\n",
      "step: 676989, loss: 0.0610077828168869, data time: 0.04227805137634277\n",
      "step: 676990, loss: 0.06941848248243332, data time: 0.03413100242614746\n",
      "step: 676991, loss: 0.06195042282342911, data time: 0.0287015438079834\n",
      "step: 676992, loss: 0.06693623960018158, data time: 0.024816581181117466\n",
      "step: 676993, loss: 0.06905873119831085, data time: 0.021979719400405884\n",
      "step: 676994, loss: 0.053071003407239914, data time: 0.01969093746609158\n",
      "step: 676995, loss: 0.06461058557033539, data time: 0.017930126190185545\n",
      "step: 676996, loss: 0.06780044734477997, data time: 0.016505631533536045\n",
      "step: 676997, loss: 0.06737595796585083, data time: 0.015323519706726074\n",
      "step: 676998, loss: 0.06829580664634705, data time: 0.014316173700185923\n",
      "step: 676999, loss: 0.0599190816283226, data time: 0.013444781303405762\n",
      "step: 677000, loss: 0.06540632247924805, data time: 0.012692276636759441\n",
      "step: 677001, loss: 0.06660361588001251, data time: 0.012037545442581177\n",
      "step: 677002, loss: 0.058140795677900314, data time: 0.01146083719590131\n",
      "step: 677003, loss: 0.06107000261545181, data time: 0.010941545168558756\n",
      "step: 677004, loss: 0.06298605352640152, data time: 0.010475296723215203\n",
      "step: 677005, loss: 0.05969721078872681, data time: 0.010068702697753906\n",
      "step: 677006, loss: 0.06536159664392471, data time: 0.009699367341541108\n",
      "step: 677007, loss: 0.06255154311656952, data time: 0.009364962577819824\n",
      "step: 677008, loss: 0.06345333158969879, data time: 0.009045600891113281\n",
      "step: 677009, loss: 0.06476883590221405, data time: 0.008759826421737671\n",
      "step: 677010, loss: 0.05706965923309326, data time: 0.00849858283996582\n",
      "step: 677011, loss: 0.05875179171562195, data time: 0.008262029060950646\n",
      "step: 677012, loss: 0.05880093574523926, data time: 0.008032578009146231\n",
      "step: 677013, loss: 0.05900675058364868, data time: 0.007817540849958147\n",
      "step: 677014, loss: 0.06472160667181015, data time: 0.007625390743387157\n",
      "step: 677015, loss: 0.06695514172315598, data time: 0.0074520190556844074\n",
      "step: 677016, loss: 0.05942850559949875, data time: 0.007281726406466576\n",
      "step: 677017, loss: 0.06431090086698532, data time: 0.0071253702044487\n",
      "step: 677018, loss: 0.06155088171362877, data time: 0.0069715398730653706\n",
      "step: 677019, loss: 0.05904276296496391, data time: 0.0068277751698213466\n",
      "step: 677020, loss: 0.058008428663015366, data time: 0.00669025012425014\n",
      "step: 677021, loss: 0.06317423284053802, data time: 0.006557491090562608\n",
      "step: 677022, loss: 0.06550387293100357, data time: 0.006431134971412453\n",
      "step: 677023, loss: 0.05796125531196594, data time: 0.006319353454991391\n",
      "step: 677024, loss: 0.06109928339719772, data time: 0.0062135060628255205\n",
      "step: 677025, loss: 0.06575056165456772, data time: 0.006109088659286499\n",
      "step: 677026, loss: 0.05642939731478691, data time: 0.15878033638000488\n",
      "step: 677027, loss: 0.061740487813949585, data time: 0.0802081823348999\n",
      "step: 677028, loss: 0.05606712028384209, data time: 0.054850498835245766\n",
      "step: 677029, loss: 0.05810588598251343, data time: 0.04153484106063843\n",
      "step: 677030, loss: 0.06146307289600372, data time: 0.033542299270629884\n",
      "step: 677031, loss: 0.06330831348896027, data time: 0.028221686681111652\n",
      "step: 677032, loss: 0.0620126947760582, data time: 0.02450925963265555\n",
      "step: 677033, loss: 0.05896235629916191, data time: 0.02163475751876831\n",
      "step: 677034, loss: 0.06612326949834824, data time: 0.019385099411010742\n",
      "step: 677035, loss: 0.059412650763988495, data time: 0.01764662265777588\n",
      "step: 677036, loss: 0.0696452260017395, data time: 0.016251780770041725\n",
      "step: 677037, loss: 0.06414603441953659, data time: 0.015101273854573568\n",
      "step: 677038, loss: 0.06555277109146118, data time: 0.014113481228168193\n",
      "step: 677039, loss: 0.05705796927213669, data time: 0.013257912227085658\n",
      "step: 677040, loss: 0.05730709061026573, data time: 0.012515449523925781\n",
      "step: 677041, loss: 0.05780050903558731, data time: 0.011873841285705566\n",
      "step: 677042, loss: 0.061637308448553085, data time: 0.011298516217400046\n",
      "step: 677043, loss: 0.05719771981239319, data time: 0.010784347852071127\n",
      "step: 677044, loss: 0.05977807566523552, data time: 0.010327251333939401\n",
      "step: 677045, loss: 0.05826465040445328, data time: 0.009925293922424316\n",
      "step: 677046, loss: 0.06563891470432281, data time: 0.009566000529697962\n",
      "step: 677047, loss: 0.05675729736685753, data time: 0.00922846794128418\n",
      "step: 677048, loss: 0.0631239116191864, data time: 0.00892192384471064\n",
      "step: 677049, loss: 0.05782720446586609, data time: 0.008642464876174927\n",
      "step: 677050, loss: 0.06676310300827026, data time: 0.00838273048400879\n",
      "step: 677051, loss: 0.0695766732096672, data time: 0.0081407198539147\n",
      "step: 677052, loss: 0.05629981681704521, data time: 0.00791595600269459\n",
      "step: 677053, loss: 0.059850260615348816, data time: 0.007707391466413226\n",
      "step: 677054, loss: 0.057952847331762314, data time: 0.007521662218817349\n",
      "step: 677055, loss: 0.058643367141485214, data time: 0.007348450024922689\n",
      "step: 677056, loss: 0.06363746523857117, data time: 0.00718348257003292\n",
      "step: 677057, loss: 0.06178481504321098, data time: 0.007034070789813995\n",
      "step: 677058, loss: 0.06126652657985687, data time: 0.006881562146273526\n",
      "step: 677059, loss: 0.06439604610204697, data time: 0.0067388310151941634\n",
      "step: 677060, loss: 0.06579996645450592, data time: 0.006604446683611188\n",
      "step: 677061, loss: 0.05909168720245361, data time: 0.006475494967566596\n",
      "step: 677062, loss: 0.05790551006793976, data time: 0.006355749594198691\n",
      "step: 677063, loss: 0.06453385949134827, data time: 0.006246102483649003\n",
      "step: 677064, loss: 0.05731688067317009, data time: 0.006142035508767152\n",
      "step: 677065, loss: 0.05538272112607956, data time: 0.006040096282958984\n",
      "step: 677066, loss: 0.06259635835886002, data time: 0.158111572265625\n",
      "step: 677067, loss: 0.06241036579012871, data time: 0.07988715171813965\n",
      "step: 677068, loss: 0.06250882893800735, data time: 0.05422290166219076\n",
      "step: 677069, loss: 0.06492375582456589, data time: 0.041568875312805176\n",
      "step: 677070, loss: 0.06469617784023285, data time: 0.03353772163391113\n",
      "step: 677071, loss: 0.05797847360372543, data time: 0.02820265293121338\n",
      "step: 677072, loss: 0.0653521940112114, data time: 0.02439389910016741\n",
      "step: 677073, loss: 0.06925802677869797, data time: 0.02161964774131775\n",
      "step: 677074, loss: 0.059222083538770676, data time: 0.01936973465813531\n",
      "step: 677075, loss: 0.05903837829828262, data time: 0.01764507293701172\n",
      "step: 677076, loss: 0.061888184398412704, data time: 0.0162408785386519\n",
      "step: 677077, loss: 0.06696685403585434, data time: 0.015081306298573812\n",
      "step: 677078, loss: 0.05728266388177872, data time: 0.014092041895939754\n",
      "step: 677079, loss: 0.0627996027469635, data time: 0.013229353087288993\n",
      "step: 677080, loss: 0.05912923440337181, data time: 0.012498267491658529\n",
      "step: 677081, loss: 0.05792555212974548, data time: 0.011850029230117798\n",
      "step: 677082, loss: 0.06483109295368195, data time: 0.011278194539687213\n",
      "step: 677083, loss: 0.05984418839216232, data time: 0.010761088795132108\n",
      "step: 677084, loss: 0.0638275295495987, data time: 0.010304275311921773\n",
      "step: 677085, loss: 0.06424038112163544, data time: 0.009901618957519532\n",
      "step: 677086, loss: 0.0589829757809639, data time: 0.009544088726951963\n",
      "step: 677087, loss: 0.05447883903980255, data time: 0.009211789477955212\n",
      "step: 677088, loss: 0.05891575664281845, data time: 0.008897781372070312\n",
      "step: 677089, loss: 0.0639086663722992, data time: 0.008617719014485678\n",
      "step: 677090, loss: 0.06347550451755524, data time: 0.008362607955932617\n",
      "step: 677091, loss: 0.06758640706539154, data time: 0.00812219656430758\n",
      "step: 677092, loss: 0.06262124329805374, data time: 0.007898489634195963\n",
      "step: 677093, loss: 0.06488284468650818, data time: 0.007688709667750767\n",
      "step: 677094, loss: 0.062379151582717896, data time: 0.007499308421693999\n",
      "step: 677095, loss: 0.060065291821956635, data time: 0.007327628135681152\n",
      "step: 677096, loss: 0.06325971335172653, data time: 0.007163316972794071\n",
      "step: 677097, loss: 0.06726965308189392, data time: 0.007021598517894745\n",
      "step: 677098, loss: 0.06448503583669662, data time: 0.006873939976547704\n",
      "step: 677099, loss: 0.05305502936244011, data time: 0.006736537989448099\n",
      "step: 677100, loss: 0.07017972320318222, data time: 0.006605441229684012\n",
      "step: 677101, loss: 0.061809223145246506, data time: 0.006488396061791314\n",
      "step: 677102, loss: 0.06116599962115288, data time: 0.006369429665642815\n",
      "step: 677103, loss: 0.06401872634887695, data time: 0.006261486756174188\n",
      "step: 677104, loss: 0.06398788094520569, data time: 0.006160094187809871\n",
      "step: 677105, loss: 0.039932381361722946, data time: 0.006063169240951538\n",
      "step: 677106, loss: 0.06113669276237488, data time: 0.1536719799041748\n",
      "step: 677107, loss: 0.06216605752706528, data time: 0.07764744758605957\n",
      "step: 677108, loss: 0.06674929708242416, data time: 0.0525667667388916\n",
      "step: 677109, loss: 0.06480221450328827, data time: 0.04030793905258179\n",
      "step: 677110, loss: 0.059966932982206345, data time: 0.032530927658081056\n",
      "step: 677111, loss: 0.05827411264181137, data time: 0.027368982632954914\n",
      "step: 677112, loss: 0.06510496139526367, data time: 0.02369335719517299\n",
      "step: 677113, loss: 0.06425677239894867, data time: 0.021027803421020508\n",
      "step: 677114, loss: 0.06008816510438919, data time: 0.018847942352294922\n",
      "step: 677115, loss: 0.055431317538022995, data time: 0.01716797351837158\n",
      "step: 677116, loss: 0.0646401047706604, data time: 0.01580754193392667\n",
      "step: 677117, loss: 0.05697914958000183, data time: 0.01467132568359375\n",
      "step: 677118, loss: 0.06488291919231415, data time: 0.01371612915625939\n",
      "step: 677119, loss: 0.06754864752292633, data time: 0.012884974479675293\n",
      "step: 677120, loss: 0.0588139109313488, data time: 0.012174018224080404\n",
      "step: 677121, loss: 0.06387929618358612, data time: 0.011545956134796143\n",
      "step: 677122, loss: 0.05757644772529602, data time: 0.010998655768001782\n",
      "step: 677123, loss: 0.05864279717206955, data time: 0.010501080089145236\n",
      "step: 677124, loss: 0.06216500699520111, data time: 0.01005427460921438\n",
      "step: 677125, loss: 0.056800585240125656, data time: 0.00966273546218872\n",
      "step: 677126, loss: 0.06344570964574814, data time: 0.009306953066871279\n",
      "step: 677127, loss: 0.06200631707906723, data time: 0.008993701501326128\n",
      "step: 677128, loss: 0.06759994477033615, data time: 0.008696068888125212\n",
      "step: 677129, loss: 0.06679320335388184, data time: 0.008430381615956625\n",
      "step: 677130, loss: 0.06558674573898315, data time: 0.008177909851074219\n",
      "step: 677131, loss: 0.05592117831110954, data time: 0.00794813266167274\n",
      "step: 677132, loss: 0.061590224504470825, data time: 0.007729751092416269\n",
      "step: 677133, loss: 0.06878669559955597, data time: 0.007525716509137835\n",
      "step: 677134, loss: 0.061804234981536865, data time: 0.007342428996645171\n",
      "step: 677135, loss: 0.06504224240779877, data time: 0.007172354062398275\n",
      "step: 677136, loss: 0.06607386469841003, data time: 0.007014651452341388\n",
      "step: 677137, loss: 0.059427522122859955, data time: 0.006865516304969788\n",
      "step: 677138, loss: 0.059031110256910324, data time: 0.0067187150319417315\n",
      "step: 677139, loss: 0.0677414983510971, data time: 0.006581390605253332\n",
      "step: 677140, loss: 0.06264220923185349, data time: 0.006451320648193359\n",
      "step: 677141, loss: 0.06434512138366699, data time: 0.006325172053443061\n",
      "step: 677142, loss: 0.059672288596630096, data time: 0.006206531782408018\n",
      "step: 677143, loss: 0.055544883012771606, data time: 0.006100058555603027\n",
      "step: 677144, loss: 0.06634363532066345, data time: 0.005997107579157903\n",
      "step: 677145, loss: 0.08068469166755676, data time: 0.005901938676834107\n",
      "step: 677146, loss: 0.06326446682214737, data time: 0.1686258316040039\n",
      "step: 677147, loss: 0.06748344749212265, data time: 0.0851813554763794\n",
      "step: 677148, loss: 0.06338612735271454, data time: 0.05729794502258301\n",
      "step: 677149, loss: 0.06299115717411041, data time: 0.04381120204925537\n",
      "step: 677150, loss: 0.06361758708953857, data time: 0.03535017967224121\n",
      "step: 677151, loss: 0.05735492706298828, data time: 0.029722213745117188\n",
      "step: 677152, loss: 0.055903807282447815, data time: 0.025695834841047014\n",
      "step: 677153, loss: 0.06460574269294739, data time: 0.022747963666915894\n",
      "step: 677154, loss: 0.06648789346218109, data time: 0.020371728473239474\n",
      "step: 677155, loss: 0.06279991567134857, data time: 0.018535351753234862\n",
      "step: 677156, loss: 0.06859090179204941, data time: 0.017053192312067204\n",
      "step: 677157, loss: 0.06374382972717285, data time: 0.015811224778493244\n",
      "step: 677158, loss: 0.06310152262449265, data time: 0.014772671919602614\n",
      "step: 677159, loss: 0.058325767517089844, data time: 0.013862916401454381\n",
      "step: 677160, loss: 0.06027916818857193, data time: 0.013079420725504557\n",
      "step: 677161, loss: 0.06445200741291046, data time: 0.012398198246955872\n",
      "step: 677162, loss: 0.06053882837295532, data time: 0.011797975091373218\n",
      "step: 677163, loss: 0.061812885105609894, data time: 0.011253767543368869\n",
      "step: 677164, loss: 0.06662660837173462, data time: 0.010768250415199682\n",
      "step: 677165, loss: 0.060798775404691696, data time: 0.010342037677764893\n",
      "step: 677166, loss: 0.06293526291847229, data time: 0.009951546078636533\n",
      "step: 677167, loss: 0.05867248773574829, data time: 0.009604876691644842\n",
      "step: 677168, loss: 0.0634894073009491, data time: 0.00927659739618716\n",
      "step: 677169, loss: 0.060169681906700134, data time: 0.008980502684911093\n",
      "step: 677170, loss: 0.06258705258369446, data time: 0.008707189559936523\n",
      "step: 677171, loss: 0.06072467193007469, data time: 0.008454075226416955\n",
      "step: 677172, loss: 0.063006192445755, data time: 0.008214968222158926\n",
      "step: 677173, loss: 0.062003783881664276, data time: 0.007993323462350028\n",
      "step: 677174, loss: 0.06403456628322601, data time: 0.007796402635245487\n",
      "step: 677175, loss: 0.06214873120188713, data time: 0.0076075315475463865\n",
      "step: 677176, loss: 0.06053566560149193, data time: 0.007438728886265908\n",
      "step: 677177, loss: 0.05814623087644577, data time: 0.007279559969902039\n",
      "step: 677178, loss: 0.06968320161104202, data time: 0.007120392539284446\n",
      "step: 677179, loss: 0.06220871955156326, data time: 0.006970531800213982\n",
      "step: 677180, loss: 0.06164190173149109, data time: 0.006829472950526647\n",
      "step: 677181, loss: 0.0598963163793087, data time: 0.006693283716837565\n",
      "step: 677182, loss: 0.06023872643709183, data time: 0.0065657512561694996\n",
      "step: 677183, loss: 0.05892718955874443, data time: 0.006449849982010691\n",
      "step: 677184, loss: 0.06671604514122009, data time: 0.00633700077350323\n",
      "step: 677185, loss: 0.06558671593666077, data time: 0.006232708692550659\n",
      "step: 677186, loss: 0.05702030286192894, data time: 0.17639636993408203\n",
      "step: 677187, loss: 0.0624675378203392, data time: 0.0894162654876709\n",
      "step: 677188, loss: 0.05821048468351364, data time: 0.0609282652537028\n",
      "step: 677189, loss: 0.06068694218993187, data time: 0.0466233491897583\n",
      "step: 677190, loss: 0.0640374943614006, data time: 0.03765420913696289\n",
      "step: 677191, loss: 0.06660516560077667, data time: 0.031687021255493164\n",
      "step: 677192, loss: 0.05419917032122612, data time: 0.027401753834315708\n",
      "step: 677193, loss: 0.06091911718249321, data time: 0.02430477738380432\n",
      "step: 677194, loss: 0.06332191079854965, data time: 0.021793074078030057\n",
      "step: 677195, loss: 0.05998631566762924, data time: 0.01985626220703125\n",
      "step: 677196, loss: 0.06135507673025131, data time: 0.01829110492359508\n",
      "step: 677197, loss: 0.06056354567408562, data time: 0.016987125078837078\n",
      "step: 677198, loss: 0.06011110171675682, data time: 0.01587840226980356\n",
      "step: 677199, loss: 0.05739342421293259, data time: 0.014914751052856445\n",
      "step: 677200, loss: 0.056804023683071136, data time: 0.014087899525960287\n",
      "step: 677201, loss: 0.05943313241004944, data time: 0.01336398720741272\n",
      "step: 677202, loss: 0.05576999485492706, data time: 0.01272128610049977\n",
      "step: 677203, loss: 0.06689002364873886, data time: 0.012147029240926107\n",
      "step: 677204, loss: 0.06106596812605858, data time: 0.011633157730102539\n",
      "step: 677205, loss: 0.06352110207080841, data time: 0.01118706464767456\n",
      "step: 677206, loss: 0.059351831674575806, data time: 0.010780527478172666\n",
      "step: 677207, loss: 0.06608570367097855, data time: 0.010408033024181019\n",
      "step: 677208, loss: 0.06205236166715622, data time: 0.01006559703661048\n",
      "step: 677209, loss: 0.05885709449648857, data time: 0.009748508532842001\n",
      "step: 677210, loss: 0.06032460927963257, data time: 0.009460115432739257\n",
      "step: 677211, loss: 0.06010861322283745, data time: 0.009192402546222393\n",
      "step: 677212, loss: 0.06396652013063431, data time: 0.008941641560307256\n",
      "step: 677213, loss: 0.06369414180517197, data time: 0.008694299629756383\n",
      "step: 677214, loss: 0.06380827724933624, data time: 0.008474284204943427\n",
      "step: 677215, loss: 0.06834453344345093, data time: 0.008268229166666667\n",
      "step: 677216, loss: 0.05925865098834038, data time: 0.008073545271350492\n",
      "step: 677217, loss: 0.0633457750082016, data time: 0.007892325520515442\n",
      "step: 677218, loss: 0.06784739345312119, data time: 0.007714098150079901\n",
      "step: 677219, loss: 0.06588900089263916, data time: 0.007545926991631003\n",
      "step: 677220, loss: 0.06045442819595337, data time: 0.0073892457144601005\n",
      "step: 677221, loss: 0.0619434230029583, data time: 0.007238063547346327\n",
      "step: 677222, loss: 0.06607044488191605, data time: 0.007096155269725903\n",
      "step: 677223, loss: 0.05891662836074829, data time: 0.006965179192392449\n",
      "step: 677224, loss: 0.05596480891108513, data time: 0.006842857752090845\n",
      "step: 677225, loss: 0.07093629240989685, data time: 0.00672345757484436\n",
      "step: 677226, loss: 0.06092542037367821, data time: 0.1565113067626953\n",
      "step: 677227, loss: 0.053862862288951874, data time: 0.08036959171295166\n",
      "step: 677228, loss: 0.06566338986158371, data time: 0.0542149543762207\n",
      "step: 677229, loss: 0.05958586931228638, data time: 0.04175591468811035\n",
      "step: 677230, loss: 0.06147981807589531, data time: 0.03373827934265137\n",
      "step: 677231, loss: 0.064858078956604, data time: 0.028417348861694336\n",
      "step: 677232, loss: 0.060611844062805176, data time: 0.024629388536725725\n",
      "step: 677233, loss: 0.06583460420370102, data time: 0.021868109703063965\n",
      "step: 677234, loss: 0.07344682514667511, data time: 0.01962301466200087\n",
      "step: 677235, loss: 0.06441915780305862, data time: 0.01791083812713623\n",
      "step: 677236, loss: 0.056471869349479675, data time: 0.016513065858320755\n",
      "step: 677237, loss: 0.06270287930965424, data time: 0.015346765518188477\n",
      "step: 677238, loss: 0.06087400019168854, data time: 0.01437986814058744\n",
      "step: 677239, loss: 0.06634776294231415, data time: 0.013526388577052526\n",
      "step: 677240, loss: 0.06386612355709076, data time: 0.012792364756266276\n",
      "step: 677241, loss: 0.06673942506313324, data time: 0.012145191431045532\n",
      "step: 677242, loss: 0.06020665913820267, data time: 0.011574254316442153\n",
      "step: 677243, loss: 0.05993877351284027, data time: 0.011068317625257704\n",
      "step: 677244, loss: 0.061802469193935394, data time: 0.010611069829840409\n",
      "step: 677245, loss: 0.057499051094055176, data time: 0.010210061073303222\n",
      "step: 677246, loss: 0.068390391767025, data time: 0.009845710936046782\n",
      "step: 677247, loss: 0.06323781609535217, data time: 0.009518980979919434\n",
      "step: 677248, loss: 0.058058001101017, data time: 0.009210742038229237\n",
      "step: 677249, loss: 0.06033405661582947, data time: 0.008933275938034058\n",
      "step: 677250, loss: 0.06786328554153442, data time: 0.008676528930664062\n",
      "step: 677251, loss: 0.06823480129241943, data time: 0.00844263113462008\n",
      "step: 677252, loss: 0.0558021180331707, data time: 0.008218606313069662\n",
      "step: 677253, loss: 0.06643302738666534, data time: 0.008012805666242327\n",
      "step: 677254, loss: 0.05945099517703056, data time: 0.007829205743197737\n",
      "step: 677255, loss: 0.06200733035802841, data time: 0.00765533447265625\n",
      "step: 677256, loss: 0.06329607963562012, data time: 0.007496341582267515\n",
      "step: 677257, loss: 0.05629705637693405, data time: 0.007343068718910217\n",
      "step: 677258, loss: 0.05925590917468071, data time: 0.007185697555541992\n",
      "step: 677259, loss: 0.05917289853096008, data time: 0.007038165541256175\n",
      "step: 677260, loss: 0.056827642023563385, data time: 0.00689849853515625\n",
      "step: 677261, loss: 0.06534921377897263, data time: 0.006763557593027751\n",
      "step: 677262, loss: 0.07122361660003662, data time: 0.006635363037521775\n",
      "step: 677263, loss: 0.06106015667319298, data time: 0.006518571000350149\n",
      "step: 677264, loss: 0.05818025395274162, data time: 0.006413777669270833\n",
      "step: 677265, loss: 0.08193930238485336, data time: 0.006311392784118653\n",
      "step: 677266, loss: 0.058263182640075684, data time: 0.1578054428100586\n",
      "step: 677267, loss: 0.06077706441283226, data time: 0.08036410808563232\n",
      "step: 677268, loss: 0.06090933084487915, data time: 0.05454667409261068\n",
      "step: 677269, loss: 0.06757652759552002, data time: 0.04168885946273804\n",
      "step: 677270, loss: 0.06337960064411163, data time: 0.033640050888061525\n",
      "step: 677271, loss: 0.06038385629653931, data time: 0.028280059496561687\n",
      "step: 677272, loss: 0.0630197525024414, data time: 0.024457318442208425\n",
      "step: 677273, loss: 0.06382277607917786, data time: 0.021686077117919922\n",
      "step: 677274, loss: 0.06431757658720016, data time: 0.01943129963344998\n",
      "step: 677275, loss: 0.0600925050675869, data time: 0.017731428146362305\n",
      "step: 677276, loss: 0.06192046403884888, data time: 0.016360998153686523\n",
      "step: 677277, loss: 0.05797750502824783, data time: 0.015218893686930338\n",
      "step: 677278, loss: 0.06535007059574127, data time: 0.014216863192044772\n",
      "step: 677279, loss: 0.059858690947294235, data time: 0.013354573931012834\n",
      "step: 677280, loss: 0.062126826494932175, data time: 0.012614250183105469\n",
      "step: 677281, loss: 0.06239355728030205, data time: 0.0119667649269104\n",
      "step: 677282, loss: 0.061601217836141586, data time: 0.011386969510246725\n",
      "step: 677283, loss: 0.05856437608599663, data time: 0.010885251892937554\n",
      "step: 677284, loss: 0.06312614679336548, data time: 0.010436886235287315\n",
      "step: 677285, loss: 0.06526191532611847, data time: 0.010046267509460449\n",
      "step: 677286, loss: 0.057952918112277985, data time: 0.009697914123535156\n",
      "step: 677287, loss: 0.06199689209461212, data time: 0.009356596253134987\n",
      "step: 677288, loss: 0.06513157486915588, data time: 0.009037774542103643\n",
      "step: 677289, loss: 0.06067768856883049, data time: 0.008748521407445272\n",
      "step: 677290, loss: 0.05837222933769226, data time: 0.00848383903503418\n",
      "step: 677291, loss: 0.058852892369031906, data time: 0.0082395993746244\n",
      "step: 677292, loss: 0.05760177969932556, data time: 0.008010555196691442\n",
      "step: 677293, loss: 0.06684520095586777, data time: 0.00779773507799421\n",
      "step: 677294, loss: 0.05841373652219772, data time: 0.007607205160732927\n",
      "step: 677295, loss: 0.06548916548490524, data time: 0.00742793083190918\n",
      "step: 677296, loss: 0.060360223054885864, data time: 0.007255969509001701\n",
      "step: 677297, loss: 0.06210695207118988, data time: 0.0070995911955833435\n",
      "step: 677298, loss: 0.06387224793434143, data time: 0.006945501674305309\n",
      "step: 677299, loss: 0.06051192432641983, data time: 0.006804936072405647\n",
      "step: 677300, loss: 0.060721855610609055, data time: 0.006670045852661133\n",
      "step: 677301, loss: 0.0671912282705307, data time: 0.00653667582405938\n",
      "step: 677302, loss: 0.0596165657043457, data time: 0.0064110240420779665\n",
      "step: 677303, loss: 0.059457212686538696, data time: 0.006299006311517013\n",
      "step: 677304, loss: 0.061859406530857086, data time: 0.006191895558283879\n",
      "step: 677305, loss: 0.08807271718978882, data time: 0.006088745594024658\n",
      "step: 677306, loss: 0.06226146221160889, data time: 0.1690218448638916\n",
      "step: 677307, loss: 0.0696079283952713, data time: 0.0859379768371582\n",
      "step: 677308, loss: 0.06342296302318573, data time: 0.05835493405659994\n",
      "step: 677309, loss: 0.06132810562849045, data time: 0.04444986581802368\n",
      "step: 677310, loss: 0.06099963188171387, data time: 0.035851240158081055\n",
      "step: 677311, loss: 0.06071300059556961, data time: 0.030137379964192707\n",
      "step: 677312, loss: 0.05610588192939758, data time: 0.026044300624302456\n",
      "step: 677313, loss: 0.05903753638267517, data time: 0.023053377866744995\n",
      "step: 677314, loss: 0.05547887086868286, data time: 0.02064500914679633\n",
      "step: 677315, loss: 0.06761515140533447, data time: 0.018784070014953615\n",
      "step: 677316, loss: 0.05966784805059433, data time: 0.017298416657881302\n",
      "step: 677317, loss: 0.06332407891750336, data time: 0.01605101426442464\n",
      "step: 677318, loss: 0.06323052942752838, data time: 0.014986606744619517\n",
      "step: 677319, loss: 0.05922697111964226, data time: 0.014061314719063895\n",
      "step: 677320, loss: 0.06064104661345482, data time: 0.01327374776204427\n",
      "step: 677321, loss: 0.05894330143928528, data time: 0.012579619884490967\n",
      "step: 677322, loss: 0.06284230202436447, data time: 0.011966466903686523\n",
      "step: 677323, loss: 0.056675635278224945, data time: 0.011416647169325087\n",
      "step: 677324, loss: 0.05774201452732086, data time: 0.010920135598433646\n",
      "step: 677325, loss: 0.06386399269104004, data time: 0.01049802303314209\n",
      "step: 677326, loss: 0.05675804615020752, data time: 0.010104009083339147\n",
      "step: 677327, loss: 0.06206812709569931, data time: 0.009747971187938343\n",
      "step: 677328, loss: 0.06378279626369476, data time: 0.00941672532454781\n",
      "step: 677329, loss: 0.06089784950017929, data time: 0.009112407763799032\n",
      "step: 677330, loss: 0.06859508156776428, data time: 0.008836793899536132\n",
      "step: 677331, loss: 0.05974484607577324, data time: 0.00858614077934852\n",
      "step: 677332, loss: 0.06664040684700012, data time: 0.008345153596666124\n",
      "step: 677333, loss: 0.06684274971485138, data time: 0.008119557585035051\n",
      "step: 677334, loss: 0.06036287546157837, data time: 0.007919903459220097\n",
      "step: 677335, loss: 0.06246306747198105, data time: 0.007730595270792643\n",
      "step: 677336, loss: 0.06981080770492554, data time: 0.007550347235894973\n",
      "step: 677337, loss: 0.06381493806838989, data time: 0.007385715842247009\n",
      "step: 677338, loss: 0.06266123056411743, data time: 0.00722242124152906\n",
      "step: 677339, loss: 0.06562928855419159, data time: 0.00706939136280733\n",
      "step: 677340, loss: 0.07039456814527512, data time: 0.0069237572806222095\n",
      "step: 677341, loss: 0.07186802476644516, data time: 0.006783995363447402\n",
      "step: 677342, loss: 0.06533300876617432, data time: 0.006651923463151262\n",
      "step: 677343, loss: 0.06381972134113312, data time: 0.006533710580123098\n",
      "step: 677344, loss: 0.06344112753868103, data time: 0.006420538975642278\n",
      "step: 677345, loss: 0.07257741689682007, data time: 0.006310540437698364\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40882420539855957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 677346, loss: 0.0606670044362545, data time: 0.1492443084716797\n",
      "step: 677347, loss: 0.05876324698328972, data time: 0.07646369934082031\n",
      "step: 677348, loss: 0.061556361615657806, data time: 0.05151589711507162\n",
      "step: 677349, loss: 0.058936964720487595, data time: 0.03942227363586426\n",
      "step: 677350, loss: 0.06200053542852402, data time: 0.03180389404296875\n",
      "step: 677351, loss: 0.06340433657169342, data time: 0.026750922203063965\n",
      "step: 677352, loss: 0.06053696200251579, data time: 0.023129429136003767\n",
      "step: 677353, loss: 0.061497896909713745, data time: 0.020542889833450317\n",
      "step: 677354, loss: 0.06563620269298553, data time: 0.018405755360921223\n",
      "step: 677355, loss: 0.057111870497465134, data time: 0.016770148277282716\n",
      "step: 677356, loss: 0.06272599846124649, data time: 0.015441764484752308\n",
      "step: 677357, loss: 0.06235728785395622, data time: 0.014337380727132162\n",
      "step: 677358, loss: 0.06077728793025017, data time: 0.01341555668757512\n",
      "step: 677359, loss: 0.05961717665195465, data time: 0.012605445725577218\n",
      "step: 677360, loss: 0.06207433342933655, data time: 0.011901760101318359\n",
      "step: 677361, loss: 0.06299267709255219, data time: 0.0112830251455307\n",
      "step: 677362, loss: 0.06422442197799683, data time: 0.010736675823436063\n",
      "step: 677363, loss: 0.06233395263552666, data time: 0.010252449247572158\n",
      "step: 677364, loss: 0.0630946084856987, data time: 0.009822933297408255\n",
      "step: 677365, loss: 0.06060247868299484, data time: 0.009437572956085206\n",
      "step: 677366, loss: 0.06350795179605484, data time: 0.009095021656581334\n",
      "step: 677367, loss: 0.06220775097608566, data time: 0.008776924826882103\n",
      "step: 677368, loss: 0.059774599969387054, data time: 0.008480621420818827\n",
      "step: 677369, loss: 0.06694015860557556, data time: 0.0082149604956309\n",
      "step: 677370, loss: 0.06359212845563889, data time: 0.00796703338623047\n",
      "step: 677371, loss: 0.06395269185304642, data time: 0.007741121145395132\n",
      "step: 677372, loss: 0.05555356666445732, data time: 0.00752613279554579\n",
      "step: 677373, loss: 0.06237266585230827, data time: 0.007328510284423828\n",
      "step: 677374, loss: 0.06241112947463989, data time: 0.00714917018495757\n",
      "step: 677375, loss: 0.06393343210220337, data time: 0.006982032457987467\n",
      "step: 677376, loss: 0.061586543917655945, data time: 0.006828584978657384\n",
      "step: 677377, loss: 0.06010115146636963, data time: 0.00668509304523468\n",
      "step: 677378, loss: 0.06064557284116745, data time: 0.006542364756266276\n",
      "step: 677379, loss: 0.0643303245306015, data time: 0.006405956604901482\n",
      "step: 677380, loss: 0.06553879380226135, data time: 0.006276341847011022\n",
      "step: 677381, loss: 0.06140407919883728, data time: 0.006153676244947646\n",
      "step: 677382, loss: 0.06656011939048767, data time: 0.006040624670080237\n",
      "step: 677383, loss: 0.06570294499397278, data time: 0.005936327733491596\n",
      "step: 677384, loss: 0.05884639918804169, data time: 0.005837850081614959\n",
      "step: 677385, loss: 0.048195045441389084, data time: 0.005746579170227051\n",
      "step: 677386, loss: 0.06316377222537994, data time: 0.16009926795959473\n",
      "step: 677387, loss: 0.0585419237613678, data time: 0.08139586448669434\n",
      "step: 677388, loss: 0.06533917039632797, data time: 0.054760138193766274\n",
      "step: 677389, loss: 0.062163516879081726, data time: 0.04185140132904053\n",
      "step: 677390, loss: 0.06312256306409836, data time: 0.0337623119354248\n",
      "step: 677391, loss: 0.0571432039141655, data time: 0.028383930524190266\n",
      "step: 677392, loss: 0.06071307510137558, data time: 0.024537972041538784\n",
      "step: 677393, loss: 0.06505878269672394, data time: 0.02174985408782959\n",
      "step: 677394, loss: 0.06329318135976791, data time: 0.019485049777560763\n",
      "step: 677395, loss: 0.060004860162734985, data time: 0.01773250102996826\n",
      "step: 677396, loss: 0.055743277072906494, data time: 0.016316890716552734\n",
      "step: 677397, loss: 0.054765745997428894, data time: 0.015138725439707438\n",
      "step: 677398, loss: 0.06592901051044464, data time: 0.01414319185110239\n",
      "step: 677399, loss: 0.06637585163116455, data time: 0.013283508164542062\n",
      "step: 677400, loss: 0.06299836933612823, data time: 0.012531328201293945\n",
      "step: 677401, loss: 0.05817512422800064, data time: 0.011873632669448853\n",
      "step: 677402, loss: 0.06437044590711594, data time: 0.01129914732540355\n",
      "step: 677403, loss: 0.06382893025875092, data time: 0.010783844523959689\n",
      "step: 677404, loss: 0.06093708798289299, data time: 0.010320826580649927\n",
      "step: 677405, loss: 0.05875377357006073, data time: 0.00991729497909546\n",
      "step: 677406, loss: 0.061715640127658844, data time: 0.009545837129865373\n",
      "step: 677407, loss: 0.05728042125701904, data time: 0.009217728268016468\n",
      "step: 677408, loss: 0.05874370038509369, data time: 0.008904384530108908\n",
      "step: 677409, loss: 0.05914640426635742, data time: 0.008621384700139364\n",
      "step: 677410, loss: 0.058952685445547104, data time: 0.008361654281616211\n",
      "step: 677411, loss: 0.06075635179877281, data time: 0.008123388657203088\n",
      "step: 677412, loss: 0.06387807428836823, data time: 0.007896485152067962\n",
      "step: 677413, loss: 0.05917038768529892, data time: 0.007686895983559745\n",
      "step: 677414, loss: 0.062098972499370575, data time: 0.0074961021028715985\n",
      "step: 677415, loss: 0.06389120221138, data time: 0.007317384084065755\n",
      "step: 677416, loss: 0.05940105766057968, data time: 0.007150626951648343\n",
      "step: 677417, loss: 0.0576593391597271, data time: 0.007000051438808441\n",
      "step: 677418, loss: 0.06187785789370537, data time: 0.006845676537716027\n",
      "step: 677419, loss: 0.0680893287062645, data time: 0.00670019318075741\n",
      "step: 677420, loss: 0.06163666397333145, data time: 0.006565598079136439\n",
      "step: 677421, loss: 0.06361746788024902, data time: 0.006436023447248671\n",
      "step: 677422, loss: 0.06005002185702324, data time: 0.006314528954995645\n",
      "step: 677423, loss: 0.061715465039014816, data time: 0.006206041888186806\n",
      "step: 677424, loss: 0.0671396255493164, data time: 0.006100306144127479\n",
      "step: 677425, loss: 0.04195726662874222, data time: 0.005997800827026367\n",
      "step: 677426, loss: 0.06335373222827911, data time: 0.13519978523254395\n",
      "step: 677427, loss: 0.058387260884046555, data time: 0.06954693794250488\n",
      "step: 677428, loss: 0.057529013603925705, data time: 0.04687253634134928\n",
      "step: 677429, loss: 0.05914958566427231, data time: 0.03581416606903076\n",
      "step: 677430, loss: 0.06082681566476822, data time: 0.028916454315185545\n",
      "step: 677431, loss: 0.06547845900058746, data time: 0.0243760347366333\n",
      "step: 677432, loss: 0.063532255589962, data time: 0.021090950284685408\n",
      "step: 677433, loss: 0.05553921312093735, data time: 0.018715232610702515\n",
      "step: 677434, loss: 0.05903639644384384, data time: 0.016781224144829646\n",
      "step: 677435, loss: 0.06403419375419617, data time: 0.015315580368041991\n",
      "step: 677436, loss: 0.05992479622364044, data time: 0.01412565057927912\n",
      "step: 677437, loss: 0.06254324316978455, data time: 0.013127028942108154\n",
      "step: 677438, loss: 0.055987220257520676, data time: 0.01228484740624061\n",
      "step: 677439, loss: 0.06518100202083588, data time: 0.011578798294067383\n",
      "step: 677440, loss: 0.06510210037231445, data time: 0.010955905914306641\n",
      "step: 677441, loss: 0.06229446083307266, data time: 0.01040254533290863\n",
      "step: 677442, loss: 0.05830882117152214, data time: 0.009906768798828125\n",
      "step: 677443, loss: 0.06306102871894836, data time: 0.009464873207939995\n",
      "step: 677444, loss: 0.0673924908041954, data time: 0.00907017055310701\n",
      "step: 677445, loss: 0.05675786733627319, data time: 0.008724451065063477\n",
      "step: 677446, loss: 0.0583253912627697, data time: 0.00841129393804641\n",
      "step: 677447, loss: 0.06467418372631073, data time: 0.008125879547812721\n",
      "step: 677448, loss: 0.06179381534457207, data time: 0.007860235545946203\n",
      "step: 677449, loss: 0.06652672588825226, data time: 0.00762378176053365\n",
      "step: 677450, loss: 0.06430025398731232, data time: 0.007398128509521484\n",
      "step: 677451, loss: 0.06595820933580399, data time: 0.007189769011277419\n",
      "step: 677452, loss: 0.06756171584129333, data time: 0.006993805920636213\n",
      "step: 677453, loss: 0.06176448613405228, data time: 0.006816250937325614\n",
      "step: 677454, loss: 0.06633146852254868, data time: 0.006656441195257779\n",
      "step: 677455, loss: 0.06288962811231613, data time: 0.006506125132242839\n",
      "step: 677456, loss: 0.06893078982830048, data time: 0.00636416865933326\n",
      "step: 677457, loss: 0.06067030876874924, data time: 0.006234712898731232\n",
      "step: 677458, loss: 0.06113913282752037, data time: 0.006108731934518525\n",
      "step: 677459, loss: 0.053538545966148376, data time: 0.005987763404846191\n",
      "step: 677460, loss: 0.06253227591514587, data time: 0.005871200561523437\n",
      "step: 677461, loss: 0.05776919424533844, data time: 0.005759391519758437\n",
      "step: 677462, loss: 0.06020001322031021, data time: 0.005655366021233636\n",
      "step: 677463, loss: 0.06724777072668076, data time: 0.005560473391884251\n",
      "step: 677464, loss: 0.06176602095365524, data time: 0.00547026976560935\n",
      "step: 677465, loss: 0.07852131128311157, data time: 0.005384129285812378\n",
      "step: 677466, loss: 0.06139488145709038, data time: 0.15691041946411133\n",
      "step: 677467, loss: 0.059288591146469116, data time: 0.07922625541687012\n",
      "step: 677468, loss: 0.06420966982841492, data time: 0.05335426330566406\n",
      "step: 677469, loss: 0.06256567686796188, data time: 0.04076707363128662\n",
      "step: 677470, loss: 0.06138896569609642, data time: 0.03290243148803711\n",
      "step: 677471, loss: 0.06133471056818962, data time: 0.027653972307840984\n",
      "step: 677472, loss: 0.05393468588590622, data time: 0.023907388959612166\n",
      "step: 677473, loss: 0.06953906267881393, data time: 0.02120131254196167\n",
      "step: 677474, loss: 0.05536035820841789, data time: 0.0190022521548801\n",
      "step: 677475, loss: 0.06172845885157585, data time: 0.017310142517089844\n",
      "step: 677476, loss: 0.06279253214597702, data time: 0.01593195308338512\n",
      "step: 677477, loss: 0.06114993989467621, data time: 0.014792104562123617\n",
      "step: 677478, loss: 0.06683732569217682, data time: 0.013820519814124474\n",
      "step: 677479, loss: 0.05879753828048706, data time: 0.012978332383292062\n",
      "step: 677480, loss: 0.0602710098028183, data time: 0.012274599075317383\n",
      "step: 677481, loss: 0.0636696070432663, data time: 0.011634066700935364\n",
      "step: 677482, loss: 0.06733962893486023, data time: 0.011076127781587489\n",
      "step: 677483, loss: 0.06201943755149841, data time: 0.010575837559170194\n",
      "step: 677484, loss: 0.061756283044815063, data time: 0.0101293262682463\n",
      "step: 677485, loss: 0.05971990153193474, data time: 0.009739518165588379\n",
      "step: 677486, loss: 0.060028284788131714, data time: 0.00937871705918085\n",
      "step: 677487, loss: 0.06154611334204674, data time: 0.009051073681224476\n",
      "step: 677488, loss: 0.06179839372634888, data time: 0.008747401444808296\n",
      "step: 677489, loss: 0.057733118534088135, data time: 0.008469184239705404\n",
      "step: 677490, loss: 0.059720754623413086, data time: 0.00821218490600586\n",
      "step: 677491, loss: 0.06267007440328598, data time: 0.007979512214660645\n",
      "step: 677492, loss: 0.06264255195856094, data time: 0.007757045604564526\n",
      "step: 677493, loss: 0.06397846341133118, data time: 0.007554352283477783\n",
      "step: 677494, loss: 0.060548439621925354, data time: 0.007367989112590921\n",
      "step: 677495, loss: 0.06531988084316254, data time: 0.007193994522094726\n",
      "step: 677496, loss: 0.06638291478157043, data time: 0.007031317680112777\n",
      "step: 677497, loss: 0.05901063606142998, data time: 0.006883434951305389\n",
      "step: 677498, loss: 0.0628218799829483, data time: 0.0067330634955203895\n",
      "step: 677499, loss: 0.06231212615966797, data time: 0.0065918599858003505\n",
      "step: 677500, loss: 0.06798718124628067, data time: 0.006461831501552037\n",
      "step: 677501, loss: 0.07255339622497559, data time: 0.006336238649156358\n",
      "step: 677502, loss: 0.05912794917821884, data time: 0.006218291617728569\n",
      "step: 677503, loss: 0.06543947756290436, data time: 0.006107813433596962\n",
      "step: 677504, loss: 0.05841965600848198, data time: 0.006003392048371144\n",
      "step: 677505, loss: 0.07546515017747879, data time: 0.005903404951095581\n",
      "step: 677506, loss: 0.061593204736709595, data time: 0.15470004081726074\n",
      "step: 677507, loss: 0.06278833746910095, data time: 0.07893967628479004\n",
      "step: 677508, loss: 0.06119459867477417, data time: 0.05315995216369629\n",
      "step: 677509, loss: 0.0634365901350975, data time: 0.040741562843322754\n",
      "step: 677510, loss: 0.058468520641326904, data time: 0.03287773132324219\n",
      "step: 677511, loss: 0.06482275575399399, data time: 0.027631282806396484\n",
      "step: 677512, loss: 0.064969003200531, data time: 0.023881946291242327\n",
      "step: 677513, loss: 0.059029243886470795, data time: 0.021197974681854248\n",
      "step: 677514, loss: 0.06102467328310013, data time: 0.018991549809773762\n",
      "step: 677515, loss: 0.06326383352279663, data time: 0.01729114055633545\n",
      "step: 677516, loss: 0.060332607477903366, data time: 0.015920790758999912\n",
      "step: 677517, loss: 0.0656077116727829, data time: 0.014772733052571615\n",
      "step: 677518, loss: 0.05795110762119293, data time: 0.013805132645827074\n",
      "step: 677519, loss: 0.06759601086378098, data time: 0.012966394424438477\n",
      "step: 677520, loss: 0.06082921475172043, data time: 0.012247435251871745\n",
      "step: 677521, loss: 0.05740216374397278, data time: 0.011608809232711792\n",
      "step: 677522, loss: 0.05759766697883606, data time: 0.011057573206284466\n",
      "step: 677523, loss: 0.06383294612169266, data time: 0.01055235332912869\n",
      "step: 677524, loss: 0.060222383588552475, data time: 0.010105321281834653\n",
      "step: 677525, loss: 0.06728266924619675, data time: 0.009707677364349365\n",
      "step: 677526, loss: 0.062488656491041183, data time: 0.009352627254667737\n",
      "step: 677527, loss: 0.06091763824224472, data time: 0.009025649590925737\n",
      "step: 677528, loss: 0.06510033458471298, data time: 0.008724575457365616\n",
      "step: 677529, loss: 0.05935385823249817, data time: 0.00844535231590271\n",
      "step: 677530, loss: 0.06084208935499191, data time: 0.008186521530151368\n",
      "step: 677531, loss: 0.06870294362306595, data time: 0.007955707036531888\n",
      "step: 677532, loss: 0.05962806195020676, data time: 0.007734925658614548\n",
      "step: 677533, loss: 0.062381427735090256, data time: 0.007534520966666085\n",
      "step: 677534, loss: 0.05769551917910576, data time: 0.0073565367994637325\n",
      "step: 677535, loss: 0.053253769874572754, data time: 0.00718232790629069\n",
      "step: 677536, loss: 0.06512818485498428, data time: 0.007021442536384829\n",
      "step: 677537, loss: 0.05674077570438385, data time: 0.006873637437820435\n",
      "step: 677538, loss: 0.05804421007633209, data time: 0.0067233894810532074\n",
      "step: 677539, loss: 0.06512321531772614, data time: 0.006582547636593089\n",
      "step: 677540, loss: 0.06217978522181511, data time: 0.006452172143118722\n",
      "step: 677541, loss: 0.06051415205001831, data time: 0.0063248541620042585\n",
      "step: 677542, loss: 0.060741763561964035, data time: 0.006205558776855469\n",
      "step: 677543, loss: 0.06349653005599976, data time: 0.006095014120403089\n",
      "step: 677544, loss: 0.06465213000774384, data time: 0.005990994282257862\n",
      "step: 677545, loss: 0.043272148817777634, data time: 0.005892318487167358\n",
      "step: 677546, loss: 0.05760050565004349, data time: 0.1581263542175293\n",
      "step: 677547, loss: 0.0627562627196312, data time: 0.0801854133605957\n",
      "step: 677548, loss: 0.05655793473124504, data time: 0.05434807141621908\n",
      "step: 677549, loss: 0.06586165726184845, data time: 0.04164940118789673\n",
      "step: 677550, loss: 0.06146467849612236, data time: 0.03360328674316406\n",
      "step: 677551, loss: 0.060496509075164795, data time: 0.028267184893290203\n",
      "step: 677552, loss: 0.06399090588092804, data time: 0.02444924627031599\n",
      "step: 677553, loss: 0.06267189234495163, data time: 0.02164563536643982\n",
      "step: 677554, loss: 0.06354397535324097, data time: 0.019387165705362957\n",
      "step: 677555, loss: 0.06454625725746155, data time: 0.01765310764312744\n",
      "step: 677556, loss: 0.06519252061843872, data time: 0.016243934631347656\n",
      "step: 677557, loss: 0.06087369844317436, data time: 0.01506646474202474\n",
      "step: 677558, loss: 0.06407143175601959, data time: 0.014071061060978817\n",
      "step: 677559, loss: 0.06349968910217285, data time: 0.013207895415169852\n",
      "step: 677560, loss: 0.057016532868146896, data time: 0.01249367396036784\n",
      "step: 677561, loss: 0.06672371923923492, data time: 0.011855214834213257\n",
      "step: 677562, loss: 0.06304411590099335, data time: 0.011286917854757869\n",
      "step: 677563, loss: 0.06113244220614433, data time: 0.01076960563659668\n",
      "step: 677564, loss: 0.05900826305150986, data time: 0.010306985754715768\n",
      "step: 677565, loss: 0.06590434163808823, data time: 0.009902143478393554\n",
      "step: 677566, loss: 0.061654578894376755, data time: 0.009537560599190848\n",
      "step: 677567, loss: 0.06319408118724823, data time: 0.00920470194383101\n",
      "step: 677568, loss: 0.05642111226916313, data time: 0.008892619091531506\n",
      "step: 677569, loss: 0.061885952949523926, data time: 0.00861060619354248\n",
      "step: 677570, loss: 0.06140037998557091, data time: 0.00834878921508789\n",
      "step: 677571, loss: 0.06904716789722443, data time: 0.008111045910761906\n",
      "step: 677572, loss: 0.05497007817029953, data time: 0.007883354469581886\n",
      "step: 677573, loss: 0.059294771403074265, data time: 0.0076748984200613836\n",
      "step: 677574, loss: 0.06095881760120392, data time: 0.007484066075292127\n",
      "step: 677575, loss: 0.0590863935649395, data time: 0.007305089632670085\n",
      "step: 677576, loss: 0.06669524312019348, data time: 0.007138129203550277\n",
      "step: 677577, loss: 0.05953831598162651, data time: 0.0069846585392951965\n",
      "step: 677578, loss: 0.06823603808879852, data time: 0.006835735205448035\n",
      "step: 677579, loss: 0.06671787798404694, data time: 0.0066942186916575715\n",
      "step: 677580, loss: 0.05876777321100235, data time: 0.006557444163731167\n",
      "step: 677581, loss: 0.06414229422807693, data time: 0.006427632437811958\n",
      "step: 677582, loss: 0.061985261738300323, data time: 0.006307086429080447\n",
      "step: 677583, loss: 0.054947346448898315, data time: 0.006197822721380936\n",
      "step: 677584, loss: 0.05796484276652336, data time: 0.006090475962712214\n",
      "step: 677585, loss: 0.05360671877861023, data time: 0.005988967418670654\n",
      "step: 677586, loss: 0.0622665099799633, data time: 0.17242932319641113\n",
      "step: 677587, loss: 0.06136756390333176, data time: 0.08699750900268555\n",
      "step: 677588, loss: 0.06531932950019836, data time: 0.05902544657389323\n",
      "step: 677589, loss: 0.0641261413693428, data time: 0.045047223567962646\n",
      "step: 677590, loss: 0.06042948365211487, data time: 0.036321830749511716\n",
      "step: 677591, loss: 0.057791467756032944, data time: 0.03051761786142985\n",
      "step: 677592, loss: 0.06060014292597771, data time: 0.026368686131068637\n",
      "step: 677593, loss: 0.06654272228479385, data time: 0.023369550704956055\n",
      "step: 677594, loss: 0.059241876006126404, data time: 0.0209248595767551\n",
      "step: 677595, loss: 0.06347911059856415, data time: 0.01903820037841797\n",
      "step: 677596, loss: 0.05903417617082596, data time: 0.017497886310924183\n",
      "step: 677597, loss: 0.059652410447597504, data time: 0.01622086763381958\n",
      "step: 677598, loss: 0.05910496041178703, data time: 0.01513840601994441\n",
      "step: 677599, loss: 0.06084676831960678, data time: 0.014205149241856166\n",
      "step: 677600, loss: 0.058405354619026184, data time: 0.013418197631835938\n",
      "step: 677601, loss: 0.06415091454982758, data time: 0.012718543410301208\n",
      "step: 677602, loss: 0.06124205142259598, data time: 0.012087373172535616\n",
      "step: 677603, loss: 0.0606878399848938, data time: 0.011529816521538628\n",
      "step: 677604, loss: 0.05548156425356865, data time: 0.011032706812808388\n",
      "step: 677605, loss: 0.06297919154167175, data time: 0.010589969158172608\n",
      "step: 677606, loss: 0.06373061239719391, data time: 0.010191258930024646\n",
      "step: 677607, loss: 0.05832874774932861, data time: 0.009841116991910067\n",
      "step: 677608, loss: 0.05873870104551315, data time: 0.009503602981567383\n",
      "step: 677609, loss: 0.060938458889722824, data time: 0.009196589390436808\n",
      "step: 677610, loss: 0.06564895808696747, data time: 0.008911046981811523\n",
      "step: 677611, loss: 0.058642737567424774, data time: 0.008650394586416392\n",
      "step: 677612, loss: 0.060134511440992355, data time: 0.008402727268360279\n",
      "step: 677613, loss: 0.06362010538578033, data time: 0.008174300193786621\n",
      "step: 677614, loss: 0.06004083901643753, data time: 0.007967998241556102\n",
      "step: 677615, loss: 0.061664603650569916, data time: 0.007772994041442871\n",
      "step: 677616, loss: 0.05850748345255852, data time: 0.007591147576608966\n",
      "step: 677617, loss: 0.05681955814361572, data time: 0.007425278425216675\n",
      "step: 677618, loss: 0.06076522171497345, data time: 0.007261001702510949\n",
      "step: 677619, loss: 0.059826724231243134, data time: 0.007106234045589671\n",
      "step: 677620, loss: 0.05965154618024826, data time: 0.006957919257027763\n",
      "step: 677621, loss: 0.06833118945360184, data time: 0.006817493173811171\n",
      "step: 677622, loss: 0.06011508032679558, data time: 0.0066843806086359795\n",
      "step: 677623, loss: 0.06555712223052979, data time: 0.006566298635382401\n",
      "step: 677624, loss: 0.06494487822055817, data time: 0.006449473209870167\n",
      "step: 677625, loss: 0.058035098016262054, data time: 0.006339079141616822\n",
      "step: 677626, loss: 0.06345134973526001, data time: 0.17560029029846191\n",
      "step: 677627, loss: 0.058901768177747726, data time: 0.0885847806930542\n",
      "step: 677628, loss: 0.06798212230205536, data time: 0.05997610092163086\n",
      "step: 677629, loss: 0.06344859302043915, data time: 0.045748889446258545\n",
      "step: 677630, loss: 0.06046392023563385, data time: 0.03688149452209473\n",
      "step: 677631, loss: 0.06267222762107849, data time: 0.030979514122009277\n",
      "step: 677632, loss: 0.056573446840047836, data time: 0.02677117075238909\n",
      "step: 677633, loss: 0.06110916659235954, data time: 0.023692607879638672\n",
      "step: 677634, loss: 0.06197880208492279, data time: 0.02121522691514757\n",
      "step: 677635, loss: 0.05888479948043823, data time: 0.019292759895324706\n",
      "step: 677636, loss: 0.06022794172167778, data time: 0.017738819122314453\n",
      "step: 677637, loss: 0.07057717442512512, data time: 0.016451597213745117\n",
      "step: 677638, loss: 0.05728687345981598, data time: 0.0153657656449538\n",
      "step: 677639, loss: 0.0596647672355175, data time: 0.014409661293029785\n",
      "step: 677640, loss: 0.06151854246854782, data time: 0.013589906692504882\n",
      "step: 677641, loss: 0.05730241537094116, data time: 0.012873440980911255\n",
      "step: 677642, loss: 0.06894087791442871, data time: 0.012242920258465935\n",
      "step: 677643, loss: 0.055596724152565, data time: 0.011673384242587619\n",
      "step: 677644, loss: 0.062120288610458374, data time: 0.011166986666227641\n",
      "step: 677645, loss: 0.06306157261133194, data time: 0.010716891288757325\n",
      "step: 677646, loss: 0.06470158696174622, data time: 0.010311648959205263\n",
      "step: 677647, loss: 0.06890787929296494, data time: 0.00994829698042436\n",
      "step: 677648, loss: 0.06266842782497406, data time: 0.009605604669322138\n",
      "step: 677649, loss: 0.06388509273529053, data time: 0.00929945707321167\n",
      "step: 677650, loss: 0.054141394793987274, data time: 0.009015827178955079\n",
      "step: 677651, loss: 0.058755338191986084, data time: 0.00874938414647029\n",
      "step: 677652, loss: 0.058204375207424164, data time: 0.00849994906672725\n",
      "step: 677653, loss: 0.06333504617214203, data time: 0.008274461541857039\n",
      "step: 677654, loss: 0.06163399666547775, data time: 0.008066498000046303\n",
      "step: 677655, loss: 0.06655770540237427, data time: 0.00787525177001953\n",
      "step: 677656, loss: 0.05482487380504608, data time: 0.007690991124799175\n",
      "step: 677657, loss: 0.061969053000211716, data time: 0.007523275911808014\n",
      "step: 677658, loss: 0.06359857320785522, data time: 0.007355458808667732\n",
      "step: 677659, loss: 0.05915036052465439, data time: 0.00719760445987477\n",
      "step: 677660, loss: 0.062013767659664154, data time: 0.007046542848859514\n",
      "step: 677661, loss: 0.06440441310405731, data time: 0.0069038669268290205\n",
      "step: 677662, loss: 0.06496887654066086, data time: 0.006768890329309412\n",
      "step: 677663, loss: 0.06778434664011002, data time: 0.006645403410259046\n",
      "step: 677664, loss: 0.06680279225111008, data time: 0.006526623016748673\n",
      "step: 677665, loss: 0.041037317365407944, data time: 0.006414556503295898\n",
      "step: 677666, loss: 0.06492846459150314, data time: 0.1602637767791748\n",
      "step: 677667, loss: 0.05979437381029129, data time: 0.08152997493743896\n",
      "step: 677668, loss: 0.05814216658473015, data time: 0.054871559143066406\n",
      "step: 677669, loss: 0.06358525156974792, data time: 0.04197448492050171\n",
      "step: 677670, loss: 0.06223633140325546, data time: 0.03387913703918457\n",
      "step: 677671, loss: 0.06028503179550171, data time: 0.028490185737609863\n",
      "step: 677672, loss: 0.06058322638273239, data time: 0.02461542401994978\n",
      "step: 677673, loss: 0.05712901055812836, data time: 0.021823406219482422\n",
      "step: 677674, loss: 0.0629742369055748, data time: 0.019553581873575848\n",
      "step: 677675, loss: 0.05617564171552658, data time: 0.017818760871887208\n",
      "step: 677676, loss: 0.06646290421485901, data time: 0.016393097964200107\n",
      "step: 677677, loss: 0.06092696636915207, data time: 0.015211542447408041\n",
      "step: 677678, loss: 0.05914219468832016, data time: 0.01420455712538499\n",
      "step: 677679, loss: 0.059715598821640015, data time: 0.013337697301592146\n",
      "step: 677680, loss: 0.06131727620959282, data time: 0.012593952814737956\n",
      "step: 677681, loss: 0.06185447424650192, data time: 0.011932849884033203\n",
      "step: 677682, loss: 0.06415185332298279, data time: 0.011363478267894071\n",
      "step: 677683, loss: 0.05710883438587189, data time: 0.010838190714518229\n",
      "step: 677684, loss: 0.06372459977865219, data time: 0.010374094310559724\n",
      "step: 677685, loss: 0.06380018591880798, data time: 0.009962010383605956\n",
      "step: 677686, loss: 0.06587573140859604, data time: 0.00958871841430664\n",
      "step: 677687, loss: 0.06547971814870834, data time: 0.009250196543606844\n",
      "step: 677688, loss: 0.0661885142326355, data time: 0.008936000906902811\n",
      "step: 677689, loss: 0.06423964351415634, data time: 0.008652150630950928\n",
      "step: 677690, loss: 0.062228791415691376, data time: 0.008388071060180665\n",
      "step: 677691, loss: 0.07134731113910675, data time: 0.008150577545166016\n",
      "step: 677692, loss: 0.06402657926082611, data time: 0.007919779530277959\n",
      "step: 677693, loss: 0.06536900997161865, data time: 0.007709383964538574\n",
      "step: 677694, loss: 0.05991156026721001, data time: 0.007517633766963564\n",
      "step: 677695, loss: 0.061826761811971664, data time: 0.007338786125183105\n",
      "step: 677696, loss: 0.06091472506523132, data time: 0.00717178467781313\n",
      "step: 677697, loss: 0.05819454416632652, data time: 0.007025495171546936\n",
      "step: 677698, loss: 0.060561537742614746, data time: 0.006876931045994614\n",
      "step: 677699, loss: 0.06440193951129913, data time: 0.006730416241814108\n",
      "step: 677700, loss: 0.06298951804637909, data time: 0.006597246442522321\n",
      "step: 677701, loss: 0.05802864953875542, data time: 0.0064672960175408255\n",
      "step: 677702, loss: 0.05646973103284836, data time: 0.006344247508693386\n",
      "step: 677703, loss: 0.06305008381605148, data time: 0.0062304107766402396\n",
      "step: 677704, loss: 0.06454219669103622, data time: 0.006122466845390124\n",
      "step: 677705, loss: 0.05716658756136894, data time: 0.006021994352340698\n",
      "step: 677706, loss: 0.06032116338610649, data time: 0.17189526557922363\n",
      "step: 677707, loss: 0.07001905888319016, data time: 0.08724784851074219\n",
      "step: 677708, loss: 0.06453893333673477, data time: 0.05949179331461588\n",
      "step: 677709, loss: 0.06529326736927032, data time: 0.045002102851867676\n",
      "step: 677710, loss: 0.06716444343328476, data time: 0.03631114959716797\n",
      "step: 677711, loss: 0.057871028780937195, data time: 0.0305331548055013\n",
      "step: 677712, loss: 0.06271550059318542, data time: 0.026562282017299106\n",
      "step: 677713, loss: 0.06266816705465317, data time: 0.02341693639755249\n",
      "step: 677714, loss: 0.06374343484640121, data time: 0.020977550082736544\n",
      "step: 677715, loss: 0.05949600785970688, data time: 0.01907825469970703\n",
      "step: 677716, loss: 0.06196926161646843, data time: 0.017556515606966885\n",
      "step: 677717, loss: 0.05645282194018364, data time: 0.01628198226292928\n",
      "step: 677718, loss: 0.05840291082859039, data time: 0.015196433434119591\n",
      "step: 677719, loss: 0.06268344819545746, data time: 0.01426051344190325\n",
      "step: 677720, loss: 0.06211553141474724, data time: 0.013454167048136394\n",
      "step: 677721, loss: 0.0598173588514328, data time: 0.012757524847984314\n",
      "step: 677722, loss: 0.06696891784667969, data time: 0.01212899825152229\n",
      "step: 677723, loss: 0.05644143745303154, data time: 0.011569486724005805\n",
      "step: 677724, loss: 0.06338946521282196, data time: 0.011073175229524312\n",
      "step: 677725, loss: 0.06315801292657852, data time: 0.01062685251235962\n",
      "step: 677726, loss: 0.06010913848876953, data time: 0.010222469057355608\n",
      "step: 677727, loss: 0.059354983270168304, data time: 0.009854392571882769\n",
      "step: 677728, loss: 0.06618025153875351, data time: 0.009521847185881241\n",
      "step: 677729, loss: 0.056972771883010864, data time: 0.00921618938446045\n",
      "step: 677730, loss: 0.057943396270275116, data time: 0.008934783935546874\n",
      "step: 677731, loss: 0.06236087530851364, data time: 0.008668330999521108\n",
      "step: 677732, loss: 0.0660080760717392, data time: 0.008422886883770977\n",
      "step: 677733, loss: 0.06366634368896484, data time: 0.008193441799708776\n",
      "step: 677734, loss: 0.06294986605644226, data time: 0.008002174311670763\n",
      "step: 677735, loss: 0.061347559094429016, data time: 0.007809329032897949\n",
      "step: 677736, loss: 0.06827828288078308, data time: 0.007626218180502614\n",
      "step: 677737, loss: 0.062177740037441254, data time: 0.00745873898267746\n",
      "step: 677738, loss: 0.06083041429519653, data time: 0.007295435125177557\n",
      "step: 677739, loss: 0.05926772207021713, data time: 0.007140643456402947\n",
      "step: 677740, loss: 0.06340473890304565, data time: 0.006992026737758092\n",
      "step: 677741, loss: 0.06764663755893707, data time: 0.006851289007398818\n",
      "step: 677742, loss: 0.061463963240385056, data time: 0.006717810759673247\n",
      "step: 677743, loss: 0.07006653398275375, data time: 0.006598039677268581\n",
      "step: 677744, loss: 0.06412120163440704, data time: 0.006481286806937976\n",
      "step: 677745, loss: 0.06283871084451675, data time: 0.00637090802192688\n",
      "step: 677746, loss: 0.0621970109641552, data time: 0.1595935821533203\n",
      "step: 677747, loss: 0.06508505344390869, data time: 0.0816032886505127\n",
      "step: 677748, loss: 0.06044639274477959, data time: 0.05491224924723307\n",
      "step: 677749, loss: 0.05789033696055412, data time: 0.041960179805755615\n",
      "step: 677750, loss: 0.06328609585762024, data time: 0.0338627815246582\n",
      "step: 677751, loss: 0.060602836310863495, data time: 0.0284731388092041\n",
      "step: 677752, loss: 0.06424722820520401, data time: 0.02462278093610491\n",
      "step: 677753, loss: 0.06675701588392258, data time: 0.021808147430419922\n",
      "step: 677754, loss: 0.06856928765773773, data time: 0.019528256522284612\n",
      "step: 677755, loss: 0.059722453355789185, data time: 0.017780613899230958\n",
      "step: 677756, loss: 0.06370361149311066, data time: 0.01635930754921653\n",
      "step: 677757, loss: 0.058003123849630356, data time: 0.015176912148793539\n",
      "step: 677758, loss: 0.06790390610694885, data time: 0.014171783740703877\n",
      "step: 677759, loss: 0.05891428142786026, data time: 0.013306771005902971\n",
      "step: 677760, loss: 0.06455132365226746, data time: 0.012558380762736002\n",
      "step: 677761, loss: 0.07365454733371735, data time: 0.011904925107955933\n",
      "step: 677762, loss: 0.05763320252299309, data time: 0.011327575234805836\n",
      "step: 677763, loss: 0.05753348022699356, data time: 0.010813395182291666\n",
      "step: 677764, loss: 0.06355269998311996, data time: 0.010350315194380911\n",
      "step: 677765, loss: 0.06080542132258415, data time: 0.009940040111541749\n",
      "step: 677766, loss: 0.06255124509334564, data time: 0.009568259829566592\n",
      "step: 677767, loss: 0.062002234160900116, data time: 0.009233539754694159\n",
      "step: 677768, loss: 0.0635160505771637, data time: 0.008922431779944378\n",
      "step: 677769, loss: 0.05826854705810547, data time: 0.008640746275583902\n",
      "step: 677770, loss: 0.06061677634716034, data time: 0.008379440307617187\n",
      "step: 677771, loss: 0.06169920414686203, data time: 0.008145946722764235\n",
      "step: 677772, loss: 0.06638972461223602, data time: 0.007920697883323388\n",
      "step: 677773, loss: 0.06556519865989685, data time: 0.007710422788347516\n",
      "step: 677774, loss: 0.06907401978969574, data time: 0.007519302697017275\n",
      "step: 677775, loss: 0.06178229674696922, data time: 0.007340057690938314\n",
      "step: 677776, loss: 0.05927681177854538, data time: 0.0071726229883009385\n",
      "step: 677777, loss: 0.060081999748945236, data time: 0.007020719349384308\n",
      "step: 677778, loss: 0.06357499957084656, data time: 0.006868485248450077\n",
      "step: 677779, loss: 0.062064897269010544, data time: 0.006729392444386202\n",
      "step: 677780, loss: 0.06647348403930664, data time: 0.006596040725708008\n",
      "step: 677781, loss: 0.05842239782214165, data time: 0.006472362412346734\n",
      "step: 677782, loss: 0.05643949657678604, data time: 0.0063491511989284205\n",
      "step: 677783, loss: 0.06329697370529175, data time: 0.006235750097977488\n",
      "step: 677784, loss: 0.06512618064880371, data time: 0.006127736507317958\n",
      "step: 677785, loss: 0.06551092863082886, data time: 0.006025165319442749\n",
      "step: 677786, loss: 0.06597055494785309, data time: 0.17257952690124512\n",
      "step: 677787, loss: 0.05144770070910454, data time: 0.0870516300201416\n",
      "step: 677788, loss: 0.06073090434074402, data time: 0.058561245600382485\n",
      "step: 677789, loss: 0.05932918190956116, data time: 0.044826626777648926\n",
      "step: 677790, loss: 0.06904247403144836, data time: 0.036212158203125\n",
      "step: 677791, loss: 0.06208491325378418, data time: 0.030467828114827473\n",
      "step: 677792, loss: 0.06562922894954681, data time: 0.02635489191327776\n",
      "step: 677793, loss: 0.06803243607282639, data time: 0.0233745276927948\n",
      "step: 677794, loss: 0.05506297945976257, data time: 0.020952383677164715\n",
      "step: 677795, loss: 0.06808849424123764, data time: 0.019108510017395018\n",
      "step: 677796, loss: 0.0650278851389885, data time: 0.017595898021351208\n",
      "step: 677797, loss: 0.05979256331920624, data time: 0.01634313662846883\n",
      "step: 677798, loss: 0.05793142318725586, data time: 0.015280576852651743\n",
      "step: 677799, loss: 0.06130087375640869, data time: 0.014360410826546805\n",
      "step: 677800, loss: 0.06324395537376404, data time: 0.013569148381551106\n",
      "step: 677801, loss: 0.059268347918987274, data time: 0.01287841796875\n",
      "step: 677802, loss: 0.06128718703985214, data time: 0.012271839029648724\n",
      "step: 677803, loss: 0.06242953985929489, data time: 0.011715028021070693\n",
      "step: 677804, loss: 0.06558703631162643, data time: 0.01122661640769557\n",
      "step: 677805, loss: 0.06686466932296753, data time: 0.010791981220245361\n",
      "step: 677806, loss: 0.05844996124505997, data time: 0.010398433321998232\n",
      "step: 677807, loss: 0.055181246250867844, data time: 0.010037498040632769\n",
      "step: 677808, loss: 0.0612693689763546, data time: 0.00970552278601605\n",
      "step: 677809, loss: 0.05840468406677246, data time: 0.009403477112452189\n",
      "step: 677810, loss: 0.05819970369338989, data time: 0.009126911163330078\n",
      "step: 677811, loss: 0.05879434198141098, data time: 0.008874141252957858\n",
      "step: 677812, loss: 0.0574285052716732, data time: 0.008632236056857638\n",
      "step: 677813, loss: 0.059900593012571335, data time: 0.00840951715196882\n",
      "step: 677814, loss: 0.0642194002866745, data time: 0.008207904881444471\n",
      "step: 677815, loss: 0.05757632106542587, data time: 0.0080199400583903\n",
      "step: 677816, loss: 0.0612960122525692, data time: 0.007841833176151398\n",
      "step: 677817, loss: 0.0658867284655571, data time: 0.007670797407627106\n",
      "step: 677818, loss: 0.05900419503450394, data time: 0.007503422823819247\n",
      "step: 677819, loss: 0.05846880003809929, data time: 0.007340788841247559\n",
      "step: 677820, loss: 0.054824307560920715, data time: 0.007188326971871512\n",
      "step: 677821, loss: 0.05919448286294937, data time: 0.007043831878238254\n",
      "step: 677822, loss: 0.06141720339655876, data time: 0.006908371641829207\n",
      "step: 677823, loss: 0.06471045315265656, data time: 0.006781207887749923\n",
      "step: 677824, loss: 0.0626772940158844, data time: 0.006660100741264148\n",
      "step: 677825, loss: 0.0789330005645752, data time: 0.0065449953079223635\n",
      "step: 677826, loss: 0.06808724999427795, data time: 0.15081381797790527\n",
      "step: 677827, loss: 0.05936422944068909, data time: 0.0773996114730835\n",
      "step: 677828, loss: 0.06826305389404297, data time: 0.05211186408996582\n",
      "step: 677829, loss: 0.06512801349163055, data time: 0.039764344692230225\n",
      "step: 677830, loss: 0.06561822444200516, data time: 0.03209524154663086\n",
      "step: 677831, loss: 0.061353206634521484, data time: 0.027022679646809895\n",
      "step: 677832, loss: 0.06020566076040268, data time: 0.02336175101143973\n",
      "step: 677833, loss: 0.062214724719524384, data time: 0.020702242851257324\n",
      "step: 677834, loss: 0.06180153414607048, data time: 0.01855799886915419\n",
      "step: 677835, loss: 0.05764637142419815, data time: 0.016909050941467284\n",
      "step: 677836, loss: 0.06233825907111168, data time: 0.01556983861056241\n",
      "step: 677837, loss: 0.06650853157043457, data time: 0.014447848002115885\n",
      "step: 677838, loss: 0.05887189507484436, data time: 0.013500103583702674\n",
      "step: 677839, loss: 0.06405103951692581, data time: 0.012684600693838937\n",
      "step: 677840, loss: 0.06323468685150146, data time: 0.0119874636332194\n",
      "step: 677841, loss: 0.06599201261997223, data time: 0.011363223195075989\n",
      "step: 677842, loss: 0.059226393699645996, data time: 0.010817934485042797\n",
      "step: 677843, loss: 0.05840551480650902, data time: 0.010323762893676758\n",
      "step: 677844, loss: 0.06018855795264244, data time: 0.009884370000738846\n",
      "step: 677845, loss: 0.060998618602752686, data time: 0.009494912624359132\n",
      "step: 677846, loss: 0.062346722930669785, data time: 0.009147984640938895\n",
      "step: 677847, loss: 0.06265616416931152, data time: 0.008831782774491743\n",
      "step: 677848, loss: 0.06327997148036957, data time: 0.008538474207339079\n",
      "step: 677849, loss: 0.06890662759542465, data time: 0.008273273706436157\n",
      "step: 677850, loss: 0.05227024480700493, data time: 0.008026199340820312\n",
      "step: 677851, loss: 0.05915123224258423, data time: 0.007798891801100511\n",
      "step: 677852, loss: 0.05908949300646782, data time: 0.007582346598307292\n",
      "step: 677853, loss: 0.06937249004840851, data time: 0.007384112903050014\n",
      "step: 677854, loss: 0.05921725183725357, data time: 0.007203891359526536\n",
      "step: 677855, loss: 0.06151653826236725, data time: 0.007033514976501465\n",
      "step: 677856, loss: 0.054276496171951294, data time: 0.006874661291799237\n",
      "step: 677857, loss: 0.05737380310893059, data time: 0.006745778024196625\n",
      "step: 677858, loss: 0.060598697513341904, data time: 0.00660384785045277\n",
      "step: 677859, loss: 0.060578953474760056, data time: 0.006467447561376235\n",
      "step: 677860, loss: 0.06329917162656784, data time: 0.006340633119855608\n",
      "step: 677861, loss: 0.06396903097629547, data time: 0.006226095888349745\n",
      "step: 677862, loss: 0.0645119696855545, data time: 0.006109160345953864\n",
      "step: 677863, loss: 0.06459688395261765, data time: 0.006003486482720626\n",
      "step: 677864, loss: 0.06191606819629669, data time: 0.005902644915458484\n",
      "step: 677865, loss: 0.027044696733355522, data time: 0.005805826187133789\n",
      "step: 677866, loss: 0.06197522208094597, data time: 0.15849852561950684\n",
      "step: 677867, loss: 0.05788695812225342, data time: 0.08078634738922119\n",
      "step: 677868, loss: 0.06857746094465256, data time: 0.054442644119262695\n",
      "step: 677869, loss: 0.0658365786075592, data time: 0.04150897264480591\n",
      "step: 677870, loss: 0.06499308347702026, data time: 0.03351802825927734\n",
      "step: 677871, loss: 0.06137743592262268, data time: 0.028171857198079426\n",
      "step: 677872, loss: 0.06803479790687561, data time: 0.02434441021510533\n",
      "step: 677873, loss: 0.05595184862613678, data time: 0.02161511778831482\n",
      "step: 677874, loss: 0.06063929200172424, data time: 0.019404252370198567\n",
      "step: 677875, loss: 0.057688795030117035, data time: 0.017706537246704103\n",
      "step: 677876, loss: 0.05810987949371338, data time: 0.016321940855546432\n",
      "step: 677877, loss: 0.05794268846511841, data time: 0.015164832274119059\n",
      "step: 677878, loss: 0.06226157397031784, data time: 0.014193369792057918\n",
      "step: 677879, loss: 0.05806529521942139, data time: 0.013351576668875558\n",
      "step: 677880, loss: 0.06473871320486069, data time: 0.01263283093770345\n",
      "step: 677881, loss: 0.05973879620432854, data time: 0.011969953775405884\n",
      "step: 677882, loss: 0.0611620768904686, data time: 0.011393533033483168\n",
      "step: 677883, loss: 0.057597093284130096, data time: 0.010869569248623319\n",
      "step: 677884, loss: 0.0639348030090332, data time: 0.01041000767758018\n",
      "step: 677885, loss: 0.06722893565893173, data time: 0.009998810291290284\n",
      "step: 677886, loss: 0.06519981473684311, data time: 0.009624560674031576\n",
      "step: 677887, loss: 0.06139788031578064, data time: 0.009282859888943758\n",
      "step: 677888, loss: 0.06138473004102707, data time: 0.008967658747797426\n",
      "step: 677889, loss: 0.06075594946742058, data time: 0.00868108868598938\n",
      "step: 677890, loss: 0.06560834497213364, data time: 0.008413772583007812\n",
      "step: 677891, loss: 0.06576015055179596, data time: 0.008171595059908353\n",
      "step: 677892, loss: 0.0603904128074646, data time: 0.007943577236599393\n",
      "step: 677893, loss: 0.06274010241031647, data time: 0.007732595716203962\n",
      "step: 677894, loss: 0.057112570852041245, data time: 0.007542297757905105\n",
      "step: 677895, loss: 0.06227552145719528, data time: 0.007361618677775065\n",
      "step: 677896, loss: 0.06179133057594299, data time: 0.0071928654947588525\n",
      "step: 677897, loss: 0.06440355628728867, data time: 0.007039882242679596\n",
      "step: 677898, loss: 0.0647316724061966, data time: 0.00688705299839829\n",
      "step: 677899, loss: 0.06441725790500641, data time: 0.006740745376138126\n",
      "step: 677900, loss: 0.05809994041919708, data time: 0.006605509349278041\n",
      "step: 677901, loss: 0.06238628551363945, data time: 0.006473826037512885\n",
      "step: 677902, loss: 0.0623558908700943, data time: 0.006350659035347603\n",
      "step: 677903, loss: 0.054750315845012665, data time: 0.0062363210477327045\n",
      "step: 677904, loss: 0.06610879302024841, data time: 0.006128525122618064\n",
      "step: 677905, loss: 0.06395436823368073, data time: 0.0060267806053161625\n",
      "step: 677906, loss: 0.06552198529243469, data time: 0.17127752304077148\n",
      "step: 677907, loss: 0.06162907928228378, data time: 0.08699381351470947\n",
      "step: 677908, loss: 0.060506995767354965, data time: 0.05850823720296224\n",
      "step: 677909, loss: 0.06407516449689865, data time: 0.044679462909698486\n",
      "step: 677910, loss: 0.0688880980014801, data time: 0.036028146743774414\n",
      "step: 677911, loss: 0.060860782861709595, data time: 0.03028285503387451\n",
      "step: 677912, loss: 0.06102433800697327, data time: 0.02615325791495187\n",
      "step: 677913, loss: 0.060039155185222626, data time: 0.02314835786819458\n",
      "step: 677914, loss: 0.06023579090833664, data time: 0.02073282665676541\n",
      "step: 677915, loss: 0.06739795207977295, data time: 0.018860006332397462\n",
      "step: 677916, loss: 0.06125101447105408, data time: 0.017350066791881214\n",
      "step: 677917, loss: 0.05960891768336296, data time: 0.016084671020507812\n",
      "step: 677918, loss: 0.06463049352169037, data time: 0.01501184243422288\n",
      "step: 677919, loss: 0.06068137288093567, data time: 0.014082346643720354\n",
      "step: 677920, loss: 0.06439746916294098, data time: 0.013288497924804688\n",
      "step: 677921, loss: 0.058576278388500214, data time: 0.012582466006278992\n",
      "step: 677922, loss: 0.06205189228057861, data time: 0.011968191932229436\n",
      "step: 677923, loss: 0.06158491224050522, data time: 0.011414753066168891\n",
      "step: 677924, loss: 0.06639210134744644, data time: 0.010921892366911235\n",
      "step: 677925, loss: 0.0601588599383831, data time: 0.01048448085784912\n",
      "step: 677926, loss: 0.05926460027694702, data time: 0.010085639499482654\n",
      "step: 677927, loss: 0.06570906937122345, data time: 0.009726285934448242\n",
      "step: 677928, loss: 0.06159209460020065, data time: 0.00939426214798637\n",
      "step: 677929, loss: 0.06680972874164581, data time: 0.009095112482706705\n",
      "step: 677930, loss: 0.06345400214195251, data time: 0.008811960220336914\n",
      "step: 677931, loss: 0.057426296174526215, data time: 0.00855548565204327\n",
      "step: 677932, loss: 0.0614047572016716, data time: 0.008311262837162724\n",
      "step: 677933, loss: 0.06280892342329025, data time: 0.008085489273071289\n",
      "step: 677934, loss: 0.0625041127204895, data time: 0.007880959017523404\n",
      "step: 677935, loss: 0.059103719890117645, data time: 0.0076903820037841795\n",
      "step: 677936, loss: 0.06846047192811966, data time: 0.007513169319398941\n",
      "step: 677937, loss: 0.05893810838460922, data time: 0.00735066831111908\n",
      "step: 677938, loss: 0.0611165389418602, data time: 0.007191369027802439\n",
      "step: 677939, loss: 0.06359396874904633, data time: 0.007034855730393354\n",
      "step: 677940, loss: 0.062245696783065796, data time: 0.006893682479858399\n",
      "step: 677941, loss: 0.06216223165392876, data time: 0.006752894984351264\n",
      "step: 677942, loss: 0.05787825584411621, data time: 0.006623332564895217\n",
      "step: 677943, loss: 0.05609101802110672, data time: 0.006503519259001079\n",
      "step: 677944, loss: 0.0520298108458519, data time: 0.006389024930122571\n",
      "step: 677945, loss: 0.04542610049247742, data time: 0.006280672550201416\n",
      "step: 677946, loss: 0.06087560951709747, data time: 0.18518710136413574\n",
      "step: 677947, loss: 0.06572819501161575, data time: 0.09336566925048828\n",
      "step: 677948, loss: 0.06283339858055115, data time: 0.06275725364685059\n",
      "step: 677949, loss: 0.06362312287092209, data time: 0.047823548316955566\n",
      "step: 677950, loss: 0.06156047806143761, data time: 0.0385380744934082\n",
      "step: 677951, loss: 0.061798930168151855, data time: 0.03237048784891764\n",
      "step: 677952, loss: 0.06226973980665207, data time: 0.027974230902535573\n",
      "step: 677953, loss: 0.06037820503115654, data time: 0.02473124861717224\n",
      "step: 677954, loss: 0.06352454423904419, data time: 0.022206094529893663\n",
      "step: 677955, loss: 0.06135247275233269, data time: 0.020185422897338868\n",
      "step: 677956, loss: 0.058352917432785034, data time: 0.018549984151666813\n",
      "step: 677957, loss: 0.0596931055188179, data time: 0.017187337080637615\n",
      "step: 677958, loss: 0.05640442296862602, data time: 0.01603218225332407\n",
      "step: 677959, loss: 0.06633666157722473, data time: 0.015038490295410156\n",
      "step: 677960, loss: 0.062272876501083374, data time: 0.014184808731079102\n",
      "step: 677961, loss: 0.0642329752445221, data time: 0.013429954648017883\n",
      "step: 677962, loss: 0.06086350232362747, data time: 0.012759587344001322\n",
      "step: 677963, loss: 0.05956654995679855, data time: 0.01217027505238851\n",
      "step: 677964, loss: 0.059297651052474976, data time: 0.011636219526592054\n",
      "step: 677965, loss: 0.06203630566596985, data time: 0.011161363124847412\n",
      "step: 677966, loss: 0.06266111135482788, data time: 0.010730936413719541\n",
      "step: 677967, loss: 0.06589958071708679, data time: 0.010339400985024193\n",
      "step: 677968, loss: 0.06215720623731613, data time: 0.009976998619411303\n",
      "step: 677969, loss: 0.059410303831100464, data time: 0.009666621685028076\n",
      "step: 677970, loss: 0.05870877951383591, data time: 0.009384469985961914\n",
      "step: 677971, loss: 0.0577235221862793, data time: 0.009118777055006761\n",
      "step: 677972, loss: 0.06413870304822922, data time: 0.008868120334766529\n",
      "step: 677973, loss: 0.06143761798739433, data time: 0.008635784898485457\n",
      "step: 677974, loss: 0.06105629727244377, data time: 0.008422185634744579\n",
      "step: 677975, loss: 0.06391144543886185, data time: 0.008226235707600912\n",
      "step: 677976, loss: 0.06675107777118683, data time: 0.008043512221305602\n",
      "step: 677977, loss: 0.06950444728136063, data time: 0.00787370651960373\n",
      "step: 677978, loss: 0.061940647661685944, data time: 0.007700667236790512\n",
      "step: 677979, loss: 0.061179470270872116, data time: 0.007537400021272547\n",
      "step: 677980, loss: 0.061760567128658295, data time: 0.007381064551217216\n",
      "step: 677981, loss: 0.06093573197722435, data time: 0.007232520315382216\n",
      "step: 677982, loss: 0.06603985279798508, data time: 0.007090774742332664\n",
      "step: 677983, loss: 0.054499756544828415, data time: 0.006961226463317871\n",
      "step: 677984, loss: 0.06356104463338852, data time: 0.006838743503277118\n",
      "step: 677985, loss: 0.06813384592533112, data time: 0.006722193956375122\n",
      "step: 677986, loss: 0.05570076033473015, data time: 0.1605372428894043\n",
      "step: 677987, loss: 0.05908121168613434, data time: 0.08139955997467041\n",
      "step: 677988, loss: 0.06880941241979599, data time: 0.0553132692972819\n",
      "step: 677989, loss: 0.05789506062865257, data time: 0.04236721992492676\n",
      "step: 677990, loss: 0.05326297879219055, data time: 0.03418412208557129\n",
      "step: 677991, loss: 0.06229248642921448, data time: 0.028734882672627766\n",
      "step: 677992, loss: 0.06321630626916885, data time: 0.02484869956970215\n",
      "step: 677993, loss: 0.061572086066007614, data time: 0.02200537919998169\n",
      "step: 677994, loss: 0.06239182502031326, data time: 0.019715441597832575\n",
      "step: 677995, loss: 0.057760532945394516, data time: 0.01794710159301758\n",
      "step: 677996, loss: 0.06388425827026367, data time: 0.016518982973965732\n",
      "step: 677997, loss: 0.060314081609249115, data time: 0.015318373839060465\n",
      "step: 677998, loss: 0.0604952797293663, data time: 0.014304252771230845\n",
      "step: 677999, loss: 0.06040599197149277, data time: 0.013428040913173131\n",
      "step: 678000, loss: 0.06713201105594635, data time: 0.012676763534545898\n",
      "step: 678001, loss: 0.06078781932592392, data time: 0.012016624212265015\n",
      "step: 678002, loss: 0.06029066443443298, data time: 0.011430950725779813\n",
      "step: 678003, loss: 0.05856476351618767, data time: 0.010905411508348253\n",
      "step: 678004, loss: 0.060799580067396164, data time: 0.010439006905806692\n",
      "step: 678005, loss: 0.05871575325727463, data time: 0.010039329528808594\n",
      "step: 678006, loss: 0.06447559595108032, data time: 0.009682995932442802\n",
      "step: 678007, loss: 0.058513909578323364, data time: 0.009359251369129528\n",
      "step: 678008, loss: 0.05883454531431198, data time: 0.009059356606524923\n",
      "step: 678009, loss: 0.0670461356639862, data time: 0.00878863533337911\n",
      "step: 678010, loss: 0.06228972598910332, data time: 0.008540945053100586\n",
      "step: 678011, loss: 0.06327245384454727, data time: 0.008306750884422889\n",
      "step: 678012, loss: 0.06147952377796173, data time: 0.008086690196284541\n",
      "step: 678013, loss: 0.06206660717725754, data time: 0.007887491158076696\n",
      "step: 678014, loss: 0.06363417208194733, data time: 0.007700895440989527\n",
      "step: 678015, loss: 0.05920186638832092, data time: 0.007528909047444661\n",
      "step: 678016, loss: 0.06467220187187195, data time: 0.007369210643153037\n",
      "step: 678017, loss: 0.059816669672727585, data time: 0.007227353751659393\n",
      "step: 678018, loss: 0.06340856850147247, data time: 0.0070733157071200285\n",
      "step: 678019, loss: 0.06261111795902252, data time: 0.006929643013898064\n",
      "step: 678020, loss: 0.06234016269445419, data time: 0.006790774209158761\n",
      "step: 678021, loss: 0.06189923733472824, data time: 0.006658169958326552\n",
      "step: 678022, loss: 0.06637421995401382, data time: 0.006533719397879936\n",
      "step: 678023, loss: 0.06339091062545776, data time: 0.0064178391506797385\n",
      "step: 678024, loss: 0.06766249239444733, data time: 0.006308396657307942\n",
      "step: 678025, loss: 0.06371458619832993, data time: 0.006204599142074585\n",
      "step: 678026, loss: 0.06337262690067291, data time: 0.16814303398132324\n",
      "step: 678027, loss: 0.06750761717557907, data time: 0.08481228351593018\n",
      "step: 678028, loss: 0.06342661380767822, data time: 0.057461818059285484\n",
      "step: 678029, loss: 0.055562131106853485, data time: 0.04388022422790527\n",
      "step: 678030, loss: 0.059392303228378296, data time: 0.03540835380554199\n",
      "step: 678031, loss: 0.061701685190200806, data time: 0.029761751492818195\n",
      "step: 678032, loss: 0.0689714252948761, data time: 0.02572601182120187\n",
      "step: 678033, loss: 0.05801969766616821, data time: 0.022761881351470947\n",
      "step: 678034, loss: 0.06412874162197113, data time: 0.02038211292690701\n",
      "step: 678035, loss: 0.0616707019507885, data time: 0.018600106239318848\n",
      "step: 678036, loss: 0.06172057241201401, data time: 0.0171051025390625\n",
      "step: 678037, loss: 0.061249732971191406, data time: 0.015863815943400066\n",
      "step: 678038, loss: 0.06332595646381378, data time: 0.014805463644174429\n",
      "step: 678039, loss: 0.06283976882696152, data time: 0.013892582484654017\n",
      "step: 678040, loss: 0.061635419726371765, data time: 0.013110701243082683\n",
      "step: 678041, loss: 0.05903426930308342, data time: 0.012425541877746582\n",
      "step: 678042, loss: 0.06338285654783249, data time: 0.01181944678811466\n",
      "step: 678043, loss: 0.05597023665904999, data time: 0.011272894011603462\n",
      "step: 678044, loss: 0.06222681701183319, data time: 0.010785140489277086\n",
      "step: 678045, loss: 0.06535881757736206, data time: 0.010351729393005372\n",
      "step: 678046, loss: 0.06481261551380157, data time: 0.00996309234982445\n",
      "step: 678047, loss: 0.06130534037947655, data time: 0.009608983993530273\n",
      "step: 678048, loss: 0.06494246423244476, data time: 0.009279354758884596\n",
      "step: 678049, loss: 0.061184581369161606, data time: 0.00898053248723348\n",
      "step: 678050, loss: 0.06409814208745956, data time: 0.00870600700378418\n",
      "step: 678051, loss: 0.05957468971610069, data time: 0.008447518715491662\n",
      "step: 678052, loss: 0.06587822735309601, data time: 0.008208442617345739\n",
      "step: 678053, loss: 0.058179523795843124, data time: 0.007988248552594866\n",
      "step: 678054, loss: 0.06394139677286148, data time: 0.007787778459746262\n",
      "step: 678055, loss: 0.0626533254981041, data time: 0.0075993220011393225\n",
      "step: 678056, loss: 0.060281336307525635, data time: 0.007424077680034022\n",
      "step: 678057, loss: 0.06165332719683647, data time: 0.007262729108333588\n",
      "step: 678058, loss: 0.06283625215291977, data time: 0.007106976075605912\n",
      "step: 678059, loss: 0.06629408150911331, data time: 0.006958211169523352\n",
      "step: 678060, loss: 0.05637922137975693, data time: 0.006814765930175781\n",
      "step: 678061, loss: 0.059919100254774094, data time: 0.00667796532313029\n",
      "step: 678062, loss: 0.06085093319416046, data time: 0.006552554465628959\n",
      "step: 678063, loss: 0.05903918296098709, data time: 0.006432884617855674\n",
      "step: 678064, loss: 0.06708408892154694, data time: 0.006321142881344526\n",
      "step: 678065, loss: 0.053993385285139084, data time: 0.006215614080429077\n",
      "step: 678066, loss: 0.06220070645213127, data time: 0.16072964668273926\n",
      "step: 678067, loss: 0.057251110672950745, data time: 0.08149075508117676\n",
      "step: 678068, loss: 0.05957787483930588, data time: 0.05547785758972168\n",
      "step: 678069, loss: 0.06372185051441193, data time: 0.042274653911590576\n",
      "step: 678070, loss: 0.06360946595668793, data time: 0.03409876823425293\n",
      "step: 678071, loss: 0.06727844476699829, data time: 0.028666973114013672\n",
      "step: 678072, loss: 0.06271690130233765, data time: 0.02479168346949986\n",
      "step: 678073, loss: 0.05543459206819534, data time: 0.021970361471176147\n",
      "step: 678074, loss: 0.06462115794420242, data time: 0.019678036371866863\n",
      "step: 678075, loss: 0.059616535902023315, data time: 0.017905235290527344\n",
      "step: 678076, loss: 0.050946276634931564, data time: 0.01647628437389027\n",
      "step: 678077, loss: 0.06824101507663727, data time: 0.015279213587443033\n",
      "step: 678078, loss: 0.0645514652132988, data time: 0.014270984209500827\n",
      "step: 678079, loss: 0.06382841616868973, data time: 0.0134002651487078\n",
      "step: 678080, loss: 0.06267189234495163, data time: 0.012648328145345052\n",
      "step: 678081, loss: 0.06080862879753113, data time: 0.011993378400802612\n",
      "step: 678082, loss: 0.06058109551668167, data time: 0.01140903024112477\n",
      "step: 678083, loss: 0.05468931049108505, data time: 0.01088094711303711\n",
      "step: 678084, loss: 0.062151484191417694, data time: 0.01041672104283383\n",
      "step: 678085, loss: 0.061090707778930664, data time: 0.010004210472106933\n",
      "step: 678086, loss: 0.062458332628011703, data time: 0.009632553373064314\n",
      "step: 678087, loss: 0.06188744306564331, data time: 0.009293437004089355\n",
      "step: 678088, loss: 0.06435255706310272, data time: 0.00897954857867697\n",
      "step: 678089, loss: 0.06186319887638092, data time: 0.008697450160980225\n",
      "step: 678090, loss: 0.060347191989421844, data time: 0.008436565399169921\n",
      "step: 678091, loss: 0.06791151314973831, data time: 0.008191878979022685\n",
      "step: 678092, loss: 0.06419547647237778, data time: 0.007976125787805629\n",
      "step: 678093, loss: 0.06316279619932175, data time: 0.007776158196585519\n",
      "step: 678094, loss: 0.061252497136592865, data time: 0.007592439651489258\n",
      "step: 678095, loss: 0.06525285542011261, data time: 0.0074229558308919275\n",
      "step: 678096, loss: 0.0726785957813263, data time: 0.007261207026820029\n",
      "step: 678097, loss: 0.0586274079978466, data time: 0.007115490734577179\n",
      "step: 678098, loss: 0.0584665946662426, data time: 0.006964683532714844\n",
      "step: 678099, loss: 0.06415450572967529, data time: 0.006824051632600672\n",
      "step: 678100, loss: 0.05845437943935394, data time: 0.006687961305890764\n",
      "step: 678101, loss: 0.05772038921713829, data time: 0.006557186444600423\n",
      "step: 678102, loss: 0.055786579847335815, data time: 0.006435349180891707\n",
      "step: 678103, loss: 0.06052134558558464, data time: 0.0063231179588719415\n",
      "step: 678104, loss: 0.05857403203845024, data time: 0.006216501578306541\n",
      "step: 678105, loss: 0.05209621787071228, data time: 0.006114637851715088\n",
      "step: 678106, loss: 0.06210247799754143, data time: 0.17434310913085938\n",
      "step: 678107, loss: 0.0697551742196083, data time: 0.08850717544555664\n",
      "step: 678108, loss: 0.05309058353304863, data time: 0.05998365084330241\n",
      "step: 678109, loss: 0.06040608882904053, data time: 0.045667171478271484\n",
      "step: 678110, loss: 0.060281433165073395, data time: 0.03682270050048828\n",
      "step: 678111, loss: 0.0653921440243721, data time: 0.030953407287597656\n",
      "step: 678112, loss: 0.05554131790995598, data time: 0.026759113584245955\n",
      "step: 678113, loss: 0.06621815264225006, data time: 0.023743778467178345\n",
      "step: 678114, loss: 0.06103579327464104, data time: 0.021257797876993816\n",
      "step: 678115, loss: 0.06656598299741745, data time: 0.019372725486755372\n",
      "step: 678116, loss: 0.06181924045085907, data time: 0.01780854571949352\n",
      "step: 678117, loss: 0.05997254699468613, data time: 0.016498665014902752\n",
      "step: 678118, loss: 0.061295222491025925, data time: 0.015401106614332933\n",
      "step: 678119, loss: 0.06936661899089813, data time: 0.014445900917053223\n",
      "step: 678120, loss: 0.062476661056280136, data time: 0.013627560933430989\n",
      "step: 678121, loss: 0.06441052258014679, data time: 0.012910231947898865\n",
      "step: 678122, loss: 0.05857359617948532, data time: 0.012277869617237765\n",
      "step: 678123, loss: 0.060844361782073975, data time: 0.011720736821492514\n",
      "step: 678124, loss: 0.05716782808303833, data time: 0.011207906823409232\n",
      "step: 678125, loss: 0.06826414167881012, data time: 0.01075444221496582\n",
      "step: 678126, loss: 0.061717938631772995, data time: 0.010344085239228747\n",
      "step: 678127, loss: 0.06534886360168457, data time: 0.009978803721341219\n",
      "step: 678128, loss: 0.06104900687932968, data time: 0.009633333786674168\n",
      "step: 678129, loss: 0.06790772080421448, data time: 0.009320070346196493\n",
      "step: 678130, loss: 0.05576249584555626, data time: 0.009035835266113281\n",
      "step: 678131, loss: 0.060990508645772934, data time: 0.008766266015859751\n",
      "step: 678132, loss: 0.061465710401535034, data time: 0.00851491645530418\n",
      "step: 678133, loss: 0.05629381164908409, data time: 0.00828183548791068\n",
      "step: 678134, loss: 0.06535890698432922, data time: 0.008070748427818561\n",
      "step: 678135, loss: 0.06220617890357971, data time: 0.007874067624409993\n",
      "step: 678136, loss: 0.06201711297035217, data time: 0.007688853048509167\n",
      "step: 678137, loss: 0.062125563621520996, data time: 0.007519543170928955\n",
      "step: 678138, loss: 0.05958394333720207, data time: 0.007353948824333422\n",
      "step: 678139, loss: 0.06076233088970184, data time: 0.007198761491214528\n",
      "step: 678140, loss: 0.05947096645832062, data time: 0.007046481541224888\n",
      "step: 678141, loss: 0.06279908120632172, data time: 0.0069022443559434675\n",
      "step: 678142, loss: 0.05523660033941269, data time: 0.006767150518056509\n",
      "step: 678143, loss: 0.06448793411254883, data time: 0.0066441485756321954\n",
      "step: 678144, loss: 0.06292669475078583, data time: 0.006525693795619867\n",
      "step: 678145, loss: 0.0572403222322464, data time: 0.006413066387176513\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4071989059448242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 678146, loss: 0.0564994215965271, data time: 0.1629633903503418\n",
      "step: 678147, loss: 0.06223207712173462, data time: 0.08304882049560547\n",
      "step: 678148, loss: 0.0582767128944397, data time: 0.05588142077128092\n",
      "step: 678149, loss: 0.06003851816058159, data time: 0.04276317358016968\n",
      "step: 678150, loss: 0.06126003712415695, data time: 0.03449668884277344\n",
      "step: 678151, loss: 0.061379607766866684, data time: 0.028974135716756184\n",
      "step: 678152, loss: 0.058778055012226105, data time: 0.02503861699785505\n",
      "step: 678153, loss: 0.051595836877822876, data time: 0.022158056497573853\n",
      "step: 678154, loss: 0.06296254694461823, data time: 0.019847181108262803\n",
      "step: 678155, loss: 0.05837615579366684, data time: 0.01805415153503418\n",
      "step: 678156, loss: 0.06384392082691193, data time: 0.016625751148570667\n",
      "step: 678157, loss: 0.059750091284513474, data time: 0.015412092208862305\n",
      "step: 678158, loss: 0.0575106106698513, data time: 0.014388781327467699\n",
      "step: 678159, loss: 0.06359733641147614, data time: 0.013514842305864607\n",
      "step: 678160, loss: 0.059387218207120895, data time: 0.012752040227254232\n",
      "step: 678161, loss: 0.05822807550430298, data time: 0.012079834938049316\n",
      "step: 678162, loss: 0.061708495020866394, data time: 0.01148686689489028\n",
      "step: 678163, loss: 0.059668611735105515, data time: 0.010956896675957574\n",
      "step: 678164, loss: 0.061971329152584076, data time: 0.01048731803894043\n",
      "step: 678165, loss: 0.05756588280200958, data time: 0.01007702350616455\n",
      "step: 678166, loss: 0.05893365666270256, data time: 0.009697607585362025\n",
      "step: 678167, loss: 0.06204009801149368, data time: 0.009352564811706543\n",
      "step: 678168, loss: 0.06195298582315445, data time: 0.009036333664603855\n",
      "step: 678169, loss: 0.0600360631942749, data time: 0.008744249741236368\n",
      "step: 678170, loss: 0.0614248588681221, data time: 0.008477392196655274\n",
      "step: 678171, loss: 0.057588085532188416, data time: 0.008239250916701097\n",
      "step: 678172, loss: 0.06577256321907043, data time: 0.00800686412387424\n",
      "step: 678173, loss: 0.05923282355070114, data time: 0.007789450032370431\n",
      "step: 678174, loss: 0.058334462344646454, data time: 0.007594001704248889\n",
      "step: 678175, loss: 0.06371410191059113, data time: 0.007415397961934408\n",
      "step: 678176, loss: 0.06373468041419983, data time: 0.0072495783528973975\n",
      "step: 678177, loss: 0.06005958840250969, data time: 0.007093466818332672\n",
      "step: 678178, loss: 0.06438817083835602, data time: 0.006937157024036755\n",
      "step: 678179, loss: 0.06442248821258545, data time: 0.006789172396940344\n",
      "step: 678180, loss: 0.060653142631053925, data time: 0.006653574534824916\n",
      "step: 678181, loss: 0.06040564179420471, data time: 0.006518224875132243\n",
      "step: 678182, loss: 0.0572223961353302, data time: 0.006404380540590029\n",
      "step: 678183, loss: 0.05606655403971672, data time: 0.006289714261105186\n",
      "step: 678184, loss: 0.05883605405688286, data time: 0.006180384220221104\n",
      "step: 678185, loss: 0.05258340761065483, data time: 0.00607953667640686\n",
      "step: 678186, loss: 0.06322142481803894, data time: 0.17138242721557617\n",
      "step: 678187, loss: 0.06468638777732849, data time: 0.08648431301116943\n",
      "step: 678188, loss: 0.062324684113264084, data time: 0.05816316604614258\n",
      "step: 678189, loss: 0.06779660284519196, data time: 0.04442721605300903\n",
      "step: 678190, loss: 0.06243058294057846, data time: 0.035853004455566405\n",
      "step: 678191, loss: 0.05981656163930893, data time: 0.03011588255564372\n",
      "step: 678192, loss: 0.06261777877807617, data time: 0.026037488664899553\n",
      "step: 678193, loss: 0.06208815798163414, data time: 0.023044347763061523\n",
      "step: 678194, loss: 0.05454476922750473, data time: 0.020631604724460177\n",
      "step: 678195, loss: 0.05903759226202965, data time: 0.01876969337463379\n",
      "step: 678196, loss: 0.06070364639163017, data time: 0.017259944568980824\n",
      "step: 678197, loss: 0.05543997883796692, data time: 0.016005635261535645\n",
      "step: 678198, loss: 0.06223286688327789, data time: 0.014958601731520433\n",
      "step: 678199, loss: 0.05858948826789856, data time: 0.014061995915004186\n",
      "step: 678200, loss: 0.06358374655246735, data time: 0.013285207748413085\n",
      "step: 678201, loss: 0.06148509308695793, data time: 0.012611493468284607\n",
      "step: 678202, loss: 0.06044027954339981, data time: 0.012009985306683709\n",
      "step: 678203, loss: 0.06275547295808792, data time: 0.011473973592122396\n",
      "step: 678204, loss: 0.05705534666776657, data time: 0.010990782787925318\n",
      "step: 678205, loss: 0.060906797647476196, data time: 0.010562920570373535\n",
      "step: 678206, loss: 0.06130429357290268, data time: 0.010179587772914342\n",
      "step: 678207, loss: 0.06026514619588852, data time: 0.009831591085954145\n",
      "step: 678208, loss: 0.06005462259054184, data time: 0.009511543356853983\n",
      "step: 678209, loss: 0.06091718375682831, data time: 0.009211490551630655\n",
      "step: 678210, loss: 0.05702865868806839, data time: 0.00894390106201172\n",
      "step: 678211, loss: 0.05997586250305176, data time: 0.008678958966181828\n",
      "step: 678212, loss: 0.06421183049678802, data time: 0.008429642076845522\n",
      "step: 678213, loss: 0.06436721235513687, data time: 0.008198542254311698\n",
      "step: 678214, loss: 0.06503143161535263, data time: 0.007992547133873248\n",
      "step: 678215, loss: 0.05766085535287857, data time: 0.007798186937967936\n",
      "step: 678216, loss: 0.06519519537687302, data time: 0.007615989254366967\n",
      "step: 678217, loss: 0.06216883659362793, data time: 0.0074475109577178955\n",
      "step: 678218, loss: 0.06386466324329376, data time: 0.007278767499056729\n",
      "step: 678219, loss: 0.05512086674571037, data time: 0.007123884032754337\n",
      "step: 678220, loss: 0.06581415981054306, data time: 0.006974778856549944\n",
      "step: 678221, loss: 0.060460008680820465, data time: 0.006837381256951226\n",
      "step: 678222, loss: 0.06308315694332123, data time: 0.006704994150110193\n",
      "step: 678223, loss: 0.06700794398784637, data time: 0.006584493737471731\n",
      "step: 678224, loss: 0.06173671782016754, data time: 0.006467226224067884\n",
      "step: 678225, loss: 0.04655498266220093, data time: 0.006356364488601685\n",
      "step: 678226, loss: 0.062287695705890656, data time: 0.1787734031677246\n",
      "step: 678227, loss: 0.061793528497219086, data time: 0.0901346206665039\n",
      "step: 678228, loss: 0.06349597871303558, data time: 0.06113680203755697\n",
      "step: 678229, loss: 0.06291718780994415, data time: 0.04651212692260742\n",
      "step: 678230, loss: 0.06163227558135986, data time: 0.037538766860961914\n",
      "step: 678231, loss: 0.06983445584774017, data time: 0.031529863675435386\n",
      "step: 678232, loss: 0.06010152027010918, data time: 0.0272430351802281\n",
      "step: 678233, loss: 0.06496857106685638, data time: 0.02409309148788452\n",
      "step: 678234, loss: 0.06291008740663528, data time: 0.021567344665527344\n",
      "step: 678235, loss: 0.060719989240169525, data time: 0.01962277889251709\n",
      "step: 678236, loss: 0.06366033852100372, data time: 0.018037817694924095\n",
      "step: 678237, loss: 0.06255920231342316, data time: 0.01671179135640462\n",
      "step: 678238, loss: 0.06132548674941063, data time: 0.01558982408963717\n",
      "step: 678239, loss: 0.061591364443302155, data time: 0.01462083203451974\n",
      "step: 678240, loss: 0.06318199634552002, data time: 0.013779497146606446\n",
      "step: 678241, loss: 0.0646512508392334, data time: 0.013052582740783691\n",
      "step: 678242, loss: 0.059083618223667145, data time: 0.012427400140201343\n",
      "step: 678243, loss: 0.06518330425024033, data time: 0.011848462952507867\n",
      "step: 678244, loss: 0.06103816628456116, data time: 0.011331859387849507\n",
      "step: 678245, loss: 0.05934765934944153, data time: 0.010873353481292725\n",
      "step: 678246, loss: 0.057166606187820435, data time: 0.010461671011788505\n",
      "step: 678247, loss: 0.06162421032786369, data time: 0.010084726593711159\n",
      "step: 678248, loss: 0.06310112774372101, data time: 0.00974074653957201\n",
      "step: 678249, loss: 0.06079447269439697, data time: 0.009417474269866943\n",
      "step: 678250, loss: 0.06566087156534195, data time: 0.009126901626586914\n",
      "step: 678251, loss: 0.05774480104446411, data time: 0.00885306871854342\n",
      "step: 678252, loss: 0.06315585225820541, data time: 0.008599572711520724\n",
      "step: 678253, loss: 0.05769926309585571, data time: 0.0083635790007455\n",
      "step: 678254, loss: 0.06489743292331696, data time: 0.008151383235536772\n",
      "step: 678255, loss: 0.06216725707054138, data time: 0.007951505978902181\n",
      "step: 678256, loss: 0.06656541675329208, data time: 0.007762647444202054\n",
      "step: 678257, loss: 0.06264669448137283, data time: 0.007590942084789276\n",
      "step: 678258, loss: 0.05916088819503784, data time: 0.007417519887288411\n",
      "step: 678259, loss: 0.055664293467998505, data time: 0.007261395454406738\n",
      "step: 678260, loss: 0.06322559714317322, data time: 0.007111876351492745\n",
      "step: 678261, loss: 0.061606526374816895, data time: 0.0069668226771884495\n",
      "step: 678262, loss: 0.06551743298768997, data time: 0.0068314655407055004\n",
      "step: 678263, loss: 0.06505995988845825, data time: 0.006706225244622482\n",
      "step: 678264, loss: 0.059136293828487396, data time: 0.0065863621540558645\n",
      "step: 678265, loss: 0.06612582504749298, data time: 0.006471693515777588\n",
      "step: 678266, loss: 0.06501258909702301, data time: 0.16719460487365723\n",
      "step: 678267, loss: 0.06379513442516327, data time: 0.08495664596557617\n",
      "step: 678268, loss: 0.06117692589759827, data time: 0.05769101778666178\n",
      "step: 678269, loss: 0.06535229086875916, data time: 0.04395681619644165\n",
      "step: 678270, loss: 0.05702213570475578, data time: 0.03543701171875\n",
      "step: 678271, loss: 0.06464561074972153, data time: 0.02980971336364746\n",
      "step: 678272, loss: 0.06238088011741638, data time: 0.025752578462873186\n",
      "step: 678273, loss: 0.06650790572166443, data time: 0.022794127464294434\n",
      "step: 678274, loss: 0.0622822642326355, data time: 0.020415915383232966\n",
      "step: 678275, loss: 0.06377473473548889, data time: 0.018625307083129882\n",
      "step: 678276, loss: 0.06394083797931671, data time: 0.01713397286155007\n",
      "step: 678277, loss: 0.06285050511360168, data time: 0.015886545181274414\n",
      "step: 678278, loss: 0.06066092103719711, data time: 0.014827434833233174\n",
      "step: 678279, loss: 0.06256359070539474, data time: 0.013913699558803014\n",
      "step: 678280, loss: 0.05565905570983887, data time: 0.013141457239786785\n",
      "step: 678281, loss: 0.060176655650138855, data time: 0.012453287839889526\n",
      "step: 678282, loss: 0.05678665637969971, data time: 0.011838828816133387\n",
      "step: 678283, loss: 0.06473995745182037, data time: 0.011289384629991319\n",
      "step: 678284, loss: 0.059430427849292755, data time: 0.010803285397981343\n",
      "step: 678285, loss: 0.0601569227874279, data time: 0.010377538204193116\n",
      "step: 678286, loss: 0.06344200670719147, data time: 0.00998914809454055\n",
      "step: 678287, loss: 0.055231258273124695, data time: 0.009632143107327547\n",
      "step: 678288, loss: 0.05917952582240105, data time: 0.00929979656053626\n",
      "step: 678289, loss: 0.06052849441766739, data time: 0.009001761674880981\n",
      "step: 678290, loss: 0.06258229911327362, data time: 0.008722906112670898\n",
      "step: 678291, loss: 0.056648384779691696, data time: 0.008469682473402757\n",
      "step: 678292, loss: 0.05556201934814453, data time: 0.008231631031742803\n",
      "step: 678293, loss: 0.05870058387517929, data time: 0.008013503892081124\n",
      "step: 678294, loss: 0.06295931339263916, data time: 0.007813404346334523\n",
      "step: 678295, loss: 0.0622204914689064, data time: 0.0076247453689575195\n",
      "step: 678296, loss: 0.06051536649465561, data time: 0.007449780741045552\n",
      "step: 678297, loss: 0.06551508605480194, data time: 0.0072867125272750854\n",
      "step: 678298, loss: 0.06111413985490799, data time: 0.007130528941298976\n",
      "step: 678299, loss: 0.06495758891105652, data time: 0.006978371564079733\n",
      "step: 678300, loss: 0.06939969956874847, data time: 0.006834016527448382\n",
      "step: 678301, loss: 0.06150026246905327, data time: 0.0066967738999260795\n",
      "step: 678302, loss: 0.06108751893043518, data time: 0.006567619942330025\n",
      "step: 678303, loss: 0.06142941117286682, data time: 0.0064479740042435495\n",
      "step: 678304, loss: 0.059811435639858246, data time: 0.006334842779697516\n",
      "step: 678305, loss: 0.057684846222400665, data time: 0.006226098537445069\n",
      "step: 678306, loss: 0.06250123679637909, data time: 0.1793813705444336\n",
      "step: 678307, loss: 0.06107715889811516, data time: 0.09150838851928711\n",
      "step: 678308, loss: 0.06234215945005417, data time: 0.061507781346639\n",
      "step: 678309, loss: 0.06750790774822235, data time: 0.04680073261260986\n",
      "step: 678310, loss: 0.060973428189754486, data time: 0.03773674964904785\n",
      "step: 678311, loss: 0.06365124881267548, data time: 0.031678358713785805\n",
      "step: 678312, loss: 0.05892135947942734, data time: 0.027380807059151784\n",
      "step: 678313, loss: 0.06086152791976929, data time: 0.024225562810897827\n",
      "step: 678314, loss: 0.06364534795284271, data time: 0.021754291322496202\n",
      "step: 678315, loss: 0.06266187131404877, data time: 0.019774770736694335\n",
      "step: 678316, loss: 0.06213168427348137, data time: 0.01817235079678622\n",
      "step: 678317, loss: 0.062433816492557526, data time: 0.016838133335113525\n",
      "step: 678318, loss: 0.059754423797130585, data time: 0.01570897835951585\n",
      "step: 678319, loss: 0.06512247025966644, data time: 0.014744162559509277\n",
      "step: 678320, loss: 0.0580701008439064, data time: 0.013893620173136393\n",
      "step: 678321, loss: 0.05844529718160629, data time: 0.01316368579864502\n",
      "step: 678322, loss: 0.06447553634643555, data time: 0.012509612476124483\n",
      "step: 678323, loss: 0.05786985903978348, data time: 0.01192490259806315\n",
      "step: 678324, loss: 0.06568437814712524, data time: 0.011405467987060547\n",
      "step: 678325, loss: 0.06054745241999626, data time: 0.01094522476196289\n",
      "step: 678326, loss: 0.06802479922771454, data time: 0.010528076262701126\n",
      "step: 678327, loss: 0.05758931487798691, data time: 0.010150237516923384\n",
      "step: 678328, loss: 0.0679803341627121, data time: 0.009798454201739767\n",
      "step: 678329, loss: 0.06473028659820557, data time: 0.009473840395609537\n",
      "step: 678330, loss: 0.0604868158698082, data time: 0.009181833267211914\n",
      "step: 678331, loss: 0.06173957884311676, data time: 0.00890654783982497\n",
      "step: 678332, loss: 0.060223840177059174, data time: 0.008651053463971173\n",
      "step: 678333, loss: 0.05924282968044281, data time: 0.008411696978977748\n",
      "step: 678334, loss: 0.06800035387277603, data time: 0.00819407660385658\n",
      "step: 678335, loss: 0.06089208647608757, data time: 0.007991973559061687\n",
      "step: 678336, loss: 0.05917413532733917, data time: 0.007805132096813571\n",
      "step: 678337, loss: 0.06568007916212082, data time: 0.007636137306690216\n",
      "step: 678338, loss: 0.06042943522334099, data time: 0.007463679169163559\n",
      "step: 678339, loss: 0.06597352027893066, data time: 0.007303819936864516\n",
      "step: 678340, loss: 0.06575791537761688, data time: 0.007149744033813477\n",
      "step: 678341, loss: 0.0618501752614975, data time: 0.007004115316602919\n",
      "step: 678342, loss: 0.06440508365631104, data time: 0.006864921466724292\n",
      "step: 678343, loss: 0.06319399178028107, data time: 0.006737125547308671\n",
      "step: 678344, loss: 0.06342482566833496, data time: 0.006616066663693159\n",
      "step: 678345, loss: 0.05255145952105522, data time: 0.0065032958984375\n",
      "step: 678346, loss: 0.05888277292251587, data time: 0.15378665924072266\n",
      "step: 678347, loss: 0.06509208679199219, data time: 0.07824957370758057\n",
      "step: 678348, loss: 0.0691254660487175, data time: 0.0532073179880778\n",
      "step: 678349, loss: 0.06591670960187912, data time: 0.040583670139312744\n",
      "step: 678350, loss: 0.06338217854499817, data time: 0.03275737762451172\n",
      "step: 678351, loss: 0.0637122318148613, data time: 0.027557094891866047\n",
      "step: 678352, loss: 0.061518557369709015, data time: 0.023821115493774414\n",
      "step: 678353, loss: 0.06470730155706406, data time: 0.021098703145980835\n",
      "step: 678354, loss: 0.05787423998117447, data time: 0.018900023566351995\n",
      "step: 678355, loss: 0.07277531921863556, data time: 0.017224955558776855\n",
      "step: 678356, loss: 0.060431789606809616, data time: 0.015856829556551846\n",
      "step: 678357, loss: 0.057164326310157776, data time: 0.014713903268178305\n",
      "step: 678358, loss: 0.05987795442342758, data time: 0.013747875507061299\n",
      "step: 678359, loss: 0.06107698008418083, data time: 0.01291743346623012\n",
      "step: 678360, loss: 0.059709660708904266, data time: 0.01220717430114746\n",
      "step: 678361, loss: 0.0673496350646019, data time: 0.011573314666748047\n",
      "step: 678362, loss: 0.06394018232822418, data time: 0.011008851668413948\n",
      "step: 678363, loss: 0.0648002102971077, data time: 0.010505265659756131\n",
      "step: 678364, loss: 0.0583689920604229, data time: 0.010060034300151625\n",
      "step: 678365, loss: 0.05686226487159729, data time: 0.009662115573883056\n",
      "step: 678366, loss: 0.0679541602730751, data time: 0.009319725490751721\n",
      "step: 678367, loss: 0.06588166952133179, data time: 0.009010531685569069\n",
      "step: 678368, loss: 0.06131073832511902, data time: 0.008724150450333305\n",
      "step: 678369, loss: 0.06822233647108078, data time: 0.008481740951538086\n",
      "step: 678370, loss: 0.06293362379074097, data time: 0.00824188232421875\n",
      "step: 678371, loss: 0.05606508255004883, data time: 0.008016641323383037\n",
      "step: 678372, loss: 0.06049133464694023, data time: 0.0078078729135018805\n",
      "step: 678373, loss: 0.0643160492181778, data time: 0.007616034575871059\n",
      "step: 678374, loss: 0.062291115522384644, data time: 0.007441150731053846\n",
      "step: 678375, loss: 0.06500944495201111, data time: 0.007277774810791016\n",
      "step: 678376, loss: 0.061118870973587036, data time: 0.007110672612344065\n",
      "step: 678377, loss: 0.063758984208107, data time: 0.006962783634662628\n",
      "step: 678378, loss: 0.06293484568595886, data time: 0.006813620076035008\n",
      "step: 678379, loss: 0.055223703384399414, data time: 0.006673322004430434\n",
      "step: 678380, loss: 0.06190606206655502, data time: 0.006540897914341518\n",
      "step: 678381, loss: 0.06533603370189667, data time: 0.00641195641623603\n",
      "step: 678382, loss: 0.06456206738948822, data time: 0.006290435791015625\n",
      "step: 678383, loss: 0.0619894377887249, data time: 0.006178228478682668\n",
      "step: 678384, loss: 0.060782723128795624, data time: 0.006071842633760893\n",
      "step: 678385, loss: 0.07387866824865341, data time: 0.005970579385757446\n",
      "step: 678386, loss: 0.05996866151690483, data time: 0.16011667251586914\n",
      "step: 678387, loss: 0.05935203284025192, data time: 0.08097565174102783\n",
      "step: 678388, loss: 0.06149819493293762, data time: 0.055198113123575844\n",
      "step: 678389, loss: 0.06846370548009872, data time: 0.04176884889602661\n",
      "step: 678390, loss: 0.05673620104789734, data time: 0.03369741439819336\n",
      "step: 678391, loss: 0.0645691379904747, data time: 0.02832182248433431\n",
      "step: 678392, loss: 0.06545957922935486, data time: 0.02458909579685756\n",
      "step: 678393, loss: 0.05764351040124893, data time: 0.021683186292648315\n",
      "step: 678394, loss: 0.06603780388832092, data time: 0.01950769954257541\n",
      "step: 678395, loss: 0.06514071673154831, data time: 0.017755007743835448\n",
      "step: 678396, loss: 0.060820214450359344, data time: 0.016334251923994583\n",
      "step: 678397, loss: 0.0566861554980278, data time: 0.015162527561187744\n",
      "step: 678398, loss: 0.056181650608778, data time: 0.014171930459829478\n",
      "step: 678399, loss: 0.06229376792907715, data time: 0.01330392701285226\n",
      "step: 678400, loss: 0.056996606290340424, data time: 0.012555742263793945\n",
      "step: 678401, loss: 0.05952101945877075, data time: 0.011903032660484314\n",
      "step: 678402, loss: 0.06181119382381439, data time: 0.011319062289069681\n",
      "step: 678403, loss: 0.06228918582201004, data time: 0.010814163419935439\n",
      "step: 678404, loss: 0.06265250593423843, data time: 0.010351607674046567\n",
      "step: 678405, loss: 0.0600728914141655, data time: 0.009951794147491455\n",
      "step: 678406, loss: 0.059392061084508896, data time: 0.00958417710803804\n",
      "step: 678407, loss: 0.062338657677173615, data time: 0.00924703207882968\n",
      "step: 678408, loss: 0.06651752442121506, data time: 0.008935192356938902\n",
      "step: 678409, loss: 0.06280453503131866, data time: 0.008648296197255453\n",
      "step: 678410, loss: 0.06042664498090744, data time: 0.00839371681213379\n",
      "step: 678411, loss: 0.060934778302907944, data time: 0.008150834303635817\n",
      "step: 678412, loss: 0.0648636519908905, data time: 0.007925342630456996\n",
      "step: 678413, loss: 0.06382846087217331, data time: 0.0077122535024370465\n",
      "step: 678414, loss: 0.06736951321363449, data time: 0.007521423800238247\n",
      "step: 678415, loss: 0.05929286405444145, data time: 0.007343347867329915\n",
      "step: 678416, loss: 0.06250421702861786, data time: 0.007176260794362714\n",
      "step: 678417, loss: 0.06164216995239258, data time: 0.007023580372333527\n",
      "step: 678418, loss: 0.05945442244410515, data time: 0.006869438922766483\n",
      "step: 678419, loss: 0.05618277192115784, data time: 0.006727765588199391\n",
      "step: 678420, loss: 0.06152021884918213, data time: 0.006593649727957589\n",
      "step: 678421, loss: 0.06153973191976547, data time: 0.006466269493103027\n",
      "step: 678422, loss: 0.057192813605070114, data time: 0.006345484707806562\n",
      "step: 678423, loss: 0.05987445265054703, data time: 0.006234313312329744\n",
      "step: 678424, loss: 0.06361621618270874, data time: 0.00613107436742538\n",
      "step: 678425, loss: 0.07905684411525726, data time: 0.006032413244247437\n",
      "step: 678426, loss: 0.0667143166065216, data time: 0.1676015853881836\n",
      "step: 678427, loss: 0.06109040975570679, data time: 0.0851588249206543\n",
      "step: 678428, loss: 0.05615384131669998, data time: 0.05768092473347982\n",
      "step: 678429, loss: 0.06630799174308777, data time: 0.044013917446136475\n",
      "step: 678430, loss: 0.060003094375133514, data time: 0.03548951148986816\n",
      "step: 678431, loss: 0.055793970823287964, data time: 0.029816746711730957\n",
      "step: 678432, loss: 0.06074697896838188, data time: 0.025783470698765347\n",
      "step: 678433, loss: 0.06423085927963257, data time: 0.022810935974121094\n",
      "step: 678434, loss: 0.06637232005596161, data time: 0.02043257819281684\n",
      "step: 678435, loss: 0.06359679996967316, data time: 0.018589305877685546\n",
      "step: 678436, loss: 0.06663345545530319, data time: 0.017096844586459072\n",
      "step: 678437, loss: 0.06275619566440582, data time: 0.015854299068450928\n",
      "step: 678438, loss: 0.06388238072395325, data time: 0.014793946192814754\n",
      "step: 678439, loss: 0.06321819126605988, data time: 0.013877170426504952\n",
      "step: 678440, loss: 0.062097251415252686, data time: 0.013088734944661458\n",
      "step: 678441, loss: 0.06368666887283325, data time: 0.01240558922290802\n",
      "step: 678442, loss: 0.05980836972594261, data time: 0.011799279381247127\n",
      "step: 678443, loss: 0.06113576143980026, data time: 0.011252588695949979\n",
      "step: 678444, loss: 0.06412625312805176, data time: 0.010769367218017578\n",
      "step: 678445, loss: 0.060612570494413376, data time: 0.010339868068695069\n",
      "step: 678446, loss: 0.06765871495008469, data time: 0.009948253631591797\n",
      "step: 678447, loss: 0.06305305659770966, data time: 0.00959124348380349\n",
      "step: 678448, loss: 0.06230488419532776, data time: 0.009259783703347912\n",
      "step: 678449, loss: 0.06455366313457489, data time: 0.00896035631497701\n",
      "step: 678450, loss: 0.058352820575237274, data time: 0.008689193725585938\n",
      "step: 678451, loss: 0.06354539096355438, data time: 0.008431150363041805\n",
      "step: 678452, loss: 0.059116628021001816, data time: 0.008208124725906938\n",
      "step: 678453, loss: 0.06056199222803116, data time: 0.00799870491027832\n",
      "step: 678454, loss: 0.06762652099132538, data time: 0.007809836288978313\n",
      "step: 678455, loss: 0.06131290644407272, data time: 0.00763547420501709\n",
      "step: 678456, loss: 0.06002017855644226, data time: 0.007467885171213458\n",
      "step: 678457, loss: 0.06635283678770065, data time: 0.007314689457416534\n",
      "step: 678458, loss: 0.054950933903455734, data time: 0.007155830209905451\n",
      "step: 678459, loss: 0.05363820120692253, data time: 0.00700977269340964\n",
      "step: 678460, loss: 0.06211980804800987, data time: 0.006867109026227679\n",
      "step: 678461, loss: 0.062430769205093384, data time: 0.0067301856146918405\n",
      "step: 678462, loss: 0.061681341379880905, data time: 0.0066026352547310495\n",
      "step: 678463, loss: 0.06152811646461487, data time: 0.006485242592661004\n",
      "step: 678464, loss: 0.05918010324239731, data time: 0.006374084032498873\n",
      "step: 678465, loss: 0.05911669880151749, data time: 0.006267583370208741\n",
      "step: 678466, loss: 0.0592622309923172, data time: 0.170548677444458\n",
      "step: 678467, loss: 0.062491338700056076, data time: 0.08606767654418945\n",
      "step: 678468, loss: 0.055072933435440063, data time: 0.0583038330078125\n",
      "step: 678469, loss: 0.07114323228597641, data time: 0.044494688510894775\n",
      "step: 678470, loss: 0.06289595365524292, data time: 0.03587627410888672\n",
      "step: 678471, loss: 0.059817444533109665, data time: 0.030143221219380695\n",
      "step: 678472, loss: 0.06601440906524658, data time: 0.02605945723397391\n",
      "step: 678473, loss: 0.05593816563487053, data time: 0.023058533668518066\n",
      "step: 678474, loss: 0.06759512424468994, data time: 0.020645962821112737\n",
      "step: 678475, loss: 0.06916438788175583, data time: 0.018783378601074218\n",
      "step: 678476, loss: 0.06310828775167465, data time: 0.017272862521084873\n",
      "step: 678477, loss: 0.06996836513280869, data time: 0.016019701957702637\n",
      "step: 678478, loss: 0.05970318242907524, data time: 0.014957611377422627\n",
      "step: 678479, loss: 0.06174737215042114, data time: 0.01403982298714774\n",
      "step: 678480, loss: 0.06375470012426376, data time: 0.013239049911499023\n",
      "step: 678481, loss: 0.05657416582107544, data time: 0.012568265199661255\n",
      "step: 678482, loss: 0.06087943911552429, data time: 0.011974488987642177\n",
      "step: 678483, loss: 0.06127236410975456, data time: 0.011438820097181533\n",
      "step: 678484, loss: 0.0627884715795517, data time: 0.010960566370110763\n",
      "step: 678485, loss: 0.06742250919342041, data time: 0.01054055690765381\n",
      "step: 678486, loss: 0.06218148022890091, data time: 0.010154315403529577\n",
      "step: 678487, loss: 0.06378263235092163, data time: 0.009788805788213556\n",
      "step: 678488, loss: 0.0632205456495285, data time: 0.009448424629543139\n",
      "step: 678489, loss: 0.061875056475400925, data time: 0.009144286314646402\n",
      "step: 678490, loss: 0.06046280264854431, data time: 0.008864707946777343\n",
      "step: 678491, loss: 0.06204969808459282, data time: 0.008602610001197228\n",
      "step: 678492, loss: 0.061375316232442856, data time: 0.008355290801436812\n",
      "step: 678493, loss: 0.05963923782110214, data time: 0.008125288145882743\n",
      "step: 678494, loss: 0.07007700204849243, data time: 0.00791809476655105\n",
      "step: 678495, loss: 0.06113201379776001, data time: 0.007725842793782552\n",
      "step: 678496, loss: 0.05853386968374252, data time: 0.007558899541055003\n",
      "step: 678497, loss: 0.056958429515361786, data time: 0.0073942095041275024\n",
      "step: 678498, loss: 0.06035361438989639, data time: 0.0072314522483132105\n",
      "step: 678499, loss: 0.05730050057172775, data time: 0.0070800570880665496\n",
      "step: 678500, loss: 0.05897381901741028, data time: 0.006932565144130162\n",
      "step: 678501, loss: 0.06204979866743088, data time: 0.0067906975746154785\n",
      "step: 678502, loss: 0.05684613436460495, data time: 0.006656968915784681\n",
      "step: 678503, loss: 0.06291284412145615, data time: 0.006535467348600689\n",
      "step: 678504, loss: 0.0619354173541069, data time: 0.006421303137754783\n",
      "step: 678505, loss: 0.05158037692308426, data time: 0.006310844421386718\n",
      "step: 678506, loss: 0.05615764483809471, data time: 0.141096830368042\n",
      "step: 678507, loss: 0.060948632657527924, data time: 0.07207345962524414\n",
      "step: 678508, loss: 0.06486885994672775, data time: 0.04879577954610189\n",
      "step: 678509, loss: 0.062031980603933334, data time: 0.03737431764602661\n",
      "step: 678510, loss: 0.05916067957878113, data time: 0.03016490936279297\n",
      "step: 678511, loss: 0.059976473450660706, data time: 0.025409936904907227\n",
      "step: 678512, loss: 0.06075754761695862, data time: 0.021993228367396762\n",
      "step: 678513, loss: 0.06320495903491974, data time: 0.01949608325958252\n",
      "step: 678514, loss: 0.07146976888179779, data time: 0.017476346757676866\n",
      "step: 678515, loss: 0.06206081435084343, data time: 0.01593458652496338\n",
      "step: 678516, loss: 0.0581309050321579, data time: 0.014680363915183327\n",
      "step: 678517, loss: 0.06074313446879387, data time: 0.013647675514221191\n",
      "step: 678518, loss: 0.06574783474206924, data time: 0.01276392203110915\n",
      "step: 678519, loss: 0.06315448880195618, data time: 0.011995809418814523\n",
      "step: 678520, loss: 0.05941479653120041, data time: 0.011341571807861328\n",
      "step: 678521, loss: 0.060050155967473984, data time: 0.010758176445960999\n",
      "step: 678522, loss: 0.059993844479322433, data time: 0.010249502518597771\n",
      "step: 678523, loss: 0.07184936851263046, data time: 0.009791798061794706\n",
      "step: 678524, loss: 0.06124691292643547, data time: 0.009386112815455386\n",
      "step: 678525, loss: 0.062075432389974594, data time: 0.009023678302764893\n",
      "step: 678526, loss: 0.06383584439754486, data time: 0.008695216405959357\n",
      "step: 678527, loss: 0.05957075208425522, data time: 0.008404677564447577\n",
      "step: 678528, loss: 0.06198021396994591, data time: 0.008125533228335173\n",
      "step: 678529, loss: 0.06410177052021027, data time: 0.007878661155700684\n",
      "step: 678530, loss: 0.05879281088709831, data time: 0.0076469039916992184\n",
      "step: 678531, loss: 0.06436902284622192, data time: 0.007433240230266864\n",
      "step: 678532, loss: 0.06107527017593384, data time: 0.007231544565271448\n",
      "step: 678533, loss: 0.06114020198583603, data time: 0.007047082696642194\n",
      "step: 678534, loss: 0.0662393718957901, data time: 0.006879140590799266\n",
      "step: 678535, loss: 0.06397871673107147, data time: 0.0067228158315022785\n",
      "step: 678536, loss: 0.06445321440696716, data time: 0.00657574592098113\n",
      "step: 678537, loss: 0.0642039105296135, data time: 0.006439462304115295\n",
      "step: 678538, loss: 0.059082504361867905, data time: 0.006307385184548118\n",
      "step: 678539, loss: 0.06690536439418793, data time: 0.006177818073945887\n",
      "step: 678540, loss: 0.06069464609026909, data time: 0.006055280140468053\n",
      "step: 678541, loss: 0.07208764553070068, data time: 0.005938609441121419\n",
      "step: 678542, loss: 0.059715162962675095, data time: 0.0058290958404541016\n",
      "step: 678543, loss: 0.05837200582027435, data time: 0.005729869792335912\n",
      "step: 678544, loss: 0.061134885996580124, data time: 0.0056389600802690554\n",
      "step: 678545, loss: 0.09435079991817474, data time: 0.005549287796020508\n",
      "step: 678546, loss: 0.06172659248113632, data time: 0.16262483596801758\n",
      "step: 678547, loss: 0.061285194009542465, data time: 0.08268845081329346\n",
      "step: 678548, loss: 0.057252489030361176, data time: 0.055906216303507485\n",
      "step: 678549, loss: 0.06107190251350403, data time: 0.042825937271118164\n",
      "step: 678550, loss: 0.062097810208797455, data time: 0.03454866409301758\n",
      "step: 678551, loss: 0.057992614805698395, data time: 0.029046376546223957\n",
      "step: 678552, loss: 0.05672707408666611, data time: 0.02510932513645717\n",
      "step: 678553, loss: 0.06512843817472458, data time: 0.022221684455871582\n",
      "step: 678554, loss: 0.06130024045705795, data time: 0.019896056916978624\n",
      "step: 678555, loss: 0.06422159075737, data time: 0.01810638904571533\n",
      "step: 678556, loss: 0.06502775847911835, data time: 0.016653732819990677\n",
      "step: 678557, loss: 0.06573191285133362, data time: 0.015449245770772299\n",
      "step: 678558, loss: 0.05776290223002434, data time: 0.014437363697932316\n",
      "step: 678559, loss: 0.059244513511657715, data time: 0.013548459325517927\n",
      "step: 678560, loss: 0.0661662295460701, data time: 0.012785053253173828\n",
      "step: 678561, loss: 0.06335826218128204, data time: 0.01211431622505188\n",
      "step: 678562, loss: 0.06185045465826988, data time: 0.011522152844597311\n",
      "step: 678563, loss: 0.057018741965293884, data time: 0.010989189147949219\n",
      "step: 678564, loss: 0.06732866168022156, data time: 0.010523005535728052\n",
      "step: 678565, loss: 0.060617607086896896, data time: 0.010113191604614259\n",
      "step: 678566, loss: 0.061772413551807404, data time: 0.009738842646280924\n",
      "step: 678567, loss: 0.0646737739443779, data time: 0.009395263411782005\n",
      "step: 678568, loss: 0.060456033796072006, data time: 0.009080368539561396\n",
      "step: 678569, loss: 0.05472332239151001, data time: 0.00878883401552836\n",
      "step: 678570, loss: 0.06594318151473999, data time: 0.00852482795715332\n",
      "step: 678571, loss: 0.06160300225019455, data time: 0.008283037405747633\n",
      "step: 678572, loss: 0.06072790548205376, data time: 0.008053532353153935\n",
      "step: 678573, loss: 0.059223324060440063, data time: 0.007837201867784773\n",
      "step: 678574, loss: 0.062059350311756134, data time: 0.007639819178087958\n",
      "step: 678575, loss: 0.0635242611169815, data time: 0.007455635070800781\n",
      "step: 678576, loss: 0.06406670808792114, data time: 0.0072854565035912295\n",
      "step: 678577, loss: 0.06551456451416016, data time: 0.007128827273845673\n",
      "step: 678578, loss: 0.06316228210926056, data time: 0.006974473144068863\n",
      "step: 678579, loss: 0.05815775692462921, data time: 0.0068297877031214095\n",
      "step: 678580, loss: 0.0529199056327343, data time: 0.006690202440534319\n",
      "step: 678581, loss: 0.06367181241512299, data time: 0.006554378403557671\n",
      "step: 678582, loss: 0.06141163408756256, data time: 0.006427861548758842\n",
      "step: 678583, loss: 0.055234842002391815, data time: 0.006312175800925807\n",
      "step: 678584, loss: 0.0663176104426384, data time: 0.006202813906547351\n",
      "step: 678585, loss: 0.07310417294502258, data time: 0.006099987030029297\n",
      "step: 678586, loss: 0.057834576815366745, data time: 0.17981433868408203\n",
      "step: 678587, loss: 0.058477580547332764, data time: 0.0906592607498169\n",
      "step: 678588, loss: 0.06353475153446198, data time: 0.06148409843444824\n",
      "step: 678589, loss: 0.06378988921642303, data time: 0.04680699110031128\n",
      "step: 678590, loss: 0.05985681340098381, data time: 0.037724637985229494\n",
      "step: 678591, loss: 0.062144890427589417, data time: 0.03169973691304525\n",
      "step: 678592, loss: 0.0658244639635086, data time: 0.02739827973502023\n",
      "step: 678593, loss: 0.05577018857002258, data time: 0.024244368076324463\n",
      "step: 678594, loss: 0.06310027092695236, data time: 0.021699481540256076\n",
      "step: 678595, loss: 0.06223280355334282, data time: 0.01973288059234619\n",
      "step: 678596, loss: 0.060739219188690186, data time: 0.018135460940274326\n",
      "step: 678597, loss: 0.06090432405471802, data time: 0.01680841048558553\n",
      "step: 678598, loss: 0.060137584805488586, data time: 0.015690363370455228\n",
      "step: 678599, loss: 0.06623208522796631, data time: 0.01471970762525286\n",
      "step: 678600, loss: 0.06310371309518814, data time: 0.013876549402872721\n",
      "step: 678601, loss: 0.061097633093595505, data time: 0.013150572776794434\n",
      "step: 678602, loss: 0.06030227988958359, data time: 0.012497509227079503\n",
      "step: 678603, loss: 0.05901734158396721, data time: 0.011916478474934896\n",
      "step: 678604, loss: 0.06380622088909149, data time: 0.01140653459649337\n",
      "step: 678605, loss: 0.06246970221400261, data time: 0.010947477817535401\n",
      "step: 678606, loss: 0.06432448327541351, data time: 0.010527224767775763\n",
      "step: 678607, loss: 0.061035849153995514, data time: 0.010144352912902832\n",
      "step: 678608, loss: 0.06115350127220154, data time: 0.009792835816093113\n",
      "step: 678609, loss: 0.056049250066280365, data time: 0.009474019209543863\n",
      "step: 678610, loss: 0.06221705675125122, data time: 0.009185218811035156\n",
      "step: 678611, loss: 0.05494897812604904, data time: 0.008916643949655386\n",
      "step: 678612, loss: 0.06858079135417938, data time: 0.008659848460444697\n",
      "step: 678613, loss: 0.06731441617012024, data time: 0.008425329412732805\n",
      "step: 678614, loss: 0.06547276675701141, data time: 0.008210009542004815\n",
      "step: 678615, loss: 0.05919231101870537, data time: 0.008008042971293131\n",
      "step: 678616, loss: 0.0654970183968544, data time: 0.007820237067437941\n",
      "step: 678617, loss: 0.06452515721321106, data time: 0.007649749517440796\n",
      "step: 678618, loss: 0.05901942774653435, data time: 0.007478352748986446\n",
      "step: 678619, loss: 0.057577311992645264, data time: 0.007318012854632209\n",
      "step: 678620, loss: 0.06305542588233948, data time: 0.007164376122610909\n",
      "step: 678621, loss: 0.06891941279172897, data time: 0.007016380627950032\n",
      "step: 678622, loss: 0.06547167152166367, data time: 0.0068776994138150605\n",
      "step: 678623, loss: 0.06964483857154846, data time: 0.006762435561732242\n",
      "step: 678624, loss: 0.06617714464664459, data time: 0.006642983509944036\n",
      "step: 678625, loss: 0.054595716297626495, data time: 0.006528908014297485\n",
      "step: 678626, loss: 0.057832591235637665, data time: 0.1821911334991455\n",
      "step: 678627, loss: 0.06562086194753647, data time: 0.09219670295715332\n",
      "step: 678628, loss: 0.06704230606555939, data time: 0.06237014134724935\n",
      "step: 678629, loss: 0.06000242754817009, data time: 0.047585368156433105\n",
      "step: 678630, loss: 0.0555468425154686, data time: 0.03836765289306641\n",
      "step: 678631, loss: 0.06700988113880157, data time: 0.03220637639363607\n",
      "step: 678632, loss: 0.06568273156881332, data time: 0.027819020407540456\n",
      "step: 678633, loss: 0.061857499182224274, data time: 0.024593770503997803\n",
      "step: 678634, loss: 0.06420381367206573, data time: 0.022085322274102107\n",
      "step: 678635, loss: 0.06766917556524277, data time: 0.020093154907226563\n",
      "step: 678636, loss: 0.059886686503887177, data time: 0.01846567067232999\n",
      "step: 678637, loss: 0.06535249948501587, data time: 0.01711020867029826\n",
      "step: 678638, loss: 0.05810035020112991, data time: 0.015961206876314603\n",
      "step: 678639, loss: 0.06031687185168266, data time: 0.014966777392796107\n",
      "step: 678640, loss: 0.05833135172724724, data time: 0.014113855361938477\n",
      "step: 678641, loss: 0.0644969493150711, data time: 0.013366550207138062\n",
      "step: 678642, loss: 0.06190776452422142, data time: 0.01270284372217515\n",
      "step: 678643, loss: 0.06377149373292923, data time: 0.012107743157280816\n",
      "step: 678644, loss: 0.06226895749568939, data time: 0.011574745178222656\n",
      "step: 678645, loss: 0.05615554377436638, data time: 0.011106634140014648\n",
      "step: 678646, loss: 0.06013062223792076, data time: 0.010683127811976842\n",
      "step: 678647, loss: 0.06657256931066513, data time: 0.010298707268454811\n",
      "step: 678648, loss: 0.06065256893634796, data time: 0.00994762130405592\n",
      "step: 678649, loss: 0.05869496986269951, data time: 0.009623448053995768\n",
      "step: 678650, loss: 0.05831503868103027, data time: 0.009324731826782227\n",
      "step: 678651, loss: 0.06307082623243332, data time: 0.00904394113100492\n",
      "step: 678652, loss: 0.06934039294719696, data time: 0.00878411752206308\n",
      "step: 678653, loss: 0.06244353577494621, data time: 0.008541209357125419\n",
      "step: 678654, loss: 0.06252050399780273, data time: 0.008320964615920494\n",
      "step: 678655, loss: 0.05986306816339493, data time: 0.008119551340738933\n",
      "step: 678656, loss: 0.05517251417040825, data time: 0.007926579444639145\n",
      "step: 678657, loss: 0.06029979884624481, data time: 0.007759064435958862\n",
      "step: 678658, loss: 0.0659242570400238, data time: 0.007585670008803859\n",
      "step: 678659, loss: 0.06040045619010925, data time: 0.0074268088621251725\n",
      "step: 678660, loss: 0.056946080178022385, data time: 0.007272134508405413\n",
      "step: 678661, loss: 0.06508944928646088, data time: 0.0071252187093098955\n",
      "step: 678662, loss: 0.06331633031368256, data time: 0.006987346185220254\n",
      "step: 678663, loss: 0.05757421255111694, data time: 0.0068561466116654245\n",
      "step: 678664, loss: 0.06034765765070915, data time: 0.006738589360163762\n",
      "step: 678665, loss: 0.08197829127311707, data time: 0.006620627641677856\n",
      "step: 678666, loss: 0.06463466584682465, data time: 0.17351245880126953\n",
      "step: 678667, loss: 0.057084888219833374, data time: 0.08819842338562012\n",
      "step: 678668, loss: 0.05901431292295456, data time: 0.059688568115234375\n",
      "step: 678669, loss: 0.05724173039197922, data time: 0.045537352561950684\n",
      "step: 678670, loss: 0.06544985622167587, data time: 0.036714792251586914\n",
      "step: 678671, loss: 0.06250935047864914, data time: 0.030838608741760254\n",
      "step: 678672, loss: 0.06406204402446747, data time: 0.026657002312796458\n",
      "step: 678673, loss: 0.06094970926642418, data time: 0.023583054542541504\n",
      "step: 678674, loss: 0.05728659778833389, data time: 0.021106481552124023\n",
      "step: 678675, loss: 0.05678616091609001, data time: 0.019199490547180176\n",
      "step: 678676, loss: 0.06208769232034683, data time: 0.01766428080472079\n",
      "step: 678677, loss: 0.05847173184156418, data time: 0.016375958919525146\n",
      "step: 678678, loss: 0.05859197676181793, data time: 0.015278155987079326\n",
      "step: 678679, loss: 0.059529826045036316, data time: 0.01432973997933524\n",
      "step: 678680, loss: 0.06588461995124817, data time: 0.013511991500854493\n",
      "step: 678681, loss: 0.06432686746120453, data time: 0.012812703847885132\n",
      "step: 678682, loss: 0.060542091727256775, data time: 0.012175994760849896\n",
      "step: 678683, loss: 0.06407397985458374, data time: 0.011607090632120768\n",
      "step: 678684, loss: 0.06535670161247253, data time: 0.011103291260568719\n",
      "step: 678685, loss: 0.06065509468317032, data time: 0.010655379295349121\n",
      "step: 678686, loss: 0.07104475051164627, data time: 0.010249785014561244\n",
      "step: 678687, loss: 0.06288395076990128, data time: 0.009880477731878107\n",
      "step: 678688, loss: 0.06213393062353134, data time: 0.009545388429061226\n",
      "step: 678689, loss: 0.06835800409317017, data time: 0.009232292572657267\n",
      "step: 678690, loss: 0.061658572405576706, data time: 0.008948526382446288\n",
      "step: 678691, loss: 0.06731925904750824, data time: 0.008682471055250902\n",
      "step: 678692, loss: 0.05829446762800217, data time: 0.008434348636203341\n",
      "step: 678693, loss: 0.062280066311359406, data time: 0.008211893694741386\n",
      "step: 678694, loss: 0.06147744879126549, data time: 0.008001360399969694\n",
      "step: 678695, loss: 0.06563328951597214, data time: 0.007805562019348145\n",
      "step: 678696, loss: 0.059055671095848083, data time: 0.007621580554592994\n",
      "step: 678697, loss: 0.06340615451335907, data time: 0.007452704012393951\n",
      "step: 678698, loss: 0.06399937719106674, data time: 0.007284446196122603\n",
      "step: 678699, loss: 0.058130405843257904, data time: 0.007130153038922478\n",
      "step: 678700, loss: 0.0562601201236248, data time: 0.006981079918997628\n",
      "step: 678701, loss: 0.05857732146978378, data time: 0.006838242212931315\n",
      "step: 678702, loss: 0.05883896350860596, data time: 0.006707481435827307\n",
      "step: 678703, loss: 0.06608011573553085, data time: 0.006584217673853824\n",
      "step: 678704, loss: 0.06537595391273499, data time: 0.006466394815689478\n",
      "step: 678705, loss: 0.05469149351119995, data time: 0.006354892253875732\n",
      "step: 678706, loss: 0.06665974855422974, data time: 0.1738126277923584\n",
      "step: 678707, loss: 0.05948122590780258, data time: 0.08766615390777588\n",
      "step: 678708, loss: 0.06314090639352798, data time: 0.058954477310180664\n",
      "step: 678709, loss: 0.06575003266334534, data time: 0.044996023178100586\n",
      "step: 678710, loss: 0.06692823767662048, data time: 0.03627958297729492\n",
      "step: 678711, loss: 0.05684609338641167, data time: 0.030469298362731934\n",
      "step: 678712, loss: 0.05797117203474045, data time: 0.026327541896275113\n",
      "step: 678713, loss: 0.0662696436047554, data time: 0.02330157160758972\n",
      "step: 678714, loss: 0.06649482250213623, data time: 0.02085799641079373\n",
      "step: 678715, loss: 0.060452353209257126, data time: 0.018975996971130372\n",
      "step: 678716, loss: 0.06330032646656036, data time: 0.0174520882693204\n",
      "step: 678717, loss: 0.06185157224535942, data time: 0.016176859537760418\n",
      "step: 678718, loss: 0.05915617197751999, data time: 0.015094830439640926\n",
      "step: 678719, loss: 0.06655842065811157, data time: 0.014159577233450753\n",
      "step: 678720, loss: 0.06473154574632645, data time: 0.013354746500651042\n",
      "step: 678721, loss: 0.061949215829372406, data time: 0.012648820877075195\n",
      "step: 678722, loss: 0.063094362616539, data time: 0.012033336302813362\n",
      "step: 678723, loss: 0.0627535879611969, data time: 0.01147649023267958\n",
      "step: 678724, loss: 0.061264656484127045, data time: 0.010977443895841899\n",
      "step: 678725, loss: 0.0635998323559761, data time: 0.01053624153137207\n",
      "step: 678726, loss: 0.049143001437187195, data time: 0.010138523010980515\n",
      "step: 678727, loss: 0.06488567590713501, data time: 0.009778488766063343\n",
      "step: 678728, loss: 0.06629373878240585, data time: 0.009446403254633364\n",
      "step: 678729, loss: 0.06895018368959427, data time: 0.009137948354085287\n",
      "step: 678730, loss: 0.06846287101507187, data time: 0.008856353759765625\n",
      "step: 678731, loss: 0.06916430592536926, data time: 0.00859689712524414\n",
      "step: 678732, loss: 0.06013576313853264, data time: 0.008351299497816298\n",
      "step: 678733, loss: 0.06334272027015686, data time: 0.008124266351972307\n",
      "step: 678734, loss: 0.05569048225879669, data time: 0.007920002115183863\n",
      "step: 678735, loss: 0.05927396938204765, data time: 0.007730380694071452\n",
      "step: 678736, loss: 0.07339370250701904, data time: 0.007549301270515688\n",
      "step: 678737, loss: 0.060424529016017914, data time: 0.007383517920970917\n",
      "step: 678738, loss: 0.06467898190021515, data time: 0.007217176032788826\n",
      "step: 678739, loss: 0.06104036420583725, data time: 0.007062673568725586\n",
      "step: 678740, loss: 0.058736756443977356, data time: 0.00692007201058524\n",
      "step: 678741, loss: 0.05780762434005737, data time: 0.00677975681093004\n",
      "step: 678742, loss: 0.058354079723358154, data time: 0.0066496681522678685\n",
      "step: 678743, loss: 0.06492725014686584, data time: 0.006529136707908229\n",
      "step: 678744, loss: 0.061971716582775116, data time: 0.006414933082384941\n",
      "step: 678745, loss: 0.04968909174203873, data time: 0.006306290626525879\n",
      "step: 678746, loss: 0.06112566217780113, data time: 0.17433428764343262\n",
      "step: 678747, loss: 0.06085358187556267, data time: 0.08793330192565918\n",
      "step: 678748, loss: 0.06161743402481079, data time: 0.05940707524617513\n",
      "step: 678749, loss: 0.06003212556242943, data time: 0.04552525281906128\n",
      "step: 678750, loss: 0.06407000124454498, data time: 0.03672599792480469\n",
      "step: 678751, loss: 0.06160217523574829, data time: 0.030852119127909344\n",
      "step: 678752, loss: 0.05912929028272629, data time: 0.02666858264378139\n",
      "step: 678753, loss: 0.05920597165822983, data time: 0.023601949214935303\n",
      "step: 678754, loss: 0.06777844578027725, data time: 0.021134879853990342\n",
      "step: 678755, loss: 0.05695956572890282, data time: 0.019226336479187013\n",
      "step: 678756, loss: 0.06205813214182854, data time: 0.017680948430841618\n",
      "step: 678757, loss: 0.057912372052669525, data time: 0.01638948917388916\n",
      "step: 678758, loss: 0.06605519354343414, data time: 0.015297596271221455\n",
      "step: 678759, loss: 0.06462496519088745, data time: 0.014346054622105189\n",
      "step: 678760, loss: 0.06607779860496521, data time: 0.013530683517456055\n",
      "step: 678761, loss: 0.06456860899925232, data time: 0.012820109724998474\n",
      "step: 678762, loss: 0.06325836479663849, data time: 0.012201575671925265\n",
      "step: 678763, loss: 0.06094425916671753, data time: 0.01164241631825765\n",
      "step: 678764, loss: 0.0676012709736824, data time: 0.011139568529630961\n",
      "step: 678765, loss: 0.065061055123806, data time: 0.010692059993743896\n",
      "step: 678766, loss: 0.058163922280073166, data time: 0.010286501475742884\n",
      "step: 678767, loss: 0.06263096630573273, data time: 0.00991721586747603\n",
      "step: 678768, loss: 0.05776647478342056, data time: 0.009575750516808552\n",
      "step: 678769, loss: 0.05905947834253311, data time: 0.009265075127283732\n",
      "step: 678770, loss: 0.06604129076004028, data time: 0.008980264663696289\n",
      "step: 678771, loss: 0.06253637373447418, data time: 0.008717032579275278\n",
      "step: 678772, loss: 0.06128127872943878, data time: 0.00846776255854854\n",
      "step: 678773, loss: 0.05995511636137962, data time: 0.008236970220293318\n",
      "step: 678774, loss: 0.05753075331449509, data time: 0.008032478135207603\n",
      "step: 678775, loss: 0.06005839630961418, data time: 0.007852816581726074\n",
      "step: 678776, loss: 0.0558982789516449, data time: 0.007681938909715222\n",
      "step: 678777, loss: 0.06104201823472977, data time: 0.007522903382778168\n",
      "step: 678778, loss: 0.06584961712360382, data time: 0.007361968358357747\n",
      "step: 678779, loss: 0.06337036937475204, data time: 0.0072097848443424\n",
      "step: 678780, loss: 0.05915379524230957, data time: 0.007064989634922573\n",
      "step: 678781, loss: 0.06284007430076599, data time: 0.0069242848290337455\n",
      "step: 678782, loss: 0.05834738537669182, data time: 0.006792268237552127\n",
      "step: 678783, loss: 0.06289605796337128, data time: 0.006670336974294562\n",
      "step: 678784, loss: 0.05609974265098572, data time: 0.006553961680485652\n",
      "step: 678785, loss: 0.05743124336004257, data time: 0.006445020437240601\n",
      "step: 678786, loss: 0.06111876294016838, data time: 0.17426443099975586\n",
      "step: 678787, loss: 0.0600472092628479, data time: 0.08845770359039307\n",
      "step: 678788, loss: 0.058997731655836105, data time: 0.05987230936686198\n",
      "step: 678789, loss: 0.05814921110868454, data time: 0.04586726427078247\n",
      "step: 678790, loss: 0.061678677797317505, data time: 0.03704257011413574\n",
      "step: 678791, loss: 0.06254179030656815, data time: 0.031176050504048664\n",
      "step: 678792, loss: 0.06462135910987854, data time: 0.02697171483721052\n",
      "step: 678793, loss: 0.05798449367284775, data time: 0.023911625146865845\n",
      "step: 678794, loss: 0.05890124291181564, data time: 0.02143253220452203\n",
      "step: 678795, loss: 0.06444944441318512, data time: 0.019533634185791016\n",
      "step: 678796, loss: 0.0636078268289566, data time: 0.017984867095947266\n",
      "step: 678797, loss: 0.057724036276340485, data time: 0.01669927438100179\n",
      "step: 678798, loss: 0.05686819180846214, data time: 0.015611355121319111\n",
      "step: 678799, loss: 0.060549795627593994, data time: 0.01467132568359375\n",
      "step: 678800, loss: 0.05638431757688522, data time: 0.013863865534464519\n",
      "step: 678801, loss: 0.06436675041913986, data time: 0.013153076171875\n",
      "step: 678802, loss: 0.06442862749099731, data time: 0.012525249929989086\n",
      "step: 678803, loss: 0.05652487277984619, data time: 0.011960705121358236\n",
      "step: 678804, loss: 0.06123729795217514, data time: 0.011456941303453948\n",
      "step: 678805, loss: 0.06271277368068695, data time: 0.011015355587005615\n",
      "step: 678806, loss: 0.05975417420268059, data time: 0.010611931482950846\n",
      "step: 678807, loss: 0.0655025988817215, data time: 0.010241562669927423\n",
      "step: 678808, loss: 0.05921679735183716, data time: 0.009897885115250298\n",
      "step: 678809, loss: 0.0596705786883831, data time: 0.00958980123202006\n",
      "step: 678810, loss: 0.06079588457942009, data time: 0.009307889938354493\n",
      "step: 678811, loss: 0.06706926226615906, data time: 0.009047389030456543\n",
      "step: 678812, loss: 0.06325021386146545, data time: 0.008801875291047272\n",
      "step: 678813, loss: 0.05917605757713318, data time: 0.008573753493172782\n",
      "step: 678814, loss: 0.05948653072118759, data time: 0.00836260565396013\n",
      "step: 678815, loss: 0.0594727098941803, data time: 0.008167537053426106\n",
      "step: 678816, loss: 0.05858966335654259, data time: 0.00798639174430601\n",
      "step: 678817, loss: 0.05941399186849594, data time: 0.007817372679710388\n",
      "step: 678818, loss: 0.062193527817726135, data time: 0.007647839459505948\n",
      "step: 678819, loss: 0.06755484640598297, data time: 0.007487023577970617\n",
      "step: 678820, loss: 0.0600670725107193, data time: 0.007335042953491211\n",
      "step: 678821, loss: 0.05826001986861229, data time: 0.007187644640604655\n",
      "step: 678822, loss: 0.0639718547463417, data time: 0.007047865841839765\n",
      "step: 678823, loss: 0.0641309916973114, data time: 0.006918141716404965\n",
      "step: 678824, loss: 0.059607308357954025, data time: 0.006795693666507036\n",
      "step: 678825, loss: 0.05592411011457443, data time: 0.006679320335388183\n",
      "step: 678826, loss: 0.05794757604598999, data time: 0.1710665225982666\n",
      "step: 678827, loss: 0.058029405772686005, data time: 0.0863029956817627\n",
      "step: 678828, loss: 0.05968617647886276, data time: 0.058056036631266274\n",
      "step: 678829, loss: 0.06434620916843414, data time: 0.04429817199707031\n",
      "step: 678830, loss: 0.05624256655573845, data time: 0.03574080467224121\n",
      "step: 678831, loss: 0.06067141145467758, data time: 0.03003982702891032\n",
      "step: 678832, loss: 0.06372478604316711, data time: 0.025971957615443637\n",
      "step: 678833, loss: 0.06429031491279602, data time: 0.023000657558441162\n",
      "step: 678834, loss: 0.06232191249728203, data time: 0.020597722795274522\n",
      "step: 678835, loss: 0.06392906606197357, data time: 0.018744349479675293\n",
      "step: 678836, loss: 0.05917477607727051, data time: 0.017233458432284268\n",
      "step: 678837, loss: 0.060989558696746826, data time: 0.01597350835800171\n",
      "step: 678838, loss: 0.06155373156070709, data time: 0.014906571461604191\n",
      "step: 678839, loss: 0.053012169897556305, data time: 0.01398641722542899\n",
      "step: 678840, loss: 0.060460954904556274, data time: 0.013195435206095377\n",
      "step: 678841, loss: 0.05507656931877136, data time: 0.01250353455543518\n",
      "step: 678842, loss: 0.05101880058646202, data time: 0.011891421149758732\n",
      "step: 678843, loss: 0.05686930939555168, data time: 0.011340128050910102\n",
      "step: 678844, loss: 0.061350323259830475, data time: 0.010847254803306177\n",
      "step: 678845, loss: 0.061695173382759094, data time: 0.010411989688873292\n",
      "step: 678846, loss: 0.06129752844572067, data time: 0.010022186097644624\n",
      "step: 678847, loss: 0.05857693403959274, data time: 0.00966710394079035\n",
      "step: 678848, loss: 0.058869898319244385, data time: 0.009336046550584875\n",
      "step: 678849, loss: 0.06360280513763428, data time: 0.00904590884844462\n",
      "step: 678850, loss: 0.05588022992014885, data time: 0.008769454956054688\n",
      "step: 678851, loss: 0.057517170906066895, data time: 0.008516018207256611\n",
      "step: 678852, loss: 0.05820778012275696, data time: 0.00827495257059733\n",
      "step: 678853, loss: 0.0635211318731308, data time: 0.008055048329489571\n",
      "step: 678854, loss: 0.054376885294914246, data time: 0.007853327126338565\n",
      "step: 678855, loss: 0.060517966747283936, data time: 0.007662145296732584\n",
      "step: 678856, loss: 0.06093708425760269, data time: 0.00748287477800923\n",
      "step: 678857, loss: 0.06205323338508606, data time: 0.007319927215576172\n",
      "step: 678858, loss: 0.06626776605844498, data time: 0.007160728627985174\n",
      "step: 678859, loss: 0.062044136226177216, data time: 0.007013860870810116\n",
      "step: 678860, loss: 0.06763246655464172, data time: 0.006875051770891462\n",
      "step: 678861, loss: 0.06379620730876923, data time: 0.0067370931307474775\n",
      "step: 678862, loss: 0.059793539345264435, data time: 0.006606166427199905\n",
      "step: 678863, loss: 0.06640604883432388, data time: 0.006484715562117727\n",
      "step: 678864, loss: 0.06699065864086151, data time: 0.006369951443794446\n",
      "step: 678865, loss: 0.04795939847826958, data time: 0.006261491775512695\n",
      "step: 678866, loss: 0.06093558669090271, data time: 0.17358040809631348\n",
      "step: 678867, loss: 0.060504160821437836, data time: 0.08754932880401611\n",
      "step: 678868, loss: 0.059298813343048096, data time: 0.05939038594563802\n",
      "step: 678869, loss: 0.0660591572523117, data time: 0.04520910978317261\n",
      "step: 678870, loss: 0.0612366646528244, data time: 0.03647289276123047\n",
      "step: 678871, loss: 0.05720070004463196, data time: 0.030643184979756672\n",
      "step: 678872, loss: 0.06339426338672638, data time: 0.026496103831699917\n",
      "step: 678873, loss: 0.06069735810160637, data time: 0.02345702052116394\n",
      "step: 678874, loss: 0.06299012154340744, data time: 0.02101702160305447\n",
      "step: 678875, loss: 0.06441523134708405, data time: 0.019124150276184082\n",
      "step: 678876, loss: 0.06811890751123428, data time: 0.01761494983326305\n",
      "step: 678877, loss: 0.06280715763568878, data time: 0.016327182451883953\n",
      "step: 678878, loss: 0.05837254971265793, data time: 0.015244740706223708\n",
      "step: 678879, loss: 0.06473226845264435, data time: 0.014303326606750488\n",
      "step: 678880, loss: 0.06612595170736313, data time: 0.013501977920532227\n",
      "step: 678881, loss: 0.053884923458099365, data time: 0.012797877192497253\n",
      "step: 678882, loss: 0.06335066258907318, data time: 0.012172965442433077\n",
      "step: 678883, loss: 0.06059906631708145, data time: 0.011607090632120768\n",
      "step: 678884, loss: 0.05875731259584427, data time: 0.011107620440031352\n",
      "step: 678885, loss: 0.06271596252918243, data time: 0.01065971851348877\n",
      "step: 678886, loss: 0.06186114251613617, data time: 0.01025783447992234\n",
      "step: 678887, loss: 0.061853110790252686, data time: 0.00989168340509588\n",
      "step: 678888, loss: 0.06205702945590019, data time: 0.009553380634473719\n",
      "step: 678889, loss: 0.06410835683345795, data time: 0.009249975283940634\n",
      "step: 678890, loss: 0.06584300845861435, data time: 0.008967981338500977\n",
      "step: 678891, loss: 0.06176255643367767, data time: 0.008705230859609751\n",
      "step: 678892, loss: 0.06716472655534744, data time: 0.00845762535377785\n",
      "step: 678893, loss: 0.05443865805864334, data time: 0.00822864259992327\n",
      "step: 678894, loss: 0.059239160269498825, data time: 0.008023229138604525\n",
      "step: 678895, loss: 0.06501065194606781, data time: 0.007827520370483398\n",
      "step: 678896, loss: 0.06527499854564667, data time: 0.007655435992825416\n",
      "step: 678897, loss: 0.054998185485601425, data time: 0.007498443126678467\n",
      "step: 678898, loss: 0.06083226948976517, data time: 0.007336941632357511\n",
      "step: 678899, loss: 0.06279470026493073, data time: 0.007185052422916188\n",
      "step: 678900, loss: 0.062015872448682785, data time: 0.007040521076747349\n",
      "step: 678901, loss: 0.06251004338264465, data time: 0.006900409857432048\n",
      "step: 678902, loss: 0.058657705783843994, data time: 0.006769805341153531\n",
      "step: 678903, loss: 0.05993051454424858, data time: 0.006647586822509766\n",
      "step: 678904, loss: 0.0638590157032013, data time: 0.006532412308913011\n",
      "step: 678905, loss: 0.05678406357765198, data time: 0.006422191858291626\n",
      "step: 678906, loss: 0.05976715683937073, data time: 0.18767046928405762\n",
      "step: 678907, loss: 0.05987253785133362, data time: 0.09456861019134521\n",
      "step: 678908, loss: 0.05749194324016571, data time: 0.06396047274271648\n",
      "step: 678909, loss: 0.060339730232954025, data time: 0.04863172769546509\n",
      "step: 678910, loss: 0.06345489621162415, data time: 0.03920564651489258\n",
      "step: 678911, loss: 0.062161654233932495, data time: 0.03293744723002116\n",
      "step: 678912, loss: 0.06707239151000977, data time: 0.028448070798601423\n",
      "step: 678913, loss: 0.061510901898145676, data time: 0.025154709815979004\n",
      "step: 678914, loss: 0.05679358169436455, data time: 0.022591643863254122\n",
      "step: 678915, loss: 0.05717922002077103, data time: 0.02054860591888428\n",
      "step: 678916, loss: 0.05774427950382233, data time: 0.01887598904696378\n",
      "step: 678917, loss: 0.06079583615064621, data time: 0.017483115196228027\n",
      "step: 678918, loss: 0.06096529960632324, data time: 0.016304878088144157\n",
      "step: 678919, loss: 0.060622360557317734, data time: 0.015288829803466797\n",
      "step: 678920, loss: 0.06378400325775146, data time: 0.01444093386332194\n",
      "step: 678921, loss: 0.05896661803126335, data time: 0.013671308755874634\n",
      "step: 678922, loss: 0.06144550442695618, data time: 0.012991764966179343\n",
      "step: 678923, loss: 0.06484436988830566, data time: 0.0123798582288954\n",
      "step: 678924, loss: 0.06402449309825897, data time: 0.011831634923031456\n",
      "step: 678925, loss: 0.06171613186597824, data time: 0.011346888542175294\n",
      "step: 678926, loss: 0.06421932578086853, data time: 0.010927983692714147\n",
      "step: 678927, loss: 0.06109724938869476, data time: 0.010544083335182884\n",
      "step: 678928, loss: 0.06238502264022827, data time: 0.010190663130387016\n",
      "step: 678929, loss: 0.06064465641975403, data time: 0.009868810574213663\n",
      "step: 678930, loss: 0.06378635764122009, data time: 0.00957376480102539\n",
      "step: 678931, loss: 0.05764159560203552, data time: 0.009299956835233249\n",
      "step: 678932, loss: 0.058214977383613586, data time: 0.00904475318060981\n",
      "step: 678933, loss: 0.06111789867281914, data time: 0.008808084896632604\n",
      "step: 678934, loss: 0.062383364886045456, data time: 0.008589292394703832\n",
      "step: 678935, loss: 0.07421445101499557, data time: 0.008385801315307617\n",
      "step: 678936, loss: 0.06236088275909424, data time: 0.008195054146551316\n",
      "step: 678937, loss: 0.0642370879650116, data time: 0.00802302360534668\n",
      "step: 678938, loss: 0.059736475348472595, data time: 0.007846015872377338\n",
      "step: 678939, loss: 0.06273025274276733, data time: 0.0076786630293902225\n",
      "step: 678940, loss: 0.05421943962574005, data time: 0.0075196198054722375\n",
      "step: 678941, loss: 0.06173668056726456, data time: 0.00736624002456665\n",
      "step: 678942, loss: 0.06420071423053741, data time: 0.007221969398292336\n",
      "step: 678943, loss: 0.05643275007605553, data time: 0.007087833002993935\n",
      "step: 678944, loss: 0.05689360946416855, data time: 0.006960190259493315\n",
      "step: 678945, loss: 0.047493211925029755, data time: 0.006839263439178467\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4093017578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 678946, loss: 0.06198719143867493, data time: 0.1711561679840088\n",
      "step: 678947, loss: 0.06297548860311508, data time: 0.08697021007537842\n",
      "step: 678948, loss: 0.06463517248630524, data time: 0.05903871854146322\n",
      "step: 678949, loss: 0.06473414599895477, data time: 0.04497098922729492\n",
      "step: 678950, loss: 0.06279145181179047, data time: 0.03625059127807617\n",
      "step: 678951, loss: 0.06493528187274933, data time: 0.03046262264251709\n",
      "step: 678952, loss: 0.05912159010767937, data time: 0.02630720819745745\n",
      "step: 678953, loss: 0.06593553721904755, data time: 0.023274600505828857\n",
      "step: 678954, loss: 0.06393687427043915, data time: 0.020836220847235784\n",
      "step: 678955, loss: 0.06510508060455322, data time: 0.018966364860534667\n",
      "step: 678956, loss: 0.05816800519824028, data time: 0.017435962503606624\n",
      "step: 678957, loss: 0.06395342946052551, data time: 0.016157567501068115\n",
      "step: 678958, loss: 0.06221110746264458, data time: 0.015081240580632137\n",
      "step: 678959, loss: 0.0651901364326477, data time: 0.014145561626979284\n",
      "step: 678960, loss: 0.06681369990110397, data time: 0.013355604807535807\n",
      "step: 678961, loss: 0.060495179146528244, data time: 0.012646928429603577\n",
      "step: 678962, loss: 0.05960792303085327, data time: 0.012030489304486443\n",
      "step: 678963, loss: 0.06784778833389282, data time: 0.01147052976820204\n",
      "step: 678964, loss: 0.06998474895954132, data time: 0.010975837707519531\n",
      "step: 678965, loss: 0.0598575696349144, data time: 0.010535478591918945\n",
      "step: 678966, loss: 0.06231332942843437, data time: 0.010134674253917876\n",
      "step: 678967, loss: 0.06616196036338806, data time: 0.009771206162192604\n",
      "step: 678968, loss: 0.056099891662597656, data time: 0.009435435999994692\n",
      "step: 678969, loss: 0.0627140924334526, data time: 0.00912897785504659\n",
      "step: 678970, loss: 0.05639391392469406, data time: 0.008843889236450195\n",
      "step: 678971, loss: 0.06311942636966705, data time: 0.008579300000117375\n",
      "step: 678972, loss: 0.055209316313266754, data time: 0.008332950097543222\n",
      "step: 678973, loss: 0.06557174026966095, data time: 0.00810995272227696\n",
      "step: 678974, loss: 0.06155044957995415, data time: 0.007907530357097757\n",
      "step: 678975, loss: 0.05997857451438904, data time: 0.007715527216593424\n",
      "step: 678976, loss: 0.05836695060133934, data time: 0.007534496245845672\n",
      "step: 678977, loss: 0.060557764023542404, data time: 0.007368937134742737\n",
      "step: 678978, loss: 0.06105152517557144, data time: 0.007205132282141483\n",
      "step: 678979, loss: 0.06503678113222122, data time: 0.007048691020292395\n",
      "step: 678980, loss: 0.06262905150651932, data time: 0.006900460379464286\n",
      "step: 678981, loss: 0.06241200491786003, data time: 0.006758431593577067\n",
      "step: 678982, loss: 0.06035929545760155, data time: 0.006627817411680479\n",
      "step: 678983, loss: 0.060776934027671814, data time: 0.006508174695466694\n",
      "step: 678984, loss: 0.05936778709292412, data time: 0.006394508557441907\n",
      "step: 678985, loss: 0.05603494495153427, data time: 0.006287288665771484\n",
      "step: 678986, loss: 0.06406773626804352, data time: 0.15262961387634277\n",
      "step: 678987, loss: 0.0639406368136406, data time: 0.07789695262908936\n",
      "step: 678988, loss: 0.05805576965212822, data time: 0.05295483271280924\n",
      "step: 678989, loss: 0.06366187334060669, data time: 0.04038578271865845\n",
      "step: 678990, loss: 0.05670228227972984, data time: 0.032583761215209964\n",
      "step: 678991, loss: 0.06310488283634186, data time: 0.02742457389831543\n",
      "step: 678992, loss: 0.06881336867809296, data time: 0.023712669100080217\n",
      "step: 678993, loss: 0.06681138277053833, data time: 0.02101457118988037\n",
      "step: 678994, loss: 0.06433576345443726, data time: 0.018823862075805664\n",
      "step: 678995, loss: 0.06122767925262451, data time: 0.017146205902099608\n",
      "step: 678996, loss: 0.06338685005903244, data time: 0.01579119942405007\n",
      "step: 678997, loss: 0.061644237488508224, data time: 0.01465904712677002\n",
      "step: 678998, loss: 0.06592708826065063, data time: 0.013695991956270658\n",
      "step: 678999, loss: 0.0618620440363884, data time: 0.012856534549168177\n",
      "step: 679000, loss: 0.06507247686386108, data time: 0.012141021092732747\n",
      "step: 679001, loss: 0.060554295778274536, data time: 0.011506408452987671\n",
      "step: 679002, loss: 0.06271727383136749, data time: 0.010948826284969555\n",
      "step: 679003, loss: 0.06576569378376007, data time: 0.010483198695712619\n",
      "step: 679004, loss: 0.060582827776670456, data time: 0.010047561243960732\n",
      "step: 679005, loss: 0.06542886793613434, data time: 0.009650921821594239\n",
      "step: 679006, loss: 0.06072228401899338, data time: 0.009293908164614723\n",
      "step: 679007, loss: 0.06373336166143417, data time: 0.008967800573869185\n",
      "step: 679008, loss: 0.06682220101356506, data time: 0.008665665336277174\n",
      "step: 679009, loss: 0.05888804793357849, data time: 0.008394340674082438\n",
      "step: 679010, loss: 0.06368310749530792, data time: 0.008141403198242187\n",
      "step: 679011, loss: 0.0647171288728714, data time: 0.007905006408691406\n",
      "step: 679012, loss: 0.06462620943784714, data time: 0.007684504544293439\n",
      "step: 679013, loss: 0.06616199016571045, data time: 0.0074839166232517785\n",
      "step: 679014, loss: 0.05869406461715698, data time: 0.007300179580162312\n",
      "step: 679015, loss: 0.06678728759288788, data time: 0.00712881882985433\n",
      "step: 679016, loss: 0.06763498485088348, data time: 0.0069675445556640625\n",
      "step: 679017, loss: 0.062285833060741425, data time: 0.006819024682044983\n",
      "step: 679018, loss: 0.061643943190574646, data time: 0.006674036835179184\n",
      "step: 679019, loss: 0.06832647323608398, data time: 0.006534232812769273\n",
      "step: 679020, loss: 0.06146133691072464, data time: 0.0064015592847551615\n",
      "step: 679021, loss: 0.05199851095676422, data time: 0.00627666711807251\n",
      "step: 679022, loss: 0.053848396986722946, data time: 0.006162733645052524\n",
      "step: 679023, loss: 0.0594494454562664, data time: 0.006055160572654323\n",
      "step: 679024, loss: 0.06587176024913788, data time: 0.005954717978453025\n",
      "step: 679025, loss: 0.07318191975355148, data time: 0.005861753225326538\n",
      "step: 679026, loss: 0.05936063453555107, data time: 0.14422917366027832\n",
      "step: 679027, loss: 0.05567867308855057, data time: 0.07391583919525146\n",
      "step: 679028, loss: 0.06888209283351898, data time: 0.0503389040629069\n",
      "step: 679029, loss: 0.05685260146856308, data time: 0.03854936361312866\n",
      "step: 679030, loss: 0.06221847981214523, data time: 0.03116884231567383\n",
      "step: 679031, loss: 0.060641657561063766, data time: 0.026269952456156414\n",
      "step: 679032, loss: 0.060521796345710754, data time: 0.022754907608032227\n",
      "step: 679033, loss: 0.062447383999824524, data time: 0.02022138237953186\n",
      "step: 679034, loss: 0.05779273435473442, data time: 0.01814964082505968\n",
      "step: 679035, loss: 0.05843306705355644, data time: 0.016569066047668456\n",
      "step: 679036, loss: 0.05638577789068222, data time: 0.015287052501331676\n",
      "step: 679037, loss: 0.05718236416578293, data time: 0.014220197995503744\n",
      "step: 679038, loss: 0.06710010021924973, data time: 0.013323068618774414\n",
      "step: 679039, loss: 0.059000760316848755, data time: 0.012538518224443709\n",
      "step: 679040, loss: 0.0647432804107666, data time: 0.01185453732808431\n",
      "step: 679041, loss: 0.06823310256004333, data time: 0.011239036917686462\n",
      "step: 679042, loss: 0.06421296298503876, data time: 0.010696803822236903\n",
      "step: 679043, loss: 0.06650064140558243, data time: 0.01021062003241645\n",
      "step: 679044, loss: 0.06117507815361023, data time: 0.009780080694901315\n",
      "step: 679045, loss: 0.055797502398490906, data time: 0.009399449825286866\n",
      "step: 679046, loss: 0.06163932383060455, data time: 0.009056341080438523\n",
      "step: 679047, loss: 0.06676408648490906, data time: 0.008745247667486017\n",
      "step: 679048, loss: 0.05960056185722351, data time: 0.008453047793844471\n",
      "step: 679049, loss: 0.05734458565711975, data time: 0.00818943977355957\n",
      "step: 679050, loss: 0.06463469564914703, data time: 0.007942695617675782\n",
      "step: 679051, loss: 0.06631257385015488, data time: 0.007717352647047777\n",
      "step: 679052, loss: 0.06155645102262497, data time: 0.007503739109745732\n",
      "step: 679053, loss: 0.0642598420381546, data time: 0.00731351545878819\n",
      "step: 679054, loss: 0.05885029584169388, data time: 0.007135596768609409\n",
      "step: 679055, loss: 0.05826748162508011, data time: 0.0069714546203613285\n",
      "step: 679056, loss: 0.060051754117012024, data time: 0.006817486978346302\n",
      "step: 679057, loss: 0.06410052627325058, data time: 0.0066762566566467285\n",
      "step: 679058, loss: 0.059094615280628204, data time: 0.00653577573371656\n",
      "step: 679059, loss: 0.058826278895139694, data time: 0.006399400093976189\n",
      "step: 679060, loss: 0.06434619426727295, data time: 0.006271709714617048\n",
      "step: 679061, loss: 0.061736881732940674, data time: 0.0061492058965894915\n",
      "step: 679062, loss: 0.06118523329496384, data time: 0.006038330696724556\n",
      "step: 679063, loss: 0.06350398063659668, data time: 0.0059355622843692175\n",
      "step: 679064, loss: 0.05824039503931999, data time: 0.005843822772686298\n",
      "step: 679065, loss: 0.06843482702970505, data time: 0.005751007795333862\n",
      "step: 679066, loss: 0.05615686625242233, data time: 0.17056989669799805\n",
      "step: 679067, loss: 0.0681818276643753, data time: 0.08702099323272705\n",
      "step: 679068, loss: 0.06067168340086937, data time: 0.05854074160257975\n",
      "step: 679069, loss: 0.05954229086637497, data time: 0.044704973697662354\n",
      "step: 679070, loss: 0.06741738319396973, data time: 0.03604788780212402\n",
      "step: 679071, loss: 0.06600865721702576, data time: 0.030280470848083496\n",
      "step: 679072, loss: 0.06092546880245209, data time: 0.02615720885140555\n",
      "step: 679073, loss: 0.06128900498151779, data time: 0.023143917322158813\n",
      "step: 679074, loss: 0.05947080999612808, data time: 0.020717541376749676\n",
      "step: 679075, loss: 0.060355424880981445, data time: 0.01884958744049072\n",
      "step: 679076, loss: 0.06120755523443222, data time: 0.01733760400251909\n",
      "step: 679077, loss: 0.06644900143146515, data time: 0.016067524751027424\n",
      "step: 679078, loss: 0.06250862777233124, data time: 0.015000636761005107\n",
      "step: 679079, loss: 0.05761978030204773, data time: 0.014071805136544364\n",
      "step: 679080, loss: 0.06662267446517944, data time: 0.013272968928019206\n",
      "step: 679081, loss: 0.060741931200027466, data time: 0.012570694088935852\n",
      "step: 679082, loss: 0.06273573637008667, data time: 0.011962049147661994\n",
      "step: 679083, loss: 0.060620903968811035, data time: 0.011404262648688423\n",
      "step: 679084, loss: 0.05781328305602074, data time: 0.01091558054873818\n",
      "step: 679085, loss: 0.0641593486070633, data time: 0.010475707054138184\n",
      "step: 679086, loss: 0.0603434257209301, data time: 0.010080292111351377\n",
      "step: 679087, loss: 0.06325452774763107, data time: 0.009719241749156605\n",
      "step: 679088, loss: 0.05822179839015007, data time: 0.009385492490685505\n",
      "step: 679089, loss: 0.05700492113828659, data time: 0.00909621516863505\n",
      "step: 679090, loss: 0.06011730432510376, data time: 0.008827924728393555\n",
      "step: 679091, loss: 0.061483558267354965, data time: 0.008580693831810584\n",
      "step: 679092, loss: 0.06317147612571716, data time: 0.008348279529147677\n",
      "step: 679093, loss: 0.059155821800231934, data time: 0.008134092603410994\n",
      "step: 679094, loss: 0.05717567726969719, data time: 0.007940012833167767\n",
      "step: 679095, loss: 0.051633071154356, data time: 0.007759666442871094\n",
      "step: 679096, loss: 0.07044292986392975, data time: 0.007593039543397965\n",
      "step: 679097, loss: 0.06281423568725586, data time: 0.007436864078044891\n",
      "step: 679098, loss: 0.06602850556373596, data time: 0.007274201422026663\n",
      "step: 679099, loss: 0.058688171207904816, data time: 0.007119690670686609\n",
      "step: 679100, loss: 0.06004335731267929, data time: 0.006976536342075893\n",
      "step: 679101, loss: 0.061218149960041046, data time: 0.0068391164143880205\n",
      "step: 679102, loss: 0.06318993866443634, data time: 0.006709672309256889\n",
      "step: 679103, loss: 0.06604423373937607, data time: 0.0065905357662000156\n",
      "step: 679104, loss: 0.05980603024363518, data time: 0.006476591794918745\n",
      "step: 679105, loss: 0.037422262132167816, data time: 0.006369107961654663\n",
      "step: 679106, loss: 0.05887327343225479, data time: 0.17439889907836914\n",
      "step: 679107, loss: 0.06803511083126068, data time: 0.08794450759887695\n",
      "step: 679108, loss: 0.05895154923200607, data time: 0.059540351231892906\n",
      "step: 679109, loss: 0.0615721270442009, data time: 0.045408785343170166\n",
      "step: 679110, loss: 0.05844760686159134, data time: 0.03664069175720215\n",
      "step: 679111, loss: 0.062037985771894455, data time: 0.03078194459279378\n",
      "step: 679112, loss: 0.05581914633512497, data time: 0.026604550225394114\n",
      "step: 679113, loss: 0.05893854796886444, data time: 0.02353808283805847\n",
      "step: 679114, loss: 0.06394209712743759, data time: 0.021070241928100586\n",
      "step: 679115, loss: 0.05632265657186508, data time: 0.0191601037979126\n",
      "step: 679116, loss: 0.06713045388460159, data time: 0.017613866112448952\n",
      "step: 679117, loss: 0.060374222695827484, data time: 0.0163232684135437\n",
      "step: 679118, loss: 0.06022736057639122, data time: 0.015230233852679912\n",
      "step: 679119, loss: 0.05970345064997673, data time: 0.01428520679473877\n",
      "step: 679120, loss: 0.061867862939834595, data time: 0.013478247324625652\n",
      "step: 679121, loss: 0.06052236258983612, data time: 0.012767210602760315\n",
      "step: 679122, loss: 0.061707478016614914, data time: 0.012134229435640223\n",
      "step: 679123, loss: 0.06716661155223846, data time: 0.011569036377800835\n",
      "step: 679124, loss: 0.06217528507113457, data time: 0.011065859543649774\n",
      "step: 679125, loss: 0.06679411977529526, data time: 0.010616564750671386\n",
      "step: 679126, loss: 0.05717453360557556, data time: 0.010216202054704939\n",
      "step: 679127, loss: 0.06277637183666229, data time: 0.009848280386491255\n",
      "step: 679128, loss: 0.06517216563224792, data time: 0.009510610414587933\n",
      "step: 679129, loss: 0.06221405789256096, data time: 0.009202231963475546\n",
      "step: 679130, loss: 0.06202574819326401, data time: 0.008935050964355469\n",
      "step: 679131, loss: 0.06020753085613251, data time: 0.008669046255258413\n",
      "step: 679132, loss: 0.06616060435771942, data time: 0.008419928727326569\n",
      "step: 679133, loss: 0.0564766488969326, data time: 0.008191091673714774\n",
      "step: 679134, loss: 0.06694747507572174, data time: 0.007985435683151772\n",
      "step: 679135, loss: 0.061938852071762085, data time: 0.0077900568644205725\n",
      "step: 679136, loss: 0.05537959188222885, data time: 0.007608298332460465\n",
      "step: 679137, loss: 0.06000462919473648, data time: 0.007440939545631409\n",
      "step: 679138, loss: 0.061185553669929504, data time: 0.007278933669581558\n",
      "step: 679139, loss: 0.06715211272239685, data time: 0.007123596527997185\n",
      "step: 679140, loss: 0.060291364789009094, data time: 0.006974043164934431\n",
      "step: 679141, loss: 0.0665196031332016, data time: 0.006831056541866726\n",
      "step: 679142, loss: 0.06389119476079941, data time: 0.00669648840620711\n",
      "step: 679143, loss: 0.06398186832666397, data time: 0.006571374441448011\n",
      "step: 679144, loss: 0.056768715381622314, data time: 0.006454926270705003\n",
      "step: 679145, loss: 0.05476224049925804, data time: 0.006344026327133179\n",
      "step: 679146, loss: 0.0610298328101635, data time: 0.16391420364379883\n",
      "step: 679147, loss: 0.06529200077056885, data time: 0.08331477642059326\n",
      "step: 679148, loss: 0.06498977541923523, data time: 0.05629301071166992\n",
      "step: 679149, loss: 0.056661587208509445, data time: 0.04310035705566406\n",
      "step: 679150, loss: 0.06278347223997116, data time: 0.03475627899169922\n",
      "step: 679151, loss: 0.06427185237407684, data time: 0.029210567474365234\n",
      "step: 679152, loss: 0.0634567141532898, data time: 0.02524753979274205\n",
      "step: 679153, loss: 0.05794713646173477, data time: 0.022350996732711792\n",
      "step: 679154, loss: 0.06174395605921745, data time: 0.020018710030449763\n",
      "step: 679155, loss: 0.07062392681837082, data time: 0.018223023414611815\n",
      "step: 679156, loss: 0.06690990179777145, data time: 0.016756296157836914\n",
      "step: 679157, loss: 0.059899263083934784, data time: 0.015539288520812988\n",
      "step: 679158, loss: 0.061015546321868896, data time: 0.014506376706636868\n",
      "step: 679159, loss: 0.0632406622171402, data time: 0.013613734926496233\n",
      "step: 679160, loss: 0.05992059409618378, data time: 0.01284772555033366\n",
      "step: 679161, loss: 0.06233187019824982, data time: 0.012172490358352661\n",
      "step: 679162, loss: 0.0622730478644371, data time: 0.011578658047844382\n",
      "step: 679163, loss: 0.058776941150426865, data time: 0.011042488945855035\n",
      "step: 679164, loss: 0.06427772343158722, data time: 0.010568267420718544\n",
      "step: 679165, loss: 0.05804171785712242, data time: 0.010147571563720703\n",
      "step: 679166, loss: 0.06360965967178345, data time: 0.009779010500226702\n",
      "step: 679167, loss: 0.05963129922747612, data time: 0.009450143033807928\n",
      "step: 679168, loss: 0.06304357945919037, data time: 0.009129710819410242\n",
      "step: 679169, loss: 0.05892559140920639, data time: 0.00884401798248291\n",
      "step: 679170, loss: 0.056505218148231506, data time: 0.008575515747070312\n",
      "step: 679171, loss: 0.059931397438049316, data time: 0.008325292513920711\n",
      "step: 679172, loss: 0.06650810688734055, data time: 0.008089957413850006\n",
      "step: 679173, loss: 0.06178624555468559, data time: 0.007871627807617188\n",
      "step: 679174, loss: 0.05840842053294182, data time: 0.007678023700056405\n",
      "step: 679175, loss: 0.05862397700548172, data time: 0.0074951410293579105\n",
      "step: 679176, loss: 0.05876738578081131, data time: 0.007327018245573967\n",
      "step: 679177, loss: 0.06457053124904633, data time: 0.007167413830757141\n",
      "step: 679178, loss: 0.06167556345462799, data time: 0.007009867465857304\n",
      "step: 679179, loss: 0.06575815379619598, data time: 0.006860866266138414\n",
      "step: 679180, loss: 0.06163224205374718, data time: 0.006720829010009766\n",
      "step: 679181, loss: 0.06489069759845734, data time: 0.006585088041093614\n",
      "step: 679182, loss: 0.06816774606704712, data time: 0.006458359795647698\n",
      "step: 679183, loss: 0.058420002460479736, data time: 0.006341846365677683\n",
      "step: 679184, loss: 0.059189990162849426, data time: 0.006231943766276042\n",
      "step: 679185, loss: 0.037116654217243195, data time: 0.0061263501644134525\n",
      "step: 679186, loss: 0.06146723031997681, data time: 0.17909860610961914\n",
      "step: 679187, loss: 0.06529407203197479, data time: 0.0917360782623291\n",
      "step: 679188, loss: 0.05942373722791672, data time: 0.06166855494181315\n",
      "step: 679189, loss: 0.05781714618206024, data time: 0.04692196846008301\n",
      "step: 679190, loss: 0.06570994108915329, data time: 0.03784666061401367\n",
      "step: 679191, loss: 0.06730780750513077, data time: 0.03179121017456055\n",
      "step: 679192, loss: 0.059882115572690964, data time: 0.027473245348249162\n",
      "step: 679193, loss: 0.05868014693260193, data time: 0.024296849966049194\n",
      "step: 679194, loss: 0.06736928224563599, data time: 0.021745151943630643\n",
      "step: 679195, loss: 0.06131614372134209, data time: 0.019789624214172363\n",
      "step: 679196, loss: 0.06642736494541168, data time: 0.018187176097523083\n",
      "step: 679197, loss: 0.06410752981901169, data time: 0.016856571038564045\n",
      "step: 679198, loss: 0.06603756546974182, data time: 0.015724622286283053\n",
      "step: 679199, loss: 0.05669920891523361, data time: 0.014752388000488281\n",
      "step: 679200, loss: 0.06204443797469139, data time: 0.013912375768025715\n",
      "step: 679201, loss: 0.06618849188089371, data time: 0.013174518942832947\n",
      "step: 679202, loss: 0.06637924164533615, data time: 0.012526624342974494\n",
      "step: 679203, loss: 0.06835650652647018, data time: 0.0119451814227634\n",
      "step: 679204, loss: 0.05757637321949005, data time: 0.011428029913651315\n",
      "step: 679205, loss: 0.06578182429075241, data time: 0.010962092876434326\n",
      "step: 679206, loss: 0.06535598635673523, data time: 0.01054578735714867\n",
      "step: 679207, loss: 0.06312718987464905, data time: 0.010166796770962801\n",
      "step: 679208, loss: 0.06275646388530731, data time: 0.009815692901611328\n",
      "step: 679209, loss: 0.06375713646411896, data time: 0.009496033191680908\n",
      "step: 679210, loss: 0.06186487525701523, data time: 0.009198188781738281\n",
      "step: 679211, loss: 0.06244088336825371, data time: 0.008927491995004507\n",
      "step: 679212, loss: 0.060967884957790375, data time: 0.008668643456918222\n",
      "step: 679213, loss: 0.058164551854133606, data time: 0.008433886936732702\n",
      "step: 679214, loss: 0.06768389046192169, data time: 0.008219233874616951\n",
      "step: 679215, loss: 0.06759463250637054, data time: 0.008017373085021973\n",
      "step: 679216, loss: 0.05776076763868332, data time: 0.007826274441134545\n",
      "step: 679217, loss: 0.06307073682546616, data time: 0.007653534412384033\n",
      "step: 679218, loss: 0.06242290884256363, data time: 0.007481878454034979\n",
      "step: 679219, loss: 0.06651145219802856, data time: 0.007319920203265022\n",
      "step: 679220, loss: 0.06694018095731735, data time: 0.007165765762329102\n",
      "step: 679221, loss: 0.06980998814105988, data time: 0.007020288043551975\n",
      "step: 679222, loss: 0.05970054864883423, data time: 0.006885831420486038\n",
      "step: 679223, loss: 0.061051271855831146, data time: 0.0067576922868427475\n",
      "step: 679224, loss: 0.06483962386846542, data time: 0.006635830952570989\n",
      "step: 679225, loss: 0.07041610032320023, data time: 0.006520044803619385\n",
      "step: 679226, loss: 0.0637037605047226, data time: 0.17343807220458984\n",
      "step: 679227, loss: 0.05898166820406914, data time: 0.08841252326965332\n",
      "step: 679228, loss: 0.05929751321673393, data time: 0.05948630968729655\n",
      "step: 679229, loss: 0.06810804456472397, data time: 0.04548978805541992\n",
      "step: 679230, loss: 0.06322474032640457, data time: 0.03667559623718262\n",
      "step: 679231, loss: 0.061300843954086304, data time: 0.030807097752888996\n",
      "step: 679232, loss: 0.057170119136571884, data time: 0.026616368974958147\n",
      "step: 679233, loss: 0.06299225986003876, data time: 0.02355891466140747\n",
      "step: 679234, loss: 0.0642390325665474, data time: 0.02108727561102973\n",
      "step: 679235, loss: 0.06052708625793457, data time: 0.019181227684020995\n",
      "step: 679236, loss: 0.05801738426089287, data time: 0.01762752099470659\n",
      "step: 679237, loss: 0.06467039883136749, data time: 0.016334692637125652\n",
      "step: 679238, loss: 0.059651270508766174, data time: 0.015256276497474084\n",
      "step: 679239, loss: 0.061080317944288254, data time: 0.014314327921186174\n",
      "step: 679240, loss: 0.05913923308253288, data time: 0.013505808512369792\n",
      "step: 679241, loss: 0.0611652135848999, data time: 0.012799978256225586\n",
      "step: 679242, loss: 0.06962162256240845, data time: 0.012172376408296473\n",
      "step: 679243, loss: 0.06160183250904083, data time: 0.01160373952653673\n",
      "step: 679244, loss: 0.060771435499191284, data time: 0.01109908756456877\n",
      "step: 679245, loss: 0.057975687086582184, data time: 0.01065000295639038\n",
      "step: 679246, loss: 0.06014856696128845, data time: 0.01024574325198219\n",
      "step: 679247, loss: 0.05841150879859924, data time: 0.009878776290199974\n",
      "step: 679248, loss: 0.061376407742500305, data time: 0.009536815726238749\n",
      "step: 679249, loss: 0.06259506195783615, data time: 0.009225318829218546\n",
      "step: 679250, loss: 0.06772343814373016, data time: 0.008939905166625977\n",
      "step: 679251, loss: 0.05800672620534897, data time: 0.008679371613722581\n",
      "step: 679252, loss: 0.06776794046163559, data time: 0.008430145404956959\n",
      "step: 679253, loss: 0.05972158908843994, data time: 0.00820101158959525\n",
      "step: 679254, loss: 0.05930275842547417, data time: 0.007992670453827956\n",
      "step: 679255, loss: 0.06504896283149719, data time: 0.007797114054361979\n",
      "step: 679256, loss: 0.06110599637031555, data time: 0.0076135819958102315\n",
      "step: 679257, loss: 0.06227912753820419, data time: 0.0074588581919670105\n",
      "step: 679258, loss: 0.06039213389158249, data time: 0.0072920250170158615\n",
      "step: 679259, loss: 0.0632561445236206, data time: 0.007136961993049173\n",
      "step: 679260, loss: 0.06639742851257324, data time: 0.00699056897844587\n",
      "step: 679261, loss: 0.060338132083415985, data time: 0.006848500834570991\n",
      "step: 679262, loss: 0.05856337398290634, data time: 0.00671495618046941\n",
      "step: 679263, loss: 0.06359212845563889, data time: 0.00659024715423584\n",
      "step: 679264, loss: 0.060116421431303024, data time: 0.0064724164131360175\n",
      "step: 679265, loss: 0.0654027909040451, data time: 0.006360495090484619\n",
      "step: 679266, loss: 0.06037008389830589, data time: 0.1730034351348877\n",
      "step: 679267, loss: 0.06272509694099426, data time: 0.08796823024749756\n",
      "step: 679268, loss: 0.0635167583823204, data time: 0.059742132822672524\n",
      "step: 679269, loss: 0.06345133483409882, data time: 0.04549813270568848\n",
      "step: 679270, loss: 0.07330767810344696, data time: 0.03668227195739746\n",
      "step: 679271, loss: 0.054938904941082, data time: 0.030842900276184082\n",
      "step: 679272, loss: 0.06462424993515015, data time: 0.02665243829999651\n",
      "step: 679273, loss: 0.06025376915931702, data time: 0.023574084043502808\n",
      "step: 679274, loss: 0.06966404616832733, data time: 0.02110025617811415\n",
      "step: 679275, loss: 0.06282331794500351, data time: 0.01919434070587158\n",
      "step: 679276, loss: 0.06825266778469086, data time: 0.017646659504283558\n",
      "step: 679277, loss: 0.0591401569545269, data time: 0.01635889212290446\n",
      "step: 679278, loss: 0.06390032172203064, data time: 0.01526946287888747\n",
      "step: 679279, loss: 0.060572803020477295, data time: 0.014327781541006905\n",
      "step: 679280, loss: 0.05998769402503967, data time: 0.013522100448608399\n",
      "step: 679281, loss: 0.06550680845975876, data time: 0.012815937399864197\n",
      "step: 679282, loss: 0.06893256306648254, data time: 0.012183203416712144\n",
      "step: 679283, loss: 0.06943504512310028, data time: 0.011616177029079862\n",
      "step: 679284, loss: 0.06332851201295853, data time: 0.011112953487195466\n",
      "step: 679285, loss: 0.056519053876399994, data time: 0.010665369033813477\n",
      "step: 679286, loss: 0.06240119785070419, data time: 0.010262194133940198\n",
      "step: 679287, loss: 0.05739381164312363, data time: 0.009894284335049715\n",
      "step: 679288, loss: 0.06057451292872429, data time: 0.009565705838410751\n",
      "step: 679289, loss: 0.06205550953745842, data time: 0.009256869554519653\n",
      "step: 679290, loss: 0.05915123224258423, data time: 0.008973703384399415\n",
      "step: 679291, loss: 0.06014206260442734, data time: 0.008710549427912785\n",
      "step: 679292, loss: 0.059667378664016724, data time: 0.008463815406516747\n",
      "step: 679293, loss: 0.06560640782117844, data time: 0.008235548223767961\n",
      "step: 679294, loss: 0.05702417343854904, data time: 0.008026098382884058\n",
      "step: 679295, loss: 0.05868024751543999, data time: 0.007836476961771647\n",
      "step: 679296, loss: 0.05877113342285156, data time: 0.007652651879095262\n",
      "step: 679297, loss: 0.06132820248603821, data time: 0.007484324276447296\n",
      "step: 679298, loss: 0.05683936923742294, data time: 0.007317680301088275\n",
      "step: 679299, loss: 0.0651339441537857, data time: 0.007162283448611989\n",
      "step: 679300, loss: 0.0699700266122818, data time: 0.007013811383928571\n",
      "step: 679301, loss: 0.058310773223638535, data time: 0.006871912214491103\n",
      "step: 679302, loss: 0.06141863763332367, data time: 0.006737354639414194\n",
      "step: 679303, loss: 0.06241810321807861, data time: 0.006613191805387798\n",
      "step: 679304, loss: 0.05925080180168152, data time: 0.006496429443359375\n",
      "step: 679305, loss: 0.06536003947257996, data time: 0.00638427734375\n",
      "step: 679306, loss: 0.06217686086893082, data time: 0.1692335605621338\n",
      "step: 679307, loss: 0.06471969932317734, data time: 0.08542454242706299\n",
      "step: 679308, loss: 0.06096251308917999, data time: 0.058017333348592125\n",
      "step: 679309, loss: 0.06379479169845581, data time: 0.043892741203308105\n",
      "step: 679310, loss: 0.06442645192146301, data time: 0.0353968620300293\n",
      "step: 679311, loss: 0.06782285869121552, data time: 0.029747525850931805\n",
      "step: 679312, loss: 0.05924487113952637, data time: 0.02593653542654855\n",
      "step: 679313, loss: 0.06346234679222107, data time: 0.022865116596221924\n",
      "step: 679314, loss: 0.06039748713374138, data time: 0.02047909630669488\n",
      "step: 679315, loss: 0.05475703626871109, data time: 0.01862812042236328\n",
      "step: 679316, loss: 0.0662098228931427, data time: 0.017129139466719193\n",
      "step: 679317, loss: 0.05794775113463402, data time: 0.01587877670923869\n",
      "step: 679318, loss: 0.05790601670742035, data time: 0.014823601796076847\n",
      "step: 679319, loss: 0.05846615135669708, data time: 0.013912609645298548\n",
      "step: 679320, loss: 0.06023078411817551, data time: 0.013132158915201824\n",
      "step: 679321, loss: 0.06582260131835938, data time: 0.012446805834770203\n",
      "step: 679322, loss: 0.05953064188361168, data time: 0.011831115273868336\n",
      "step: 679323, loss: 0.06268374621868134, data time: 0.011282470491197374\n",
      "step: 679324, loss: 0.0638963133096695, data time: 0.010798366446244089\n",
      "step: 679325, loss: 0.06483268737792969, data time: 0.01036585569381714\n",
      "step: 679326, loss: 0.06518223136663437, data time: 0.009976205371675036\n",
      "step: 679327, loss: 0.06101186200976372, data time: 0.009618596597151323\n",
      "step: 679328, loss: 0.06340368092060089, data time: 0.00928956529368525\n",
      "step: 679329, loss: 0.062228377908468246, data time: 0.008991231520970663\n",
      "step: 679330, loss: 0.06054189056158066, data time: 0.008717031478881835\n",
      "step: 679331, loss: 0.06467476487159729, data time: 0.008458531819857083\n",
      "step: 679332, loss: 0.06925426423549652, data time: 0.008224222395155165\n",
      "step: 679333, loss: 0.052542537450790405, data time: 0.008001165730612618\n",
      "step: 679334, loss: 0.06021278724074364, data time: 0.007798605951769599\n",
      "step: 679335, loss: 0.06571000069379807, data time: 0.00760962168375651\n",
      "step: 679336, loss: 0.06140918657183647, data time: 0.007435260280486076\n",
      "step: 679337, loss: 0.055327557027339935, data time: 0.0072774142026901245\n",
      "step: 679338, loss: 0.05693903565406799, data time: 0.007118470741040779\n",
      "step: 679339, loss: 0.059874504804611206, data time: 0.006974451682146858\n",
      "step: 679340, loss: 0.06748978793621063, data time: 0.006829561505998884\n",
      "step: 679341, loss: 0.05455682799220085, data time: 0.006692045264773899\n",
      "step: 679342, loss: 0.05995941162109375, data time: 0.006562323183626742\n",
      "step: 679343, loss: 0.06522094458341599, data time: 0.006443124068410773\n",
      "step: 679344, loss: 0.06093274801969528, data time: 0.006329585344363482\n",
      "step: 679345, loss: 0.0660502165555954, data time: 0.006221908330917359\n",
      "step: 679346, loss: 0.06124880909919739, data time: 0.180100679397583\n",
      "step: 679347, loss: 0.06607137620449066, data time: 0.09114730358123779\n",
      "step: 679348, loss: 0.06738452613353729, data time: 0.06198620796203613\n",
      "step: 679349, loss: 0.06653722375631332, data time: 0.04712986946105957\n",
      "step: 679350, loss: 0.06468713283538818, data time: 0.038039445877075195\n",
      "step: 679351, loss: 0.05725890025496483, data time: 0.031986633936564125\n",
      "step: 679352, loss: 0.05921865627169609, data time: 0.027772460665021623\n",
      "step: 679353, loss: 0.062334030866622925, data time: 0.024595648050308228\n",
      "step: 679354, loss: 0.060456328094005585, data time: 0.022122091717190213\n",
      "step: 679355, loss: 0.05900174379348755, data time: 0.02014913558959961\n",
      "step: 679356, loss: 0.06322488188743591, data time: 0.01855691996487704\n",
      "step: 679357, loss: 0.06034114211797714, data time: 0.017225980758666992\n",
      "step: 679358, loss: 0.06377072632312775, data time: 0.016095088078425482\n",
      "step: 679359, loss: 0.06179146096110344, data time: 0.01511556761605399\n",
      "step: 679360, loss: 0.057917386293411255, data time: 0.014281129837036133\n",
      "step: 679361, loss: 0.05999574065208435, data time: 0.013545259833335876\n",
      "step: 679362, loss: 0.05926791951060295, data time: 0.01288760409635656\n",
      "step: 679363, loss: 0.06101752445101738, data time: 0.012300080723232694\n",
      "step: 679364, loss: 0.07173599302768707, data time: 0.011779132642244039\n",
      "step: 679365, loss: 0.06510201096534729, data time: 0.011313390731811524\n",
      "step: 679366, loss: 0.0621415376663208, data time: 0.010892675036475771\n",
      "step: 679367, loss: 0.05995725840330124, data time: 0.010512384501370516\n",
      "step: 679368, loss: 0.06362050026655197, data time: 0.01016204253486965\n",
      "step: 679369, loss: 0.061019670218229294, data time: 0.009843935569127401\n",
      "step: 679370, loss: 0.06060855835676193, data time: 0.009549312591552735\n",
      "step: 679371, loss: 0.0657346248626709, data time: 0.00926803625546969\n",
      "step: 679372, loss: 0.06907385587692261, data time: 0.008997767059891313\n",
      "step: 679373, loss: 0.06714512407779694, data time: 0.008749731949397497\n",
      "step: 679374, loss: 0.05262118577957153, data time: 0.008521787051496834\n",
      "step: 679375, loss: 0.062015384435653687, data time: 0.008311025301615397\n",
      "step: 679376, loss: 0.058225177228450775, data time: 0.008112146008399225\n",
      "step: 679377, loss: 0.05896633118391037, data time: 0.007929936051368713\n",
      "step: 679378, loss: 0.061575233936309814, data time: 0.007751313122836026\n",
      "step: 679379, loss: 0.058854274451732635, data time: 0.0075829940683701455\n",
      "step: 679380, loss: 0.05558555945754051, data time: 0.007420880453927177\n",
      "step: 679381, loss: 0.061556752771139145, data time: 0.007266673776838515\n",
      "step: 679382, loss: 0.06076095253229141, data time: 0.0071239084810824005\n",
      "step: 679383, loss: 0.06552635133266449, data time: 0.00698945396824887\n",
      "step: 679384, loss: 0.06826802343130112, data time: 0.006899699186667418\n",
      "step: 679385, loss: 0.055714190006256104, data time: 0.006778335571289063\n",
      "step: 679386, loss: 0.06367333233356476, data time: 0.18880963325500488\n",
      "step: 679387, loss: 0.05706373602151871, data time: 0.09517872333526611\n",
      "step: 679388, loss: 0.06325450539588928, data time: 0.06444565455118816\n",
      "step: 679389, loss: 0.06138021498918533, data time: 0.049204111099243164\n",
      "step: 679390, loss: 0.05459487438201904, data time: 0.03966073989868164\n",
      "step: 679391, loss: 0.055328965187072754, data time: 0.033318559328715004\n",
      "step: 679392, loss: 0.05701726675033569, data time: 0.028772456305367605\n",
      "step: 679393, loss: 0.059118516743183136, data time: 0.02542552351951599\n",
      "step: 679394, loss: 0.058303724974393845, data time: 0.022760550181070965\n",
      "step: 679395, loss: 0.054226383566856384, data time: 0.02069125175476074\n",
      "step: 679396, loss: 0.05822485685348511, data time: 0.01901630921797319\n",
      "step: 679397, loss: 0.06058656424283981, data time: 0.017612894376118977\n",
      "step: 679398, loss: 0.06276635825634003, data time: 0.01642181323124812\n",
      "step: 679399, loss: 0.05938040465116501, data time: 0.015400375638689314\n",
      "step: 679400, loss: 0.05950850248336792, data time: 0.01451438268025716\n",
      "step: 679401, loss: 0.06639419496059418, data time: 0.013739585876464844\n",
      "step: 679402, loss: 0.06499163806438446, data time: 0.013050584232105929\n",
      "step: 679403, loss: 0.05729120224714279, data time: 0.012436641587151421\n",
      "step: 679404, loss: 0.06134841963648796, data time: 0.011886835098266602\n",
      "step: 679405, loss: 0.06552450358867645, data time: 0.01139688491821289\n",
      "step: 679406, loss: 0.06294076889753342, data time: 0.010954618453979492\n",
      "step: 679407, loss: 0.06496153771877289, data time: 0.010554974729364569\n",
      "step: 679408, loss: 0.06385143101215363, data time: 0.010186568550441576\n",
      "step: 679409, loss: 0.06390324980020523, data time: 0.009853571653366089\n",
      "step: 679410, loss: 0.0689273402094841, data time: 0.009546842575073242\n",
      "step: 679411, loss: 0.06087444722652435, data time: 0.009256949791541466\n",
      "step: 679412, loss: 0.06102868169546127, data time: 0.008988071371007848\n",
      "step: 679413, loss: 0.06099134683609009, data time: 0.008739769458770752\n",
      "step: 679414, loss: 0.06045794486999512, data time: 0.008512809358794114\n",
      "step: 679415, loss: 0.06286395341157913, data time: 0.008301202456156414\n",
      "step: 679416, loss: 0.06257966160774231, data time: 0.008103147629768617\n",
      "step: 679417, loss: 0.05773039162158966, data time: 0.007921941578388214\n",
      "step: 679418, loss: 0.0638410896062851, data time: 0.007744796348340584\n",
      "step: 679419, loss: 0.05928410217165947, data time: 0.0075764375574448525\n",
      "step: 679420, loss: 0.06711864471435547, data time: 0.007414674758911133\n",
      "step: 679421, loss: 0.05764105170965195, data time: 0.0072639452086554635\n",
      "step: 679422, loss: 0.06009253114461899, data time: 0.007119964908909153\n",
      "step: 679423, loss: 0.06196614354848862, data time: 0.006987678377251876\n",
      "step: 679424, loss: 0.06110799312591553, data time: 0.006863575715285081\n",
      "step: 679425, loss: 0.04888109117746353, data time: 0.006742268800735474\n",
      "step: 679426, loss: 0.06507523357868195, data time: 0.1638627052307129\n",
      "step: 679427, loss: 0.068697988986969, data time: 0.08327627182006836\n",
      "step: 679428, loss: 0.06075623258948326, data time: 0.056560516357421875\n",
      "step: 679429, loss: 0.05549337714910507, data time: 0.04309934377670288\n",
      "step: 679430, loss: 0.05854211375117302, data time: 0.03476567268371582\n",
      "step: 679431, loss: 0.06661668419837952, data time: 0.02924613157908122\n",
      "step: 679432, loss: 0.06100807711482048, data time: 0.025288922446114675\n",
      "step: 679433, loss: 0.06809401512145996, data time: 0.022376686334609985\n",
      "step: 679434, loss: 0.059157561510801315, data time: 0.02004082997639974\n",
      "step: 679435, loss: 0.06183420866727829, data time: 0.01824338436126709\n",
      "step: 679436, loss: 0.05970821902155876, data time: 0.01678410443392667\n",
      "step: 679437, loss: 0.06173231452703476, data time: 0.01556617021560669\n",
      "step: 679438, loss: 0.06476001441478729, data time: 0.014541185819185697\n",
      "step: 679439, loss: 0.05656960606575012, data time: 0.013648901666913713\n",
      "step: 679440, loss: 0.05894479155540466, data time: 0.012880261739095051\n",
      "step: 679441, loss: 0.05894525349140167, data time: 0.012208372354507446\n",
      "step: 679442, loss: 0.06111837550997734, data time: 0.01161031162037569\n",
      "step: 679443, loss: 0.061661120504140854, data time: 0.011080265045166016\n",
      "step: 679444, loss: 0.06138097494840622, data time: 0.010604732914974815\n",
      "step: 679445, loss: 0.06102338433265686, data time: 0.010191059112548828\n",
      "step: 679446, loss: 0.061710160225629807, data time: 0.009808267865862166\n",
      "step: 679447, loss: 0.06772181391716003, data time: 0.00946420972997492\n",
      "step: 679448, loss: 0.05947761982679367, data time: 0.00914218114769977\n",
      "step: 679449, loss: 0.06384696811437607, data time: 0.008857289950052897\n",
      "step: 679450, loss: 0.0612177811563015, data time: 0.008589038848876953\n",
      "step: 679451, loss: 0.059781067073345184, data time: 0.00833751605107234\n",
      "step: 679452, loss: 0.05966467782855034, data time: 0.00810720302440502\n",
      "step: 679453, loss: 0.06434676051139832, data time: 0.007889441081455775\n",
      "step: 679454, loss: 0.06637263298034668, data time: 0.007694326598068763\n",
      "step: 679455, loss: 0.06520617753267288, data time: 0.007510964075724284\n",
      "step: 679456, loss: 0.060867976397275925, data time: 0.007337993191134545\n",
      "step: 679457, loss: 0.06201521307229996, data time: 0.007179535925388336\n",
      "step: 679458, loss: 0.057258106768131256, data time: 0.007022807092377634\n",
      "step: 679459, loss: 0.062364619225263596, data time: 0.006874946986927706\n",
      "step: 679460, loss: 0.058292921632528305, data time: 0.006731986999511719\n",
      "step: 679461, loss: 0.06284739822149277, data time: 0.006597479184468587\n",
      "step: 679462, loss: 0.057634979486465454, data time: 0.006471060417793892\n",
      "step: 679463, loss: 0.05767051503062248, data time: 0.006354670775564094\n",
      "step: 679464, loss: 0.06074845790863037, data time: 0.006243852468637319\n",
      "step: 679465, loss: 0.045750942081213, data time: 0.006139200925827026\n",
      "step: 679466, loss: 0.05819256603717804, data time: 0.17986774444580078\n",
      "step: 679467, loss: 0.06196894869208336, data time: 0.09077036380767822\n",
      "step: 679468, loss: 0.06248484179377556, data time: 0.06143482526143392\n",
      "step: 679469, loss: 0.06041193753480911, data time: 0.046623945236206055\n",
      "step: 679470, loss: 0.06888394057750702, data time: 0.037602710723876956\n",
      "step: 679471, loss: 0.06623731553554535, data time: 0.03160520394643148\n",
      "step: 679472, loss: 0.060577403753995895, data time: 0.027456794466291155\n",
      "step: 679473, loss: 0.06116165965795517, data time: 0.02420961856842041\n",
      "step: 679474, loss: 0.060430340468883514, data time: 0.021750158733791776\n",
      "step: 679475, loss: 0.05634111166000366, data time: 0.019771432876586913\n",
      "step: 679476, loss: 0.061932723969221115, data time: 0.018166780471801758\n",
      "step: 679477, loss: 0.0678790807723999, data time: 0.016829232374827068\n",
      "step: 679478, loss: 0.057030659168958664, data time: 0.01570019355187049\n",
      "step: 679479, loss: 0.0652063712477684, data time: 0.01472156388419015\n",
      "step: 679480, loss: 0.06313931941986084, data time: 0.013889026641845704\n",
      "step: 679481, loss: 0.06397095322608948, data time: 0.013153061270713806\n",
      "step: 679482, loss: 0.06377393007278442, data time: 0.012495615903068991\n",
      "step: 679483, loss: 0.05903440713882446, data time: 0.011918160650465224\n",
      "step: 679484, loss: 0.057806678116321564, data time: 0.011397662915681539\n",
      "step: 679485, loss: 0.0667266845703125, data time: 0.010938930511474609\n",
      "step: 679486, loss: 0.06533174961805344, data time: 0.010522013618832543\n",
      "step: 679487, loss: 0.0630243569612503, data time: 0.01014278151772239\n",
      "step: 679488, loss: 0.0644669234752655, data time: 0.009789332099582838\n",
      "step: 679489, loss: 0.05621206760406494, data time: 0.009469548861185709\n",
      "step: 679490, loss: 0.0642891526222229, data time: 0.009175825119018554\n",
      "step: 679491, loss: 0.07066786289215088, data time: 0.008931875228881836\n",
      "step: 679492, loss: 0.06782973557710648, data time: 0.008679195686622902\n",
      "step: 679493, loss: 0.061093997210264206, data time: 0.00844071592603411\n",
      "step: 679494, loss: 0.060729023069143295, data time: 0.008222292209493703\n",
      "step: 679495, loss: 0.057511106133461, data time: 0.008019081751505534\n",
      "step: 679496, loss: 0.06106991693377495, data time: 0.007838218442855342\n",
      "step: 679497, loss: 0.06176392361521721, data time: 0.007675185799598694\n",
      "step: 679498, loss: 0.06574983894824982, data time: 0.007508631908532345\n",
      "step: 679499, loss: 0.06081636995077133, data time: 0.007350423756767721\n",
      "step: 679500, loss: 0.059264108538627625, data time: 0.00719787733895438\n",
      "step: 679501, loss: 0.061912037432193756, data time: 0.007054494486914741\n",
      "step: 679502, loss: 0.06383365392684937, data time: 0.006918668746948242\n",
      "step: 679503, loss: 0.06282399594783783, data time: 0.006792244158293072\n",
      "step: 679504, loss: 0.06259167194366455, data time: 0.006672547413752629\n",
      "step: 679505, loss: 0.04333985596895218, data time: 0.006559503078460693\n",
      "step: 679506, loss: 0.06472808122634888, data time: 0.17943501472473145\n",
      "step: 679507, loss: 0.062173523008823395, data time: 0.09047746658325195\n",
      "step: 679508, loss: 0.06974396109580994, data time: 0.060849269231160484\n",
      "step: 679509, loss: 0.06221112608909607, data time: 0.046528160572052\n",
      "step: 679510, loss: 0.0607503280043602, data time: 0.03750720024108887\n",
      "step: 679511, loss: 0.05860389024019241, data time: 0.03150626023610433\n",
      "step: 679512, loss: 0.06379138678312302, data time: 0.02723421369280134\n",
      "step: 679513, loss: 0.062326010316610336, data time: 0.02409982681274414\n",
      "step: 679514, loss: 0.06306546926498413, data time: 0.021574444240993924\n",
      "step: 679515, loss: 0.05836285650730133, data time: 0.01961958408355713\n",
      "step: 679516, loss: 0.06693984568119049, data time: 0.018037839369340378\n",
      "step: 679517, loss: 0.06009049341082573, data time: 0.016714632511138916\n",
      "step: 679518, loss: 0.05260432884097099, data time: 0.015595436096191406\n",
      "step: 679519, loss: 0.06247527897357941, data time: 0.014632667813982283\n",
      "step: 679520, loss: 0.06175782531499863, data time: 0.013800954818725586\n",
      "step: 679521, loss: 0.06411328911781311, data time: 0.013070419430732727\n",
      "step: 679522, loss: 0.0629567950963974, data time: 0.012422701891730814\n",
      "step: 679523, loss: 0.06583865731954575, data time: 0.011845535702175565\n",
      "step: 679524, loss: 0.06215638667345047, data time: 0.011331257067228618\n",
      "step: 679525, loss: 0.057831041514873505, data time: 0.010881185531616211\n",
      "step: 679526, loss: 0.06084014102816582, data time: 0.010465076991489955\n",
      "step: 679527, loss: 0.06457991898059845, data time: 0.010084499012340199\n",
      "step: 679528, loss: 0.061422668397426605, data time: 0.009734018989231276\n",
      "step: 679529, loss: 0.06384836137294769, data time: 0.009415666262308756\n",
      "step: 679530, loss: 0.06138492375612259, data time: 0.009129266738891601\n",
      "step: 679531, loss: 0.06218094751238823, data time: 0.008858341437119704\n",
      "step: 679532, loss: 0.06064077466726303, data time: 0.00860444704691569\n",
      "step: 679533, loss: 0.06723792850971222, data time: 0.00836773429598127\n",
      "step: 679534, loss: 0.05999733507633209, data time: 0.00815372631467622\n",
      "step: 679535, loss: 0.06685294210910797, data time: 0.007955217361450195\n",
      "step: 679536, loss: 0.05831228196620941, data time: 0.007768892472790134\n",
      "step: 679537, loss: 0.06436614692211151, data time: 0.007597662508487701\n",
      "step: 679538, loss: 0.06329640746116638, data time: 0.007431788878007369\n",
      "step: 679539, loss: 0.06095527857542038, data time: 0.0072724188075346105\n",
      "step: 679540, loss: 0.061509497463703156, data time: 0.007120425360543387\n",
      "step: 679541, loss: 0.06499220430850983, data time: 0.006976717048221164\n",
      "step: 679542, loss: 0.06752088665962219, data time: 0.006838946729092984\n",
      "step: 679543, loss: 0.06935268640518188, data time: 0.006711721420288086\n",
      "step: 679544, loss: 0.062098558992147446, data time: 0.006592261485564403\n",
      "step: 679545, loss: 0.07650575041770935, data time: 0.006477844715118408\n",
      "step: 679546, loss: 0.06060672923922539, data time: 0.1688387393951416\n",
      "step: 679547, loss: 0.05828315019607544, data time: 0.08622908592224121\n",
      "step: 679548, loss: 0.05374830216169357, data time: 0.05799412727355957\n",
      "step: 679549, loss: 0.06445803493261337, data time: 0.04425770044326782\n",
      "step: 679550, loss: 0.06492792069911957, data time: 0.03571538925170899\n",
      "step: 679551, loss: 0.05815906450152397, data time: 0.03002003828684489\n",
      "step: 679552, loss: 0.061173997819423676, data time: 0.025940895080566406\n",
      "step: 679553, loss: 0.05912167951464653, data time: 0.0229625403881073\n",
      "step: 679554, loss: 0.0667000412940979, data time: 0.020558410220675997\n",
      "step: 679555, loss: 0.06475464999675751, data time: 0.018703436851501463\n",
      "step: 679556, loss: 0.061134934425354004, data time: 0.017199884761463512\n",
      "step: 679557, loss: 0.06637723743915558, data time: 0.015952905019124348\n",
      "step: 679558, loss: 0.05876540765166283, data time: 0.014889276944673978\n",
      "step: 679559, loss: 0.06108522042632103, data time: 0.013975790568760462\n",
      "step: 679560, loss: 0.06305517256259918, data time: 0.013184372584025066\n",
      "step: 679561, loss: 0.05845162272453308, data time: 0.012490808963775635\n",
      "step: 679562, loss: 0.0615701787173748, data time: 0.011882094775929171\n",
      "step: 679563, loss: 0.062041621655225754, data time: 0.011333094702826606\n",
      "step: 679564, loss: 0.062137361615896225, data time: 0.010845849388524106\n",
      "step: 679565, loss: 0.06093961000442505, data time: 0.010414969921112061\n",
      "step: 679566, loss: 0.06416008621454239, data time: 0.010021425428844634\n",
      "step: 679567, loss: 0.06078045070171356, data time: 0.009667114777998491\n",
      "step: 679568, loss: 0.062376659363508224, data time: 0.009337984997293224\n",
      "step: 679569, loss: 0.06430023908615112, data time: 0.009040643771489462\n",
      "step: 679570, loss: 0.05943742394447327, data time: 0.008764972686767578\n",
      "step: 679571, loss: 0.062294602394104004, data time: 0.008510800508352427\n",
      "step: 679572, loss: 0.06559444963932037, data time: 0.008269115730568214\n",
      "step: 679573, loss: 0.06734994053840637, data time: 0.008044915539877755\n",
      "step: 679574, loss: 0.05878164991736412, data time: 0.007842844930188409\n",
      "step: 679575, loss: 0.061093732714653015, data time: 0.0076555649439493815\n",
      "step: 679576, loss: 0.05723389983177185, data time: 0.007478414043303459\n",
      "step: 679577, loss: 0.06138905882835388, data time: 0.007314577698707581\n",
      "step: 679578, loss: 0.06491391360759735, data time: 0.0071534749233361445\n",
      "step: 679579, loss: 0.06122574210166931, data time: 0.007000993279849782\n",
      "step: 679580, loss: 0.06363452970981598, data time: 0.006858124051775251\n",
      "step: 679581, loss: 0.05669720098376274, data time: 0.006719675328996446\n",
      "step: 679582, loss: 0.059468746185302734, data time: 0.0065903470322892475\n",
      "step: 679583, loss: 0.05726194381713867, data time: 0.006475109803049188\n",
      "step: 679584, loss: 0.06697532534599304, data time: 0.006361826872214293\n",
      "step: 679585, loss: 0.07381325960159302, data time: 0.006253904104232788\n",
      "step: 679586, loss: 0.05871658772230148, data time: 0.16409063339233398\n",
      "step: 679587, loss: 0.05712252855300903, data time: 0.08346796035766602\n",
      "step: 679588, loss: 0.06537618488073349, data time: 0.056692043940226235\n",
      "step: 679589, loss: 0.06805925816297531, data time: 0.04321044683456421\n",
      "step: 679590, loss: 0.05481679365038872, data time: 0.03490290641784668\n",
      "step: 679591, loss: 0.05575830861926079, data time: 0.0293805996576945\n",
      "step: 679592, loss: 0.05901534482836723, data time: 0.025434459958757673\n",
      "step: 679593, loss: 0.05975664407014847, data time: 0.022508710622787476\n",
      "step: 679594, loss: 0.05783090740442276, data time: 0.02016072803073459\n",
      "step: 679595, loss: 0.06618183851242065, data time: 0.01835134029388428\n",
      "step: 679596, loss: 0.06511706113815308, data time: 0.0168901573527943\n",
      "step: 679597, loss: 0.05840623006224632, data time: 0.01566372315088908\n",
      "step: 679598, loss: 0.06376530230045319, data time: 0.014628080221322866\n",
      "step: 679599, loss: 0.05841904878616333, data time: 0.013725808688572474\n",
      "step: 679600, loss: 0.05968853831291199, data time: 0.012955252329508464\n",
      "step: 679601, loss: 0.05994785577058792, data time: 0.012280121445655823\n",
      "step: 679602, loss: 0.06044446676969528, data time: 0.011677868225995232\n",
      "step: 679603, loss: 0.06640976667404175, data time: 0.011142876413133409\n",
      "step: 679604, loss: 0.06042777746915817, data time: 0.010670875248156096\n",
      "step: 679605, loss: 0.06769087165594101, data time: 0.010245585441589355\n",
      "step: 679606, loss: 0.0656266137957573, data time: 0.009864546003795806\n",
      "step: 679607, loss: 0.06161688268184662, data time: 0.009517051956870339\n",
      "step: 679608, loss: 0.061576467007398605, data time: 0.009191886238429857\n",
      "step: 679609, loss: 0.061516281217336655, data time: 0.008898297945658365\n",
      "step: 679610, loss: 0.06396132707595825, data time: 0.008628082275390626\n",
      "step: 679611, loss: 0.06412084400653839, data time: 0.00837393907400278\n",
      "step: 679612, loss: 0.06432574987411499, data time: 0.008136413715503834\n",
      "step: 679613, loss: 0.057931266725063324, data time: 0.007918902805873327\n",
      "step: 679614, loss: 0.06330603361129761, data time: 0.0077208321670006064\n",
      "step: 679615, loss: 0.055845364928245544, data time: 0.007536745071411133\n",
      "step: 679616, loss: 0.06359781324863434, data time: 0.00736234264989053\n",
      "step: 679617, loss: 0.062254227697849274, data time: 0.007202260196208954\n",
      "step: 679618, loss: 0.05809621140360832, data time: 0.007044886097763524\n",
      "step: 679619, loss: 0.0637408047914505, data time: 0.0068962644128238455\n",
      "step: 679620, loss: 0.06238307058811188, data time: 0.006754575456891741\n",
      "step: 679621, loss: 0.05903145298361778, data time: 0.0066183871693081325\n",
      "step: 679622, loss: 0.061184611171483994, data time: 0.006490984478512326\n",
      "step: 679623, loss: 0.06606921553611755, data time: 0.006374014051336991\n",
      "step: 679624, loss: 0.060240279883146286, data time: 0.006262932068262345\n",
      "step: 679625, loss: 0.07103633880615234, data time: 0.006159061193466186\n",
      "step: 679626, loss: 0.061626266688108444, data time: 0.16951847076416016\n",
      "step: 679627, loss: 0.06670279800891876, data time: 0.0855255126953125\n",
      "step: 679628, loss: 0.06658519804477692, data time: 0.058335065841674805\n",
      "step: 679629, loss: 0.06565222144126892, data time: 0.04413473606109619\n",
      "step: 679630, loss: 0.057133518159389496, data time: 0.03558826446533203\n",
      "step: 679631, loss: 0.06545858085155487, data time: 0.029921730359395344\n",
      "step: 679632, loss: 0.06916936486959457, data time: 0.0259702205657959\n",
      "step: 679633, loss: 0.058512650430202484, data time: 0.022903025150299072\n",
      "step: 679634, loss: 0.06234358251094818, data time: 0.02051332261827257\n",
      "step: 679635, loss: 0.06331987679004669, data time: 0.018662667274475096\n",
      "step: 679636, loss: 0.059873372316360474, data time: 0.017193880948153408\n",
      "step: 679637, loss: 0.06439261883497238, data time: 0.01593613624572754\n",
      "step: 679638, loss: 0.06724748760461807, data time: 0.01487282606271597\n",
      "step: 679639, loss: 0.06347104907035828, data time: 0.013956342424665178\n",
      "step: 679640, loss: 0.0583709254860878, data time: 0.013180716832478841\n",
      "step: 679641, loss: 0.07315036654472351, data time: 0.012496128678321838\n",
      "step: 679642, loss: 0.06697329133749008, data time: 0.011880341698141658\n",
      "step: 679643, loss: 0.0611359104514122, data time: 0.011330538325839572\n",
      "step: 679644, loss: 0.06422961503267288, data time: 0.010842938172189813\n",
      "step: 679645, loss: 0.060471147298812866, data time: 0.010422623157501221\n",
      "step: 679646, loss: 0.060916703194379807, data time: 0.010045891716366722\n",
      "step: 679647, loss: 0.06662319600582123, data time: 0.009705532680858265\n",
      "step: 679648, loss: 0.061814915388822556, data time: 0.009388819984767748\n",
      "step: 679649, loss: 0.061017997562885284, data time: 0.009104857842127482\n",
      "step: 679650, loss: 0.06515832245349884, data time: 0.008839960098266602\n",
      "step: 679651, loss: 0.06486719846725464, data time: 0.008594210331256572\n",
      "step: 679652, loss: 0.059398531913757324, data time: 0.00836436836807816\n",
      "step: 679653, loss: 0.06179637461900711, data time: 0.008150603090013777\n",
      "step: 679654, loss: 0.05874332785606384, data time: 0.007955263400899953\n",
      "step: 679655, loss: 0.06133294850587845, data time: 0.0077729463577270504\n",
      "step: 679656, loss: 0.06278295814990997, data time: 0.007603783761301349\n",
      "step: 679657, loss: 0.06371548771858215, data time: 0.0074475109577178955\n",
      "step: 679658, loss: 0.05970553308725357, data time: 0.007286909854773319\n",
      "step: 679659, loss: 0.060313135385513306, data time: 0.0071356016046860635\n",
      "step: 679660, loss: 0.06213918328285217, data time: 0.0069905894143240796\n",
      "step: 679661, loss: 0.0630391538143158, data time: 0.006853640079498291\n",
      "step: 679662, loss: 0.06593988090753555, data time: 0.006724086967674461\n",
      "step: 679663, loss: 0.065187469124794, data time: 0.006603623691358064\n",
      "step: 679664, loss: 0.06259433180093765, data time: 0.0064893380189553285\n",
      "step: 679665, loss: 0.09752479195594788, data time: 0.0063801109790802\n",
      "step: 679666, loss: 0.05891767144203186, data time: 0.17113971710205078\n",
      "step: 679667, loss: 0.06111651659011841, data time: 0.08632266521453857\n",
      "step: 679668, loss: 0.05702605098485947, data time: 0.05855290095011393\n",
      "step: 679669, loss: 0.06270620226860046, data time: 0.04457521438598633\n",
      "step: 679670, loss: 0.06247827410697937, data time: 0.03594508171081543\n",
      "step: 679671, loss: 0.06611409038305283, data time: 0.030212958653767902\n",
      "step: 679672, loss: 0.06468968838453293, data time: 0.026114906583513533\n",
      "step: 679673, loss: 0.06237392872571945, data time: 0.0231035053730011\n",
      "step: 679674, loss: 0.06268086284399033, data time: 0.020685672760009766\n",
      "step: 679675, loss: 0.06012687087059021, data time: 0.018816184997558594\n",
      "step: 679676, loss: 0.06086946651339531, data time: 0.017297224564985794\n",
      "step: 679677, loss: 0.06415726244449615, data time: 0.016040603319803875\n",
      "step: 679678, loss: 0.059247128665447235, data time: 0.01498115979708158\n",
      "step: 679679, loss: 0.06821944564580917, data time: 0.014054792267935616\n",
      "step: 679680, loss: 0.055087946355342865, data time: 0.013285811742146809\n",
      "step: 679681, loss: 0.05954461917281151, data time: 0.012612015008926392\n",
      "step: 679682, loss: 0.06054515391588211, data time: 0.012013561585370232\n",
      "step: 679683, loss: 0.056106530129909515, data time: 0.011476344532436795\n",
      "step: 679684, loss: 0.0568050816655159, data time: 0.010997860055220755\n",
      "step: 679685, loss: 0.059936463832855225, data time: 0.010570383071899414\n",
      "step: 679686, loss: 0.06280562281608582, data time: 0.010186354319254557\n",
      "step: 679687, loss: 0.06032591313123703, data time: 0.009839448061856356\n",
      "step: 679688, loss: 0.0629056841135025, data time: 0.00952048923658288\n",
      "step: 679689, loss: 0.06439395248889923, data time: 0.009229302406311035\n",
      "step: 679690, loss: 0.05511055514216423, data time: 0.008958702087402343\n",
      "step: 679691, loss: 0.057830147445201874, data time: 0.008706120344308706\n",
      "step: 679692, loss: 0.06128076836466789, data time: 0.008471709710580332\n",
      "step: 679693, loss: 0.06385548412799835, data time: 0.008253863879612513\n",
      "step: 679694, loss: 0.05597643181681633, data time: 0.008046857241926521\n",
      "step: 679695, loss: 0.05724722146987915, data time: 0.007850050926208496\n",
      "step: 679696, loss: 0.06195253133773804, data time: 0.007665418809460056\n",
      "step: 679697, loss: 0.059272777289152145, data time: 0.007496044039726257\n",
      "step: 679698, loss: 0.06120990961790085, data time: 0.007331111214377664\n",
      "step: 679699, loss: 0.06738001108169556, data time: 0.007174281512989718\n",
      "step: 679700, loss: 0.06687112152576447, data time: 0.007023559297834124\n",
      "step: 679701, loss: 0.058667875826358795, data time: 0.0068797336684332955\n",
      "step: 679702, loss: 0.05500072240829468, data time: 0.006745615520992795\n",
      "step: 679703, loss: 0.06128381937742233, data time: 0.00662097178007427\n",
      "step: 679704, loss: 0.05350736528635025, data time: 0.006504266689985226\n",
      "step: 679705, loss: 0.10060307383537292, data time: 0.006393200159072876\n",
      "step: 679706, loss: 0.06668657064437866, data time: 0.16843390464782715\n",
      "step: 679707, loss: 0.059875018894672394, data time: 0.0849912166595459\n",
      "step: 679708, loss: 0.05942755937576294, data time: 0.057578484217325844\n",
      "step: 679709, loss: 0.06979724019765854, data time: 0.04404556751251221\n",
      "step: 679710, loss: 0.060645561665296555, data time: 0.035569047927856444\n",
      "step: 679711, loss: 0.06266127526760101, data time: 0.02992435296376546\n",
      "step: 679712, loss: 0.05787014216184616, data time: 0.02590445109776088\n",
      "step: 679713, loss: 0.06819428503513336, data time: 0.02297261357307434\n",
      "step: 679714, loss: 0.06273747235536575, data time: 0.020595868428548176\n",
      "step: 679715, loss: 0.06561927497386932, data time: 0.018783998489379884\n",
      "step: 679716, loss: 0.06356975436210632, data time: 0.01730955730785023\n",
      "step: 679717, loss: 0.0608331523835659, data time: 0.016077240308125813\n",
      "step: 679718, loss: 0.06721313297748566, data time: 0.015032694889948918\n",
      "step: 679719, loss: 0.06371106207370758, data time: 0.014133504458836146\n",
      "step: 679720, loss: 0.06344617158174515, data time: 0.013354969024658204\n",
      "step: 679721, loss: 0.05933843180537224, data time: 0.012678489089012146\n",
      "step: 679722, loss: 0.06429405510425568, data time: 0.01207404978135053\n",
      "step: 679723, loss: 0.06278073042631149, data time: 0.01153172387017144\n",
      "step: 679724, loss: 0.06261774897575378, data time: 0.011053712744461862\n",
      "step: 679725, loss: 0.06012120097875595, data time: 0.010629916191101074\n",
      "step: 679726, loss: 0.07219129800796509, data time: 0.010242132913498651\n",
      "step: 679727, loss: 0.0537194162607193, data time: 0.009889255870472301\n",
      "step: 679728, loss: 0.06106933206319809, data time: 0.009561901507170303\n",
      "step: 679729, loss: 0.06290188431739807, data time: 0.009266565243403116\n",
      "step: 679730, loss: 0.06001420319080353, data time: 0.008998289108276367\n",
      "step: 679731, loss: 0.06321971118450165, data time: 0.008746761542100173\n",
      "step: 679732, loss: 0.06217340752482414, data time: 0.008508231904771592\n",
      "step: 679733, loss: 0.06795334815979004, data time: 0.008287157331194197\n",
      "step: 679734, loss: 0.06462226808071136, data time: 0.008089920570110452\n",
      "step: 679735, loss: 0.06466957926750183, data time: 0.00790235201517741\n",
      "step: 679736, loss: 0.06518541276454926, data time: 0.007728453605405746\n",
      "step: 679737, loss: 0.06038100644946098, data time: 0.007568955421447754\n",
      "step: 679738, loss: 0.06459000706672668, data time: 0.007403893904252486\n",
      "step: 679739, loss: 0.062113016843795776, data time: 0.007249046774471507\n",
      "step: 679740, loss: 0.059713512659072876, data time: 0.007102475847516741\n",
      "step: 679741, loss: 0.06455528736114502, data time: 0.0069595641560024684\n",
      "step: 679742, loss: 0.0576852485537529, data time: 0.006826065682076119\n",
      "step: 679743, loss: 0.061386194080114365, data time: 0.006702366628144917\n",
      "step: 679744, loss: 0.05983757972717285, data time: 0.006585420706333258\n",
      "step: 679745, loss: 0.043465957045555115, data time: 0.006474816799163818\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4079720973968506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 679746, loss: 0.06284049153327942, data time: 0.17832446098327637\n",
      "step: 679747, loss: 0.06393066048622131, data time: 0.09053266048431396\n",
      "step: 679748, loss: 0.06565412133932114, data time: 0.0613861878712972\n",
      "step: 679749, loss: 0.06079898774623871, data time: 0.04667884111404419\n",
      "step: 679750, loss: 0.058440715074539185, data time: 0.03763742446899414\n",
      "step: 679751, loss: 0.06161315366625786, data time: 0.03161243597666422\n",
      "step: 679752, loss: 0.0623682364821434, data time: 0.027308293751307895\n",
      "step: 679753, loss: 0.05687183886766434, data time: 0.02414083480834961\n",
      "step: 679754, loss: 0.061058949679136276, data time: 0.02160440550910102\n",
      "step: 679755, loss: 0.060553789138793945, data time: 0.019641304016113283\n",
      "step: 679756, loss: 0.06518791615962982, data time: 0.01804772290316495\n",
      "step: 679757, loss: 0.06527353823184967, data time: 0.01672796408335368\n",
      "step: 679758, loss: 0.059192758053541183, data time: 0.015596353090726413\n",
      "step: 679759, loss: 0.05939801037311554, data time: 0.014624885150364466\n",
      "step: 679760, loss: 0.06417937576770782, data time: 0.013794708251953124\n",
      "step: 679761, loss: 0.0618649497628212, data time: 0.013062462210655212\n",
      "step: 679762, loss: 0.0606507770717144, data time: 0.012415619457469267\n",
      "step: 679763, loss: 0.059614360332489014, data time: 0.011852555804782443\n",
      "step: 679764, loss: 0.06029411777853966, data time: 0.011331307260613693\n",
      "step: 679765, loss: 0.0595247745513916, data time: 0.01087406873703003\n",
      "step: 679766, loss: 0.06259331852197647, data time: 0.01045853751046317\n",
      "step: 679767, loss: 0.06505639851093292, data time: 0.010077454827048561\n",
      "step: 679768, loss: 0.07006809115409851, data time: 0.009728151818980341\n",
      "step: 679769, loss: 0.059549279510974884, data time: 0.009411593278249105\n",
      "step: 679770, loss: 0.06457621604204178, data time: 0.009116916656494141\n",
      "step: 679771, loss: 0.05936725065112114, data time: 0.00884259664095365\n",
      "step: 679772, loss: 0.054945170879364014, data time: 0.008599793469464337\n",
      "step: 679773, loss: 0.06051888316869736, data time: 0.008375976766858782\n",
      "step: 679774, loss: 0.0699726939201355, data time: 0.008174764698949354\n",
      "step: 679775, loss: 0.05702420324087143, data time: 0.007987356185913086\n",
      "step: 679776, loss: 0.06068195402622223, data time: 0.007807985428840884\n",
      "step: 679777, loss: 0.056611910462379456, data time: 0.007643900811672211\n",
      "step: 679778, loss: 0.05952782556414604, data time: 0.007476243105801669\n",
      "step: 679779, loss: 0.06420352309942245, data time: 0.007316259776844698\n",
      "step: 679780, loss: 0.061206694692373276, data time: 0.00716475078037807\n",
      "step: 679781, loss: 0.05854938551783562, data time: 0.007019744979010688\n",
      "step: 679782, loss: 0.06704509258270264, data time: 0.0068832088161159205\n",
      "step: 679783, loss: 0.0586896613240242, data time: 0.006758583219427811\n",
      "step: 679784, loss: 0.07322726398706436, data time: 0.006640776609763121\n",
      "step: 679785, loss: 0.043070629239082336, data time: 0.006525915861129761\n",
      "step: 679786, loss: 0.06492055952548981, data time: 0.19267511367797852\n",
      "step: 679787, loss: 0.06699492782354355, data time: 0.09709000587463379\n",
      "step: 679788, loss: 0.0673859640955925, data time: 0.06560818354288737\n",
      "step: 679789, loss: 0.05891730636358261, data time: 0.04996669292449951\n",
      "step: 679790, loss: 0.06166304275393486, data time: 0.04024934768676758\n",
      "step: 679791, loss: 0.0696062296628952, data time: 0.033770362536112465\n",
      "step: 679792, loss: 0.06309738010168076, data time: 0.0291616576058524\n",
      "step: 679793, loss: 0.05536038801074028, data time: 0.0257720947265625\n",
      "step: 679794, loss: 0.06237664818763733, data time: 0.023061407936943903\n",
      "step: 679795, loss: 0.0667542889714241, data time: 0.020948266983032225\n",
      "step: 679796, loss: 0.06288603693246841, data time: 0.01923548091541637\n",
      "step: 679797, loss: 0.06214969605207443, data time: 0.0178108016649882\n",
      "step: 679798, loss: 0.06210549920797348, data time: 0.0166323918562669\n",
      "step: 679799, loss: 0.06095218285918236, data time: 0.015616110392979212\n",
      "step: 679800, loss: 0.05913182720541954, data time: 0.014736477533976238\n",
      "step: 679801, loss: 0.064443439245224, data time: 0.013969451189041138\n",
      "step: 679802, loss: 0.06372074782848358, data time: 0.0132903772241929\n",
      "step: 679803, loss: 0.0691952034831047, data time: 0.012678080134921603\n",
      "step: 679804, loss: 0.06440138816833496, data time: 0.012136572285702354\n",
      "step: 679805, loss: 0.0631563737988472, data time: 0.011666989326477051\n",
      "step: 679806, loss: 0.06428854912519455, data time: 0.011231853848411924\n",
      "step: 679807, loss: 0.06270058453083038, data time: 0.010834184559908781\n",
      "step: 679808, loss: 0.057226937264204025, data time: 0.010464813398278278\n",
      "step: 679809, loss: 0.06473792344331741, data time: 0.010127842426300049\n",
      "step: 679810, loss: 0.06661242246627808, data time: 0.00982264518737793\n",
      "step: 679811, loss: 0.05893312022089958, data time: 0.009535844509418193\n",
      "step: 679812, loss: 0.0632009208202362, data time: 0.009268142558910229\n",
      "step: 679813, loss: 0.06646299362182617, data time: 0.009020796843937464\n",
      "step: 679814, loss: 0.060193877667188644, data time: 0.008795294268377897\n",
      "step: 679815, loss: 0.06104966253042221, data time: 0.008586986859639486\n",
      "step: 679816, loss: 0.06124858185648918, data time: 0.008390118998865928\n",
      "step: 679817, loss: 0.05846942216157913, data time: 0.008208885788917542\n",
      "step: 679818, loss: 0.06395798921585083, data time: 0.008021925434921726\n",
      "step: 679819, loss: 0.060551878064870834, data time: 0.007848739624023438\n",
      "step: 679820, loss: 0.0633353516459465, data time: 0.007682793481009347\n",
      "step: 679821, loss: 0.06044686958193779, data time: 0.007523099581400554\n",
      "step: 679822, loss: 0.05912297964096069, data time: 0.007373017233771247\n",
      "step: 679823, loss: 0.058832429349422455, data time: 0.007235288619995117\n",
      "step: 679824, loss: 0.06608139723539352, data time: 0.007103913869613256\n",
      "step: 679825, loss: 0.060981348156929016, data time: 0.006978863477706909\n",
      "step: 679826, loss: 0.06421265006065369, data time: 0.17510390281677246\n",
      "step: 679827, loss: 0.05810936912894249, data time: 0.08832848072052002\n",
      "step: 679828, loss: 0.06298859417438507, data time: 0.05991593996683756\n",
      "step: 679829, loss: 0.0635610818862915, data time: 0.045564472675323486\n",
      "step: 679830, loss: 0.06120717525482178, data time: 0.036759042739868165\n",
      "step: 679831, loss: 0.06485863775014877, data time: 0.030884663263956707\n",
      "step: 679832, loss: 0.06370748579502106, data time: 0.026705469403948103\n",
      "step: 679833, loss: 0.06266550719738007, data time: 0.02362123131752014\n",
      "step: 679834, loss: 0.07010984420776367, data time: 0.021141025755140517\n",
      "step: 679835, loss: 0.06063484400510788, data time: 0.019239091873168947\n",
      "step: 679836, loss: 0.07073119282722473, data time: 0.017683982849121094\n",
      "step: 679837, loss: 0.06662578880786896, data time: 0.016388873259226482\n",
      "step: 679838, loss: 0.06578727066516876, data time: 0.015287105853740986\n",
      "step: 679839, loss: 0.060632094740867615, data time: 0.014337488583156041\n",
      "step: 679840, loss: 0.05940474197268486, data time: 0.013523213068644206\n",
      "step: 679841, loss: 0.06362064927816391, data time: 0.012813642621040344\n",
      "step: 679842, loss: 0.06398326903581619, data time: 0.012177397223079907\n",
      "step: 679843, loss: 0.06207222864031792, data time: 0.011610560946994357\n",
      "step: 679844, loss: 0.061775773763656616, data time: 0.011103943774574682\n",
      "step: 679845, loss: 0.06175047159194946, data time: 0.010676610469818115\n",
      "step: 679846, loss: 0.06509342789649963, data time: 0.010287352970668249\n",
      "step: 679847, loss: 0.0636913925409317, data time: 0.0099321495402943\n",
      "step: 679848, loss: 0.07183808088302612, data time: 0.009608424228170643\n",
      "step: 679849, loss: 0.06563413143157959, data time: 0.009312798579533895\n",
      "step: 679850, loss: 0.06574446707963943, data time: 0.009038457870483399\n",
      "step: 679851, loss: 0.05818525701761246, data time: 0.008779764175415039\n",
      "step: 679852, loss: 0.06786201894283295, data time: 0.008539367605138707\n",
      "step: 679853, loss: 0.06263785809278488, data time: 0.008321881294250488\n",
      "step: 679854, loss: 0.05630263686180115, data time: 0.008121613798470333\n",
      "step: 679855, loss: 0.057655103504657745, data time: 0.007936414082845051\n",
      "step: 679856, loss: 0.06826060265302658, data time: 0.007756925398303616\n",
      "step: 679857, loss: 0.06666933000087738, data time: 0.007595807313919067\n",
      "step: 679858, loss: 0.05876871570944786, data time: 0.007430307792894768\n",
      "step: 679859, loss: 0.06068459898233414, data time: 0.007274676771724925\n",
      "step: 679860, loss: 0.06031995266675949, data time: 0.007123395374843053\n",
      "step: 679861, loss: 0.06952224671840668, data time: 0.006980021794637044\n",
      "step: 679862, loss: 0.058453284204006195, data time: 0.006847059404527819\n",
      "step: 679863, loss: 0.05450382083654404, data time: 0.006723071399487947\n",
      "step: 679864, loss: 0.06347683072090149, data time: 0.0066055395664312904\n",
      "step: 679865, loss: 0.05361101031303406, data time: 0.006491434574127197\n",
      "step: 679866, loss: 0.0626162737607956, data time: 0.18717360496520996\n",
      "step: 679867, loss: 0.0617937408387661, data time: 0.09433948993682861\n",
      "step: 679868, loss: 0.059344422072172165, data time: 0.06421224276224773\n",
      "step: 679869, loss: 0.05973136052489281, data time: 0.04852229356765747\n",
      "step: 679870, loss: 0.05831890553236008, data time: 0.03910059928894043\n",
      "step: 679871, loss: 0.06672472506761551, data time: 0.0328452984491984\n",
      "step: 679872, loss: 0.05819384381175041, data time: 0.02845750536237444\n",
      "step: 679873, loss: 0.060836710035800934, data time: 0.02507460117340088\n",
      "step: 679874, loss: 0.0654166042804718, data time: 0.022440804375542536\n",
      "step: 679875, loss: 0.06018998846411705, data time: 0.020393180847167968\n",
      "step: 679876, loss: 0.05781244486570358, data time: 0.01873692599209872\n",
      "step: 679877, loss: 0.06554977595806122, data time: 0.017355481783548992\n",
      "step: 679878, loss: 0.057284872978925705, data time: 0.016186035596407376\n",
      "step: 679879, loss: 0.059940092265605927, data time: 0.015172600746154785\n",
      "step: 679880, loss: 0.05872093886137009, data time: 0.014301506678263347\n",
      "step: 679881, loss: 0.06566349416971207, data time: 0.013538539409637451\n",
      "step: 679882, loss: 0.0672762393951416, data time: 0.012864757986629711\n",
      "step: 679883, loss: 0.05586327612400055, data time: 0.012262609269883897\n",
      "step: 679884, loss: 0.06615889072418213, data time: 0.011730583090531198\n",
      "step: 679885, loss: 0.06425011157989502, data time: 0.011252033710479736\n",
      "step: 679886, loss: 0.05977253243327141, data time: 0.010816335678100586\n",
      "step: 679887, loss: 0.06341788172721863, data time: 0.010419498790394176\n",
      "step: 679888, loss: 0.06034260243177414, data time: 0.010058123132456903\n",
      "step: 679889, loss: 0.0635579451918602, data time: 0.009727646907170614\n",
      "step: 679890, loss: 0.06755191087722778, data time: 0.009436750411987304\n",
      "step: 679891, loss: 0.06203039735555649, data time: 0.00915820782001202\n",
      "step: 679892, loss: 0.06607028096914291, data time: 0.008896103611698857\n",
      "step: 679893, loss: 0.06717312335968018, data time: 0.008650158132825578\n",
      "step: 679894, loss: 0.06541730463504791, data time: 0.00842616475861648\n",
      "step: 679895, loss: 0.06148283928632736, data time: 0.008216325441996257\n",
      "step: 679896, loss: 0.06703443080186844, data time: 0.008017686105543567\n",
      "step: 679897, loss: 0.05849402770400047, data time: 0.007839031517505646\n",
      "step: 679898, loss: 0.0605333112180233, data time: 0.007662317969582297\n",
      "step: 679899, loss: 0.0579051747918129, data time: 0.007494106012232164\n",
      "step: 679900, loss: 0.0610051155090332, data time: 0.007334797722952706\n",
      "step: 679901, loss: 0.0601685494184494, data time: 0.00718260473675198\n",
      "step: 679902, loss: 0.060754161328077316, data time: 0.007038554629764041\n",
      "step: 679903, loss: 0.06459492444992065, data time: 0.006905750224464818\n",
      "step: 679904, loss: 0.06518785655498505, data time: 0.006780061966333634\n",
      "step: 679905, loss: 0.10479657351970673, data time: 0.0066587507724761965\n",
      "step: 679906, loss: 0.06814215332269669, data time: 0.16997551918029785\n",
      "step: 679907, loss: 0.061572473496198654, data time: 0.08633625507354736\n",
      "step: 679908, loss: 0.06466764211654663, data time: 0.05859454472859701\n",
      "step: 679909, loss: 0.06064945459365845, data time: 0.044617414474487305\n",
      "step: 679910, loss: 0.06442052125930786, data time: 0.035964345932006835\n",
      "step: 679911, loss: 0.0670166015625, data time: 0.03024117151896159\n",
      "step: 679912, loss: 0.06456606835126877, data time: 0.02612570353916713\n",
      "step: 679913, loss: 0.06599446386098862, data time: 0.02312120795249939\n",
      "step: 679914, loss: 0.06178714334964752, data time: 0.02069507704840766\n",
      "step: 679915, loss: 0.06860735267400742, data time: 0.018842601776123048\n",
      "step: 679916, loss: 0.06427346169948578, data time: 0.017321976748379795\n",
      "step: 679917, loss: 0.061863988637924194, data time: 0.01605359713236491\n",
      "step: 679918, loss: 0.06276056170463562, data time: 0.014981361535879282\n",
      "step: 679919, loss: 0.05885279178619385, data time: 0.014058538845607213\n",
      "step: 679920, loss: 0.05841746926307678, data time: 0.013260269165039062\n",
      "step: 679921, loss: 0.059966862201690674, data time: 0.012585505843162537\n",
      "step: 679922, loss: 0.06548026949167252, data time: 0.011986718458287856\n",
      "step: 679923, loss: 0.07039245963096619, data time: 0.011446807119581435\n",
      "step: 679924, loss: 0.06264698505401611, data time: 0.010974946774934468\n",
      "step: 679925, loss: 0.06371629983186722, data time: 0.010549795627593995\n",
      "step: 679926, loss: 0.06437190622091293, data time: 0.010167984735398065\n",
      "step: 679927, loss: 0.05977056175470352, data time: 0.009820667180148039\n",
      "step: 679928, loss: 0.05142934247851372, data time: 0.009498129720273226\n",
      "step: 679929, loss: 0.05868469923734665, data time: 0.009206116199493408\n",
      "step: 679930, loss: 0.059588607400655746, data time: 0.008934822082519531\n",
      "step: 679931, loss: 0.054534345865249634, data time: 0.00868681760934683\n",
      "step: 679932, loss: 0.064874067902565, data time: 0.008448733223809136\n",
      "step: 679933, loss: 0.06632618606090546, data time: 0.00823143550327846\n",
      "step: 679934, loss: 0.06501545757055283, data time: 0.00803460745975889\n",
      "step: 679935, loss: 0.059470318257808685, data time: 0.007851155598958333\n",
      "step: 679936, loss: 0.05966398864984512, data time: 0.007682331146732453\n",
      "step: 679937, loss: 0.05886425822973251, data time: 0.007523037493228912\n",
      "step: 679938, loss: 0.059617217630147934, data time: 0.00735945412606904\n",
      "step: 679939, loss: 0.059775494039058685, data time: 0.007203810355242561\n",
      "step: 679940, loss: 0.05962241813540459, data time: 0.007054941994803292\n",
      "step: 679941, loss: 0.06558805704116821, data time: 0.006912648677825928\n",
      "step: 679942, loss: 0.06284172832965851, data time: 0.006779619165368982\n",
      "step: 679943, loss: 0.0636897012591362, data time: 0.00665677221197831\n",
      "step: 679944, loss: 0.06329823285341263, data time: 0.006540726392697065\n",
      "step: 679945, loss: 0.055053241550922394, data time: 0.006431585550308228\n",
      "step: 679946, loss: 0.057947807013988495, data time: 0.16635632514953613\n",
      "step: 679947, loss: 0.06205451488494873, data time: 0.08482933044433594\n",
      "step: 679948, loss: 0.06824148446321487, data time: 0.05706596374511719\n",
      "step: 679949, loss: 0.06489212810993195, data time: 0.04364800453186035\n",
      "step: 679950, loss: 0.06776945292949677, data time: 0.03519144058227539\n",
      "step: 679951, loss: 0.06083109974861145, data time: 0.029568155606587727\n",
      "step: 679952, loss: 0.051649194210767746, data time: 0.02553738866533552\n",
      "step: 679953, loss: 0.054611414670944214, data time: 0.022617995738983154\n",
      "step: 679954, loss: 0.0641702264547348, data time: 0.020246929592556424\n",
      "step: 679955, loss: 0.06759534776210785, data time: 0.018423247337341308\n",
      "step: 679956, loss: 0.06433267146348953, data time: 0.016942110928622158\n",
      "step: 679957, loss: 0.06546779721975327, data time: 0.015706419944763184\n",
      "step: 679958, loss: 0.0597585067152977, data time: 0.01466089028578538\n",
      "step: 679959, loss: 0.05983392521739006, data time: 0.013759374618530273\n",
      "step: 679960, loss: 0.059795331209897995, data time: 0.01298352877298991\n",
      "step: 679961, loss: 0.06390804052352905, data time: 0.012298598885536194\n",
      "step: 679962, loss: 0.06004514545202255, data time: 0.011699157602646771\n",
      "step: 679963, loss: 0.06654352694749832, data time: 0.011162837346394857\n",
      "step: 679964, loss: 0.06840892136096954, data time: 0.010682858918842516\n",
      "step: 679965, loss: 0.06503976881504059, data time: 0.010255730152130127\n",
      "step: 679966, loss: 0.06502161175012589, data time: 0.009867214021228608\n",
      "step: 679967, loss: 0.06285342574119568, data time: 0.009516976096413353\n",
      "step: 679968, loss: 0.06768600642681122, data time: 0.009195400320965311\n",
      "step: 679969, loss: 0.06334170699119568, data time: 0.008899003267288208\n",
      "step: 679970, loss: 0.057575058192014694, data time: 0.008624343872070313\n",
      "step: 679971, loss: 0.05831203982234001, data time: 0.00838608925159161\n",
      "step: 679972, loss: 0.0642874538898468, data time: 0.00815882506193938\n",
      "step: 679973, loss: 0.0592273585498333, data time: 0.007949088300977434\n",
      "step: 679974, loss: 0.05899377912282944, data time: 0.007762177237148942\n",
      "step: 679975, loss: 0.06392461806535721, data time: 0.007587869962056478\n",
      "step: 679976, loss: 0.06634309887886047, data time: 0.007423654679329165\n",
      "step: 679977, loss: 0.06167852133512497, data time: 0.007272101938724518\n",
      "step: 679978, loss: 0.06188362464308739, data time: 0.007114598245331736\n",
      "step: 679979, loss: 0.06378909945487976, data time: 0.006963575587553137\n",
      "step: 679980, loss: 0.06164588779211044, data time: 0.006823791776384626\n",
      "step: 679981, loss: 0.06440265476703644, data time: 0.0066886213090684675\n",
      "step: 679982, loss: 0.05662048980593681, data time: 0.006563869682518211\n",
      "step: 679983, loss: 0.06642420589923859, data time: 0.006447685392279374\n",
      "step: 679984, loss: 0.05868970602750778, data time: 0.0063375387436304335\n",
      "step: 679985, loss: 0.05144300311803818, data time: 0.006233769655227661\n",
      "step: 679986, loss: 0.06598571687936783, data time: 0.17035794258117676\n",
      "step: 679987, loss: 0.06206207722425461, data time: 0.08595025539398193\n",
      "step: 679988, loss: 0.06137885898351669, data time: 0.057808876037597656\n",
      "step: 679989, loss: 0.06085928529500961, data time: 0.044208824634552\n",
      "step: 679990, loss: 0.06176428124308586, data time: 0.03565711975097656\n",
      "step: 679991, loss: 0.0622786208987236, data time: 0.029959797859191895\n",
      "step: 679992, loss: 0.06424738466739655, data time: 0.02590193067278181\n",
      "step: 679993, loss: 0.06154460459947586, data time: 0.022929608821868896\n",
      "step: 679994, loss: 0.05978144705295563, data time: 0.020530435774061415\n",
      "step: 679995, loss: 0.06667298823595047, data time: 0.018677306175231934\n",
      "step: 679996, loss: 0.06562232971191406, data time: 0.017172293229536575\n",
      "step: 679997, loss: 0.060228101909160614, data time: 0.01592065890630086\n",
      "step: 679998, loss: 0.06335916370153427, data time: 0.014861877147967998\n",
      "step: 679999, loss: 0.06434053182601929, data time: 0.013949206897190638\n",
      "step: 680000, loss: 0.0633493959903717, data time: 0.01315755844116211\n",
      "step: 680001, loss: 0.059249795973300934, data time: 0.012467101216316223\n",
      "step: 680002, loss: 0.05813580006361008, data time: 0.011857804130105412\n",
      "step: 680003, loss: 0.06037379428744316, data time: 0.011305888493855795\n",
      "step: 680004, loss: 0.06382765620946884, data time: 0.010815105940166273\n",
      "step: 680005, loss: 0.07095464318990707, data time: 0.010381758213043213\n",
      "step: 680006, loss: 0.06511592864990234, data time: 0.009990010942731584\n",
      "step: 680007, loss: 0.059958718717098236, data time: 0.009633454409512606\n",
      "step: 680008, loss: 0.06312575191259384, data time: 0.00930128926816194\n",
      "step: 680009, loss: 0.05913093686103821, data time: 0.008998324473698934\n",
      "step: 680010, loss: 0.05973408743739128, data time: 0.008721418380737304\n",
      "step: 680011, loss: 0.060942888259887695, data time: 0.008467435836791992\n",
      "step: 680012, loss: 0.06119470298290253, data time: 0.00822813422591598\n",
      "step: 680013, loss: 0.06665261089801788, data time: 0.008005406175340925\n",
      "step: 680014, loss: 0.06486416608095169, data time: 0.007802667288944639\n",
      "step: 680015, loss: 0.060249313712120056, data time: 0.007613285382588705\n",
      "step: 680016, loss: 0.061546944081783295, data time: 0.007437375284010364\n",
      "step: 680017, loss: 0.057817619293928146, data time: 0.007288768887519836\n",
      "step: 680018, loss: 0.06740044057369232, data time: 0.007130687887018377\n",
      "step: 680019, loss: 0.05838654190301895, data time: 0.00697776149300968\n",
      "step: 680020, loss: 0.06143695116043091, data time: 0.006836012431553432\n",
      "step: 680021, loss: 0.05890566110610962, data time: 0.00669705867767334\n",
      "step: 680022, loss: 0.06460152566432953, data time: 0.006568882916424726\n",
      "step: 680023, loss: 0.06272529065608978, data time: 0.0064479740042435495\n",
      "step: 680024, loss: 0.06099232658743858, data time: 0.006334359829242413\n",
      "step: 680025, loss: 0.05361517518758774, data time: 0.006227099895477295\n",
      "step: 680026, loss: 0.06263270229101181, data time: 0.16753268241882324\n",
      "step: 680027, loss: 0.053500473499298096, data time: 0.08513772487640381\n",
      "step: 680028, loss: 0.058598749339580536, data time: 0.05777923266092936\n",
      "step: 680029, loss: 0.057469308376312256, data time: 0.04401427507400513\n",
      "step: 680030, loss: 0.06054156646132469, data time: 0.03548579216003418\n",
      "step: 680031, loss: 0.0634116679430008, data time: 0.02983268102010091\n",
      "step: 680032, loss: 0.05457007884979248, data time: 0.025806869779314314\n",
      "step: 680033, loss: 0.06716616451740265, data time: 0.022857338190078735\n",
      "step: 680034, loss: 0.05804531276226044, data time: 0.020463069279988606\n",
      "step: 680035, loss: 0.05702762305736542, data time: 0.01861703395843506\n",
      "step: 680036, loss: 0.065165676176548, data time: 0.017115744677456943\n",
      "step: 680037, loss: 0.06208604574203491, data time: 0.01586868365605672\n",
      "step: 680038, loss: 0.061249375343322754, data time: 0.014811130670400767\n",
      "step: 680039, loss: 0.06378345936536789, data time: 0.01389467716217041\n",
      "step: 680040, loss: 0.057897329330444336, data time: 0.013109620412190754\n",
      "step: 680041, loss: 0.065892294049263, data time: 0.012420043349266052\n",
      "step: 680042, loss: 0.059938155114650726, data time: 0.011805239845724666\n",
      "step: 680043, loss: 0.060801587998867035, data time: 0.011259714762369791\n",
      "step: 680044, loss: 0.06477390229701996, data time: 0.010779054541336862\n",
      "step: 680045, loss: 0.06866054236888885, data time: 0.010348272323608399\n",
      "step: 680046, loss: 0.06131096929311752, data time: 0.009958675929478236\n",
      "step: 680047, loss: 0.06637491285800934, data time: 0.009603478691794655\n",
      "step: 680048, loss: 0.06469640880823135, data time: 0.00927151804384978\n",
      "step: 680049, loss: 0.06658919155597687, data time: 0.00897453228632609\n",
      "step: 680050, loss: 0.061238281428813934, data time: 0.00871108055114746\n",
      "step: 680051, loss: 0.06414754688739777, data time: 0.008453800128056453\n",
      "step: 680052, loss: 0.06256578862667084, data time: 0.00821497705247667\n",
      "step: 680053, loss: 0.05838220566511154, data time: 0.007993451186588832\n",
      "step: 680054, loss: 0.05991542339324951, data time: 0.0077939691214725885\n",
      "step: 680055, loss: 0.06255139410495758, data time: 0.007607388496398926\n",
      "step: 680056, loss: 0.06929998844861984, data time: 0.007430353472309728\n",
      "step: 680057, loss: 0.058191828429698944, data time: 0.007268160581588745\n",
      "step: 680058, loss: 0.06146557256579399, data time: 0.007109642028808594\n",
      "step: 680059, loss: 0.06834086030721664, data time: 0.0069582041572122015\n",
      "step: 680060, loss: 0.0625767856836319, data time: 0.006812988008771624\n",
      "step: 680061, loss: 0.06027994304895401, data time: 0.006675753328535292\n",
      "step: 680062, loss: 0.06050105392932892, data time: 0.006547637887903162\n",
      "step: 680063, loss: 0.06263872981071472, data time: 0.006429634596172132\n",
      "step: 680064, loss: 0.06097067520022392, data time: 0.006317896720690605\n",
      "step: 680065, loss: 0.0583355538547039, data time: 0.006211215257644653\n",
      "step: 680066, loss: 0.06043964624404907, data time: 0.17429280281066895\n",
      "step: 680067, loss: 0.059691302478313446, data time: 0.08831930160522461\n",
      "step: 680068, loss: 0.06472014635801315, data time: 0.05991681416829427\n",
      "step: 680069, loss: 0.06311880797147751, data time: 0.045704662799835205\n",
      "step: 680070, loss: 0.05774904415011406, data time: 0.03684101104736328\n",
      "step: 680071, loss: 0.06408610939979553, data time: 0.03095523516337077\n",
      "step: 680072, loss: 0.06030821427702904, data time: 0.026744944708687917\n",
      "step: 680073, loss: 0.0598815493285656, data time: 0.023656725883483887\n",
      "step: 680074, loss: 0.06135621294379234, data time: 0.021172391043768987\n",
      "step: 680075, loss: 0.059229444712400436, data time: 0.019257307052612305\n",
      "step: 680076, loss: 0.05919146537780762, data time: 0.017704551870172672\n",
      "step: 680077, loss: 0.06489373743534088, data time: 0.016423781712849934\n",
      "step: 680078, loss: 0.0628230944275856, data time: 0.01533181850726788\n",
      "step: 680079, loss: 0.05878017097711563, data time: 0.01439108167375837\n",
      "step: 680080, loss: 0.06501132994890213, data time: 0.013570213317871093\n",
      "step: 680081, loss: 0.06612614542245865, data time: 0.012852340936660767\n",
      "step: 680082, loss: 0.06651683896780014, data time: 0.012218475341796875\n",
      "step: 680083, loss: 0.061219632625579834, data time: 0.011657688352796767\n",
      "step: 680084, loss: 0.06289007514715195, data time: 0.011149168014526367\n",
      "step: 680085, loss: 0.06476302444934845, data time: 0.010700273513793945\n",
      "step: 680086, loss: 0.05793439596891403, data time: 0.010291303907121931\n",
      "step: 680087, loss: 0.057871878147125244, data time: 0.009923729029568758\n",
      "step: 680088, loss: 0.0628349781036377, data time: 0.009582239648570185\n",
      "step: 680089, loss: 0.0655093565583229, data time: 0.009270558754603067\n",
      "step: 680090, loss: 0.0640982910990715, data time: 0.008984231948852539\n",
      "step: 680091, loss: 0.05713707208633423, data time: 0.008717720325176533\n",
      "step: 680092, loss: 0.06356841325759888, data time: 0.008466755902325665\n",
      "step: 680093, loss: 0.0626256987452507, data time: 0.008237123489379883\n",
      "step: 680094, loss: 0.06289108842611313, data time: 0.008026468342748182\n",
      "step: 680095, loss: 0.0623752698302269, data time: 0.007832018534342448\n",
      "step: 680096, loss: 0.0552675724029541, data time: 0.007650890657978673\n",
      "step: 680097, loss: 0.054924361407756805, data time: 0.007491700351238251\n",
      "step: 680098, loss: 0.057464346289634705, data time: 0.0073279106255733605\n",
      "step: 680099, loss: 0.06055028736591339, data time: 0.007170971702126896\n",
      "step: 680100, loss: 0.06422483175992966, data time: 0.007020773206438337\n",
      "step: 680101, loss: 0.05758803337812424, data time: 0.006879846254984538\n",
      "step: 680102, loss: 0.06092613935470581, data time: 0.006746247008040144\n",
      "step: 680103, loss: 0.06229241192340851, data time: 0.00662138587550113\n",
      "step: 680104, loss: 0.065004363656044, data time: 0.006503447508200621\n",
      "step: 680105, loss: 0.05647461116313934, data time: 0.00639304518699646\n",
      "step: 680106, loss: 0.05849359557032585, data time: 0.1711592674255371\n",
      "step: 680107, loss: 0.059872500598430634, data time: 0.08719992637634277\n",
      "step: 680108, loss: 0.06149566173553467, data time: 0.05865335464477539\n",
      "step: 680109, loss: 0.06590649485588074, data time: 0.04485499858856201\n",
      "step: 680110, loss: 0.06219574064016342, data time: 0.03616480827331543\n",
      "step: 680111, loss: 0.0574348084628582, data time: 0.03040150801340739\n",
      "step: 680112, loss: 0.06330566853284836, data time: 0.026268073490687778\n",
      "step: 680113, loss: 0.06456402689218521, data time: 0.02324199676513672\n",
      "step: 680114, loss: 0.05881945416331291, data time: 0.02080967691209581\n",
      "step: 680115, loss: 0.060706909745931625, data time: 0.018929362297058105\n",
      "step: 680116, loss: 0.06221979111433029, data time: 0.01740483804182573\n",
      "step: 680117, loss: 0.05934753268957138, data time: 0.01613287130991618\n",
      "step: 680118, loss: 0.06565937399864197, data time: 0.015072437433096079\n",
      "step: 680119, loss: 0.061545949429273605, data time: 0.014146532331194197\n",
      "step: 680120, loss: 0.06111861392855644, data time: 0.013346290588378907\n",
      "step: 680121, loss: 0.06328834593296051, data time: 0.012644767761230469\n",
      "step: 680122, loss: 0.07130490243434906, data time: 0.012020601945764878\n",
      "step: 680123, loss: 0.06402428448200226, data time: 0.011461046006944444\n",
      "step: 680124, loss: 0.06138836592435837, data time: 0.010964719872725638\n",
      "step: 680125, loss: 0.06451006233692169, data time: 0.010524535179138183\n",
      "step: 680126, loss: 0.06122998520731926, data time: 0.010130314599900018\n",
      "step: 680127, loss: 0.055882539600133896, data time: 0.009766882116144354\n",
      "step: 680128, loss: 0.05772861838340759, data time: 0.009434482325678286\n",
      "step: 680129, loss: 0.05821188539266586, data time: 0.00913540522257487\n",
      "step: 680130, loss: 0.06155509874224663, data time: 0.008858985900878906\n",
      "step: 680131, loss: 0.06391972303390503, data time: 0.00859683293562669\n",
      "step: 680132, loss: 0.05981789156794548, data time: 0.008349674719351309\n",
      "step: 680133, loss: 0.06411749869585037, data time: 0.008126446178981237\n",
      "step: 680134, loss: 0.061863452196121216, data time: 0.007922271202350485\n",
      "step: 680135, loss: 0.05989202857017517, data time: 0.0077324310938517255\n",
      "step: 680136, loss: 0.06179129332304001, data time: 0.007552393021122102\n",
      "step: 680137, loss: 0.06128048896789551, data time: 0.0073891207575798035\n",
      "step: 680138, loss: 0.05682861804962158, data time: 0.007226698326342033\n",
      "step: 680139, loss: 0.057635627686977386, data time: 0.007072617025936351\n",
      "step: 680140, loss: 0.06447827816009521, data time: 0.006924581527709961\n",
      "step: 680141, loss: 0.06291120499372482, data time: 0.006783478789859348\n",
      "step: 680142, loss: 0.061732497066259384, data time: 0.006652277869147224\n",
      "step: 680143, loss: 0.06674647331237793, data time: 0.006530266059072394\n",
      "step: 680144, loss: 0.062255289405584335, data time: 0.006424671564346705\n",
      "step: 680145, loss: 0.0592341348528862, data time: 0.006314909458160401\n",
      "step: 680146, loss: 0.059920310974121094, data time: 0.1730794906616211\n",
      "step: 680147, loss: 0.06304663419723511, data time: 0.08797204494476318\n",
      "step: 680148, loss: 0.057376425713300705, data time: 0.059857447942097984\n",
      "step: 680149, loss: 0.051435887813568115, data time: 0.04527825117111206\n",
      "step: 680150, loss: 0.059691570699214935, data time: 0.03652663230895996\n",
      "step: 680151, loss: 0.06435918807983398, data time: 0.030689120292663574\n",
      "step: 680152, loss: 0.06343565881252289, data time: 0.026705503463745117\n",
      "step: 680153, loss: 0.06354957818984985, data time: 0.023540198802947998\n",
      "step: 680154, loss: 0.06012297794222832, data time: 0.02107548713684082\n",
      "step: 680155, loss: 0.06798957288265228, data time: 0.019172978401184083\n",
      "step: 680156, loss: 0.060063865035772324, data time: 0.017630490389737217\n",
      "step: 680157, loss: 0.06305967271327972, data time: 0.016344924767812092\n",
      "step: 680158, loss: 0.05804409831762314, data time: 0.015265776560856746\n",
      "step: 680159, loss: 0.0616522952914238, data time: 0.014326316969735282\n",
      "step: 680160, loss: 0.0652351975440979, data time: 0.013517538706461588\n",
      "step: 680161, loss: 0.06461173295974731, data time: 0.012805089354515076\n",
      "step: 680162, loss: 0.0643492341041565, data time: 0.012170202591839959\n",
      "step: 680163, loss: 0.06419974565505981, data time: 0.01160601774851481\n",
      "step: 680164, loss: 0.06020170450210571, data time: 0.01110286461679559\n",
      "step: 680165, loss: 0.061218541115522385, data time: 0.01065995693206787\n",
      "step: 680166, loss: 0.0632489025592804, data time: 0.010260729562668573\n",
      "step: 680167, loss: 0.06089843064546585, data time: 0.009891282428394665\n",
      "step: 680168, loss: 0.06619305908679962, data time: 0.009553391000498896\n",
      "step: 680169, loss: 0.05696742981672287, data time: 0.009253174066543579\n",
      "step: 680170, loss: 0.06507516652345657, data time: 0.008972187042236329\n",
      "step: 680171, loss: 0.06272910535335541, data time: 0.008707541685837965\n",
      "step: 680172, loss: 0.06248452514410019, data time: 0.008459320774784795\n",
      "step: 680173, loss: 0.05923762544989586, data time: 0.008230711732591902\n",
      "step: 680174, loss: 0.06358589231967926, data time: 0.00801962819592706\n",
      "step: 680175, loss: 0.058624908328056335, data time: 0.007824214299519856\n",
      "step: 680176, loss: 0.05517236515879631, data time: 0.007641130878079322\n",
      "step: 680177, loss: 0.05944689363241196, data time: 0.007476203143596649\n",
      "step: 680178, loss: 0.06495997309684753, data time: 0.0073114452940044984\n",
      "step: 680179, loss: 0.06326408684253693, data time: 0.007158566923702464\n",
      "step: 680180, loss: 0.06175737828016281, data time: 0.007007428577968052\n",
      "step: 680181, loss: 0.06382955610752106, data time: 0.006864541106753879\n",
      "step: 680182, loss: 0.06243954226374626, data time: 0.006729222632743217\n",
      "step: 680183, loss: 0.0629216879606247, data time: 0.006605054202832673\n",
      "step: 680184, loss: 0.07026927918195724, data time: 0.0064885494036552235\n",
      "step: 680185, loss: 0.054364077746868134, data time: 0.006377989053726196\n",
      "step: 680186, loss: 0.07340638339519501, data time: 0.16961026191711426\n",
      "step: 680187, loss: 0.059736352413892746, data time: 0.08614075183868408\n",
      "step: 680188, loss: 0.06441307067871094, data time: 0.05832131703694662\n",
      "step: 680189, loss: 0.059475235641002655, data time: 0.04450505971908569\n",
      "step: 680190, loss: 0.05875025689601898, data time: 0.035895586013793945\n",
      "step: 680191, loss: 0.06506533920764923, data time: 0.030161579449971516\n",
      "step: 680192, loss: 0.06253514438867569, data time: 0.02606034278869629\n",
      "step: 680193, loss: 0.060496434569358826, data time: 0.02304944396018982\n",
      "step: 680194, loss: 0.07400807738304138, data time: 0.0206453800201416\n",
      "step: 680195, loss: 0.05749529227614403, data time: 0.018783950805664064\n",
      "step: 680196, loss: 0.06270070374011993, data time: 0.01728257265957919\n",
      "step: 680197, loss: 0.06461387872695923, data time: 0.016018072764078777\n",
      "step: 680198, loss: 0.0670676976442337, data time: 0.014951467514038086\n",
      "step: 680199, loss: 0.055918388068675995, data time: 0.0140261139188494\n",
      "step: 680200, loss: 0.058837052434682846, data time: 0.013233693440755208\n",
      "step: 680201, loss: 0.06646886467933655, data time: 0.012537360191345215\n",
      "step: 680202, loss: 0.06355138123035431, data time: 0.011916637420654297\n",
      "step: 680203, loss: 0.06092469021677971, data time: 0.01136269834306505\n",
      "step: 680204, loss: 0.057799115777015686, data time: 0.010870218276977539\n",
      "step: 680205, loss: 0.055583469569683075, data time: 0.01043236255645752\n",
      "step: 680206, loss: 0.06687760353088379, data time: 0.010042088372366769\n",
      "step: 680207, loss: 0.06360869109630585, data time: 0.009682742032137785\n",
      "step: 680208, loss: 0.06226106733083725, data time: 0.009353679159413214\n",
      "step: 680209, loss: 0.06659078598022461, data time: 0.009052634239196777\n",
      "step: 680210, loss: 0.06918251514434814, data time: 0.008773126602172852\n",
      "step: 680211, loss: 0.06717245280742645, data time: 0.008519264367910532\n",
      "step: 680212, loss: 0.06419104337692261, data time: 0.008278678964685511\n",
      "step: 680213, loss: 0.05184650421142578, data time: 0.008058173315865653\n",
      "step: 680214, loss: 0.05710047483444214, data time: 0.00785353265959641\n",
      "step: 680215, loss: 0.05881371349096298, data time: 0.007663329442342122\n",
      "step: 680216, loss: 0.055022090673446655, data time: 0.0074864356748519404\n",
      "step: 680217, loss: 0.06695640087127686, data time: 0.007323570549488068\n",
      "step: 680218, loss: 0.05913010984659195, data time: 0.007162563728563713\n",
      "step: 680219, loss: 0.05815557762980461, data time: 0.007011539795819451\n",
      "step: 680220, loss: 0.06113579124212265, data time: 0.00686798095703125\n",
      "step: 680221, loss: 0.06462088972330093, data time: 0.006727947129143609\n",
      "step: 680222, loss: 0.06366781890392303, data time: 0.006599548700693491\n",
      "step: 680223, loss: 0.05920376628637314, data time: 0.00647886175858347\n",
      "step: 680224, loss: 0.06632610410451889, data time: 0.006363611954909105\n",
      "step: 680225, loss: 0.06219057738780975, data time: 0.006254971027374268\n",
      "step: 680226, loss: 0.06905527412891388, data time: 0.1834700107574463\n",
      "step: 680227, loss: 0.061247728765010834, data time: 0.09250926971435547\n",
      "step: 680228, loss: 0.06808018684387207, data time: 0.06303167343139648\n",
      "step: 680229, loss: 0.057732753455638885, data time: 0.047664642333984375\n",
      "step: 680230, loss: 0.060026079416275024, data time: 0.038423776626586914\n",
      "step: 680231, loss: 0.06580021232366562, data time: 0.03227031230926514\n",
      "step: 680232, loss: 0.0595848523080349, data time: 0.02802654675074986\n",
      "step: 680233, loss: 0.06332345306873322, data time: 0.024696797132492065\n",
      "step: 680234, loss: 0.060411565005779266, data time: 0.022113058302137587\n",
      "step: 680235, loss: 0.06139051169157028, data time: 0.02011229991912842\n",
      "step: 680236, loss: 0.059550754725933075, data time: 0.0184797156940807\n",
      "step: 680237, loss: 0.060942571610212326, data time: 0.01711718241373698\n",
      "step: 680238, loss: 0.060610659420490265, data time: 0.015967277380136344\n",
      "step: 680239, loss: 0.07068116962909698, data time: 0.014977710587637765\n",
      "step: 680240, loss: 0.06781025230884552, data time: 0.014125299453735352\n",
      "step: 680241, loss: 0.06402025371789932, data time: 0.013378903269767761\n",
      "step: 680242, loss: 0.05995143577456474, data time: 0.012708411497228286\n",
      "step: 680243, loss: 0.06386007368564606, data time: 0.012112683720058866\n",
      "step: 680244, loss: 0.06146695464849472, data time: 0.011580743287739\n",
      "step: 680245, loss: 0.05440021678805351, data time: 0.011108791828155518\n",
      "step: 680246, loss: 0.06159558147192001, data time: 0.010681368055797759\n",
      "step: 680247, loss: 0.06956504285335541, data time: 0.010294795036315918\n",
      "step: 680248, loss: 0.06396318972110748, data time: 0.009934197301449984\n",
      "step: 680249, loss: 0.0636671856045723, data time: 0.00960691769917806\n",
      "step: 680250, loss: 0.059354983270168304, data time: 0.009309797286987305\n",
      "step: 680251, loss: 0.06154888868331909, data time: 0.009032909686748799\n",
      "step: 680252, loss: 0.06711801141500473, data time: 0.00877125174910934\n",
      "step: 680253, loss: 0.05664566904306412, data time: 0.008529748235430037\n",
      "step: 680254, loss: 0.06575979292392731, data time: 0.008308016020676186\n",
      "step: 680255, loss: 0.057253144681453705, data time: 0.008101320266723633\n",
      "step: 680256, loss: 0.05428103730082512, data time: 0.007909720943820092\n",
      "step: 680257, loss: 0.06473120301961899, data time: 0.007733583450317383\n",
      "step: 680258, loss: 0.05817883461713791, data time: 0.0075614813602331915\n",
      "step: 680259, loss: 0.06714127957820892, data time: 0.007404194158666274\n",
      "step: 680260, loss: 0.06001541018486023, data time: 0.007247325352260045\n",
      "step: 680261, loss: 0.06122605502605438, data time: 0.0070992641978793675\n",
      "step: 680262, loss: 0.05909089371562004, data time: 0.006959051699251742\n",
      "step: 680263, loss: 0.06068713590502739, data time: 0.006828270460429944\n",
      "step: 680264, loss: 0.060218703001737595, data time: 0.006704281537960737\n",
      "step: 680265, loss: 0.044388365000486374, data time: 0.0065877437591552734\n",
      "step: 680266, loss: 0.06081346794962883, data time: 0.1723494529724121\n",
      "step: 680267, loss: 0.06634466350078583, data time: 0.0875847339630127\n",
      "step: 680268, loss: 0.06412948668003082, data time: 0.05957420667012533\n",
      "step: 680269, loss: 0.05953928828239441, data time: 0.04535108804702759\n",
      "step: 680270, loss: 0.06404672563076019, data time: 0.03655972480773926\n",
      "step: 680271, loss: 0.056532472372055054, data time: 0.030728777249654133\n",
      "step: 680272, loss: 0.06417416036128998, data time: 0.026556321552821567\n",
      "step: 680273, loss: 0.0596633106470108, data time: 0.023499995470046997\n",
      "step: 680274, loss: 0.058392226696014404, data time: 0.02104896969265408\n",
      "step: 680275, loss: 0.06300181150436401, data time: 0.019141721725463866\n",
      "step: 680276, loss: 0.06538334488868713, data time: 0.017604286020452328\n",
      "step: 680277, loss: 0.05894559621810913, data time: 0.016324838002522785\n",
      "step: 680278, loss: 0.06592591851949692, data time: 0.015238725222074069\n",
      "step: 680279, loss: 0.058266885578632355, data time: 0.014299460819789342\n",
      "step: 680280, loss: 0.05633841082453728, data time: 0.013489103317260743\n",
      "step: 680281, loss: 0.0523906871676445, data time: 0.012782663106918335\n",
      "step: 680282, loss: 0.06038103625178337, data time: 0.012152391321518841\n",
      "step: 680283, loss: 0.06401985138654709, data time: 0.011586295233832465\n",
      "step: 680284, loss: 0.06283578276634216, data time: 0.011084217774240594\n",
      "step: 680285, loss: 0.06344646215438843, data time: 0.01064361333847046\n",
      "step: 680286, loss: 0.05985662341117859, data time: 0.010242064793904623\n",
      "step: 680287, loss: 0.07387768477201462, data time: 0.009876153685829857\n",
      "step: 680288, loss: 0.05567729473114014, data time: 0.009536971216616423\n",
      "step: 680289, loss: 0.060009703040122986, data time: 0.009229709704717001\n",
      "step: 680290, loss: 0.05966863036155701, data time: 0.008946514129638672\n",
      "step: 680291, loss: 0.06785258650779724, data time: 0.008679839280935435\n",
      "step: 680292, loss: 0.06391173601150513, data time: 0.008431143230862088\n",
      "step: 680293, loss: 0.059554602950811386, data time: 0.008206886904580253\n",
      "step: 680294, loss: 0.059472233057022095, data time: 0.007998524041011416\n",
      "step: 680295, loss: 0.057238124310970306, data time: 0.007801977793375651\n",
      "step: 680296, loss: 0.06412898749113083, data time: 0.00762020387957173\n",
      "step: 680297, loss: 0.061899539083242416, data time: 0.007454663515090942\n",
      "step: 680298, loss: 0.0624505877494812, data time: 0.007291042443477746\n",
      "step: 680299, loss: 0.06898622214794159, data time: 0.007135622641619514\n",
      "step: 680300, loss: 0.06801550090312958, data time: 0.006986590794154576\n",
      "step: 680301, loss: 0.06252844631671906, data time: 0.006843335098690457\n",
      "step: 680302, loss: 0.06293685734272003, data time: 0.006713029500600454\n",
      "step: 680303, loss: 0.05922478437423706, data time: 0.006589801687943308\n",
      "step: 680304, loss: 0.06012031435966492, data time: 0.006472722077981019\n",
      "step: 680305, loss: 0.04616324231028557, data time: 0.006362009048461914\n",
      "step: 680306, loss: 0.06190164014697075, data time: 0.1912384033203125\n",
      "step: 680307, loss: 0.06744596362113953, data time: 0.09637558460235596\n",
      "step: 680308, loss: 0.0600486621260643, data time: 0.06476338704427083\n",
      "step: 680309, loss: 0.05900169536471367, data time: 0.0494612455368042\n",
      "step: 680310, loss: 0.06434908509254456, data time: 0.03985238075256348\n",
      "step: 680311, loss: 0.062223680317401886, data time: 0.03344674905141195\n",
      "step: 680312, loss: 0.06144076585769653, data time: 0.02888594354901995\n",
      "step: 680313, loss: 0.05298328399658203, data time: 0.025544703006744385\n",
      "step: 680314, loss: 0.05716472119092941, data time: 0.022863467534383137\n",
      "step: 680315, loss: 0.058178797364234924, data time: 0.02077915668487549\n",
      "step: 680316, loss: 0.06858493387699127, data time: 0.01908642595464533\n",
      "step: 680317, loss: 0.05578433722257614, data time: 0.01767653226852417\n",
      "step: 680318, loss: 0.06315258145332336, data time: 0.01648431557875413\n",
      "step: 680319, loss: 0.06205404922366142, data time: 0.015454598835536413\n",
      "step: 680320, loss: 0.06132366508245468, data time: 0.014556264877319336\n",
      "step: 680321, loss: 0.05946798250079155, data time: 0.01378004252910614\n",
      "step: 680322, loss: 0.05858142301440239, data time: 0.013144605300005744\n",
      "step: 680323, loss: 0.06690693646669388, data time: 0.012545903523763021\n",
      "step: 680324, loss: 0.06036202237010002, data time: 0.012006094581202456\n",
      "step: 680325, loss: 0.06297402083873749, data time: 0.01152738332748413\n",
      "step: 680326, loss: 0.059467464685440063, data time: 0.011098191851661318\n",
      "step: 680327, loss: 0.06243913620710373, data time: 0.010708299550143156\n",
      "step: 680328, loss: 0.05847514793276787, data time: 0.010347646215687628\n",
      "step: 680329, loss: 0.0577077716588974, data time: 0.01001437505086263\n",
      "step: 680330, loss: 0.06044306606054306, data time: 0.009713945388793945\n",
      "step: 680331, loss: 0.06189514324069023, data time: 0.009435754555922288\n",
      "step: 680332, loss: 0.060680489987134933, data time: 0.009177022510104708\n",
      "step: 680333, loss: 0.06300722062587738, data time: 0.008937367371150426\n",
      "step: 680334, loss: 0.06153635308146477, data time: 0.00871659147328344\n",
      "step: 680335, loss: 0.05565495416522026, data time: 0.008509405453999837\n",
      "step: 680336, loss: 0.05905226618051529, data time: 0.008315517056372857\n",
      "step: 680337, loss: 0.06451760232448578, data time: 0.0081353560090065\n",
      "step: 680338, loss: 0.06127346679568291, data time: 0.007949612357399681\n",
      "step: 680339, loss: 0.05879034847021103, data time: 0.007780089097864488\n",
      "step: 680340, loss: 0.05969960615038872, data time: 0.007620034899030413\n",
      "step: 680341, loss: 0.06643986701965332, data time: 0.007465269830491807\n",
      "step: 680342, loss: 0.06371612846851349, data time: 0.007318574029046136\n",
      "step: 680343, loss: 0.07179926335811615, data time: 0.0071820648092972604\n",
      "step: 680344, loss: 0.058742016553878784, data time: 0.007052696668184721\n",
      "step: 680345, loss: 0.05510532110929489, data time: 0.00692983865737915\n",
      "step: 680346, loss: 0.05993371829390526, data time: 0.1832432746887207\n",
      "step: 680347, loss: 0.06171981990337372, data time: 0.0936659574508667\n",
      "step: 680348, loss: 0.06223078817129135, data time: 0.06298502286275227\n",
      "step: 680349, loss: 0.06194494292140007, data time: 0.04789304733276367\n",
      "step: 680350, loss: 0.060805827379226685, data time: 0.0386042594909668\n",
      "step: 680351, loss: 0.06179926544427872, data time: 0.032445708910624184\n",
      "step: 680352, loss: 0.06232037767767906, data time: 0.028008665357317244\n",
      "step: 680353, loss: 0.057240575551986694, data time: 0.02477210760116577\n",
      "step: 680354, loss: 0.06689004600048065, data time: 0.022173378202650283\n",
      "step: 680355, loss: 0.054837536066770554, data time: 0.02016284465789795\n",
      "step: 680356, loss: 0.054073404520750046, data time: 0.018520485271107067\n",
      "step: 680357, loss: 0.0660008043050766, data time: 0.017159461975097656\n",
      "step: 680358, loss: 0.06513218581676483, data time: 0.01600184807410607\n",
      "step: 680359, loss: 0.06312942504882812, data time: 0.01500943728855678\n",
      "step: 680360, loss: 0.05857406556606293, data time: 0.014151938756306966\n",
      "step: 680361, loss: 0.058679353445768356, data time: 0.013394951820373535\n",
      "step: 680362, loss: 0.05896841734647751, data time: 0.012729560627656825\n",
      "step: 680363, loss: 0.0647260993719101, data time: 0.012130684322781034\n",
      "step: 680364, loss: 0.06220805272459984, data time: 0.011597796490317896\n",
      "step: 680365, loss: 0.06386567652225494, data time: 0.011125278472900391\n",
      "step: 680366, loss: 0.0652683824300766, data time: 0.010695661817278181\n",
      "step: 680367, loss: 0.054009806364774704, data time: 0.010307290337302467\n",
      "step: 680368, loss: 0.06424932926893234, data time: 0.009948751200800356\n",
      "step: 680369, loss: 0.06290850788354874, data time: 0.00963155428568522\n",
      "step: 680370, loss: 0.05701471120119095, data time: 0.009326410293579102\n",
      "step: 680371, loss: 0.057775698602199554, data time: 0.009047764998215895\n",
      "step: 680372, loss: 0.0637144073843956, data time: 0.00878427646778248\n",
      "step: 680373, loss: 0.05916596204042435, data time: 0.008541984217507499\n",
      "step: 680374, loss: 0.06078828498721123, data time: 0.008320981058581122\n",
      "step: 680375, loss: 0.056081466376781464, data time: 0.008115180333455404\n",
      "step: 680376, loss: 0.057889632880687714, data time: 0.007925018187492125\n",
      "step: 680377, loss: 0.06486042588949203, data time: 0.007750518620014191\n",
      "step: 680378, loss: 0.05814000591635704, data time: 0.0075817108154296875\n",
      "step: 680379, loss: 0.06334061920642853, data time: 0.007415049216326545\n",
      "step: 680380, loss: 0.05990556627511978, data time: 0.007261201313563756\n",
      "step: 680381, loss: 0.06238818168640137, data time: 0.007110695044199626\n",
      "step: 680382, loss: 0.06740807741880417, data time: 0.006971024178169869\n",
      "step: 680383, loss: 0.058240827172994614, data time: 0.006839551423725329\n",
      "step: 680384, loss: 0.062857985496521, data time: 0.006716673190777118\n",
      "step: 680385, loss: 0.07991175353527069, data time: 0.00659865140914917\n",
      "step: 680386, loss: 0.06555505841970444, data time: 0.17201733589172363\n",
      "step: 680387, loss: 0.06343269348144531, data time: 0.08738255500793457\n",
      "step: 680388, loss: 0.07122998684644699, data time: 0.05922309557596842\n",
      "step: 680389, loss: 0.05704420059919357, data time: 0.045176029205322266\n",
      "step: 680390, loss: 0.056333865970373154, data time: 0.036431503295898435\n",
      "step: 680391, loss: 0.06633644551038742, data time: 0.030623555183410645\n",
      "step: 680392, loss: 0.05884125083684921, data time: 0.02645805903843471\n",
      "step: 680393, loss: 0.06154252216219902, data time: 0.023402690887451172\n",
      "step: 680394, loss: 0.06271640956401825, data time: 0.020948304070366755\n",
      "step: 680395, loss: 0.06160619109869003, data time: 0.01908717155456543\n",
      "step: 680396, loss: 0.06885506957769394, data time: 0.01756176081570712\n",
      "step: 680397, loss: 0.06527294218540192, data time: 0.016284167766571045\n",
      "step: 680398, loss: 0.06340094655752182, data time: 0.015203530971820537\n",
      "step: 680399, loss: 0.058455199003219604, data time: 0.014268670763288225\n",
      "step: 680400, loss: 0.059847377240657806, data time: 0.013463433583577473\n",
      "step: 680401, loss: 0.05678629130125046, data time: 0.012758836150169373\n",
      "step: 680402, loss: 0.0627480149269104, data time: 0.012131172067978802\n",
      "step: 680403, loss: 0.0614425465464592, data time: 0.011564665370517306\n",
      "step: 680404, loss: 0.06344246119260788, data time: 0.011061455074109529\n",
      "step: 680405, loss: 0.06367149949073792, data time: 0.010618150234222412\n",
      "step: 680406, loss: 0.06214647740125656, data time: 0.010220414116269066\n",
      "step: 680407, loss: 0.06191611289978027, data time: 0.009852777827869762\n",
      "step: 680408, loss: 0.06435620784759521, data time: 0.009513720222141432\n",
      "step: 680409, loss: 0.05989369377493858, data time: 0.009205629428227743\n",
      "step: 680410, loss: 0.06139864772558212, data time: 0.008926200866699218\n",
      "step: 680411, loss: 0.06619347631931305, data time: 0.008662581443786621\n",
      "step: 680412, loss: 0.06534040719270706, data time: 0.00841337663156015\n",
      "step: 680413, loss: 0.06450437754392624, data time: 0.008186842714037215\n",
      "step: 680414, loss: 0.06294715404510498, data time: 0.00797938478404078\n",
      "step: 680415, loss: 0.06361222267150879, data time: 0.0077875057856241865\n",
      "step: 680416, loss: 0.0625210851430893, data time: 0.007604729744695848\n",
      "step: 680417, loss: 0.06368112564086914, data time: 0.00743720680475235\n",
      "step: 680418, loss: 0.06281128525733948, data time: 0.007274815530487986\n",
      "step: 680419, loss: 0.05569864436984062, data time: 0.007119557436774759\n",
      "step: 680420, loss: 0.06770261377096176, data time: 0.006970289775303432\n",
      "step: 680421, loss: 0.06614722311496735, data time: 0.006827215353647868\n",
      "step: 680422, loss: 0.0675620436668396, data time: 0.00669550895690918\n",
      "step: 680423, loss: 0.06341508030891418, data time: 0.006572146164743524\n",
      "step: 680424, loss: 0.06692957878112793, data time: 0.006455390881269406\n",
      "step: 680425, loss: 0.0390312485396862, data time: 0.006345421075820923\n",
      "step: 680426, loss: 0.06829266995191574, data time: 0.1727139949798584\n",
      "step: 680427, loss: 0.06760486960411072, data time: 0.0871124267578125\n",
      "step: 680428, loss: 0.06763730943202972, data time: 0.05922206242879232\n",
      "step: 680429, loss: 0.059724967926740646, data time: 0.04508703947067261\n",
      "step: 680430, loss: 0.0562826506793499, data time: 0.036354446411132814\n",
      "step: 680431, loss: 0.06476003676652908, data time: 0.03054650624593099\n",
      "step: 680432, loss: 0.060754790902137756, data time: 0.026403938020978655\n",
      "step: 680433, loss: 0.06365542113780975, data time: 0.02336198091506958\n",
      "step: 680434, loss: 0.062053292989730835, data time: 0.020916011598375108\n",
      "step: 680435, loss: 0.06402060389518738, data time: 0.019024205207824708\n",
      "step: 680436, loss: 0.05463370680809021, data time: 0.01749196919527921\n",
      "step: 680437, loss: 0.0675075352191925, data time: 0.0162159005800883\n",
      "step: 680438, loss: 0.06402525305747986, data time: 0.0151368287893442\n",
      "step: 680439, loss: 0.06430260837078094, data time: 0.014200551169259208\n",
      "step: 680440, loss: 0.058884382247924805, data time: 0.013397884368896485\n",
      "step: 680441, loss: 0.0640307143330574, data time: 0.012692630290985107\n",
      "step: 680442, loss: 0.059249963611364365, data time: 0.012065326466279872\n",
      "step: 680443, loss: 0.060496531426906586, data time: 0.011504160033331977\n",
      "step: 680444, loss: 0.06146508455276489, data time: 0.01101156284934596\n",
      "step: 680445, loss: 0.06074855476617813, data time: 0.010573089122772217\n",
      "step: 680446, loss: 0.06093885749578476, data time: 0.010172003791445778\n",
      "step: 680447, loss: 0.06681330502033234, data time: 0.009808226065202192\n",
      "step: 680448, loss: 0.06243286281824112, data time: 0.00946943656257961\n",
      "step: 680449, loss: 0.06813453137874603, data time: 0.009164581696192423\n",
      "step: 680450, loss: 0.05861254781484604, data time: 0.008885107040405273\n",
      "step: 680451, loss: 0.06035839766263962, data time: 0.008621582618126502\n",
      "step: 680452, loss: 0.06613604724407196, data time: 0.008375079543502242\n",
      "step: 680453, loss: 0.06633684039115906, data time: 0.00815067972455706\n",
      "step: 680454, loss: 0.05908193439245224, data time: 0.00794615416691221\n",
      "step: 680455, loss: 0.06498091667890549, data time: 0.007752561569213867\n",
      "step: 680456, loss: 0.057843200862407684, data time: 0.007572658600345734\n",
      "step: 680457, loss: 0.0634622722864151, data time: 0.0074122026562690735\n",
      "step: 680458, loss: 0.06083321198821068, data time: 0.007248119874434037\n",
      "step: 680459, loss: 0.06539503484964371, data time: 0.0070942920797011436\n",
      "step: 680460, loss: 0.06221961975097656, data time: 0.006945058277675084\n",
      "step: 680461, loss: 0.06141570955514908, data time: 0.006803128454420302\n",
      "step: 680462, loss: 0.05851946398615837, data time: 0.006672266367319468\n",
      "step: 680463, loss: 0.06485538184642792, data time: 0.00655272132471988\n",
      "step: 680464, loss: 0.06687776744365692, data time: 0.006439960919893705\n",
      "step: 680465, loss: 0.061511944979429245, data time: 0.006332910060882569\n",
      "step: 680466, loss: 0.06546387076377869, data time: 0.1796410083770752\n",
      "step: 680467, loss: 0.06631524860858917, data time: 0.09056961536407471\n",
      "step: 680468, loss: 0.06440135836601257, data time: 0.060912370681762695\n",
      "step: 680469, loss: 0.05899275094270706, data time: 0.04646545648574829\n",
      "step: 680470, loss: 0.06331973522901535, data time: 0.03744692802429199\n",
      "step: 680471, loss: 0.06043393164873123, data time: 0.03145313262939453\n",
      "step: 680472, loss: 0.06819111108779907, data time: 0.027167865208217075\n",
      "step: 680473, loss: 0.05633305758237839, data time: 0.02403545379638672\n",
      "step: 680474, loss: 0.061764903366565704, data time: 0.02151746220058865\n",
      "step: 680475, loss: 0.06266101449728012, data time: 0.019565248489379884\n",
      "step: 680476, loss: 0.06310347467660904, data time: 0.01798469370061701\n",
      "step: 680477, loss: 0.06001977249979973, data time: 0.01667875051498413\n",
      "step: 680478, loss: 0.06652617454528809, data time: 0.015560223506047176\n",
      "step: 680479, loss: 0.06137901544570923, data time: 0.014590263366699219\n",
      "step: 680480, loss: 0.0624479204416275, data time: 0.0137574831644694\n",
      "step: 680481, loss: 0.058043330907821655, data time: 0.013026103377342224\n",
      "step: 680482, loss: 0.0655520111322403, data time: 0.012385228100944968\n",
      "step: 680483, loss: 0.061698805540800095, data time: 0.011829296747843424\n",
      "step: 680484, loss: 0.06276218593120575, data time: 0.011329788910715203\n",
      "step: 680485, loss: 0.061533372849226, data time: 0.010888171195983887\n",
      "step: 680486, loss: 0.06129565089941025, data time: 0.01048814682733445\n",
      "step: 680487, loss: 0.057791244238615036, data time: 0.010127035054293547\n",
      "step: 680488, loss: 0.06210236996412277, data time: 0.00979130164436672\n",
      "step: 680489, loss: 0.057816922664642334, data time: 0.009486565987269083\n",
      "step: 680490, loss: 0.06475567817687988, data time: 0.00920060157775879\n",
      "step: 680491, loss: 0.06800016760826111, data time: 0.008940348258385291\n",
      "step: 680492, loss: 0.06362777203321457, data time: 0.008700300146032262\n",
      "step: 680493, loss: 0.06649287790060043, data time: 0.008474103042057582\n",
      "step: 680494, loss: 0.06301271170377731, data time: 0.00827122556752172\n",
      "step: 680495, loss: 0.06698727607727051, data time: 0.008080371220906575\n",
      "step: 680496, loss: 0.06743322312831879, data time: 0.007902699132119455\n",
      "step: 680497, loss: 0.06193070858716965, data time: 0.0077367424964904785\n",
      "step: 680498, loss: 0.05857343226671219, data time: 0.007566972212357955\n",
      "step: 680499, loss: 0.061794377863407135, data time: 0.00740504264831543\n",
      "step: 680500, loss: 0.06048499047756195, data time: 0.00725376946585519\n",
      "step: 680501, loss: 0.06022236868739128, data time: 0.00710827112197876\n",
      "step: 680502, loss: 0.059122003614902496, data time: 0.0069709790719522016\n",
      "step: 680503, loss: 0.06292515993118286, data time: 0.006844081376728259\n",
      "step: 680504, loss: 0.0696820467710495, data time: 0.00672602653503418\n",
      "step: 680505, loss: 0.05429711192846298, data time: 0.006611919403076172\n",
      "step: 680506, loss: 0.06261490285396576, data time: 0.18516039848327637\n",
      "step: 680507, loss: 0.05382125824689865, data time: 0.09394216537475586\n",
      "step: 680508, loss: 0.06017739698290825, data time: 0.06314317385355632\n",
      "step: 680509, loss: 0.06046508997678757, data time: 0.04813450574874878\n",
      "step: 680510, loss: 0.06659124046564102, data time: 0.03878927230834961\n",
      "step: 680511, loss: 0.06303062289953232, data time: 0.03257648150126139\n",
      "step: 680512, loss: 0.05789155885577202, data time: 0.02812981605529785\n",
      "step: 680513, loss: 0.06340877711772919, data time: 0.02487790584564209\n",
      "step: 680514, loss: 0.060824573040008545, data time: 0.02225915590922038\n",
      "step: 680515, loss: 0.05586276948451996, data time: 0.020283770561218262\n",
      "step: 680516, loss: 0.06349201500415802, data time: 0.018643054095181553\n",
      "step: 680517, loss: 0.0680849477648735, data time: 0.017263948917388916\n",
      "step: 680518, loss: 0.06387175619602203, data time: 0.016105138338529147\n",
      "step: 680519, loss: 0.05883995443582535, data time: 0.015097873551504952\n",
      "step: 680520, loss: 0.060803286731243134, data time: 0.014233128229777018\n",
      "step: 680521, loss: 0.06126774847507477, data time: 0.013474270701408386\n",
      "step: 680522, loss: 0.06412093341350555, data time: 0.01283121109008789\n",
      "step: 680523, loss: 0.06248185783624649, data time: 0.012228118048773872\n",
      "step: 680524, loss: 0.06518024951219559, data time: 0.011693828984310753\n",
      "step: 680525, loss: 0.06351076811552048, data time: 0.011232256889343262\n",
      "step: 680526, loss: 0.05513645336031914, data time: 0.010821456000918434\n",
      "step: 680527, loss: 0.06634218990802765, data time: 0.010444456880742853\n",
      "step: 680528, loss: 0.06167767569422722, data time: 0.010095990222433338\n",
      "step: 680529, loss: 0.05706370994448662, data time: 0.00977945327758789\n",
      "step: 680530, loss: 0.07154521346092224, data time: 0.00948430061340332\n",
      "step: 680531, loss: 0.06420297920703888, data time: 0.00921646448282095\n",
      "step: 680532, loss: 0.06309778243303299, data time: 0.008965280320909288\n",
      "step: 680533, loss: 0.06180378794670105, data time: 0.008728981018066406\n",
      "step: 680534, loss: 0.060468196868896484, data time: 0.008515727931055529\n",
      "step: 680535, loss: 0.058363161981105804, data time: 0.008317176500956218\n",
      "step: 680536, loss: 0.057388536632061005, data time: 0.00813181169571415\n",
      "step: 680537, loss: 0.06435820460319519, data time: 0.007958896458148956\n",
      "step: 680538, loss: 0.058735333383083344, data time: 0.007781541708743934\n",
      "step: 680539, loss: 0.06149733066558838, data time: 0.007614283000721651\n",
      "step: 680540, loss: 0.06397052109241486, data time: 0.007457678658621652\n",
      "step: 680541, loss: 0.06355443596839905, data time: 0.007305827405717637\n",
      "step: 680542, loss: 0.06521736085414886, data time: 0.0071628931406381964\n",
      "step: 680543, loss: 0.06178952381014824, data time: 0.0070305937214901575\n",
      "step: 680544, loss: 0.05953329801559448, data time: 0.006906729478102464\n",
      "step: 680545, loss: 0.0806342363357544, data time: 0.006788969039916992\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40772294998168945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 680546, loss: 0.0577341690659523, data time: 0.17917084693908691\n",
      "step: 680547, loss: 0.06081031262874603, data time: 0.09089434146881104\n",
      "step: 680548, loss: 0.06664034724235535, data time: 0.06120077768961588\n",
      "step: 680549, loss: 0.06980681419372559, data time: 0.04684656858444214\n",
      "step: 680550, loss: 0.06304341554641724, data time: 0.037793207168579104\n",
      "step: 680551, loss: 0.064421147108078, data time: 0.03177460034688314\n",
      "step: 680552, loss: 0.059084512293338776, data time: 0.027482543672834123\n",
      "step: 680553, loss: 0.061590127646923065, data time: 0.024350225925445557\n",
      "step: 680554, loss: 0.0671626552939415, data time: 0.021814664204915363\n",
      "step: 680555, loss: 0.06314477324485779, data time: 0.019859504699707032\n",
      "step: 680556, loss: 0.057604458183050156, data time: 0.018281329761851917\n",
      "step: 680557, loss: 0.06665515154600143, data time: 0.016960124174753826\n",
      "step: 680558, loss: 0.0622236542403698, data time: 0.015847297815176156\n",
      "step: 680559, loss: 0.06447772681713104, data time: 0.014882615634373255\n",
      "step: 680560, loss: 0.06518064439296722, data time: 0.014052804311116536\n",
      "step: 680561, loss: 0.05908834561705589, data time: 0.013324052095413208\n",
      "step: 680562, loss: 0.06037462502717972, data time: 0.012682522044462316\n",
      "step: 680563, loss: 0.0625506043434143, data time: 0.012104524506462945\n",
      "step: 680564, loss: 0.06555993854999542, data time: 0.011589589871858296\n",
      "step: 680565, loss: 0.0578131303191185, data time: 0.011132526397705077\n",
      "step: 680566, loss: 0.07010354101657867, data time: 0.010721887860979353\n",
      "step: 680567, loss: 0.05835054814815521, data time: 0.010351170193065296\n",
      "step: 680568, loss: 0.06086268275976181, data time: 0.010002167328544285\n",
      "step: 680569, loss: 0.05529957264661789, data time: 0.0096873939037323\n",
      "step: 680570, loss: 0.06613432615995407, data time: 0.009400129318237305\n",
      "step: 680571, loss: 0.06075962632894516, data time: 0.009132458613469051\n",
      "step: 680572, loss: 0.05979008600115776, data time: 0.008877745381108037\n",
      "step: 680573, loss: 0.06497910618782043, data time: 0.008641328130449568\n",
      "step: 680574, loss: 0.061546504497528076, data time: 0.008430637162307212\n",
      "step: 680575, loss: 0.06036779284477234, data time: 0.008231131235758464\n",
      "step: 680576, loss: 0.06072321534156799, data time: 0.00804632709872338\n",
      "step: 680577, loss: 0.06343977898359299, data time: 0.007881700992584229\n",
      "step: 680578, loss: 0.05993009731173515, data time: 0.007705486182010535\n",
      "step: 680579, loss: 0.06605442613363266, data time: 0.007538367720211253\n",
      "step: 680580, loss: 0.05924209952354431, data time: 0.007383366993495396\n",
      "step: 680581, loss: 0.06229159235954285, data time: 0.007233719031016032\n",
      "step: 680582, loss: 0.06842175871133804, data time: 0.007091586654250686\n",
      "step: 680583, loss: 0.06112992763519287, data time: 0.006961533897801449\n",
      "step: 680584, loss: 0.06380881369113922, data time: 0.00683747193752191\n",
      "step: 680585, loss: 0.05168038606643677, data time: 0.006721043586730957\n",
      "step: 680586, loss: 0.06150447577238083, data time: 0.17900300025939941\n",
      "step: 680587, loss: 0.06288428604602814, data time: 0.09137296676635742\n",
      "step: 680588, loss: 0.06387536227703094, data time: 0.06153233846028646\n",
      "step: 680589, loss: 0.0633738562464714, data time: 0.047077834606170654\n",
      "step: 680590, loss: 0.058639995753765106, data time: 0.03798565864562988\n",
      "step: 680591, loss: 0.061905622482299805, data time: 0.031931797663370766\n",
      "step: 680592, loss: 0.060597341507673264, data time: 0.027619361877441406\n",
      "step: 680593, loss: 0.06391263008117676, data time: 0.024468958377838135\n",
      "step: 680594, loss: 0.0573616623878479, data time: 0.021918641196356878\n",
      "step: 680595, loss: 0.057972364127635956, data time: 0.019964766502380372\n",
      "step: 680596, loss: 0.06093388795852661, data time: 0.018371712077747692\n",
      "step: 680597, loss: 0.0689886212348938, data time: 0.017042815685272217\n",
      "step: 680598, loss: 0.06274000555276871, data time: 0.01592736977797288\n",
      "step: 680599, loss: 0.0662444457411766, data time: 0.014958858489990234\n",
      "step: 680600, loss: 0.06579908728599548, data time: 0.014120833079020182\n",
      "step: 680601, loss: 0.06213616579771042, data time: 0.013388857245445251\n",
      "step: 680602, loss: 0.06319069117307663, data time: 0.012746446272906135\n",
      "step: 680603, loss: 0.06020229309797287, data time: 0.012166380882263184\n",
      "step: 680604, loss: 0.06875070184469223, data time: 0.01165126499376799\n",
      "step: 680605, loss: 0.05723803862929344, data time: 0.011194050312042236\n",
      "step: 680606, loss: 0.06263823807239532, data time: 0.01078101566859654\n",
      "step: 680607, loss: 0.06290581822395325, data time: 0.010408975861289284\n",
      "step: 680608, loss: 0.05940663069486618, data time: 0.010058921316395636\n",
      "step: 680609, loss: 0.059662193059921265, data time: 0.009743203719456991\n",
      "step: 680610, loss: 0.0587763749063015, data time: 0.0094451904296875\n",
      "step: 680611, loss: 0.062097348272800446, data time: 0.009171183292682353\n",
      "step: 680612, loss: 0.06805063784122467, data time: 0.008916360360604746\n",
      "step: 680613, loss: 0.06079729646444321, data time: 0.008684022086007255\n",
      "step: 680614, loss: 0.05611567944288254, data time: 0.008475394084535796\n",
      "step: 680615, loss: 0.057637035846710205, data time: 0.008278330167134603\n",
      "step: 680616, loss: 0.06058073043823242, data time: 0.008093641650292182\n",
      "step: 680617, loss: 0.059839844703674316, data time: 0.007920503616333008\n",
      "step: 680618, loss: 0.06147041916847229, data time: 0.00774213761994333\n",
      "step: 680619, loss: 0.06168501824140549, data time: 0.007573730805340935\n",
      "step: 680620, loss: 0.05450024828314781, data time: 0.007415063040597098\n",
      "step: 680621, loss: 0.06506653130054474, data time: 0.007263700167338054\n",
      "step: 680622, loss: 0.058333870023489, data time: 0.0071222588822648335\n",
      "step: 680623, loss: 0.06007719039916992, data time: 0.006993645115902549\n",
      "step: 680624, loss: 0.058475881814956665, data time: 0.006869964110545623\n",
      "step: 680625, loss: 0.03323972225189209, data time: 0.006752139329910279\n",
      "step: 680626, loss: 0.05908435583114624, data time: 0.18150997161865234\n",
      "step: 680627, loss: 0.06169367954134941, data time: 0.09154129028320312\n",
      "step: 680628, loss: 0.06775318831205368, data time: 0.061794281005859375\n",
      "step: 680629, loss: 0.06007898598909378, data time: 0.047194063663482666\n",
      "step: 680630, loss: 0.06014150008559227, data time: 0.038032913208007814\n",
      "step: 680631, loss: 0.0639483779668808, data time: 0.03192460536956787\n",
      "step: 680632, loss: 0.06167958304286003, data time: 0.027580976486206055\n",
      "step: 680633, loss: 0.059895217418670654, data time: 0.024385660886764526\n",
      "step: 680634, loss: 0.052848391234874725, data time: 0.02182732688056098\n",
      "step: 680635, loss: 0.06227695941925049, data time: 0.019866180419921876\n",
      "step: 680636, loss: 0.05961385369300842, data time: 0.018263535066084427\n",
      "step: 680637, loss: 0.05620443820953369, data time: 0.016918619473775227\n",
      "step: 680638, loss: 0.05750111863017082, data time: 0.015787418072040264\n",
      "step: 680639, loss: 0.05856902524828911, data time: 0.014803426606314523\n",
      "step: 680640, loss: 0.06310270726680756, data time: 0.013953081766764323\n",
      "step: 680641, loss: 0.05858142301440239, data time: 0.013222545385360718\n",
      "step: 680642, loss: 0.05871627852320671, data time: 0.012564743266386144\n",
      "step: 680643, loss: 0.06513332575559616, data time: 0.011980136235555014\n",
      "step: 680644, loss: 0.06124456226825714, data time: 0.011455435501901727\n",
      "step: 680645, loss: 0.05952385067939758, data time: 0.010990750789642335\n",
      "step: 680646, loss: 0.06578902900218964, data time: 0.010567562920706612\n",
      "step: 680647, loss: 0.05738406628370285, data time: 0.010189067233692516\n",
      "step: 680648, loss: 0.06484101712703705, data time: 0.009834434675133747\n",
      "step: 680649, loss: 0.06630472093820572, data time: 0.009526083866755167\n",
      "step: 680650, loss: 0.06259514391422272, data time: 0.009234771728515626\n",
      "step: 680651, loss: 0.0628189668059349, data time: 0.008960641347444974\n",
      "step: 680652, loss: 0.062375105917453766, data time: 0.008701112535264757\n",
      "step: 680653, loss: 0.062206562608480453, data time: 0.008461534976959229\n",
      "step: 680654, loss: 0.06429536640644073, data time: 0.008244933753178037\n",
      "step: 680655, loss: 0.058467473834753036, data time: 0.008042462666829427\n",
      "step: 680656, loss: 0.05762461945414543, data time: 0.007855553780832598\n",
      "step: 680657, loss: 0.0643986165523529, data time: 0.007682882249355316\n",
      "step: 680658, loss: 0.058697909116744995, data time: 0.007507656559799657\n",
      "step: 680659, loss: 0.06477048248052597, data time: 0.007346405702478746\n",
      "step: 680660, loss: 0.06285068392753601, data time: 0.007190486363002232\n",
      "step: 680661, loss: 0.06234362721443176, data time: 0.007042944431304932\n",
      "step: 680662, loss: 0.06565321236848831, data time: 0.006903848132571658\n",
      "step: 680663, loss: 0.06470513343811035, data time: 0.00677620109758879\n",
      "step: 680664, loss: 0.06516404449939728, data time: 0.006660241347092848\n",
      "step: 680665, loss: 0.07883992791175842, data time: 0.006545311212539673\n",
      "step: 680666, loss: 0.06124911457300186, data time: 0.18024516105651855\n",
      "step: 680667, loss: 0.054911017417907715, data time: 0.09089398384094238\n",
      "step: 680668, loss: 0.06060608848929405, data time: 0.061111052831014\n",
      "step: 680669, loss: 0.06177998706698418, data time: 0.04670912027359009\n",
      "step: 680670, loss: 0.06102700158953667, data time: 0.03765091896057129\n",
      "step: 680671, loss: 0.05940708518028259, data time: 0.03161128362019857\n",
      "step: 680672, loss: 0.0629536360502243, data time: 0.027298041752406528\n",
      "step: 680673, loss: 0.06270900368690491, data time: 0.02414122223854065\n",
      "step: 680674, loss: 0.06786958128213882, data time: 0.021610551410251193\n",
      "step: 680675, loss: 0.06421642750501633, data time: 0.019650983810424804\n",
      "step: 680676, loss: 0.05759947746992111, data time: 0.018060077320445667\n",
      "step: 680677, loss: 0.06405167281627655, data time: 0.016740500926971436\n",
      "step: 680678, loss: 0.07250302284955978, data time: 0.01562665059016301\n",
      "step: 680679, loss: 0.059673160314559937, data time: 0.014658570289611816\n",
      "step: 680680, loss: 0.06462939083576202, data time: 0.013816181818644207\n",
      "step: 680681, loss: 0.06408005207777023, data time: 0.013089105486869812\n",
      "step: 680682, loss: 0.05859220772981644, data time: 0.012454944498398724\n",
      "step: 680683, loss: 0.06251729279756546, data time: 0.011873881022135416\n",
      "step: 680684, loss: 0.06553445011377335, data time: 0.011352175160458213\n",
      "step: 680685, loss: 0.0630553811788559, data time: 0.010900282859802246\n",
      "step: 680686, loss: 0.06341955065727234, data time: 0.010483582814534506\n",
      "step: 680687, loss: 0.06299331784248352, data time: 0.010106585242531517\n",
      "step: 680688, loss: 0.06451199948787689, data time: 0.009754844333814539\n",
      "step: 680689, loss: 0.061530254781246185, data time: 0.00943287213643392\n",
      "step: 680690, loss: 0.05878806114196777, data time: 0.009139070510864258\n",
      "step: 680691, loss: 0.0651804506778717, data time: 0.008866310119628906\n",
      "step: 680692, loss: 0.06011340022087097, data time: 0.008610734233149776\n",
      "step: 680693, loss: 0.06185635179281235, data time: 0.008375406265258789\n",
      "step: 680694, loss: 0.059002410620450974, data time: 0.008164488036057045\n",
      "step: 680695, loss: 0.061309508979320526, data time: 0.007967519760131835\n",
      "step: 680696, loss: 0.0642334371805191, data time: 0.007781605566701581\n",
      "step: 680697, loss: 0.059546418488025665, data time: 0.007609426975250244\n",
      "step: 680698, loss: 0.06076262518763542, data time: 0.007436304381399444\n",
      "step: 680699, loss: 0.06981728971004486, data time: 0.0072744313408346735\n",
      "step: 680700, loss: 0.059494078159332275, data time: 0.007122686931065151\n",
      "step: 680701, loss: 0.06394115835428238, data time: 0.006975630919138591\n",
      "step: 680702, loss: 0.0612439289689064, data time: 0.0068416982083707245\n",
      "step: 680703, loss: 0.06242899224162102, data time: 0.006717154854222348\n",
      "step: 680704, loss: 0.05246949940919876, data time: 0.006596987064068134\n",
      "step: 680705, loss: 0.05966508388519287, data time: 0.00648263692855835\n",
      "step: 680706, loss: 0.06182250380516052, data time: 0.17725253105163574\n",
      "step: 680707, loss: 0.0659412071108818, data time: 0.0897517204284668\n",
      "step: 680708, loss: 0.05736467242240906, data time: 0.06075676282246908\n",
      "step: 680709, loss: 0.061725642532110214, data time: 0.046463847160339355\n",
      "step: 680710, loss: 0.06672731041908264, data time: 0.03746027946472168\n",
      "step: 680711, loss: 0.060334112495183945, data time: 0.03145833810170492\n",
      "step: 680712, loss: 0.06570665538311005, data time: 0.027162892477852956\n",
      "step: 680713, loss: 0.05911386013031006, data time: 0.024020105600357056\n",
      "step: 680714, loss: 0.06069086492061615, data time: 0.021501620610555012\n",
      "step: 680715, loss: 0.05617881566286087, data time: 0.01957385540008545\n",
      "step: 680716, loss: 0.06454174220561981, data time: 0.017994577234441585\n",
      "step: 680717, loss: 0.057728707790374756, data time: 0.01668878396352132\n",
      "step: 680718, loss: 0.0617210790514946, data time: 0.015568127998938927\n",
      "step: 680719, loss: 0.06520802527666092, data time: 0.014599221093314034\n",
      "step: 680720, loss: 0.058275189250707626, data time: 0.013768545786539714\n",
      "step: 680721, loss: 0.0638035386800766, data time: 0.01303805410861969\n",
      "step: 680722, loss: 0.06104801222681999, data time: 0.012388159247005688\n",
      "step: 680723, loss: 0.05769858881831169, data time: 0.011808156967163086\n",
      "step: 680724, loss: 0.06576330959796906, data time: 0.011290424748470909\n",
      "step: 680725, loss: 0.06323999166488647, data time: 0.010831081867218017\n",
      "step: 680726, loss: 0.06505817174911499, data time: 0.010417518161592029\n",
      "step: 680727, loss: 0.061898522078990936, data time: 0.010045615109530363\n",
      "step: 680728, loss: 0.06509710848331451, data time: 0.009698297666466755\n",
      "step: 680729, loss: 0.062218956649303436, data time: 0.00938829779624939\n",
      "step: 680730, loss: 0.053491875529289246, data time: 0.00909412384033203\n",
      "step: 680731, loss: 0.058121323585510254, data time: 0.00882103809943566\n",
      "step: 680732, loss: 0.056718721985816956, data time: 0.00856726257889359\n",
      "step: 680733, loss: 0.06377774477005005, data time: 0.008332584585462297\n",
      "step: 680734, loss: 0.05934474989771843, data time: 0.008121950873013201\n",
      "step: 680735, loss: 0.05673105642199516, data time: 0.007921838760375976\n",
      "step: 680736, loss: 0.06617696583271027, data time: 0.0077343294697423135\n",
      "step: 680737, loss: 0.05655576288700104, data time: 0.007562071084976196\n",
      "step: 680738, loss: 0.059171512722969055, data time: 0.007394082618482185\n",
      "step: 680739, loss: 0.058951981365680695, data time: 0.007235232521505917\n",
      "step: 680740, loss: 0.06174314022064209, data time: 0.0070849009922572545\n",
      "step: 680741, loss: 0.06445777416229248, data time: 0.006938795248667399\n",
      "step: 680742, loss: 0.06632053852081299, data time: 0.006804066735344964\n",
      "step: 680743, loss: 0.06166359782218933, data time: 0.00667840556094521\n",
      "step: 680744, loss: 0.06596119701862335, data time: 0.006560001617822892\n",
      "step: 680745, loss: 0.07510553300380707, data time: 0.006447517871856689\n",
      "step: 680746, loss: 0.06464755535125732, data time: 0.1851956844329834\n",
      "step: 680747, loss: 0.06433619558811188, data time: 0.09337007999420166\n",
      "step: 680748, loss: 0.06277704238891602, data time: 0.06280350685119629\n",
      "step: 680749, loss: 0.05946997553110123, data time: 0.04775947332382202\n",
      "step: 680750, loss: 0.06263451278209686, data time: 0.03849191665649414\n",
      "step: 680751, loss: 0.0597127303481102, data time: 0.032307823499043785\n",
      "step: 680752, loss: 0.06741443276405334, data time: 0.027910845620291575\n",
      "step: 680753, loss: 0.07520347088575363, data time: 0.024730443954467773\n",
      "step: 680754, loss: 0.06287229061126709, data time: 0.02213475439283583\n",
      "step: 680755, loss: 0.0612955316901207, data time: 0.020127415657043457\n",
      "step: 680756, loss: 0.06053590774536133, data time: 0.018495494669133968\n",
      "step: 680757, loss: 0.06122685968875885, data time: 0.017133037249247234\n",
      "step: 680758, loss: 0.06615519523620605, data time: 0.015988771732036885\n",
      "step: 680759, loss: 0.06207680329680443, data time: 0.01498964854649135\n",
      "step: 680760, loss: 0.05711641162633896, data time: 0.01413275400797526\n",
      "step: 680761, loss: 0.05552806705236435, data time: 0.013385623693466187\n",
      "step: 680762, loss: 0.05658896267414093, data time: 0.012722492218017578\n",
      "step: 680763, loss: 0.06291354447603226, data time: 0.012129240565829806\n",
      "step: 680764, loss: 0.06615158915519714, data time: 0.011595023305792557\n",
      "step: 680765, loss: 0.06258055567741394, data time: 0.011124229431152344\n",
      "step: 680766, loss: 0.05930851027369499, data time: 0.010696308953421456\n",
      "step: 680767, loss: 0.06368600577116013, data time: 0.010307593779130415\n",
      "step: 680768, loss: 0.05970040708780289, data time: 0.009947963382886804\n",
      "step: 680769, loss: 0.057437337934970856, data time: 0.009616166353225708\n",
      "step: 680770, loss: 0.06660446524620056, data time: 0.009317026138305665\n",
      "step: 680771, loss: 0.05574166774749756, data time: 0.009039998054504395\n",
      "step: 680772, loss: 0.05906524881720543, data time: 0.008782157191523799\n",
      "step: 680773, loss: 0.06308089941740036, data time: 0.008538493088313512\n",
      "step: 680774, loss: 0.06579738110303879, data time: 0.008319739637703731\n",
      "step: 680775, loss: 0.06313377618789673, data time: 0.00811465581258138\n",
      "step: 680776, loss: 0.06343959271907806, data time: 0.00793771589955976\n",
      "step: 680777, loss: 0.06489719450473785, data time: 0.0077626705169677734\n",
      "step: 680778, loss: 0.05843689292669296, data time: 0.007584730784098308\n",
      "step: 680779, loss: 0.05995815619826317, data time: 0.0074207221760469325\n",
      "step: 680780, loss: 0.05558868870139122, data time: 0.007265254429408482\n",
      "step: 680781, loss: 0.06432776898145676, data time: 0.007114847501118978\n",
      "step: 680782, loss: 0.05388081073760986, data time: 0.006973627451303843\n",
      "step: 680783, loss: 0.06330150365829468, data time: 0.006841753658495452\n",
      "step: 680784, loss: 0.0697842463850975, data time: 0.0067182259681897285\n",
      "step: 680785, loss: 0.04882538318634033, data time: 0.006600677967071533\n",
      "step: 680786, loss: 0.06517165154218674, data time: 0.188523530960083\n",
      "step: 680787, loss: 0.0652645155787468, data time: 0.09502458572387695\n",
      "step: 680788, loss: 0.0601031593978405, data time: 0.06385890642801921\n",
      "step: 680789, loss: 0.06308074295520782, data time: 0.048641979694366455\n",
      "step: 680790, loss: 0.06071476265788078, data time: 0.03919463157653809\n",
      "step: 680791, loss: 0.06976650655269623, data time: 0.03289961814880371\n",
      "step: 680792, loss: 0.06327901780605316, data time: 0.02840679032461984\n",
      "step: 680793, loss: 0.06356869637966156, data time: 0.02512529492378235\n",
      "step: 680794, loss: 0.060083530843257904, data time: 0.022481759389241535\n",
      "step: 680795, loss: 0.07106219977140427, data time: 0.02043638229370117\n",
      "step: 680796, loss: 0.057742517441511154, data time: 0.018773555755615234\n",
      "step: 680797, loss: 0.06731369346380234, data time: 0.01738828420639038\n",
      "step: 680798, loss: 0.06138179451227188, data time: 0.01621393057016226\n",
      "step: 680799, loss: 0.06087764352560043, data time: 0.015201057706560408\n",
      "step: 680800, loss: 0.0662713423371315, data time: 0.014318323135375977\n",
      "step: 680801, loss: 0.0680282711982727, data time: 0.013554513454437256\n",
      "step: 680802, loss: 0.0617721788585186, data time: 0.012886804692885456\n",
      "step: 680803, loss: 0.0630030483007431, data time: 0.012280133035447862\n",
      "step: 680804, loss: 0.0646660253405571, data time: 0.011739141062686318\n",
      "step: 680805, loss: 0.06673033535480499, data time: 0.0112595796585083\n",
      "step: 680806, loss: 0.0640602558851242, data time: 0.01082617895943778\n",
      "step: 680807, loss: 0.06108074635267258, data time: 0.010431181300770153\n",
      "step: 680808, loss: 0.0578208863735199, data time: 0.01007042760434358\n",
      "step: 680809, loss: 0.06001822277903557, data time: 0.009735127290089926\n",
      "step: 680810, loss: 0.061670511960983276, data time: 0.009428644180297851\n",
      "step: 680811, loss: 0.05700584501028061, data time: 0.009146020962641789\n",
      "step: 680812, loss: 0.05749635398387909, data time: 0.008879317177666558\n",
      "step: 680813, loss: 0.05994325503706932, data time: 0.008635078157697405\n",
      "step: 680814, loss: 0.06650865077972412, data time: 0.008414309600303913\n",
      "step: 680815, loss: 0.0658925250172615, data time: 0.008206788698832195\n",
      "step: 680816, loss: 0.06299000233411789, data time: 0.00801165642276887\n",
      "step: 680817, loss: 0.06409626454114914, data time: 0.007830984890460968\n",
      "step: 680818, loss: 0.06701068580150604, data time: 0.007651784203269265\n",
      "step: 680819, loss: 0.056840091943740845, data time: 0.007483973222620347\n",
      "step: 680820, loss: 0.06269994378089905, data time: 0.007326296397617885\n",
      "step: 680821, loss: 0.060726579278707504, data time: 0.007174372673034668\n",
      "step: 680822, loss: 0.05826890096068382, data time: 0.007033953795561919\n",
      "step: 680823, loss: 0.05696052312850952, data time: 0.006902280606721577\n",
      "step: 680824, loss: 0.06366224586963654, data time: 0.0067814985911051435\n",
      "step: 680825, loss: 0.0605657622218132, data time: 0.0066677391529083255\n",
      "step: 680826, loss: 0.05373375862836838, data time: 0.17125368118286133\n",
      "step: 680827, loss: 0.06157168745994568, data time: 0.08742225170135498\n",
      "step: 680828, loss: 0.059578657150268555, data time: 0.05887619654337565\n",
      "step: 680829, loss: 0.05448276922106743, data time: 0.04504877328872681\n",
      "step: 680830, loss: 0.05642026662826538, data time: 0.0363344669342041\n",
      "step: 680831, loss: 0.056928977370262146, data time: 0.030529260635375977\n",
      "step: 680832, loss: 0.05792325735092163, data time: 0.026371104376656667\n",
      "step: 680833, loss: 0.05814342200756073, data time: 0.023335903882980347\n",
      "step: 680834, loss: 0.07112257927656174, data time: 0.0208895206451416\n",
      "step: 680835, loss: 0.06418643891811371, data time: 0.019018149375915526\n",
      "step: 680836, loss: 0.06311185657978058, data time: 0.017492446032437412\n",
      "step: 680837, loss: 0.06003790348768234, data time: 0.01621115207672119\n",
      "step: 680838, loss: 0.05265093967318535, data time: 0.015130996704101562\n",
      "step: 680839, loss: 0.06566701829433441, data time: 0.014192955834524972\n",
      "step: 680840, loss: 0.05368322879076004, data time: 0.013388363520304362\n",
      "step: 680841, loss: 0.058950524777173996, data time: 0.012676417827606201\n",
      "step: 680842, loss: 0.060067445039749146, data time: 0.012053265291101792\n",
      "step: 680843, loss: 0.058628425002098083, data time: 0.01149051719241672\n",
      "step: 680844, loss: 0.05993887782096863, data time: 0.010992815620020815\n",
      "step: 680845, loss: 0.05991743132472038, data time: 0.010549449920654297\n",
      "step: 680846, loss: 0.06192855164408684, data time: 0.010154020218622117\n",
      "step: 680847, loss: 0.06690948456525803, data time: 0.009789716113697399\n",
      "step: 680848, loss: 0.06033560633659363, data time: 0.009451949078103771\n",
      "step: 680849, loss: 0.05478455871343613, data time: 0.009142011404037476\n",
      "step: 680850, loss: 0.061579130589962006, data time: 0.008858165740966796\n",
      "step: 680851, loss: 0.06133047491312027, data time: 0.008600821861853966\n",
      "step: 680852, loss: 0.06487762928009033, data time: 0.00835881409821687\n",
      "step: 680853, loss: 0.06006968766450882, data time: 0.008132968630109514\n",
      "step: 680854, loss: 0.0710580050945282, data time: 0.007925831038376381\n",
      "step: 680855, loss: 0.06078822538256645, data time: 0.00773160457611084\n",
      "step: 680856, loss: 0.05687543749809265, data time: 0.007552216129918252\n",
      "step: 680857, loss: 0.06179018318653107, data time: 0.007388442754745483\n",
      "step: 680858, loss: 0.05787608027458191, data time: 0.00722323041973692\n",
      "step: 680859, loss: 0.06250912696123123, data time: 0.007068732205559225\n",
      "step: 680860, loss: 0.05933847278356552, data time: 0.006927013397216797\n",
      "step: 680861, loss: 0.05824952572584152, data time: 0.006785399383968777\n",
      "step: 680862, loss: 0.06296290457248688, data time: 0.006653502180769637\n",
      "step: 680863, loss: 0.05881361663341522, data time: 0.0065302723332455286\n",
      "step: 680864, loss: 0.06179535388946533, data time: 0.006414969762166341\n",
      "step: 680865, loss: 0.047477804124355316, data time: 0.006306838989257812\n",
      "step: 680866, loss: 0.06228487938642502, data time: 0.17629480361938477\n",
      "step: 680867, loss: 0.0658278688788414, data time: 0.0890434980392456\n",
      "step: 680868, loss: 0.05722122639417648, data time: 0.06025306383768717\n",
      "step: 680869, loss: 0.06028991937637329, data time: 0.04622548818588257\n",
      "step: 680870, loss: 0.05794749781489372, data time: 0.037304353713989255\n",
      "step: 680871, loss: 0.058556556701660156, data time: 0.03140552838643392\n",
      "step: 680872, loss: 0.055196329951286316, data time: 0.02716789926801409\n",
      "step: 680873, loss: 0.0627359002828598, data time: 0.024066686630249023\n",
      "step: 680874, loss: 0.06049906462430954, data time: 0.021561278237236872\n",
      "step: 680875, loss: 0.06424218416213989, data time: 0.019634246826171875\n",
      "step: 680876, loss: 0.06516426801681519, data time: 0.018076766620982777\n",
      "step: 680877, loss: 0.06331729888916016, data time: 0.016783674558003742\n",
      "step: 680878, loss: 0.06231693550944328, data time: 0.015686970490675706\n",
      "step: 680879, loss: 0.06454131007194519, data time: 0.014734796115330287\n",
      "step: 680880, loss: 0.06049704551696777, data time: 0.013916842142740886\n",
      "step: 680881, loss: 0.06834493577480316, data time: 0.013200938701629639\n",
      "step: 680882, loss: 0.06792305409908295, data time: 0.012564617044785443\n",
      "step: 680883, loss: 0.059433262795209885, data time: 0.01199952761332194\n",
      "step: 680884, loss: 0.06018928810954094, data time: 0.011491976286235609\n",
      "step: 680885, loss: 0.062131594866514206, data time: 0.011041927337646484\n",
      "step: 680886, loss: 0.06259723007678986, data time: 0.01063204946972075\n",
      "step: 680887, loss: 0.05802389979362488, data time: 0.01024682955308394\n",
      "step: 680888, loss: 0.062061648815870285, data time: 0.009891717330269192\n",
      "step: 680889, loss: 0.06279760599136353, data time: 0.009569287300109863\n",
      "step: 680890, loss: 0.061376433819532394, data time: 0.009270830154418945\n",
      "step: 680891, loss: 0.06372815370559692, data time: 0.008990599558903621\n",
      "step: 680892, loss: 0.0601501390337944, data time: 0.008730173110961914\n",
      "step: 680893, loss: 0.05572585016489029, data time: 0.008490434714726039\n",
      "step: 680894, loss: 0.05981484800577164, data time: 0.008270165015911234\n",
      "step: 680895, loss: 0.059076130390167236, data time: 0.008067131042480469\n",
      "step: 680896, loss: 0.06035484001040459, data time: 0.007878895728818832\n",
      "step: 680897, loss: 0.056786976754665375, data time: 0.007705084979534149\n",
      "step: 680898, loss: 0.05991523712873459, data time: 0.00753242319280451\n",
      "step: 680899, loss: 0.06352104246616364, data time: 0.007368228014777689\n",
      "step: 680900, loss: 0.0645899847149849, data time: 0.007211569377354213\n",
      "step: 680901, loss: 0.06075238436460495, data time: 0.007062468263838027\n",
      "step: 680902, loss: 0.06196246296167374, data time: 0.006922953837626689\n",
      "step: 680903, loss: 0.058770738542079926, data time: 0.006802665559869064\n",
      "step: 680904, loss: 0.06815153360366821, data time: 0.006680048429048979\n",
      "step: 680905, loss: 0.07526662945747375, data time: 0.0065636754035949705\n",
      "step: 680906, loss: 0.06688490509986877, data time: 0.16933250427246094\n",
      "step: 680907, loss: 0.06061559170484543, data time: 0.08622205257415771\n",
      "step: 680908, loss: 0.06120889633893967, data time: 0.05798602104187012\n",
      "step: 680909, loss: 0.06404222548007965, data time: 0.04435551166534424\n",
      "step: 680910, loss: 0.06378638744354248, data time: 0.035780906677246094\n",
      "step: 680911, loss: 0.06141437217593193, data time: 0.03005663553873698\n",
      "step: 680912, loss: 0.0586027130484581, data time: 0.025976044791085378\n",
      "step: 680913, loss: 0.06641187518835068, data time: 0.02300015091896057\n",
      "step: 680914, loss: 0.060248471796512604, data time: 0.020591444439358182\n",
      "step: 680915, loss: 0.0624450258910656, data time: 0.018722891807556152\n",
      "step: 680916, loss: 0.059247031807899475, data time: 0.017215316945856266\n",
      "step: 680917, loss: 0.05525832623243332, data time: 0.01595890522003174\n",
      "step: 680918, loss: 0.06522078812122345, data time: 0.01489569590641902\n",
      "step: 680919, loss: 0.0626862570643425, data time: 0.013980695179530553\n",
      "step: 680920, loss: 0.06192437931895256, data time: 0.013184404373168946\n",
      "step: 680921, loss: 0.056221917271614075, data time: 0.012491270899772644\n",
      "step: 680922, loss: 0.062013499438762665, data time: 0.011882627711576573\n",
      "step: 680923, loss: 0.05637013912200928, data time: 0.011331929100884331\n",
      "step: 680924, loss: 0.0660041868686676, data time: 0.010839349345157021\n",
      "step: 680925, loss: 0.062365978956222534, data time: 0.010401999950408936\n",
      "step: 680926, loss: 0.05900611728429794, data time: 0.010009175255185082\n",
      "step: 680927, loss: 0.06762015074491501, data time: 0.009651086547157982\n",
      "step: 680928, loss: 0.059479426592588425, data time: 0.009321772533914318\n",
      "step: 680929, loss: 0.06469123810529709, data time: 0.009024391571680704\n",
      "step: 680930, loss: 0.05892205238342285, data time: 0.008745613098144532\n",
      "step: 680931, loss: 0.06310668587684631, data time: 0.008490947576669546\n",
      "step: 680932, loss: 0.06838028132915497, data time: 0.0082516317014341\n",
      "step: 680933, loss: 0.059251707047224045, data time: 0.008028456142970495\n",
      "step: 680934, loss: 0.060010503977537155, data time: 0.007824601798221982\n",
      "step: 680935, loss: 0.06230480596423149, data time: 0.007634250322977701\n",
      "step: 680936, loss: 0.0626915767788887, data time: 0.00745796388195407\n",
      "step: 680937, loss: 0.06683915108442307, data time: 0.007297240197658539\n",
      "step: 680938, loss: 0.06511633843183517, data time: 0.007138591824155866\n",
      "step: 680939, loss: 0.06998047232627869, data time: 0.0069878592210657455\n",
      "step: 680940, loss: 0.05827253311872482, data time: 0.006848880222865513\n",
      "step: 680941, loss: 0.06006719172000885, data time: 0.0067129068904452855\n",
      "step: 680942, loss: 0.057171132415533066, data time: 0.006585668873142552\n",
      "step: 680943, loss: 0.0588245689868927, data time: 0.006469795578404477\n",
      "step: 680944, loss: 0.05833206698298454, data time: 0.006358929169483674\n",
      "step: 680945, loss: 0.06654448807239532, data time: 0.006254065036773682\n",
      "step: 680946, loss: 0.059519898146390915, data time: 0.182297945022583\n",
      "step: 680947, loss: 0.0638076514005661, data time: 0.09260320663452148\n",
      "step: 680948, loss: 0.060943182557821274, data time: 0.0622554620107015\n",
      "step: 680949, loss: 0.0630248412489891, data time: 0.047660112380981445\n",
      "step: 680950, loss: 0.05750222131609917, data time: 0.03840193748474121\n",
      "step: 680951, loss: 0.0621376559138298, data time: 0.032240867614746094\n",
      "step: 680952, loss: 0.06035652756690979, data time: 0.027840171541486467\n",
      "step: 680953, loss: 0.06267356872558594, data time: 0.024627357721328735\n",
      "step: 680954, loss: 0.06546105444431305, data time: 0.022037400139702693\n",
      "step: 680955, loss: 0.06436395645141602, data time: 0.020046472549438477\n",
      "step: 680956, loss: 0.061356257647275925, data time: 0.018426916816017845\n",
      "step: 680957, loss: 0.06916560232639313, data time: 0.017072637875874836\n",
      "step: 680958, loss: 0.06693965196609497, data time: 0.015929112067589395\n",
      "step: 680959, loss: 0.07495622336864471, data time: 0.014943616730826241\n",
      "step: 680960, loss: 0.06242047995328903, data time: 0.014081732432047526\n",
      "step: 680961, loss: 0.06512399017810822, data time: 0.013326406478881836\n",
      "step: 680962, loss: 0.05928903818130493, data time: 0.012669857810525334\n",
      "step: 680963, loss: 0.06234321743249893, data time: 0.012078973982069228\n",
      "step: 680964, loss: 0.060774318873882294, data time: 0.011549824162533409\n",
      "step: 680965, loss: 0.05869369953870773, data time: 0.011082780361175538\n",
      "step: 680966, loss: 0.06263795495033264, data time: 0.010658740997314453\n",
      "step: 680967, loss: 0.06221172958612442, data time: 0.010274941271001642\n",
      "step: 680968, loss: 0.05761877819895744, data time: 0.009921965391739555\n",
      "step: 680969, loss: 0.06359249353408813, data time: 0.009593605995178223\n",
      "step: 680970, loss: 0.0634639710187912, data time: 0.009290447235107422\n",
      "step: 680971, loss: 0.06056682765483856, data time: 0.00901467983539288\n",
      "step: 680972, loss: 0.06002914905548096, data time: 0.00875197516547309\n",
      "step: 680973, loss: 0.06038662791252136, data time: 0.008514140333448137\n",
      "step: 680974, loss: 0.06303678452968597, data time: 0.008297336512598497\n",
      "step: 680975, loss: 0.06082502007484436, data time: 0.008091990152994792\n",
      "step: 680976, loss: 0.06143102049827576, data time: 0.007902253058648879\n",
      "step: 680977, loss: 0.05985131114721298, data time: 0.007728546857833862\n",
      "step: 680978, loss: 0.06278711557388306, data time: 0.007553302880489465\n",
      "step: 680979, loss: 0.06199878081679344, data time: 0.007386537159190458\n",
      "step: 680980, loss: 0.06378965079784393, data time: 0.007232604707990374\n",
      "step: 680981, loss: 0.0642656683921814, data time: 0.007082614633772109\n",
      "step: 680982, loss: 0.05819019675254822, data time: 0.006942014436464052\n",
      "step: 680983, loss: 0.05966296046972275, data time: 0.006812697962710732\n",
      "step: 680984, loss: 0.06832803785800934, data time: 0.006689206147805238\n",
      "step: 680985, loss: 0.05788081884384155, data time: 0.006573301553726196\n",
      "step: 680986, loss: 0.06573411822319031, data time: 0.17087936401367188\n",
      "step: 680987, loss: 0.05838782340288162, data time: 0.08684420585632324\n",
      "step: 680988, loss: 0.06502021104097366, data time: 0.058410962422688804\n",
      "step: 680989, loss: 0.06458957493305206, data time: 0.04472696781158447\n",
      "step: 680990, loss: 0.06754066050052643, data time: 0.036058473587036136\n",
      "step: 680991, loss: 0.06155019998550415, data time: 0.03029942512512207\n",
      "step: 680992, loss: 0.06534802913665771, data time: 0.02617941583905901\n",
      "step: 680993, loss: 0.062072765082120895, data time: 0.023170411586761475\n",
      "step: 680994, loss: 0.06433115899562836, data time: 0.020752668380737305\n",
      "step: 680995, loss: 0.06522762030363083, data time: 0.01887953281402588\n",
      "step: 680996, loss: 0.06012876331806183, data time: 0.017354835163463245\n",
      "step: 680997, loss: 0.06267540901899338, data time: 0.016086876392364502\n",
      "step: 680998, loss: 0.06160195916891098, data time: 0.015013162906353291\n",
      "step: 680999, loss: 0.06010865420103073, data time: 0.014084168842860631\n",
      "step: 681000, loss: 0.0599806122481823, data time: 0.013284651438395183\n",
      "step: 681001, loss: 0.0642298236489296, data time: 0.012580901384353638\n",
      "step: 681002, loss: 0.05399763211607933, data time: 0.011965401032391717\n",
      "step: 681003, loss: 0.0637533962726593, data time: 0.011412832472059462\n",
      "step: 681004, loss: 0.06284929811954498, data time: 0.010917287123830695\n",
      "step: 681005, loss: 0.05854126811027527, data time: 0.010478329658508301\n",
      "step: 681006, loss: 0.06522773951292038, data time: 0.010083777563912528\n",
      "step: 681007, loss: 0.06178770586848259, data time: 0.009725841608914461\n",
      "step: 681008, loss: 0.06454622000455856, data time: 0.009390551110972528\n",
      "step: 681009, loss: 0.06450728327035904, data time: 0.009087969859441122\n",
      "step: 681010, loss: 0.06089415401220322, data time: 0.008803968429565429\n",
      "step: 681011, loss: 0.057472795248031616, data time: 0.008547058472266564\n",
      "step: 681012, loss: 0.06357329338788986, data time: 0.008302503161960177\n",
      "step: 681013, loss: 0.06519539654254913, data time: 0.008079639502934046\n",
      "step: 681014, loss: 0.05964251980185509, data time: 0.007877760920031318\n",
      "step: 681015, loss: 0.06487897038459778, data time: 0.0076868534088134766\n",
      "step: 681016, loss: 0.057367391884326935, data time: 0.007508877784975113\n",
      "step: 681017, loss: 0.05535323545336723, data time: 0.007347568869590759\n",
      "step: 681018, loss: 0.05714595317840576, data time: 0.007185134020718661\n",
      "step: 681019, loss: 0.0574115514755249, data time: 0.007031489821041331\n",
      "step: 681020, loss: 0.06042666360735893, data time: 0.006889329637799944\n",
      "step: 681021, loss: 0.0605899840593338, data time: 0.006749921374850803\n",
      "step: 681022, loss: 0.06245185062289238, data time: 0.006618332218479466\n",
      "step: 681023, loss: 0.06142086535692215, data time: 0.0064982113085295026\n",
      "step: 681024, loss: 0.06339675933122635, data time: 0.0063844766372289415\n",
      "step: 681025, loss: 0.08048154413700104, data time: 0.006280189752578736\n",
      "step: 681026, loss: 0.060953687876462936, data time: 0.1861429214477539\n",
      "step: 681027, loss: 0.05943473428487778, data time: 0.09387040138244629\n",
      "step: 681028, loss: 0.05776405334472656, data time: 0.06360832850138347\n",
      "step: 681029, loss: 0.05598254129290581, data time: 0.04848909378051758\n",
      "step: 681030, loss: 0.07106776535511017, data time: 0.039065980911254884\n",
      "step: 681031, loss: 0.058889906853437424, data time: 0.03280484676361084\n",
      "step: 681032, loss: 0.0645587295293808, data time: 0.02833376611982073\n",
      "step: 681033, loss: 0.06484398245811462, data time: 0.025045305490493774\n",
      "step: 681034, loss: 0.06149064004421234, data time: 0.022409836451212566\n",
      "step: 681035, loss: 0.05793420597910881, data time: 0.020373964309692384\n",
      "step: 681036, loss: 0.06073227524757385, data time: 0.0187172456221147\n",
      "step: 681037, loss: 0.054126136004924774, data time: 0.01733452081680298\n",
      "step: 681038, loss: 0.06357461214065552, data time: 0.01616507310133714\n",
      "step: 681039, loss: 0.0642683133482933, data time: 0.015154038156781877\n",
      "step: 681040, loss: 0.06240957975387573, data time: 0.014287296930948894\n",
      "step: 681041, loss: 0.0589204840362072, data time: 0.01353771984577179\n",
      "step: 681042, loss: 0.061794720590114594, data time: 0.012860634747673483\n",
      "step: 681043, loss: 0.06465581059455872, data time: 0.012263880835639106\n",
      "step: 681044, loss: 0.06481197476387024, data time: 0.011734121724178917\n",
      "step: 681045, loss: 0.06310009211301804, data time: 0.011259722709655761\n",
      "step: 681046, loss: 0.06464734673500061, data time: 0.010827643530709403\n",
      "step: 681047, loss: 0.05925213545560837, data time: 0.010432828556407581\n",
      "step: 681048, loss: 0.06164270639419556, data time: 0.010070686754973038\n",
      "step: 681049, loss: 0.061750076711177826, data time: 0.009742538134256998\n",
      "step: 681050, loss: 0.06180250272154808, data time: 0.00944162368774414\n",
      "step: 681051, loss: 0.06382318586111069, data time: 0.009157400864821214\n",
      "step: 681052, loss: 0.06523019075393677, data time: 0.008891096821537724\n",
      "step: 681053, loss: 0.058977626264095306, data time: 0.008647586618150984\n",
      "step: 681054, loss: 0.0647512823343277, data time: 0.008422810455848431\n",
      "step: 681055, loss: 0.05996699631214142, data time: 0.008214131991068522\n",
      "step: 681056, loss: 0.06287790089845657, data time: 0.008030568399736959\n",
      "step: 681057, loss: 0.05712094157934189, data time: 0.00786028802394867\n",
      "step: 681058, loss: 0.0603199303150177, data time: 0.0076874169436368074\n",
      "step: 681059, loss: 0.061053209006786346, data time: 0.007525009267470416\n",
      "step: 681060, loss: 0.0662703812122345, data time: 0.00737041745867048\n",
      "step: 681061, loss: 0.05944838374853134, data time: 0.007222023275163438\n",
      "step: 681062, loss: 0.061145853251218796, data time: 0.00708300358540303\n",
      "step: 681063, loss: 0.059049587696790695, data time: 0.006953992341694079\n",
      "step: 681064, loss: 0.06080641970038414, data time: 0.006830955163026467\n",
      "step: 681065, loss: 0.04671749472618103, data time: 0.006711286306381225\n",
      "step: 681066, loss: 0.06473658978939056, data time: 0.19457507133483887\n",
      "step: 681067, loss: 0.0710427314043045, data time: 0.09806358814239502\n",
      "step: 681068, loss: 0.0676998570561409, data time: 0.06630142529805501\n",
      "step: 681069, loss: 0.060255419462919235, data time: 0.05049693584442139\n",
      "step: 681070, loss: 0.06514538079500198, data time: 0.040689611434936525\n",
      "step: 681071, loss: 0.060400187969207764, data time: 0.03417412439982096\n",
      "step: 681072, loss: 0.05734739825129509, data time: 0.029503788266863142\n",
      "step: 681073, loss: 0.06474676728248596, data time: 0.026078522205352783\n",
      "step: 681074, loss: 0.06531877815723419, data time: 0.023332754770914715\n",
      "step: 681075, loss: 0.0636104941368103, data time: 0.02119901180267334\n",
      "step: 681076, loss: 0.05713643878698349, data time: 0.019462650472467594\n",
      "step: 681077, loss: 0.05767397582530975, data time: 0.018024325370788574\n",
      "step: 681078, loss: 0.06476464122533798, data time: 0.016804328331580527\n",
      "step: 681079, loss: 0.057323992252349854, data time: 0.015748960631234304\n",
      "step: 681080, loss: 0.059747807681560516, data time: 0.014844608306884766\n",
      "step: 681081, loss: 0.06922532618045807, data time: 0.014046788215637207\n",
      "step: 681082, loss: 0.061623960733413696, data time: 0.01333754202898811\n",
      "step: 681083, loss: 0.06148547679185867, data time: 0.012706014845106337\n",
      "step: 681084, loss: 0.05650141090154648, data time: 0.012143662101344058\n",
      "step: 681085, loss: 0.05965954065322876, data time: 0.011645007133483886\n",
      "step: 681086, loss: 0.0618271678686142, data time: 0.011193468457176572\n",
      "step: 681087, loss: 0.060280054807662964, data time: 0.010789773680947044\n",
      "step: 681088, loss: 0.06492569297552109, data time: 0.010407012441883917\n",
      "step: 681089, loss: 0.06618817895650864, data time: 0.010064522425333658\n",
      "step: 681090, loss: 0.05939904600381851, data time: 0.00974456787109375\n",
      "step: 681091, loss: 0.062161874026060104, data time: 0.009449344414931077\n",
      "step: 681092, loss: 0.06565403193235397, data time: 0.009171441749290184\n",
      "step: 681093, loss: 0.06557226181030273, data time: 0.008915969303676061\n",
      "step: 681094, loss: 0.06386497616767883, data time: 0.008682176984589675\n",
      "step: 681095, loss: 0.06235431879758835, data time: 0.008464185396830241\n",
      "step: 681096, loss: 0.05777336657047272, data time: 0.008261965167137885\n",
      "step: 681097, loss: 0.06060374900698662, data time: 0.008076675236225128\n",
      "step: 681098, loss: 0.0641314834356308, data time: 0.007893410595980558\n",
      "step: 681099, loss: 0.05870016664266586, data time: 0.007719369495616239\n",
      "step: 681100, loss: 0.061309926211833954, data time: 0.007552984782627651\n",
      "step: 681101, loss: 0.06620150804519653, data time: 0.007394969463348389\n",
      "step: 681102, loss: 0.06430236995220184, data time: 0.007247667054872255\n",
      "step: 681103, loss: 0.062394000589847565, data time: 0.007110162785178737\n",
      "step: 681104, loss: 0.06029941141605377, data time: 0.006980107380793645\n",
      "step: 681105, loss: 0.03960177302360535, data time: 0.006856578588485718\n",
      "step: 681106, loss: 0.06569609045982361, data time: 0.17749428749084473\n",
      "step: 681107, loss: 0.06176578998565674, data time: 0.1161339282989502\n",
      "step: 681108, loss: 0.0612003430724144, data time: 0.07835030555725098\n",
      "step: 681109, loss: 0.061067044734954834, data time: 0.05953186750411987\n",
      "step: 681110, loss: 0.05820905789732933, data time: 0.04788365364074707\n",
      "step: 681111, loss: 0.06325286626815796, data time: 0.040216286977132164\n",
      "step: 681112, loss: 0.061553459614515305, data time: 0.03472866330827985\n",
      "step: 681113, loss: 0.058263279497623444, data time: 0.030685484409332275\n",
      "step: 681114, loss: 0.059420518577098846, data time: 0.027460813522338867\n",
      "step: 681115, loss: 0.0579441636800766, data time: 0.02486896514892578\n",
      "step: 681116, loss: 0.06280141323804855, data time: 0.022803241556341\n",
      "step: 681117, loss: 0.05863693356513977, data time: 0.021081844965616863\n",
      "step: 681118, loss: 0.06451135128736496, data time: 0.019628488100492038\n",
      "step: 681119, loss: 0.0641024112701416, data time: 0.018370883805411204\n",
      "step: 681120, loss: 0.06675725430250168, data time: 0.017286650339762368\n",
      "step: 681121, loss: 0.057402580976486206, data time: 0.016336798667907715\n",
      "step: 681122, loss: 0.0549946054816246, data time: 0.015521119622623218\n",
      "step: 681123, loss: 0.06276270747184753, data time: 0.01472828123304579\n",
      "step: 681124, loss: 0.06041538715362549, data time: 0.01405905422411467\n",
      "step: 681125, loss: 0.05406990274786949, data time: 0.013461744785308838\n",
      "step: 681126, loss: 0.06016090512275696, data time: 0.012928179332188197\n",
      "step: 681127, loss: 0.06322099268436432, data time: 0.012438774108886719\n",
      "step: 681128, loss: 0.06084105372428894, data time: 0.011984369029169497\n",
      "step: 681129, loss: 0.06318783760070801, data time: 0.011574814716974894\n",
      "step: 681130, loss: 0.05696570500731468, data time: 0.011196069717407227\n",
      "step: 681131, loss: 0.05888050049543381, data time: 0.010844248991746169\n",
      "step: 681132, loss: 0.06696571409702301, data time: 0.010513667707090025\n",
      "step: 681133, loss: 0.05818495154380798, data time: 0.01021054812840053\n",
      "step: 681134, loss: 0.06129559874534607, data time: 0.009944340278362405\n",
      "step: 681135, loss: 0.06134944409132004, data time: 0.009684165318806967\n",
      "step: 681136, loss: 0.07087218761444092, data time: 0.00944378299097861\n",
      "step: 681137, loss: 0.06166202947497368, data time: 0.009228713810443878\n",
      "step: 681138, loss: 0.05541125684976578, data time: 0.00901503273935029\n",
      "step: 681139, loss: 0.05436002463102341, data time: 0.008812497643863453\n",
      "step: 681140, loss: 0.06264612078666687, data time: 0.008617843900408064\n",
      "step: 681141, loss: 0.06189711019396782, data time: 0.008430838584899902\n",
      "step: 681142, loss: 0.06167010962963104, data time: 0.008258722923897408\n",
      "step: 681143, loss: 0.06493532657623291, data time: 0.008097830571626363\n",
      "step: 681144, loss: 0.06083416938781738, data time: 0.007945054616683569\n",
      "step: 681145, loss: 0.0560871884226799, data time: 0.007800203561782837\n",
      "step: 681146, loss: 0.06373296678066254, data time: 0.19100427627563477\n",
      "step: 681147, loss: 0.056864164769649506, data time: 0.09633302688598633\n",
      "step: 681148, loss: 0.05337345600128174, data time: 0.06474121411641438\n",
      "step: 681149, loss: 0.06611881405115128, data time: 0.04943782091140747\n",
      "step: 681150, loss: 0.0559876374900341, data time: 0.03984384536743164\n",
      "step: 681151, loss: 0.061443157494068146, data time: 0.03346641858418783\n",
      "step: 681152, loss: 0.060059793293476105, data time: 0.028901032039097378\n",
      "step: 681153, loss: 0.06456863135099411, data time: 0.02555900812149048\n",
      "step: 681154, loss: 0.056205760687589645, data time: 0.022876209682888456\n",
      "step: 681155, loss: 0.07036418467760086, data time: 0.020847177505493163\n",
      "step: 681156, loss: 0.05914448946714401, data time: 0.01915576241233132\n",
      "step: 681157, loss: 0.0590510293841362, data time: 0.017739613850911457\n",
      "step: 681158, loss: 0.06266460567712784, data time: 0.01654657950768104\n",
      "step: 681159, loss: 0.06703720986843109, data time: 0.015513931001935686\n",
      "step: 681160, loss: 0.05865735933184624, data time: 0.014626153310139974\n",
      "step: 681161, loss: 0.06027603894472122, data time: 0.013850703835487366\n",
      "step: 681162, loss: 0.06278377026319504, data time: 0.013163440367754768\n",
      "step: 681163, loss: 0.06034044176340103, data time: 0.012546102205912272\n",
      "step: 681164, loss: 0.061615586280822754, data time: 0.011997975801166735\n",
      "step: 681165, loss: 0.06177808344364166, data time: 0.01150825023651123\n",
      "step: 681166, loss: 0.06275366246700287, data time: 0.011065358207339332\n",
      "step: 681167, loss: 0.05957949161529541, data time: 0.01066302169453014\n",
      "step: 681168, loss: 0.06330981850624084, data time: 0.010288870852926502\n",
      "step: 681169, loss: 0.057059720158576965, data time: 0.009951074918111166\n",
      "step: 681170, loss: 0.06520995497703552, data time: 0.00963963508605957\n",
      "step: 681171, loss: 0.057536106556653976, data time: 0.009350455724276029\n",
      "step: 681172, loss: 0.06005033850669861, data time: 0.00907856446725351\n",
      "step: 681173, loss: 0.07181046903133392, data time: 0.00882934672491891\n",
      "step: 681174, loss: 0.06046068295836449, data time: 0.00859993079612995\n",
      "step: 681175, loss: 0.061152130365371704, data time: 0.008388201395670572\n",
      "step: 681176, loss: 0.0642818808555603, data time: 0.008185940404092111\n",
      "step: 681177, loss: 0.06255470216274261, data time: 0.00800354778766632\n",
      "step: 681178, loss: 0.05898965522646904, data time: 0.007823568401914654\n",
      "step: 681179, loss: 0.06483902782201767, data time: 0.007652808638180003\n",
      "step: 681180, loss: 0.05659320205450058, data time: 0.007492542266845703\n",
      "step: 681181, loss: 0.0614326111972332, data time: 0.007337874836391873\n",
      "step: 681182, loss: 0.061718087643384933, data time: 0.007193101418984903\n",
      "step: 681183, loss: 0.06357300281524658, data time: 0.007058890242325633\n",
      "step: 681184, loss: 0.06329065561294556, data time: 0.0069334140190711385\n",
      "step: 681185, loss: 0.05234278738498688, data time: 0.006814587116241455\n",
      "step: 681186, loss: 0.056752752512693405, data time: 0.1964414119720459\n",
      "step: 681187, loss: 0.06498619168996811, data time: 0.09901189804077148\n",
      "step: 681188, loss: 0.061574824154376984, data time: 0.06691233317057292\n",
      "step: 681189, loss: 0.06635934114456177, data time: 0.050957679748535156\n",
      "step: 681190, loss: 0.061322491616010666, data time: 0.04108591079711914\n",
      "step: 681191, loss: 0.061628248542547226, data time: 0.034494916598002114\n",
      "step: 681192, loss: 0.06251242756843567, data time: 0.029790844236101423\n",
      "step: 681193, loss: 0.0564439557492733, data time: 0.026335924863815308\n",
      "step: 681194, loss: 0.061033379286527634, data time: 0.023594670825534396\n",
      "step: 681195, loss: 0.0598655641078949, data time: 0.021477365493774415\n",
      "step: 681196, loss: 0.06557232141494751, data time: 0.01977008039301092\n",
      "step: 681197, loss: 0.06147537752985954, data time: 0.018331507841746014\n",
      "step: 681198, loss: 0.05769171938300133, data time: 0.01711572133577787\n",
      "step: 681199, loss: 0.06218937039375305, data time: 0.016072613852364675\n",
      "step: 681200, loss: 0.06417503952980042, data time: 0.015167299906412761\n",
      "step: 681201, loss: 0.06349298357963562, data time: 0.014379903674125671\n",
      "step: 681202, loss: 0.06302293390035629, data time: 0.013681986752678366\n",
      "step: 681203, loss: 0.0671718642115593, data time: 0.013056940502590604\n",
      "step: 681204, loss: 0.0620012991130352, data time: 0.012494250347739771\n",
      "step: 681205, loss: 0.06107921525835991, data time: 0.01200120449066162\n",
      "step: 681206, loss: 0.0599152110517025, data time: 0.011548587254115514\n",
      "step: 681207, loss: 0.05828551575541496, data time: 0.011139652945778587\n",
      "step: 681208, loss: 0.05390060693025589, data time: 0.010764536650284477\n",
      "step: 681209, loss: 0.0646454393863678, data time: 0.010405381520589193\n",
      "step: 681210, loss: 0.06277736276388168, data time: 0.010074644088745118\n",
      "step: 681211, loss: 0.05580966919660568, data time: 0.009770558430598332\n",
      "step: 681212, loss: 0.06400096416473389, data time: 0.00948521826002333\n",
      "step: 681213, loss: 0.0598503053188324, data time: 0.009218752384185791\n",
      "step: 681214, loss: 0.06801342964172363, data time: 0.008979780920620623\n",
      "step: 681215, loss: 0.060989540070295334, data time: 0.0087510347366333\n",
      "step: 681216, loss: 0.06126188486814499, data time: 0.008538169245566092\n",
      "step: 681217, loss: 0.058867719024419785, data time: 0.008345961570739746\n",
      "step: 681218, loss: 0.05794175714254379, data time: 0.008155548211300013\n",
      "step: 681219, loss: 0.06501728296279907, data time: 0.007974743843078613\n",
      "step: 681220, loss: 0.05934218689799309, data time: 0.007808692114693778\n",
      "step: 681221, loss: 0.05887831375002861, data time: 0.0076459116405910915\n",
      "step: 681222, loss: 0.05890991538763046, data time: 0.007491897892307591\n",
      "step: 681223, loss: 0.06608535349369049, data time: 0.007350200100948936\n",
      "step: 681224, loss: 0.06909788399934769, data time: 0.007213390790499174\n",
      "step: 681225, loss: 0.057956695556640625, data time: 0.007084566354751587\n",
      "step: 681226, loss: 0.06286928057670593, data time: 0.1764819622039795\n",
      "step: 681227, loss: 0.06009998172521591, data time: 0.08985543251037598\n",
      "step: 681228, loss: 0.06270121037960052, data time: 0.06087470054626465\n",
      "step: 681229, loss: 0.06539849191904068, data time: 0.046382904052734375\n",
      "step: 681230, loss: 0.05675274133682251, data time: 0.037395286560058597\n",
      "step: 681231, loss: 0.06299605965614319, data time: 0.031413515408833824\n",
      "step: 681232, loss: 0.06313228607177734, data time: 0.027141809463500977\n",
      "step: 681233, loss: 0.058254268020391464, data time: 0.024019688367843628\n",
      "step: 681234, loss: 0.0630536675453186, data time: 0.02151510450575087\n",
      "step: 681235, loss: 0.06132536381483078, data time: 0.01959218978881836\n",
      "step: 681236, loss: 0.06330173462629318, data time: 0.01801529797640714\n",
      "step: 681237, loss: 0.06350500881671906, data time: 0.016700963179270428\n",
      "step: 681238, loss: 0.0594673827290535, data time: 0.015586229471059946\n",
      "step: 681239, loss: 0.06589756906032562, data time: 0.014620849064418249\n",
      "step: 681240, loss: 0.06776846945285797, data time: 0.013790639241536458\n",
      "step: 681241, loss: 0.060754042118787766, data time: 0.01306462287902832\n",
      "step: 681242, loss: 0.0579712837934494, data time: 0.012420233558205998\n",
      "step: 681243, loss: 0.05668455362319946, data time: 0.01184017128414578\n",
      "step: 681244, loss: 0.06928876042366028, data time: 0.011329324621903268\n",
      "step: 681245, loss: 0.0627364069223404, data time: 0.010876321792602539\n",
      "step: 681246, loss: 0.05833463370800018, data time: 0.010463498887561616\n",
      "step: 681247, loss: 0.060002487152814865, data time: 0.010088649663058195\n",
      "step: 681248, loss: 0.05541428178548813, data time: 0.009742975234985352\n",
      "step: 681249, loss: 0.06453316658735275, data time: 0.009425808986028036\n",
      "step: 681250, loss: 0.05944058299064636, data time: 0.009133739471435547\n",
      "step: 681251, loss: 0.06423944234848022, data time: 0.008863054789029635\n",
      "step: 681252, loss: 0.06241476535797119, data time: 0.008609021151507343\n",
      "step: 681253, loss: 0.05756660923361778, data time: 0.008381562573569161\n",
      "step: 681254, loss: 0.056176312267780304, data time: 0.008170086762000775\n",
      "step: 681255, loss: 0.06566976010799408, data time: 0.007968537012736003\n",
      "step: 681256, loss: 0.06348913908004761, data time: 0.007781920894499748\n",
      "step: 681257, loss: 0.05798714607954025, data time: 0.0076108649373054504\n",
      "step: 681258, loss: 0.06270915269851685, data time: 0.007443182396166252\n",
      "step: 681259, loss: 0.062298208475112915, data time: 0.007284367785734289\n",
      "step: 681260, loss: 0.05684293061494827, data time: 0.007134226390293666\n",
      "step: 681261, loss: 0.06177029013633728, data time: 0.006998519102732341\n",
      "step: 681262, loss: 0.06174018979072571, data time: 0.006868072458215662\n",
      "step: 681263, loss: 0.061515264213085175, data time: 0.006744165169565301\n",
      "step: 681264, loss: 0.06556618213653564, data time: 0.006627443509224134\n",
      "step: 681265, loss: 0.04887901991605759, data time: 0.006516939401626587\n",
      "step: 681266, loss: 0.059427544474601746, data time: 0.17882943153381348\n",
      "step: 681267, loss: 0.06438250094652176, data time: 0.09052538871765137\n",
      "step: 681268, loss: 0.059809885919094086, data time: 0.06127007802327474\n",
      "step: 681269, loss: 0.0574304461479187, data time: 0.046859145164489746\n",
      "step: 681270, loss: 0.05909023433923721, data time: 0.0377962589263916\n",
      "step: 681271, loss: 0.06104641407728195, data time: 0.031749884287516274\n",
      "step: 681272, loss: 0.06420303881168365, data time: 0.027432680130004883\n",
      "step: 681273, loss: 0.059538692235946655, data time: 0.02426701784133911\n",
      "step: 681274, loss: 0.06382781267166138, data time: 0.021727482477823894\n",
      "step: 681275, loss: 0.061797112226486206, data time: 0.019756507873535157\n",
      "step: 681276, loss: 0.0650048702955246, data time: 0.018163724379106003\n",
      "step: 681277, loss: 0.06785112619400024, data time: 0.016831378142038982\n",
      "step: 681278, loss: 0.06438185274600983, data time: 0.01570551212017353\n",
      "step: 681279, loss: 0.06122405827045441, data time: 0.014730828148978097\n",
      "step: 681280, loss: 0.05975426360964775, data time: 0.013894128799438476\n",
      "step: 681281, loss: 0.059287503361701965, data time: 0.01315772533416748\n",
      "step: 681282, loss: 0.0637858584523201, data time: 0.012534968993243049\n",
      "step: 681283, loss: 0.06368909776210785, data time: 0.011951910124884712\n",
      "step: 681284, loss: 0.0702168345451355, data time: 0.011438457589400442\n",
      "step: 681285, loss: 0.06509609520435333, data time: 0.010974597930908204\n",
      "step: 681286, loss: 0.05948411673307419, data time: 0.010553064800444104\n",
      "step: 681287, loss: 0.05674025043845177, data time: 0.01017242128198797\n",
      "step: 681288, loss: 0.05936770886182785, data time: 0.009820533835369608\n",
      "step: 681289, loss: 0.057892318814992905, data time: 0.00950405995051066\n",
      "step: 681290, loss: 0.05567115172743797, data time: 0.009212007522583008\n",
      "step: 681291, loss: 0.06569251418113708, data time: 0.008941916319040151\n",
      "step: 681292, loss: 0.059913456439971924, data time: 0.008693262382789894\n",
      "step: 681293, loss: 0.06479807198047638, data time: 0.008454339844839913\n",
      "step: 681294, loss: 0.06578287482261658, data time: 0.008240190045586947\n",
      "step: 681295, loss: 0.05923006683588028, data time: 0.008037781715393067\n",
      "step: 681296, loss: 0.06450175493955612, data time: 0.007850746954641035\n",
      "step: 681297, loss: 0.059283897280693054, data time: 0.007677115499973297\n",
      "step: 681298, loss: 0.057411350309848785, data time: 0.007508754730224609\n",
      "step: 681299, loss: 0.06465260684490204, data time: 0.007347534684573903\n",
      "step: 681300, loss: 0.06166502833366394, data time: 0.007196324212210519\n",
      "step: 681301, loss: 0.06545120477676392, data time: 0.007054143481784397\n",
      "step: 681302, loss: 0.05982036516070366, data time: 0.0069153953242946315\n",
      "step: 681303, loss: 0.05910443514585495, data time: 0.006788140849063271\n",
      "step: 681304, loss: 0.06461228430271149, data time: 0.00666661751575959\n",
      "step: 681305, loss: 0.04307399317622185, data time: 0.0065515398979187015\n",
      "step: 681306, loss: 0.056054383516311646, data time: 0.18111300468444824\n",
      "step: 681307, loss: 0.06161734461784363, data time: 0.09192466735839844\n",
      "step: 681308, loss: 0.0637434870004654, data time: 0.06180922190348307\n",
      "step: 681309, loss: 0.0613565668463707, data time: 0.047149598598480225\n",
      "step: 681310, loss: 0.06441104412078857, data time: 0.03803606033325195\n",
      "step: 681311, loss: 0.06510406732559204, data time: 0.031963467597961426\n",
      "step: 681312, loss: 0.06534507125616074, data time: 0.0276162964957101\n",
      "step: 681313, loss: 0.05852349102497101, data time: 0.024434387683868408\n",
      "step: 681314, loss: 0.06660225987434387, data time: 0.021881712807549372\n",
      "step: 681315, loss: 0.06863343715667725, data time: 0.019896388053894043\n",
      "step: 681316, loss: 0.05876116454601288, data time: 0.018284017389470882\n",
      "step: 681317, loss: 0.057633936405181885, data time: 0.01694146792093913\n",
      "step: 681318, loss: 0.060250841081142426, data time: 0.015811681747436523\n",
      "step: 681319, loss: 0.06274066865444183, data time: 0.014832803181239538\n",
      "step: 681320, loss: 0.0601382851600647, data time: 0.013994169235229493\n",
      "step: 681321, loss: 0.0627625435590744, data time: 0.01327398419380188\n",
      "step: 681322, loss: 0.06458233296871185, data time: 0.012641051236320944\n",
      "step: 681323, loss: 0.06788872927427292, data time: 0.012071808179219564\n",
      "step: 681324, loss: 0.06238074228167534, data time: 0.011546737269351357\n",
      "step: 681325, loss: 0.062046729028224945, data time: 0.011080193519592284\n",
      "step: 681326, loss: 0.060614436864852905, data time: 0.0106593200138637\n",
      "step: 681327, loss: 0.05730663985013962, data time: 0.01027700034054843\n",
      "step: 681328, loss: 0.06165768951177597, data time: 0.009921384894329569\n",
      "step: 681329, loss: 0.06054779142141342, data time: 0.009597758452097574\n",
      "step: 681330, loss: 0.06408952176570892, data time: 0.009301528930664063\n",
      "step: 681331, loss: 0.06166898459196091, data time: 0.009030653880192684\n",
      "step: 681332, loss: 0.06199611350893974, data time: 0.008771560810230396\n",
      "step: 681333, loss: 0.06851047277450562, data time: 0.00853524889264788\n",
      "step: 681334, loss: 0.05927664786577225, data time: 0.00831689505741514\n",
      "step: 681335, loss: 0.06445536017417908, data time: 0.008112223943074544\n",
      "step: 681336, loss: 0.05463111028075218, data time: 0.00792151112710276\n",
      "step: 681337, loss: 0.07260113209486008, data time: 0.007752522826194763\n",
      "step: 681338, loss: 0.06533770263195038, data time: 0.007580648769031872\n",
      "step: 681339, loss: 0.06228032335639, data time: 0.007419200504527372\n",
      "step: 681340, loss: 0.06502106785774231, data time: 0.007267427444458008\n",
      "step: 681341, loss: 0.06078963726758957, data time: 0.007118807898627387\n",
      "step: 681342, loss: 0.06242172047495842, data time: 0.006977654792167045\n",
      "step: 681343, loss: 0.05925516411662102, data time: 0.0068482599760356705\n",
      "step: 681344, loss: 0.06597857922315598, data time: 0.006725488564906976\n",
      "step: 681345, loss: 0.08580763638019562, data time: 0.006612646579742432\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40410518646240234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 681346, loss: 0.06336934864521027, data time: 0.18875503540039062\n",
      "step: 681347, loss: 0.06342142075300217, data time: 0.09515202045440674\n",
      "step: 681348, loss: 0.05746377259492874, data time: 0.06395689646402995\n",
      "step: 681349, loss: 0.06221681088209152, data time: 0.048887550830841064\n",
      "step: 681350, loss: 0.05995657294988632, data time: 0.03939309120178223\n",
      "step: 681351, loss: 0.057909123599529266, data time: 0.033073862393697105\n",
      "step: 681352, loss: 0.06638994067907333, data time: 0.028569698333740234\n",
      "step: 681353, loss: 0.06391417235136032, data time: 0.025260448455810547\n",
      "step: 681354, loss: 0.06667548418045044, data time: 0.022600730260213215\n",
      "step: 681355, loss: 0.06090294569730759, data time: 0.020548200607299803\n",
      "step: 681356, loss: 0.06408762186765671, data time: 0.018883011557839134\n",
      "step: 681357, loss: 0.0624331459403038, data time: 0.0174940824508667\n",
      "step: 681358, loss: 0.06046510487794876, data time: 0.016314873328575723\n",
      "step: 681359, loss: 0.061181604862213135, data time: 0.015297208513532366\n",
      "step: 681360, loss: 0.06722916662693024, data time: 0.014417346318562825\n",
      "step: 681361, loss: 0.06517693400382996, data time: 0.013642460107803345\n",
      "step: 681362, loss: 0.05854235216975212, data time: 0.012983560562133789\n",
      "step: 681363, loss: 0.06307334452867508, data time: 0.012371606296963163\n",
      "step: 681364, loss: 0.06955818831920624, data time: 0.011846492165013364\n",
      "step: 681365, loss: 0.05858346074819565, data time: 0.011381959915161133\n",
      "step: 681366, loss: 0.06092096492648125, data time: 0.010963144756498792\n",
      "step: 681367, loss: 0.055466845631599426, data time: 0.010583877563476562\n",
      "step: 681368, loss: 0.0682329535484314, data time: 0.010228841201118801\n",
      "step: 681369, loss: 0.05732157081365585, data time: 0.009906351566314697\n",
      "step: 681370, loss: 0.0577065646648407, data time: 0.009607229232788086\n",
      "step: 681371, loss: 0.06390585750341415, data time: 0.00933538033412053\n",
      "step: 681372, loss: 0.059506773948669434, data time: 0.00907772558706778\n",
      "step: 681373, loss: 0.065328449010849, data time: 0.008840467248644148\n",
      "step: 681374, loss: 0.059186436235904694, data time: 0.008622523011832401\n",
      "step: 681375, loss: 0.06300563365221024, data time: 0.008419068654378255\n",
      "step: 681376, loss: 0.06357047706842422, data time: 0.008230824624338457\n",
      "step: 681377, loss: 0.0598272942006588, data time: 0.008055359125137329\n",
      "step: 681378, loss: 0.06333640217781067, data time: 0.00787585431879217\n",
      "step: 681379, loss: 0.058695197105407715, data time: 0.007703998509575339\n",
      "step: 681380, loss: 0.06852062791585922, data time: 0.007543965748378209\n",
      "step: 681381, loss: 0.06487507373094559, data time: 0.007389095094468858\n",
      "step: 681382, loss: 0.06353676319122314, data time: 0.007244361413491739\n",
      "step: 681383, loss: 0.06031573936343193, data time: 0.007111022346898129\n",
      "step: 681384, loss: 0.06909427791833878, data time: 0.006985236436892779\n",
      "step: 681385, loss: 0.0583229660987854, data time: 0.006865966320037842\n",
      "step: 681386, loss: 0.057610780000686646, data time: 0.18724441528320312\n",
      "step: 681387, loss: 0.06318594515323639, data time: 0.09502005577087402\n",
      "step: 681388, loss: 0.06200886890292168, data time: 0.06466023127237956\n",
      "step: 681389, loss: 0.06752371042966843, data time: 0.048868000507354736\n",
      "step: 681390, loss: 0.06042350456118584, data time: 0.03937630653381348\n",
      "step: 681391, loss: 0.05538085103034973, data time: 0.03306726614634196\n",
      "step: 681392, loss: 0.05872928351163864, data time: 0.028654234749930247\n",
      "step: 681393, loss: 0.059416513890028, data time: 0.02524322271347046\n",
      "step: 681394, loss: 0.062075622379779816, data time: 0.022596571180555556\n",
      "step: 681395, loss: 0.06243354082107544, data time: 0.020534396171569824\n",
      "step: 681396, loss: 0.0639161467552185, data time: 0.018860816955566406\n",
      "step: 681397, loss: 0.0646284967660904, data time: 0.01747792959213257\n",
      "step: 681398, loss: 0.05851215124130249, data time: 0.016304804728581354\n",
      "step: 681399, loss: 0.06049355864524841, data time: 0.015290141105651855\n",
      "step: 681400, loss: 0.060376107692718506, data time: 0.014412975311279297\n",
      "step: 681401, loss: 0.06506749242544174, data time: 0.01364678144454956\n",
      "step: 681402, loss: 0.06504765897989273, data time: 0.012963210835176356\n",
      "step: 681403, loss: 0.06660038232803345, data time: 0.012355473306443956\n",
      "step: 681404, loss: 0.05988531559705734, data time: 0.011810578797992907\n",
      "step: 681405, loss: 0.05649268627166748, data time: 0.011330986022949218\n",
      "step: 681406, loss: 0.05265834927558899, data time: 0.01089341299874442\n",
      "step: 681407, loss: 0.060253120958805084, data time: 0.010493213480169123\n",
      "step: 681408, loss: 0.05999775975942612, data time: 0.010123553483382515\n",
      "step: 681409, loss: 0.05898843705654144, data time: 0.009808897972106934\n",
      "step: 681410, loss: 0.060072895139455795, data time: 0.009503002166748048\n",
      "step: 681411, loss: 0.06321834027767181, data time: 0.009215694207411546\n",
      "step: 681412, loss: 0.05832687020301819, data time: 0.008949200312296549\n",
      "step: 681413, loss: 0.05532646179199219, data time: 0.008701511791774206\n",
      "step: 681414, loss: 0.0563090555369854, data time: 0.008479414315059268\n",
      "step: 681415, loss: 0.06185033544898033, data time: 0.008268133799235026\n",
      "step: 681416, loss: 0.06433907151222229, data time: 0.008070953430668\n",
      "step: 681417, loss: 0.05767526850104332, data time: 0.007892437279224396\n",
      "step: 681418, loss: 0.058582525700330734, data time: 0.00771383083227909\n",
      "step: 681419, loss: 0.06110084801912308, data time: 0.007549201740938074\n",
      "step: 681420, loss: 0.06346859782934189, data time: 0.007387747083391462\n",
      "step: 681421, loss: 0.060244448482990265, data time: 0.007235209147135417\n",
      "step: 681422, loss: 0.06297695636749268, data time: 0.007092849628345386\n",
      "step: 681423, loss: 0.059307850897312164, data time: 0.0069607684486790704\n",
      "step: 681424, loss: 0.06512196362018585, data time: 0.00683445196885329\n",
      "step: 681425, loss: 0.07863111048936844, data time: 0.006713676452636719\n",
      "step: 681426, loss: 0.061079710721969604, data time: 0.20429277420043945\n",
      "step: 681427, loss: 0.06109839305281639, data time: 0.10290944576263428\n",
      "step: 681428, loss: 0.06377843022346497, data time: 0.06950235366821289\n",
      "step: 681429, loss: 0.06776683777570724, data time: 0.05288594961166382\n",
      "step: 681430, loss: 0.061123304069042206, data time: 0.04258627891540527\n",
      "step: 681431, loss: 0.0648665651679039, data time: 0.035753448804219566\n",
      "step: 681432, loss: 0.0678144320845604, data time: 0.03089918409075056\n",
      "step: 681433, loss: 0.06145866960287094, data time: 0.027340292930603027\n",
      "step: 681434, loss: 0.06504274904727936, data time: 0.024484740363226995\n",
      "step: 681435, loss: 0.06261315196752548, data time: 0.02227284908294678\n",
      "step: 681436, loss: 0.059399768710136414, data time: 0.020476232875477184\n",
      "step: 681437, loss: 0.06384891271591187, data time: 0.018980979919433594\n",
      "step: 681438, loss: 0.059560827910900116, data time: 0.017716389435988206\n",
      "step: 681439, loss: 0.062299929559230804, data time: 0.01662344591958182\n",
      "step: 681440, loss: 0.05750395357608795, data time: 0.015678040186564126\n",
      "step: 681441, loss: 0.06473162770271301, data time: 0.01483374834060669\n",
      "step: 681442, loss: 0.06302444636821747, data time: 0.014084886102115406\n",
      "step: 681443, loss: 0.06398431956768036, data time: 0.013415137926737467\n",
      "step: 681444, loss: 0.059924885630607605, data time: 0.01282005561025519\n",
      "step: 681445, loss: 0.057464323937892914, data time: 0.012288081645965575\n",
      "step: 681446, loss: 0.07104633003473282, data time: 0.011806067966279529\n",
      "step: 681447, loss: 0.0547679178416729, data time: 0.011365998875011097\n",
      "step: 681448, loss: 0.06457069516181946, data time: 0.010961159415867018\n",
      "step: 681449, loss: 0.06045933812856674, data time: 0.010595003763834635\n",
      "step: 681450, loss: 0.0646197497844696, data time: 0.010257835388183595\n",
      "step: 681451, loss: 0.06296303868293762, data time: 0.009956855040330153\n",
      "step: 681452, loss: 0.05629609152674675, data time: 0.009676668379041884\n",
      "step: 681453, loss: 0.06478369235992432, data time: 0.009415600981031145\n",
      "step: 681454, loss: 0.06062141805887222, data time: 0.009180702012160728\n",
      "step: 681455, loss: 0.05944910645484924, data time: 0.008959404627482096\n",
      "step: 681456, loss: 0.06143776699900627, data time: 0.008752276820521201\n",
      "step: 681457, loss: 0.0617198571562767, data time: 0.008560039103031158\n",
      "step: 681458, loss: 0.06476011127233505, data time: 0.008365096467914003\n",
      "step: 681459, loss: 0.06398472189903259, data time: 0.008181312504936667\n",
      "step: 681460, loss: 0.06673319637775421, data time: 0.008007029124668667\n",
      "step: 681461, loss: 0.06188802048563957, data time: 0.007840885056389702\n",
      "step: 681462, loss: 0.0580061674118042, data time: 0.00768394727964659\n",
      "step: 681463, loss: 0.05840925872325897, data time: 0.0075394040659854285\n",
      "step: 681464, loss: 0.06038367748260498, data time: 0.007401435803144406\n",
      "step: 681465, loss: 0.03835885971784592, data time: 0.007271116971969605\n",
      "step: 681466, loss: 0.06401556730270386, data time: 0.1832871437072754\n",
      "step: 681467, loss: 0.05602700635790825, data time: 0.09240865707397461\n",
      "step: 681468, loss: 0.06820818781852722, data time: 0.0623775323232015\n",
      "step: 681469, loss: 0.06151266023516655, data time: 0.04765200614929199\n",
      "step: 681470, loss: 0.06500030308961868, data time: 0.038401460647583006\n",
      "step: 681471, loss: 0.06701742857694626, data time: 0.03227412700653076\n",
      "step: 681472, loss: 0.060302503407001495, data time: 0.02788077081952776\n",
      "step: 681473, loss: 0.06935805082321167, data time: 0.024654299020767212\n",
      "step: 681474, loss: 0.060909491032361984, data time: 0.02206661966111925\n",
      "step: 681475, loss: 0.05738306790590286, data time: 0.02006485462188721\n",
      "step: 681476, loss: 0.06010543555021286, data time: 0.01843749393116344\n",
      "step: 681477, loss: 0.059530988335609436, data time: 0.017082075277964275\n",
      "step: 681478, loss: 0.06659405678510666, data time: 0.015939052288348857\n",
      "step: 681479, loss: 0.05742626637220383, data time: 0.014946596963065011\n",
      "step: 681480, loss: 0.059312693774700165, data time: 0.01409163475036621\n",
      "step: 681481, loss: 0.05361339449882507, data time: 0.013345807790756226\n",
      "step: 681482, loss: 0.05884357541799545, data time: 0.012682031182681812\n",
      "step: 681483, loss: 0.06794086843729019, data time: 0.012087835205925835\n",
      "step: 681484, loss: 0.05660919100046158, data time: 0.011558733488384047\n",
      "step: 681485, loss: 0.06223210692405701, data time: 0.011092841625213623\n",
      "step: 681486, loss: 0.06579612195491791, data time: 0.010668697811308362\n",
      "step: 681487, loss: 0.0703878402709961, data time: 0.010283242572437633\n",
      "step: 681488, loss: 0.06326848268508911, data time: 0.009923219680786133\n",
      "step: 681489, loss: 0.060869328677654266, data time: 0.00960002342859904\n",
      "step: 681490, loss: 0.06183107942342758, data time: 0.009300069808959961\n",
      "step: 681491, loss: 0.06283017247915268, data time: 0.009022841086754432\n",
      "step: 681492, loss: 0.06106403470039368, data time: 0.00876291592915853\n",
      "step: 681493, loss: 0.06832493096590042, data time: 0.008524443422045027\n",
      "step: 681494, loss: 0.06172892451286316, data time: 0.008304267094053071\n",
      "step: 681495, loss: 0.06496883928775787, data time: 0.008099842071533202\n",
      "step: 681496, loss: 0.0626140683889389, data time: 0.00790809815929782\n",
      "step: 681497, loss: 0.05963299423456192, data time: 0.007733307778835297\n",
      "step: 681498, loss: 0.06640999764204025, data time: 0.007564775871508049\n",
      "step: 681499, loss: 0.06860359758138657, data time: 0.007403948727776022\n",
      "step: 681500, loss: 0.060817014425992966, data time: 0.007249764033726284\n",
      "step: 681501, loss: 0.06853847205638885, data time: 0.007101317246754964\n",
      "step: 681502, loss: 0.056896910071372986, data time: 0.0069607528480323585\n",
      "step: 681503, loss: 0.0655730813741684, data time: 0.006831150305898566\n",
      "step: 681504, loss: 0.06352236866950989, data time: 0.006711446321927584\n",
      "step: 681505, loss: 0.04299924522638321, data time: 0.006597137451171875\n",
      "step: 681506, loss: 0.057297419756650925, data time: 0.17600655555725098\n",
      "step: 681507, loss: 0.07019361853599548, data time: 0.08917546272277832\n",
      "step: 681508, loss: 0.05991087481379509, data time: 0.060613791147867836\n",
      "step: 681509, loss: 0.06012926250696182, data time: 0.04612553119659424\n",
      "step: 681510, loss: 0.061009131371974945, data time: 0.03718276023864746\n",
      "step: 681511, loss: 0.06308875977993011, data time: 0.03123907248179118\n",
      "step: 681512, loss: 0.06624837219715118, data time: 0.02699869019644601\n",
      "step: 681513, loss: 0.06111321970820427, data time: 0.02389085292816162\n",
      "step: 681514, loss: 0.06278909742832184, data time: 0.021386888292100694\n",
      "step: 681515, loss: 0.06305187940597534, data time: 0.019449615478515626\n",
      "step: 681516, loss: 0.05738136172294617, data time: 0.01787634329362349\n",
      "step: 681517, loss: 0.06879251450300217, data time: 0.016568501790364582\n",
      "step: 681518, loss: 0.056490492075681686, data time: 0.01546617654653696\n",
      "step: 681519, loss: 0.06101510673761368, data time: 0.014508843421936035\n",
      "step: 681520, loss: 0.06224466860294342, data time: 0.013684479395548503\n",
      "step: 681521, loss: 0.06318299472332001, data time: 0.012961208820343018\n",
      "step: 681522, loss: 0.0653141662478447, data time: 0.012324683806475471\n",
      "step: 681523, loss: 0.06704190373420715, data time: 0.011750035815768771\n",
      "step: 681524, loss: 0.06615278124809265, data time: 0.011239992944817794\n",
      "step: 681525, loss: 0.06678755581378937, data time: 0.010788226127624511\n",
      "step: 681526, loss: 0.052447788417339325, data time: 0.01038383302234468\n",
      "step: 681527, loss: 0.06413063406944275, data time: 0.010010740973732689\n",
      "step: 681528, loss: 0.06286084651947021, data time: 0.009661881820015285\n",
      "step: 681529, loss: 0.0654979795217514, data time: 0.009347259998321533\n",
      "step: 681530, loss: 0.06117856130003929, data time: 0.009063053131103515\n",
      "step: 681531, loss: 0.06332816928625107, data time: 0.008798700112562913\n",
      "step: 681532, loss: 0.056775063276290894, data time: 0.008547049981576425\n",
      "step: 681533, loss: 0.05928042158484459, data time: 0.008314941610608782\n",
      "step: 681534, loss: 0.06279032677412033, data time: 0.00810818836606782\n",
      "step: 681535, loss: 0.0634220540523529, data time: 0.007908837000528971\n",
      "step: 681536, loss: 0.06285355240106583, data time: 0.007733921850881269\n",
      "step: 681537, loss: 0.06600411236286163, data time: 0.007562749087810516\n",
      "step: 681538, loss: 0.0625135600566864, data time: 0.0073938658743193655\n",
      "step: 681539, loss: 0.06099718064069748, data time: 0.007235590149374569\n",
      "step: 681540, loss: 0.06606917083263397, data time: 0.007085193906511579\n",
      "step: 681541, loss: 0.056287042796611786, data time: 0.006944947772555881\n",
      "step: 681542, loss: 0.06122838705778122, data time: 0.006808841550672376\n",
      "step: 681543, loss: 0.06451821327209473, data time: 0.006684817765888415\n",
      "step: 681544, loss: 0.060767024755477905, data time: 0.006564678289951422\n",
      "step: 681545, loss: 0.049546852707862854, data time: 0.00645056962966919\n",
      "step: 681546, loss: 0.06662417948246002, data time: 0.19362211227416992\n",
      "step: 681547, loss: 0.06200144439935684, data time: 0.09761106967926025\n",
      "step: 681548, loss: 0.05538545548915863, data time: 0.06596573193868001\n",
      "step: 681549, loss: 0.06762862205505371, data time: 0.05022484064102173\n",
      "step: 681550, loss: 0.062497012317180634, data time: 0.040482091903686526\n",
      "step: 681551, loss: 0.06287398934364319, data time: 0.0340034564336141\n",
      "step: 681552, loss: 0.06748735159635544, data time: 0.029368434633527483\n",
      "step: 681553, loss: 0.0596877858042717, data time: 0.02595856785774231\n",
      "step: 681554, loss: 0.059979554265737534, data time: 0.023230552673339844\n",
      "step: 681555, loss: 0.06297820806503296, data time: 0.021111226081848143\n",
      "step: 681556, loss: 0.06500770896673203, data time: 0.019388155503706497\n",
      "step: 681557, loss: 0.05699033662676811, data time: 0.017985204855600994\n",
      "step: 681558, loss: 0.06148166581988335, data time: 0.01679264582120455\n",
      "step: 681559, loss: 0.06024457514286041, data time: 0.01576653548649379\n",
      "step: 681560, loss: 0.06233854219317436, data time: 0.014885441462198893\n",
      "step: 681561, loss: 0.06224016100168228, data time: 0.014112010598182678\n",
      "step: 681562, loss: 0.06100190803408623, data time: 0.013428141089046703\n",
      "step: 681563, loss: 0.06140294671058655, data time: 0.0128175417582194\n",
      "step: 681564, loss: 0.06523523479700089, data time: 0.012269170660721628\n",
      "step: 681565, loss: 0.060879193246364594, data time: 0.011767172813415527\n",
      "step: 681566, loss: 0.0606258362531662, data time: 0.011308306739443824\n",
      "step: 681567, loss: 0.06138532981276512, data time: 0.010889215902848677\n",
      "step: 681568, loss: 0.06623142212629318, data time: 0.01050399697345236\n",
      "step: 681569, loss: 0.06258214265108109, data time: 0.010157605012257894\n",
      "step: 681570, loss: 0.06816117465496063, data time: 0.009837017059326172\n",
      "step: 681571, loss: 0.062036752700805664, data time: 0.00953846711378831\n",
      "step: 681572, loss: 0.06902892887592316, data time: 0.009268222031769928\n",
      "step: 681573, loss: 0.06494537740945816, data time: 0.009009020669119698\n",
      "step: 681574, loss: 0.061128612607717514, data time: 0.008774403868050411\n",
      "step: 681575, loss: 0.05211683362722397, data time: 0.008554832140604655\n",
      "step: 681576, loss: 0.060089580714702606, data time: 0.008347442073206748\n",
      "step: 681577, loss: 0.06910081952810287, data time: 0.008156254887580872\n",
      "step: 681578, loss: 0.05348711833357811, data time: 0.00797005133195357\n",
      "step: 681579, loss: 0.05965489149093628, data time: 0.007794352138743681\n",
      "step: 681580, loss: 0.06414102017879486, data time: 0.00762824331011091\n",
      "step: 681581, loss: 0.06156369298696518, data time: 0.007470753457811143\n",
      "step: 681582, loss: 0.0691710114479065, data time: 0.0073248373495565875\n",
      "step: 681583, loss: 0.06838946044445038, data time: 0.007184806622956928\n",
      "step: 681584, loss: 0.06084543466567993, data time: 0.007054885228474935\n",
      "step: 681585, loss: 0.06703869998455048, data time: 0.006928747892379761\n",
      "step: 681586, loss: 0.06154155358672142, data time: 0.19431805610656738\n",
      "step: 681587, loss: 0.06364622712135315, data time: 0.09904003143310547\n",
      "step: 681588, loss: 0.06837816536426544, data time: 0.06663131713867188\n",
      "step: 681589, loss: 0.06100800260901451, data time: 0.05101931095123291\n",
      "step: 681590, loss: 0.0627887025475502, data time: 0.04116201400756836\n",
      "step: 681591, loss: 0.058662742376327515, data time: 0.03460375467936198\n",
      "step: 681592, loss: 0.060752641409635544, data time: 0.02990903173174177\n",
      "step: 681593, loss: 0.057667993009090424, data time: 0.02648302912712097\n",
      "step: 681594, loss: 0.06119733676314354, data time: 0.023722489674886067\n",
      "step: 681595, loss: 0.061333052814006805, data time: 0.0215970516204834\n",
      "step: 681596, loss: 0.06307272613048553, data time: 0.019863757220181553\n",
      "step: 681597, loss: 0.059504203498363495, data time: 0.018423179785410564\n",
      "step: 681598, loss: 0.06216577813029289, data time: 0.017203386013324443\n",
      "step: 681599, loss: 0.0629909336566925, data time: 0.016148958887372698\n",
      "step: 681600, loss: 0.06431518495082855, data time: 0.015237919489542643\n",
      "step: 681601, loss: 0.056474603712558746, data time: 0.01444767415523529\n",
      "step: 681602, loss: 0.05904342979192734, data time: 0.013739684048820944\n",
      "step: 681603, loss: 0.059669069945812225, data time: 0.013111445638868544\n",
      "step: 681604, loss: 0.056639280170202255, data time: 0.012548170591655531\n",
      "step: 681605, loss: 0.06499352306127548, data time: 0.012048566341400146\n",
      "step: 681606, loss: 0.06443711370229721, data time: 0.011601720537458147\n",
      "step: 681607, loss: 0.06282924860715866, data time: 0.011190576986833052\n",
      "step: 681608, loss: 0.0608443021774292, data time: 0.01081097644308339\n",
      "step: 681609, loss: 0.05671065300703049, data time: 0.010464797417322794\n",
      "step: 681610, loss: 0.05856039375066757, data time: 0.010148029327392578\n",
      "step: 681611, loss: 0.06097414344549179, data time: 0.00985413331251878\n",
      "step: 681612, loss: 0.05692952871322632, data time: 0.009579888096562138\n",
      "step: 681613, loss: 0.06005951762199402, data time: 0.009310500962393624\n",
      "step: 681614, loss: 0.06144466996192932, data time: 0.009064847025377997\n",
      "step: 681615, loss: 0.07012957334518433, data time: 0.008833018938700359\n",
      "step: 681616, loss: 0.05919090658426285, data time: 0.008617516486875473\n",
      "step: 681617, loss: 0.05811909958720207, data time: 0.008419238030910492\n",
      "step: 681618, loss: 0.06204668805003166, data time: 0.008226380203709457\n",
      "step: 681619, loss: 0.0570702999830246, data time: 0.008044081575730267\n",
      "step: 681620, loss: 0.06262326240539551, data time: 0.007873126438685827\n",
      "step: 681621, loss: 0.0595732145011425, data time: 0.0077068474557664655\n",
      "step: 681622, loss: 0.05933825671672821, data time: 0.007551277005994642\n",
      "step: 681623, loss: 0.06659625470638275, data time: 0.007408788329676578\n",
      "step: 681624, loss: 0.06427250802516937, data time: 0.007272842602852063\n",
      "step: 681625, loss: 0.08656337112188339, data time: 0.0071408629417419435\n",
      "step: 681626, loss: 0.05847465991973877, data time: 0.18709516525268555\n",
      "step: 681627, loss: 0.05903308838605881, data time: 0.09490251541137695\n",
      "step: 681628, loss: 0.061717502772808075, data time: 0.06379230817159016\n",
      "step: 681629, loss: 0.059751998633146286, data time: 0.048624396324157715\n",
      "step: 681630, loss: 0.060120292007923126, data time: 0.03919658660888672\n",
      "step: 681631, loss: 0.06268542259931564, data time: 0.03292834758758545\n",
      "step: 681632, loss: 0.058719098567962646, data time: 0.028450625283377513\n",
      "step: 681633, loss: 0.0646328553557396, data time: 0.025160640478134155\n",
      "step: 681634, loss: 0.0635107085108757, data time: 0.02251924408806695\n",
      "step: 681635, loss: 0.07076521962881088, data time: 0.020474052429199217\n",
      "step: 681636, loss: 0.06110149621963501, data time: 0.01880667426369407\n",
      "step: 681637, loss: 0.05799856036901474, data time: 0.017416695753733318\n",
      "step: 681638, loss: 0.061963342130184174, data time: 0.016239661436814528\n",
      "step: 681639, loss: 0.061187755316495895, data time: 0.015232477869306291\n",
      "step: 681640, loss: 0.06619179248809814, data time: 0.014358329772949218\n",
      "step: 681641, loss: 0.06386060267686844, data time: 0.01359635591506958\n",
      "step: 681642, loss: 0.06058529019355774, data time: 0.012929818209479837\n",
      "step: 681643, loss: 0.06415362656116486, data time: 0.012323684162563749\n",
      "step: 681644, loss: 0.06062805652618408, data time: 0.011781353699533563\n",
      "step: 681645, loss: 0.06358909606933594, data time: 0.011297941207885742\n",
      "step: 681646, loss: 0.054261885583400726, data time: 0.010862475349789574\n",
      "step: 681647, loss: 0.06535841524600983, data time: 0.010468212040987883\n",
      "step: 681648, loss: 0.056720953434705734, data time: 0.010101847026659094\n",
      "step: 681649, loss: 0.06399693340063095, data time: 0.00977054238319397\n",
      "step: 681650, loss: 0.05689237639307976, data time: 0.009464950561523437\n",
      "step: 681651, loss: 0.05814304202795029, data time: 0.009183526039123535\n",
      "step: 681652, loss: 0.06147855520248413, data time: 0.008916413342511212\n",
      "step: 681653, loss: 0.0695372223854065, data time: 0.008672603539058141\n",
      "step: 681654, loss: 0.06370129436254501, data time: 0.008453402025946256\n",
      "step: 681655, loss: 0.059388212859630585, data time: 0.008246374130249024\n",
      "step: 681656, loss: 0.05991103872656822, data time: 0.008047234627508348\n",
      "step: 681657, loss: 0.06183250993490219, data time: 0.007868953049182892\n",
      "step: 681658, loss: 0.05861485004425049, data time: 0.007692315361716531\n",
      "step: 681659, loss: 0.05978360027074814, data time: 0.007525549215428969\n",
      "step: 681660, loss: 0.051522403955459595, data time: 0.007373060498918805\n",
      "step: 681661, loss: 0.06091151759028435, data time: 0.007226182354821099\n",
      "step: 681662, loss: 0.06396820396184921, data time: 0.007086947157576277\n",
      "step: 681663, loss: 0.06324923038482666, data time: 0.0069664277528461655\n",
      "step: 681664, loss: 0.06892883777618408, data time: 0.006844129317846053\n",
      "step: 681665, loss: 0.03969744220376015, data time: 0.006730389595031738\n",
      "step: 681666, loss: 0.06533143669366837, data time: 0.19781994819641113\n",
      "step: 681667, loss: 0.05524707958102226, data time: 0.0996941328048706\n",
      "step: 681668, loss: 0.06552265584468842, data time: 0.06749796867370605\n",
      "step: 681669, loss: 0.06077314168214798, data time: 0.051383793354034424\n",
      "step: 681670, loss: 0.06350143253803253, data time: 0.041388082504272464\n",
      "step: 681671, loss: 0.06518742442131042, data time: 0.03474116325378418\n",
      "step: 681672, loss: 0.064375139772892, data time: 0.030010155269077847\n",
      "step: 681673, loss: 0.06537917256355286, data time: 0.026531964540481567\n",
      "step: 681674, loss: 0.06824475526809692, data time: 0.023739920722113714\n",
      "step: 681675, loss: 0.06357245147228241, data time: 0.021573877334594725\n",
      "step: 681676, loss: 0.05655653402209282, data time: 0.019811261783946644\n",
      "step: 681677, loss: 0.06065351888537407, data time: 0.01836840311686198\n",
      "step: 681678, loss: 0.06630011647939682, data time: 0.017150328709528997\n",
      "step: 681679, loss: 0.05689927935600281, data time: 0.01609480381011963\n",
      "step: 681680, loss: 0.060914646834135056, data time: 0.015183051427205404\n",
      "step: 681681, loss: 0.056793153285980225, data time: 0.014396131038665771\n",
      "step: 681682, loss: 0.06019042059779167, data time: 0.013693711336921243\n",
      "step: 681683, loss: 0.05562721937894821, data time: 0.013067987230088975\n",
      "step: 681684, loss: 0.05838844180107117, data time: 0.012506020696539628\n",
      "step: 681685, loss: 0.05980398505926132, data time: 0.012006664276123047\n",
      "step: 681686, loss: 0.06352288275957108, data time: 0.011551709402175177\n",
      "step: 681687, loss: 0.05876901000738144, data time: 0.011139988899230957\n",
      "step: 681688, loss: 0.06577348709106445, data time: 0.010761945143989895\n",
      "step: 681689, loss: 0.06249023601412773, data time: 0.01042032241821289\n",
      "step: 681690, loss: 0.0644000992178917, data time: 0.010103254318237305\n",
      "step: 681691, loss: 0.06668601930141449, data time: 0.009810621921832744\n",
      "step: 681692, loss: 0.06579034775495529, data time: 0.009535904283876772\n",
      "step: 681693, loss: 0.05878780409693718, data time: 0.00928255489894322\n",
      "step: 681694, loss: 0.05969482660293579, data time: 0.00905092009182634\n",
      "step: 681695, loss: 0.05914853513240814, data time: 0.008836166063944498\n",
      "step: 681696, loss: 0.062160372734069824, data time: 0.008633951986989668\n",
      "step: 681697, loss: 0.06334929168224335, data time: 0.008444100618362427\n",
      "step: 681698, loss: 0.05940410494804382, data time: 0.008254990433201645\n",
      "step: 681699, loss: 0.06330844014883041, data time: 0.00807509702794692\n",
      "step: 681700, loss: 0.055417463183403015, data time: 0.007906641278948103\n",
      "step: 681701, loss: 0.06234530359506607, data time: 0.007743815581003825\n",
      "step: 681702, loss: 0.0675927996635437, data time: 0.0075897912721376165\n",
      "step: 681703, loss: 0.057718198746442795, data time: 0.007447845057437294\n",
      "step: 681704, loss: 0.06421595811843872, data time: 0.007312212234888322\n",
      "step: 681705, loss: 0.06134774908423424, data time: 0.007183778285980225\n",
      "step: 681706, loss: 0.06602640450000763, data time: 0.18993520736694336\n",
      "step: 681707, loss: 0.07257129997015, data time: 0.09573996067047119\n",
      "step: 681708, loss: 0.062182746827602386, data time: 0.06436975797017415\n",
      "step: 681709, loss: 0.05572938546538353, data time: 0.04924517869949341\n",
      "step: 681710, loss: 0.06571429967880249, data time: 0.039684295654296875\n",
      "step: 681711, loss: 0.052589453756809235, data time: 0.033342440923055015\n",
      "step: 681712, loss: 0.061018817126750946, data time: 0.02881424767630441\n",
      "step: 681713, loss: 0.06241876631975174, data time: 0.02549445629119873\n",
      "step: 681714, loss: 0.05931578576564789, data time: 0.022814247343275283\n",
      "step: 681715, loss: 0.058552972972393036, data time: 0.020736837387084962\n",
      "step: 681716, loss: 0.06507998704910278, data time: 0.01904739033092152\n",
      "step: 681717, loss: 0.06435363739728928, data time: 0.01764039198557536\n",
      "step: 681718, loss: 0.06481718271970749, data time: 0.016454678315382738\n",
      "step: 681719, loss: 0.057584263384342194, data time: 0.015424728393554688\n",
      "step: 681720, loss: 0.060395680367946625, data time: 0.014546044667561849\n",
      "step: 681721, loss: 0.059770092368125916, data time: 0.013771295547485352\n",
      "step: 681722, loss: 0.05768661946058273, data time: 0.013091914794024299\n",
      "step: 681723, loss: 0.0556948184967041, data time: 0.012481252352396647\n",
      "step: 681724, loss: 0.05521136149764061, data time: 0.011941671371459961\n",
      "step: 681725, loss: 0.059385400265455246, data time: 0.01145395040512085\n",
      "step: 681726, loss: 0.06144323945045471, data time: 0.011012224923996698\n",
      "step: 681727, loss: 0.06218666583299637, data time: 0.010609225793318315\n",
      "step: 681728, loss: 0.06383576989173889, data time: 0.010239010271818741\n",
      "step: 681729, loss: 0.05611862987279892, data time: 0.009905765453974405\n",
      "step: 681730, loss: 0.05723947286605835, data time: 0.009598722457885742\n",
      "step: 681731, loss: 0.06064724922180176, data time: 0.00932916311117319\n",
      "step: 681732, loss: 0.05739506706595421, data time: 0.009071615007188585\n",
      "step: 681733, loss: 0.0534769669175148, data time: 0.008833348751068115\n",
      "step: 681734, loss: 0.06033971160650253, data time: 0.008615641758359712\n",
      "step: 681735, loss: 0.06123451888561249, data time: 0.008413553237915039\n",
      "step: 681736, loss: 0.061293698847293854, data time: 0.008225494815457252\n",
      "step: 681737, loss: 0.06270639598369598, data time: 0.008051574230194092\n",
      "step: 681738, loss: 0.06076234579086304, data time: 0.007873412334557735\n",
      "step: 681739, loss: 0.05964893102645874, data time: 0.007705071393181296\n",
      "step: 681740, loss: 0.06065946817398071, data time: 0.007546642848423549\n",
      "step: 681741, loss: 0.05959617346525192, data time: 0.007394260830349392\n",
      "step: 681742, loss: 0.06716638803482056, data time: 0.007250424977895376\n",
      "step: 681743, loss: 0.056477636098861694, data time: 0.007116531070910002\n",
      "step: 681744, loss: 0.06347890198230743, data time: 0.006988928868220403\n",
      "step: 681745, loss: 0.07001497596502304, data time: 0.0068681597709655765\n",
      "step: 681746, loss: 0.06348370015621185, data time: 0.18850135803222656\n",
      "step: 681747, loss: 0.0715978667140007, data time: 0.09501445293426514\n",
      "step: 681748, loss: 0.06343266367912292, data time: 0.06386995315551758\n",
      "step: 681749, loss: 0.06254054605960846, data time: 0.048664629459381104\n",
      "step: 681750, loss: 0.06081674247980118, data time: 0.03921985626220703\n",
      "step: 681751, loss: 0.064930260181427, data time: 0.03293999036153158\n",
      "step: 681752, loss: 0.06634204089641571, data time: 0.028446197509765625\n",
      "step: 681753, loss: 0.06803669035434723, data time: 0.02515426278114319\n",
      "step: 681754, loss: 0.058767154812812805, data time: 0.022515535354614258\n",
      "step: 681755, loss: 0.06214423477649689, data time: 0.020479297637939452\n",
      "step: 681756, loss: 0.06700937449932098, data time: 0.018819462169300426\n",
      "step: 681757, loss: 0.06149548292160034, data time: 0.0174297293027242\n",
      "step: 681758, loss: 0.06436517834663391, data time: 0.016251619045551006\n",
      "step: 681759, loss: 0.06438009440898895, data time: 0.01523639474596296\n",
      "step: 681760, loss: 0.0658031553030014, data time: 0.014361190795898437\n",
      "step: 681761, loss: 0.05784357711672783, data time: 0.013600632548332214\n",
      "step: 681762, loss: 0.06813296675682068, data time: 0.012933324365054859\n",
      "step: 681763, loss: 0.058518439531326294, data time: 0.012327750523885092\n",
      "step: 681764, loss: 0.054321207106113434, data time: 0.011785469557109633\n",
      "step: 681765, loss: 0.05699319764971733, data time: 0.011304700374603271\n",
      "step: 681766, loss: 0.06121274456381798, data time: 0.01086977549961635\n",
      "step: 681767, loss: 0.06325529515743256, data time: 0.010475180365822533\n",
      "step: 681768, loss: 0.06054721400141716, data time: 0.010109206904535708\n",
      "step: 681769, loss: 0.06690391153097153, data time: 0.009775668382644653\n",
      "step: 681770, loss: 0.056092508137226105, data time: 0.009470529556274414\n",
      "step: 681771, loss: 0.061265237629413605, data time: 0.009190632746769832\n",
      "step: 681772, loss: 0.059508323669433594, data time: 0.008926700662683558\n",
      "step: 681773, loss: 0.06058761477470398, data time: 0.008680386202675956\n",
      "step: 681774, loss: 0.0698690116405487, data time: 0.008459831106251684\n",
      "step: 681775, loss: 0.05937379226088524, data time: 0.008249592781066895\n",
      "step: 681776, loss: 0.05868203192949295, data time: 0.00805125697966545\n",
      "step: 681777, loss: 0.06145777553319931, data time: 0.0078696608543396\n",
      "step: 681778, loss: 0.06373093277215958, data time: 0.007692900570956143\n",
      "step: 681779, loss: 0.06152322515845299, data time: 0.0075273654040168315\n",
      "step: 681780, loss: 0.059598080813884735, data time: 0.007373823438371931\n",
      "step: 681781, loss: 0.06317136436700821, data time: 0.007226579719119602\n",
      "step: 681782, loss: 0.06451599299907684, data time: 0.007084453428113782\n",
      "step: 681783, loss: 0.06231648474931717, data time: 0.006951934412906044\n",
      "step: 681784, loss: 0.06038491055369377, data time: 0.006825826106927333\n",
      "step: 681785, loss: 0.05760851502418518, data time: 0.006704998016357422\n",
      "step: 681786, loss: 0.06687195599079132, data time: 0.19435667991638184\n",
      "step: 681787, loss: 0.05869424343109131, data time: 0.09836065769195557\n",
      "step: 681788, loss: 0.057928264141082764, data time: 0.06687672932942708\n",
      "step: 681789, loss: 0.066007599234581, data time: 0.050820231437683105\n",
      "step: 681790, loss: 0.061473481357097626, data time: 0.04096174240112305\n",
      "step: 681791, loss: 0.06579969823360443, data time: 0.034382383028666176\n",
      "step: 681792, loss: 0.06577125191688538, data time: 0.02968897138323103\n",
      "step: 681793, loss: 0.0594894103705883, data time: 0.026247352361679077\n",
      "step: 681794, loss: 0.06108894944190979, data time: 0.023492919074164495\n",
      "step: 681795, loss: 0.06485198438167572, data time: 0.021347880363464355\n",
      "step: 681796, loss: 0.06683486700057983, data time: 0.019616452130404385\n",
      "step: 681797, loss: 0.06246791034936905, data time: 0.01816105842590332\n",
      "step: 681798, loss: 0.0591377429664135, data time: 0.016932597527137168\n",
      "step: 681799, loss: 0.06881646811962128, data time: 0.015876872198922292\n",
      "step: 681800, loss: 0.0739111676812172, data time: 0.014961179097493489\n",
      "step: 681801, loss: 0.060607582330703735, data time: 0.014163672924041748\n",
      "step: 681802, loss: 0.06038395315408707, data time: 0.01345352565540987\n",
      "step: 681803, loss: 0.06112157553434372, data time: 0.012821475664774576\n",
      "step: 681804, loss: 0.06243610382080078, data time: 0.012261127170763518\n",
      "step: 681805, loss: 0.0630648136138916, data time: 0.011756932735443116\n",
      "step: 681806, loss: 0.06668218970298767, data time: 0.011302471160888672\n",
      "step: 681807, loss: 0.05588933080434799, data time: 0.010890267112038353\n",
      "step: 681808, loss: 0.057334285229444504, data time: 0.010509542796922766\n",
      "step: 681809, loss: 0.05898497626185417, data time: 0.010159621636072794\n",
      "step: 681810, loss: 0.06327284872531891, data time: 0.009838533401489259\n",
      "step: 681811, loss: 0.06285078823566437, data time: 0.009545179513784556\n",
      "step: 681812, loss: 0.06083093211054802, data time: 0.009266623744258174\n",
      "step: 681813, loss: 0.05736720189452171, data time: 0.009007487978254045\n",
      "step: 681814, loss: 0.0696578398346901, data time: 0.008770030120323444\n",
      "step: 681815, loss: 0.0617908239364624, data time: 0.0085508664449056\n",
      "step: 681816, loss: 0.059498272836208344, data time: 0.008344980978196668\n",
      "step: 681817, loss: 0.06373033672571182, data time: 0.008157685399055481\n",
      "step: 681818, loss: 0.06331569701433182, data time: 0.007972753409183386\n",
      "step: 681819, loss: 0.06415922194719315, data time: 0.007798896116368911\n",
      "step: 681820, loss: 0.061594195663928986, data time: 0.007633420399257115\n",
      "step: 681821, loss: 0.05986874923110008, data time: 0.0074751244650946725\n",
      "step: 681822, loss: 0.062415845692157745, data time: 0.007325829686345281\n",
      "step: 681823, loss: 0.06290896236896515, data time: 0.007187284921344958\n",
      "step: 681824, loss: 0.06412970274686813, data time: 0.007056309626652644\n",
      "step: 681825, loss: 0.06101660057902336, data time: 0.006930881738662719\n",
      "step: 681826, loss: 0.06345391273498535, data time: 0.18378090858459473\n",
      "step: 681827, loss: 0.06265734136104584, data time: 0.09305310249328613\n",
      "step: 681828, loss: 0.06106448173522949, data time: 0.06294043858846028\n",
      "step: 681829, loss: 0.06682545691728592, data time: 0.04805868864059448\n",
      "step: 681830, loss: 0.062126390635967255, data time: 0.03873400688171387\n",
      "step: 681831, loss: 0.062391869723796844, data time: 0.03253396352132162\n",
      "step: 681832, loss: 0.058946698904037476, data time: 0.028107643127441406\n",
      "step: 681833, loss: 0.06000206992030144, data time: 0.024862140417099\n",
      "step: 681834, loss: 0.064690001308918, data time: 0.022249486711290147\n",
      "step: 681835, loss: 0.059175483882427216, data time: 0.0202254056930542\n",
      "step: 681836, loss: 0.06168133765459061, data time: 0.018579721450805664\n",
      "step: 681837, loss: 0.06740708649158478, data time: 0.017219940821329754\n",
      "step: 681838, loss: 0.054508112370967865, data time: 0.016072566692645732\n",
      "step: 681839, loss: 0.06135716289281845, data time: 0.015072737421308244\n",
      "step: 681840, loss: 0.06109580025076866, data time: 0.014211241404215496\n",
      "step: 681841, loss: 0.061577580869197845, data time: 0.013457447290420532\n",
      "step: 681842, loss: 0.060453541576862335, data time: 0.012787173776065601\n",
      "step: 681843, loss: 0.06293384730815887, data time: 0.012186156378851997\n",
      "step: 681844, loss: 0.0572710856795311, data time: 0.011656949394627622\n",
      "step: 681845, loss: 0.05552344769239426, data time: 0.011185216903686523\n",
      "step: 681846, loss: 0.06256680935621262, data time: 0.010753154754638672\n",
      "step: 681847, loss: 0.05265319347381592, data time: 0.010365507819435814\n",
      "step: 681848, loss: 0.06405306607484818, data time: 0.010005878365558126\n",
      "step: 681849, loss: 0.05837258696556091, data time: 0.009678125381469727\n",
      "step: 681850, loss: 0.05919632315635681, data time: 0.009376354217529297\n",
      "step: 681851, loss: 0.07196072489023209, data time: 0.009096741676330566\n",
      "step: 681852, loss: 0.058970630168914795, data time: 0.008833019821732133\n",
      "step: 681853, loss: 0.06662474572658539, data time: 0.008592460836683\n",
      "step: 681854, loss: 0.0569683313369751, data time: 0.008370646115007072\n",
      "step: 681855, loss: 0.062239013612270355, data time: 0.008163650830586752\n",
      "step: 681856, loss: 0.05813596770167351, data time: 0.007969694752846994\n",
      "step: 681857, loss: 0.06197061762213707, data time: 0.007794484496116638\n",
      "step: 681858, loss: 0.06068291515111923, data time: 0.007619778315226237\n",
      "step: 681859, loss: 0.06229734420776367, data time: 0.007456758443047018\n",
      "step: 681860, loss: 0.0603758804500103, data time: 0.007300376892089844\n",
      "step: 681861, loss: 0.053018417209386826, data time: 0.007149649990929497\n",
      "step: 681862, loss: 0.0646040067076683, data time: 0.007009841300345756\n",
      "step: 681863, loss: 0.07401620596647263, data time: 0.006878457571330823\n",
      "step: 681864, loss: 0.0581839382648468, data time: 0.006753664750319261\n",
      "step: 681865, loss: 0.07297926396131516, data time: 0.00663529634475708\n",
      "step: 681866, loss: 0.06199069321155548, data time: 0.2033522129058838\n",
      "step: 681867, loss: 0.06303083151578903, data time: 0.1024775505065918\n",
      "step: 681868, loss: 0.05953296273946762, data time: 0.06927267710367839\n",
      "step: 681869, loss: 0.06267445534467697, data time: 0.052624523639678955\n",
      "step: 681870, loss: 0.05999331921339035, data time: 0.04239611625671387\n",
      "step: 681871, loss: 0.05999929830431938, data time: 0.035579164822896324\n",
      "step: 681872, loss: 0.060209497809410095, data time: 0.030711582728794644\n",
      "step: 681873, loss: 0.07196612656116486, data time: 0.027122467756271362\n",
      "step: 681874, loss: 0.06403706222772598, data time: 0.024349636501736112\n",
      "step: 681875, loss: 0.06233732029795647, data time: 0.02212367057800293\n",
      "step: 681876, loss: 0.0655406042933464, data time: 0.02031608061356978\n",
      "step: 681877, loss: 0.06560440361499786, data time: 0.018800636132558186\n",
      "step: 681878, loss: 0.05988692492246628, data time: 0.017518978852492113\n",
      "step: 681879, loss: 0.05969509482383728, data time: 0.016415885516575406\n",
      "step: 681880, loss: 0.06542276591062546, data time: 0.015466451644897461\n",
      "step: 681881, loss: 0.062113773077726364, data time: 0.014635220170021057\n",
      "step: 681882, loss: 0.060006774961948395, data time: 0.013904725804048427\n",
      "step: 681883, loss: 0.062300875782966614, data time: 0.013244880570305718\n",
      "step: 681884, loss: 0.06127738207578659, data time: 0.01265322534661544\n",
      "step: 681885, loss: 0.0653344914317131, data time: 0.012127387523651122\n",
      "step: 681886, loss: 0.06185662001371384, data time: 0.011651152656191871\n",
      "step: 681887, loss: 0.052920788526535034, data time: 0.011221712285822088\n",
      "step: 681888, loss: 0.06051633507013321, data time: 0.010827002318009087\n",
      "step: 681889, loss: 0.06080509349703789, data time: 0.010468224684397379\n",
      "step: 681890, loss: 0.05988308787345886, data time: 0.010149545669555664\n",
      "step: 681891, loss: 0.06347601860761642, data time: 0.009851712446946364\n",
      "step: 681892, loss: 0.062026362866163254, data time: 0.009577627535219546\n",
      "step: 681893, loss: 0.057286761701107025, data time: 0.00932079553604126\n",
      "step: 681894, loss: 0.056456923484802246, data time: 0.009086238926854628\n",
      "step: 681895, loss: 0.05579238757491112, data time: 0.008866620063781739\n",
      "step: 681896, loss: 0.057943444699048996, data time: 0.008660893286428144\n",
      "step: 681897, loss: 0.060458578169345856, data time: 0.008471667766571045\n",
      "step: 681898, loss: 0.06086912006139755, data time: 0.008283615112304688\n",
      "step: 681899, loss: 0.06051843613386154, data time: 0.00810394567601821\n",
      "step: 681900, loss: 0.05993834137916565, data time: 0.007930987221854074\n",
      "step: 681901, loss: 0.05932329595088959, data time: 0.007767789893680149\n",
      "step: 681902, loss: 0.05792420729994774, data time: 0.007613877992372255\n",
      "step: 681903, loss: 0.057528864592313766, data time: 0.007469484680577328\n",
      "step: 681904, loss: 0.06344175338745117, data time: 0.007334452409010667\n",
      "step: 681905, loss: 0.05985529348254204, data time: 0.007205820083618164\n",
      "step: 681906, loss: 0.05807226151227951, data time: 0.1855754852294922\n",
      "step: 681907, loss: 0.06790626794099808, data time: 0.09436309337615967\n",
      "step: 681908, loss: 0.05536986514925957, data time: 0.06344175338745117\n",
      "step: 681909, loss: 0.06653498113155365, data time: 0.04824995994567871\n",
      "step: 681910, loss: 0.061997972428798676, data time: 0.038904428482055664\n",
      "step: 681911, loss: 0.0619986318051815, data time: 0.03267351786295573\n",
      "step: 681912, loss: 0.05814526602625847, data time: 0.0282289981842041\n",
      "step: 681913, loss: 0.06520789116621017, data time: 0.024976491928100586\n",
      "step: 681914, loss: 0.05988689884543419, data time: 0.02235820558336046\n",
      "step: 681915, loss: 0.0672467052936554, data time: 0.020365023612976076\n",
      "step: 681916, loss: 0.06260153651237488, data time: 0.018705909902399235\n",
      "step: 681917, loss: 0.06179407238960266, data time: 0.017325103282928467\n",
      "step: 681918, loss: 0.06275781989097595, data time: 0.01615737034724309\n",
      "step: 681919, loss: 0.06268757581710815, data time: 0.0151489291872297\n",
      "step: 681920, loss: 0.0645693689584732, data time: 0.014288520812988282\n",
      "step: 681921, loss: 0.06720840930938721, data time: 0.013528585433959961\n",
      "step: 681922, loss: 0.0603020116686821, data time: 0.0128630750319537\n",
      "step: 681923, loss: 0.054650913923978806, data time: 0.012260158856709799\n",
      "step: 681924, loss: 0.05745723471045494, data time: 0.011721184379176089\n",
      "step: 681925, loss: 0.06300818920135498, data time: 0.011248373985290527\n",
      "step: 681926, loss: 0.057780515402555466, data time: 0.010815597715831939\n",
      "step: 681927, loss: 0.059106480330228806, data time: 0.010421785441311922\n",
      "step: 681928, loss: 0.05364415794610977, data time: 0.010058672531791355\n",
      "step: 681929, loss: 0.05711536854505539, data time: 0.009729166825612387\n",
      "step: 681930, loss: 0.06504887342453003, data time: 0.009428625106811523\n",
      "step: 681931, loss: 0.06643936038017273, data time: 0.00914779076209435\n",
      "step: 681932, loss: 0.052418895065784454, data time: 0.008884120870519567\n",
      "step: 681933, loss: 0.0649423599243164, data time: 0.00864006791796003\n",
      "step: 681934, loss: 0.06120985001325607, data time: 0.008418461372112405\n",
      "step: 681935, loss: 0.06307511031627655, data time: 0.008209904034932455\n",
      "step: 681936, loss: 0.05701963230967522, data time: 0.00801363298969884\n",
      "step: 681937, loss: 0.06135449931025505, data time: 0.007833339273929596\n",
      "step: 681938, loss: 0.06366705149412155, data time: 0.007657448450724284\n",
      "step: 681939, loss: 0.06419089436531067, data time: 0.007491932195775649\n",
      "step: 681940, loss: 0.06876160949468613, data time: 0.007335512978690011\n",
      "step: 681941, loss: 0.06316807121038437, data time: 0.007184664408365886\n",
      "step: 681942, loss: 0.06438654661178589, data time: 0.007042362883284285\n",
      "step: 681943, loss: 0.054318368434906006, data time: 0.006916529253909462\n",
      "step: 681944, loss: 0.06584301590919495, data time: 0.006794947844285231\n",
      "step: 681945, loss: 0.05296236276626587, data time: 0.006680744886398316\n",
      "step: 681946, loss: 0.05439820885658264, data time: 0.19050955772399902\n",
      "step: 681947, loss: 0.06361159682273865, data time: 0.09707343578338623\n",
      "step: 681948, loss: 0.05745653435587883, data time: 0.0652627944946289\n",
      "step: 681949, loss: 0.06500010192394257, data time: 0.04972964525222778\n",
      "step: 681950, loss: 0.06325149536132812, data time: 0.04006600379943848\n",
      "step: 681951, loss: 0.05780276656150818, data time: 0.033655246098836265\n",
      "step: 681952, loss: 0.06026915833353996, data time: 0.02906642641339983\n",
      "step: 681953, loss: 0.061815422028303146, data time: 0.025705009698867798\n",
      "step: 681954, loss: 0.057216476649045944, data time: 0.02300914128621419\n",
      "step: 681955, loss: 0.06012606620788574, data time: 0.020910048484802247\n",
      "step: 681956, loss: 0.054520636796951294, data time: 0.01920426975597035\n",
      "step: 681957, loss: 0.057237591594457626, data time: 0.017782668272654217\n",
      "step: 681958, loss: 0.06460876762866974, data time: 0.01657843589782715\n",
      "step: 681959, loss: 0.06017058342695236, data time: 0.015535507883344377\n",
      "step: 681960, loss: 0.06672115623950958, data time: 0.014652188618977864\n",
      "step: 681961, loss: 0.05645652860403061, data time: 0.013892099261283875\n",
      "step: 681962, loss: 0.05818251520395279, data time: 0.013222834643195657\n",
      "step: 681963, loss: 0.05898731201887131, data time: 0.012624104817708334\n",
      "step: 681964, loss: 0.060328446328639984, data time: 0.012087332574944747\n",
      "step: 681965, loss: 0.06703397631645203, data time: 0.011605918407440186\n",
      "step: 681966, loss: 0.06304630637168884, data time: 0.011174179258800689\n",
      "step: 681967, loss: 0.06694580614566803, data time: 0.01078297875144265\n",
      "step: 681968, loss: 0.06379491090774536, data time: 0.010418674220209536\n",
      "step: 681969, loss: 0.06465192139148712, data time: 0.0100862185160319\n",
      "step: 681970, loss: 0.06415483355522156, data time: 0.009785327911376953\n",
      "step: 681971, loss: 0.057796984910964966, data time: 0.009505602029653696\n",
      "step: 681972, loss: 0.05896932631731033, data time: 0.009244159415916161\n",
      "step: 681973, loss: 0.056731995195150375, data time: 0.009000616414206368\n",
      "step: 681974, loss: 0.0605575293302536, data time: 0.00877698536576896\n",
      "step: 681975, loss: 0.060967616736888885, data time: 0.008567531903584799\n",
      "step: 681976, loss: 0.0567370280623436, data time: 0.008372668297060074\n",
      "step: 681977, loss: 0.06303620338439941, data time: 0.008198931813240051\n",
      "step: 681978, loss: 0.05303383991122246, data time: 0.008017626675692472\n",
      "step: 681979, loss: 0.06638988852500916, data time: 0.00784621519200942\n",
      "step: 681980, loss: 0.06849844008684158, data time: 0.007683794839041574\n",
      "step: 681981, loss: 0.0580911859869957, data time: 0.007526596387227376\n",
      "step: 681982, loss: 0.062123458832502365, data time: 0.007377598736737226\n",
      "step: 681983, loss: 0.06122499704360962, data time: 0.007239511138514469\n",
      "step: 681984, loss: 0.06486456841230392, data time: 0.007109238551213191\n",
      "step: 681985, loss: 0.06320980191230774, data time: 0.006987130641937256\n",
      "step: 681986, loss: 0.06311891227960587, data time: 0.1759798526763916\n",
      "step: 681987, loss: 0.06552787870168686, data time: 0.08916699886322021\n",
      "step: 681988, loss: 0.06271355599164963, data time: 0.06058144569396973\n",
      "step: 681989, loss: 0.06377555429935455, data time: 0.04612863063812256\n",
      "step: 681990, loss: 0.058668091893196106, data time: 0.037205982208251956\n",
      "step: 681991, loss: 0.06392760574817657, data time: 0.03126251697540283\n",
      "step: 681992, loss: 0.06352827697992325, data time: 0.027010508946010044\n",
      "step: 681993, loss: 0.059665631502866745, data time: 0.023886412382125854\n",
      "step: 681994, loss: 0.06396563351154327, data time: 0.02139094140794542\n",
      "step: 681995, loss: 0.058479100465774536, data time: 0.019453930854797363\n",
      "step: 681996, loss: 0.05859794467687607, data time: 0.01791433854536577\n",
      "step: 681997, loss: 0.06054701656103134, data time: 0.016598959763844807\n",
      "step: 681998, loss: 0.06524061411619186, data time: 0.015515914330115685\n",
      "step: 681999, loss: 0.06122761219739914, data time: 0.014573471886771066\n",
      "step: 682000, loss: 0.059579264372587204, data time: 0.013747819264729818\n",
      "step: 682001, loss: 0.06372296810150146, data time: 0.013022556900978088\n",
      "step: 682002, loss: 0.06306492537260056, data time: 0.012376041973338407\n",
      "step: 682003, loss: 0.06314349174499512, data time: 0.01180324289533827\n",
      "step: 682004, loss: 0.0657566636800766, data time: 0.011289634202655992\n",
      "step: 682005, loss: 0.06785474717617035, data time: 0.010835158824920654\n",
      "step: 682006, loss: 0.0637686625123024, data time: 0.010419425510224841\n",
      "step: 682007, loss: 0.07032909989356995, data time: 0.010044748132879084\n",
      "step: 682008, loss: 0.06525535881519318, data time: 0.009699645249739937\n",
      "step: 682009, loss: 0.062012434005737305, data time: 0.009386877218882242\n",
      "step: 682010, loss: 0.05686873197555542, data time: 0.009095611572265625\n",
      "step: 682011, loss: 0.06093441694974899, data time: 0.008823046317467323\n",
      "step: 682012, loss: 0.056355297565460205, data time: 0.008571872004756221\n",
      "step: 682013, loss: 0.06942085921764374, data time: 0.008341899939945765\n",
      "step: 682014, loss: 0.058096617460250854, data time: 0.008129909120757004\n",
      "step: 682015, loss: 0.058143239468336105, data time: 0.007930612564086914\n",
      "step: 682016, loss: 0.060489457100629807, data time: 0.007742374174056515\n",
      "step: 682017, loss: 0.06166671961545944, data time: 0.00757201761007309\n",
      "step: 682018, loss: 0.058084242045879364, data time: 0.007404045625166459\n",
      "step: 682019, loss: 0.061572443693876266, data time: 0.007246564416324391\n",
      "step: 682020, loss: 0.06404472887516022, data time: 0.007095657076154436\n",
      "step: 682021, loss: 0.060461968183517456, data time: 0.006951583756340874\n",
      "step: 682022, loss: 0.06614330410957336, data time: 0.006815936114336993\n",
      "step: 682023, loss: 0.05940822884440422, data time: 0.006691374276813708\n",
      "step: 682024, loss: 0.05984070897102356, data time: 0.006572925127469576\n",
      "step: 682025, loss: 0.05601140856742859, data time: 0.0064596891403198246\n",
      "step: 682026, loss: 0.061098575592041016, data time: 0.17802762985229492\n",
      "step: 682027, loss: 0.06208748742938042, data time: 0.09053945541381836\n",
      "step: 682028, loss: 0.06441864371299744, data time: 0.061422506968180336\n",
      "step: 682029, loss: 0.06027507036924362, data time: 0.04674196243286133\n",
      "step: 682030, loss: 0.06207137927412987, data time: 0.03767971992492676\n",
      "step: 682031, loss: 0.06182263046503067, data time: 0.03165483474731445\n",
      "step: 682032, loss: 0.06609350442886353, data time: 0.02734896114894322\n",
      "step: 682033, loss: 0.06128251180052757, data time: 0.024183273315429688\n",
      "step: 682034, loss: 0.06435193866491318, data time: 0.021649863984849717\n",
      "step: 682035, loss: 0.05723359435796738, data time: 0.019687747955322264\n",
      "step: 682036, loss: 0.05557028949260712, data time: 0.018089272759177467\n",
      "step: 682037, loss: 0.05780569463968277, data time: 0.016761104265848797\n",
      "step: 682038, loss: 0.06373545527458191, data time: 0.01563849815955529\n",
      "step: 682039, loss: 0.06796611100435257, data time: 0.014669435364859445\n",
      "step: 682040, loss: 0.06694374233484268, data time: 0.013843584060668945\n",
      "step: 682041, loss: 0.06634679436683655, data time: 0.013109281659126282\n",
      "step: 682042, loss: 0.06757434457540512, data time: 0.012482334585750805\n",
      "step: 682043, loss: 0.06010863929986954, data time: 0.011898848745557997\n",
      "step: 682044, loss: 0.06633102893829346, data time: 0.01137751027157432\n",
      "step: 682045, loss: 0.05823973938822746, data time: 0.010916328430175782\n",
      "step: 682046, loss: 0.06369727849960327, data time: 0.010499080022176107\n",
      "step: 682047, loss: 0.06219225376844406, data time: 0.010124715891751375\n",
      "step: 682048, loss: 0.05938562750816345, data time: 0.009775648946347444\n",
      "step: 682049, loss: 0.06280927360057831, data time: 0.009458392858505249\n",
      "step: 682050, loss: 0.06553742289543152, data time: 0.009167289733886719\n",
      "step: 682051, loss: 0.05579731985926628, data time: 0.008894736950214092\n",
      "step: 682052, loss: 0.05870600789785385, data time: 0.008638152369746455\n",
      "step: 682053, loss: 0.06525298953056335, data time: 0.008405276707240514\n",
      "step: 682054, loss: 0.06277401000261307, data time: 0.008189184912319842\n",
      "step: 682055, loss: 0.06306611001491547, data time: 0.007987173398335774\n",
      "step: 682056, loss: 0.06241121143102646, data time: 0.007800117615730532\n",
      "step: 682057, loss: 0.06320185959339142, data time: 0.007629610598087311\n",
      "step: 682058, loss: 0.06595407426357269, data time: 0.007459416533961441\n",
      "step: 682059, loss: 0.06335529685020447, data time: 0.007299696697908289\n",
      "step: 682060, loss: 0.06518682837486267, data time: 0.0071482454027448385\n",
      "step: 682061, loss: 0.0633421391248703, data time: 0.007001366880204942\n",
      "step: 682062, loss: 0.06174421310424805, data time: 0.006863980679898648\n",
      "step: 682063, loss: 0.06375625729560852, data time: 0.006736316178974353\n",
      "step: 682064, loss: 0.058832310140132904, data time: 0.006615473673893855\n",
      "step: 682065, loss: 0.0621190145611763, data time: 0.006501227617263794\n",
      "step: 682066, loss: 0.06005476415157318, data time: 0.18239521980285645\n",
      "step: 682067, loss: 0.06545218825340271, data time: 0.09303808212280273\n",
      "step: 682068, loss: 0.0697573646903038, data time: 0.0625450611114502\n",
      "step: 682069, loss: 0.06230723112821579, data time: 0.047692835330963135\n",
      "step: 682070, loss: 0.059406936168670654, data time: 0.03843812942504883\n",
      "step: 682071, loss: 0.06553646177053452, data time: 0.03230146567026774\n",
      "step: 682072, loss: 0.06445591151714325, data time: 0.02790478297642299\n",
      "step: 682073, loss: 0.06134448200464249, data time: 0.02469313144683838\n",
      "step: 682074, loss: 0.06407661736011505, data time: 0.022102753321329754\n",
      "step: 682075, loss: 0.06675374507904053, data time: 0.020121145248413085\n",
      "step: 682076, loss: 0.06244450807571411, data time: 0.018483552065762607\n",
      "step: 682077, loss: 0.06411907076835632, data time: 0.0171204408009847\n",
      "step: 682078, loss: 0.06328992545604706, data time: 0.015967350739699144\n",
      "step: 682079, loss: 0.05976414680480957, data time: 0.014969042369297572\n",
      "step: 682080, loss: 0.06473778188228607, data time: 0.014116621017456055\n",
      "step: 682081, loss: 0.06326758861541748, data time: 0.013372853398323059\n",
      "step: 682082, loss: 0.05841928347945213, data time: 0.012723894680247587\n",
      "step: 682083, loss: 0.06999556720256805, data time: 0.012127280235290527\n",
      "step: 682084, loss: 0.06852373480796814, data time: 0.011598034908896998\n",
      "step: 682085, loss: 0.06191202253103256, data time: 0.011124122142791747\n",
      "step: 682086, loss: 0.0588114932179451, data time: 0.010696774437313988\n",
      "step: 682087, loss: 0.06120942160487175, data time: 0.010316426103765314\n",
      "step: 682088, loss: 0.06495290249586105, data time: 0.009956266569054645\n",
      "step: 682089, loss: 0.057538099586963654, data time: 0.009633431831995646\n",
      "step: 682090, loss: 0.06051875278353691, data time: 0.009331159591674805\n",
      "step: 682091, loss: 0.05961218476295471, data time: 0.00905453241788424\n",
      "step: 682092, loss: 0.06400837749242783, data time: 0.008802096048990885\n",
      "step: 682093, loss: 0.06077573820948601, data time: 0.008560827800205775\n",
      "step: 682094, loss: 0.06262388825416565, data time: 0.008339857232981715\n",
      "step: 682095, loss: 0.06356029957532883, data time: 0.008133578300476074\n",
      "step: 682096, loss: 0.06376427412033081, data time: 0.007941261414558656\n",
      "step: 682097, loss: 0.06437014043331146, data time: 0.007763117551803589\n",
      "step: 682098, loss: 0.05992921441793442, data time: 0.00758903676813299\n",
      "step: 682099, loss: 0.05672278627753258, data time: 0.007423618260551901\n",
      "step: 682100, loss: 0.06392141431570053, data time: 0.00727013179234096\n",
      "step: 682101, loss: 0.0605548731982708, data time: 0.00712030463748508\n",
      "step: 682102, loss: 0.06897225975990295, data time: 0.006982854894689612\n",
      "step: 682103, loss: 0.059108540415763855, data time: 0.006855745064584832\n",
      "step: 682104, loss: 0.06226327642798424, data time: 0.00673683484395345\n",
      "step: 682105, loss: 0.046248361468315125, data time: 0.006622695922851562\n",
      "step: 682106, loss: 0.06320218741893768, data time: 0.19202160835266113\n",
      "step: 682107, loss: 0.06340310722589493, data time: 0.0973358154296875\n",
      "step: 682108, loss: 0.06640505790710449, data time: 0.06593728065490723\n",
      "step: 682109, loss: 0.06034669280052185, data time: 0.050143420696258545\n",
      "step: 682110, loss: 0.06604087352752686, data time: 0.04041256904602051\n",
      "step: 682111, loss: 0.058419402688741684, data time: 0.03392895062764486\n",
      "step: 682112, loss: 0.0647454559803009, data time: 0.029302290507725308\n",
      "step: 682113, loss: 0.06670113652944565, data time: 0.02589470148086548\n",
      "step: 682114, loss: 0.06403428316116333, data time: 0.023167265786064997\n",
      "step: 682115, loss: 0.0588710755109787, data time: 0.021066737174987794\n",
      "step: 682116, loss: 0.05925385281443596, data time: 0.019348643042824486\n",
      "step: 682117, loss: 0.05657900497317314, data time: 0.017914454142252605\n",
      "step: 682118, loss: 0.06808365881443024, data time: 0.016700322811420146\n",
      "step: 682119, loss: 0.0659889429807663, data time: 0.01564957414354597\n",
      "step: 682120, loss: 0.056458115577697754, data time: 0.014752610524495443\n",
      "step: 682121, loss: 0.060286980122327805, data time: 0.013972625136375427\n",
      "step: 682122, loss: 0.0631190836429596, data time: 0.013274613548727596\n",
      "step: 682123, loss: 0.06264762580394745, data time: 0.012649257977803549\n",
      "step: 682124, loss: 0.059996407479047775, data time: 0.012096078772293893\n",
      "step: 682125, loss: 0.06454195082187653, data time: 0.011599409580230712\n",
      "step: 682126, loss: 0.06737785041332245, data time: 0.011151722499302455\n",
      "step: 682127, loss: 0.061138611286878586, data time: 0.010744766755537554\n",
      "step: 682128, loss: 0.06795693933963776, data time: 0.010368886201278023\n",
      "step: 682129, loss: 0.06415972113609314, data time: 0.010028878847757975\n",
      "step: 682130, loss: 0.05568951368331909, data time: 0.00971405029296875\n",
      "step: 682131, loss: 0.06839490681886673, data time: 0.009420128969045786\n",
      "step: 682132, loss: 0.056794993579387665, data time: 0.009146928787231445\n",
      "step: 682133, loss: 0.06181909143924713, data time: 0.008900114468165807\n",
      "step: 682134, loss: 0.06561516970396042, data time: 0.008672072969633958\n",
      "step: 682135, loss: 0.05885738134384155, data time: 0.008454434076944987\n",
      "step: 682136, loss: 0.060711853206157684, data time: 0.008251105585405904\n",
      "step: 682137, loss: 0.06295926868915558, data time: 0.00806659460067749\n",
      "step: 682138, loss: 0.06975199282169342, data time: 0.007886323061856356\n",
      "step: 682139, loss: 0.06069415062665939, data time: 0.007723780239329618\n",
      "step: 682140, loss: 0.06218668445944786, data time: 0.007559265409197126\n",
      "step: 682141, loss: 0.062474995851516724, data time: 0.007401181591881646\n",
      "step: 682142, loss: 0.05888700485229492, data time: 0.007253782169238941\n",
      "step: 682143, loss: 0.06393398344516754, data time: 0.007116135798002544\n",
      "step: 682144, loss: 0.06136512756347656, data time: 0.006984545634343074\n",
      "step: 682145, loss: 0.05886754021048546, data time: 0.006860017776489258\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4104914665222168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 682146, loss: 0.0678999125957489, data time: 0.18549752235412598\n",
      "step: 682147, loss: 0.056536994874477386, data time: 0.0935060977935791\n",
      "step: 682148, loss: 0.06308978796005249, data time: 0.06353076299031575\n",
      "step: 682149, loss: 0.05432648956775665, data time: 0.04832935333251953\n",
      "step: 682150, loss: 0.06415939331054688, data time: 0.03894267082214355\n",
      "step: 682151, loss: 0.06555147469043732, data time: 0.03269430001576742\n",
      "step: 682152, loss: 0.06333232671022415, data time: 0.028278725487845286\n",
      "step: 682153, loss: 0.06479291617870331, data time: 0.02498719096183777\n",
      "step: 682154, loss: 0.06585418432950974, data time: 0.022354099485609267\n",
      "step: 682155, loss: 0.05754096433520317, data time: 0.020316243171691895\n",
      "step: 682156, loss: 0.0666259229183197, data time: 0.018672943115234375\n",
      "step: 682157, loss: 0.057564876973629, data time: 0.01729889710744222\n",
      "step: 682158, loss: 0.06279581785202026, data time: 0.016131089283869818\n",
      "step: 682159, loss: 0.05924306437373161, data time: 0.015126143183026994\n",
      "step: 682160, loss: 0.056110382080078125, data time: 0.014265998204549154\n",
      "step: 682161, loss: 0.06673967838287354, data time: 0.013505309820175171\n",
      "step: 682162, loss: 0.05847155302762985, data time: 0.012824689640718348\n",
      "step: 682163, loss: 0.0622406005859375, data time: 0.012220925754970975\n",
      "step: 682164, loss: 0.05419837683439255, data time: 0.011685459237349661\n",
      "step: 682165, loss: 0.05901721119880676, data time: 0.011208176612854004\n",
      "step: 682166, loss: 0.061365410685539246, data time: 0.010775861285981677\n",
      "step: 682167, loss: 0.060811739414930344, data time: 0.010383573445406828\n",
      "step: 682168, loss: 0.05289294570684433, data time: 0.010018607844477114\n",
      "step: 682169, loss: 0.05667632818222046, data time: 0.009705086549123129\n",
      "step: 682170, loss: 0.06208270788192749, data time: 0.009399890899658203\n",
      "step: 682171, loss: 0.06606253981590271, data time: 0.009113834454463078\n",
      "step: 682172, loss: 0.0642567127943039, data time: 0.008848199137934932\n",
      "step: 682173, loss: 0.06344351172447205, data time: 0.008605573858533586\n",
      "step: 682174, loss: 0.05798406898975372, data time: 0.008383964670115504\n",
      "step: 682175, loss: 0.061030834913253784, data time: 0.008176954587300618\n",
      "step: 682176, loss: 0.06418068706989288, data time: 0.007982446301367975\n",
      "step: 682177, loss: 0.060502536594867706, data time: 0.007804185152053833\n",
      "step: 682178, loss: 0.061899375170469284, data time: 0.007625731554898349\n",
      "step: 682179, loss: 0.05738591402769089, data time: 0.007459710626041188\n",
      "step: 682180, loss: 0.0680651068687439, data time: 0.007300077165876116\n",
      "step: 682181, loss: 0.060599617660045624, data time: 0.007148789034949409\n",
      "step: 682182, loss: 0.06201394647359848, data time: 0.007007334683392499\n",
      "step: 682183, loss: 0.0639302209019661, data time: 0.006877554090399491\n",
      "step: 682184, loss: 0.06255468726158142, data time: 0.006755529305873773\n",
      "step: 682185, loss: 0.04279492795467377, data time: 0.006638836860656738\n",
      "step: 682186, loss: 0.06522946059703827, data time: 0.18261480331420898\n",
      "step: 682187, loss: 0.06637240946292877, data time: 0.09245860576629639\n",
      "step: 682188, loss: 0.06290608644485474, data time: 0.06266641616821289\n",
      "step: 682189, loss: 0.06290441751480103, data time: 0.04777449369430542\n",
      "step: 682190, loss: 0.05679161846637726, data time: 0.038489341735839844\n",
      "step: 682191, loss: 0.06550951302051544, data time: 0.03232661883036295\n",
      "step: 682192, loss: 0.06848904490470886, data time: 0.027924401419503347\n",
      "step: 682193, loss: 0.06319614499807358, data time: 0.024694591760635376\n",
      "step: 682194, loss: 0.06305569410324097, data time: 0.02209509743584527\n",
      "step: 682195, loss: 0.06995560228824615, data time: 0.020089244842529295\n",
      "step: 682196, loss: 0.06313101947307587, data time: 0.018458518114956943\n",
      "step: 682197, loss: 0.06073617935180664, data time: 0.017105241616566975\n",
      "step: 682198, loss: 0.063088558614254, data time: 0.015958235814021185\n",
      "step: 682199, loss: 0.06490640342235565, data time: 0.014959182058061873\n",
      "step: 682200, loss: 0.06670554727315903, data time: 0.014098246892293295\n",
      "step: 682201, loss: 0.06587822735309601, data time: 0.013358190655708313\n",
      "step: 682202, loss: 0.062372054904699326, data time: 0.012691441704245174\n",
      "step: 682203, loss: 0.058550626039505005, data time: 0.012093517515394423\n",
      "step: 682204, loss: 0.061842333525419235, data time: 0.011564430437589946\n",
      "step: 682205, loss: 0.06393366307020187, data time: 0.011093986034393311\n",
      "step: 682206, loss: 0.05851616710424423, data time: 0.010673227764311292\n",
      "step: 682207, loss: 0.06110161542892456, data time: 0.01028475978157737\n",
      "step: 682208, loss: 0.06214211881160736, data time: 0.009927500849184782\n",
      "step: 682209, loss: 0.06391909718513489, data time: 0.009600549936294556\n",
      "step: 682210, loss: 0.06692361831665039, data time: 0.009302759170532226\n",
      "step: 682211, loss: 0.06492749601602554, data time: 0.009022795237027682\n",
      "step: 682212, loss: 0.05998867750167847, data time: 0.00875895994680899\n",
      "step: 682213, loss: 0.061244018375873566, data time: 0.008519044944218226\n",
      "step: 682214, loss: 0.06316535174846649, data time: 0.008300814135321256\n",
      "step: 682215, loss: 0.06399872899055481, data time: 0.008096973101298014\n",
      "step: 682216, loss: 0.06599180400371552, data time: 0.007905683209819178\n",
      "step: 682217, loss: 0.06456496566534042, data time: 0.007729522883892059\n",
      "step: 682218, loss: 0.06029641255736351, data time: 0.0075558171127781725\n",
      "step: 682219, loss: 0.06454376876354218, data time: 0.007393009522381951\n",
      "step: 682220, loss: 0.057678431272506714, data time: 0.007235656465802874\n",
      "step: 682221, loss: 0.061650536954402924, data time: 0.007084290186564128\n",
      "step: 682222, loss: 0.06587877124547958, data time: 0.006946583051939268\n",
      "step: 682223, loss: 0.06445176899433136, data time: 0.006818049832394249\n",
      "step: 682224, loss: 0.05759041756391525, data time: 0.006695362237783579\n",
      "step: 682225, loss: 0.06006009876728058, data time: 0.006579542160034179\n",
      "step: 682226, loss: 0.06431424617767334, data time: 0.18875575065612793\n",
      "step: 682227, loss: 0.06444618105888367, data time: 0.09578299522399902\n",
      "step: 682228, loss: 0.06530707329511642, data time: 0.06522544225056966\n",
      "step: 682229, loss: 0.054966770112514496, data time: 0.04931306838989258\n",
      "step: 682230, loss: 0.06204929202795029, data time: 0.03975963592529297\n",
      "step: 682231, loss: 0.06391832232475281, data time: 0.03338762124379476\n",
      "step: 682232, loss: 0.06411746144294739, data time: 0.028919969286237444\n",
      "step: 682233, loss: 0.06038448214530945, data time: 0.025471270084381104\n",
      "step: 682234, loss: 0.061321377754211426, data time: 0.02279935942755805\n",
      "step: 682235, loss: 0.059154070913791656, data time: 0.020738649368286132\n",
      "step: 682236, loss: 0.056959740817546844, data time: 0.019049124284224075\n",
      "step: 682237, loss: 0.061032459139823914, data time: 0.017644206682840984\n",
      "step: 682238, loss: 0.055229902267456055, data time: 0.01645214741046612\n",
      "step: 682239, loss: 0.06016106531023979, data time: 0.01542464324406215\n",
      "step: 682240, loss: 0.0619644820690155, data time: 0.014538113276163738\n",
      "step: 682241, loss: 0.06651708483695984, data time: 0.013765126466751099\n",
      "step: 682242, loss: 0.06473459303379059, data time: 0.013068844290340649\n",
      "step: 682243, loss: 0.06402748078107834, data time: 0.012450906965467665\n",
      "step: 682244, loss: 0.06398558616638184, data time: 0.01190226956417686\n",
      "step: 682245, loss: 0.06476551294326782, data time: 0.011421167850494384\n",
      "step: 682246, loss: 0.06470967829227448, data time: 0.010986430304391044\n",
      "step: 682247, loss: 0.0636642575263977, data time: 0.010587518865411932\n",
      "step: 682248, loss: 0.060750409960746765, data time: 0.010214608648548956\n",
      "step: 682249, loss: 0.06041771173477173, data time: 0.00989489754041036\n",
      "step: 682250, loss: 0.06277849525213242, data time: 0.009598188400268555\n",
      "step: 682251, loss: 0.060886651277542114, data time: 0.009318810242872972\n",
      "step: 682252, loss: 0.06488686054944992, data time: 0.009061027456212926\n",
      "step: 682253, loss: 0.06267432868480682, data time: 0.00882326705115182\n",
      "step: 682254, loss: 0.06619777530431747, data time: 0.00860830833171976\n",
      "step: 682255, loss: 0.05874613672494888, data time: 0.00840603510538737\n",
      "step: 682256, loss: 0.061119548976421356, data time: 0.008216742546327652\n",
      "step: 682257, loss: 0.058026283979415894, data time: 0.008039034903049469\n",
      "step: 682258, loss: 0.06400497257709503, data time: 0.007860855622725054\n",
      "step: 682259, loss: 0.060021862387657166, data time: 0.007689440951627844\n",
      "step: 682260, loss: 0.05769520252943039, data time: 0.007522835050310407\n",
      "step: 682261, loss: 0.061441194266080856, data time: 0.007365134027269151\n",
      "step: 682262, loss: 0.06220113858580589, data time: 0.007217516770233979\n",
      "step: 682263, loss: 0.06738145649433136, data time: 0.0070823305531551965\n",
      "step: 682264, loss: 0.061337172985076904, data time: 0.006953031588823368\n",
      "step: 682265, loss: 0.048464205116033554, data time: 0.0068300306797027584\n",
      "step: 682266, loss: 0.06124170869588852, data time: 0.17701292037963867\n",
      "step: 682267, loss: 0.06467290222644806, data time: 0.08986151218414307\n",
      "step: 682268, loss: 0.06271976232528687, data time: 0.06129995981852213\n",
      "step: 682269, loss: 0.06444855779409409, data time: 0.046350717544555664\n",
      "step: 682270, loss: 0.05633590370416641, data time: 0.03736457824707031\n",
      "step: 682271, loss: 0.06507902592420578, data time: 0.03139046827952067\n",
      "step: 682272, loss: 0.058930426836013794, data time: 0.02722774233136858\n",
      "step: 682273, loss: 0.06366828083992004, data time: 0.023991554975509644\n",
      "step: 682274, loss: 0.061745014041662216, data time: 0.021477778752644856\n",
      "step: 682275, loss: 0.05662447214126587, data time: 0.019534206390380858\n",
      "step: 682276, loss: 0.062281541526317596, data time: 0.017956473610617897\n",
      "step: 682277, loss: 0.062490008771419525, data time: 0.016638457775115967\n",
      "step: 682278, loss: 0.059119369834661484, data time: 0.015539756188025841\n",
      "step: 682279, loss: 0.06084826588630676, data time: 0.01458380903516497\n",
      "step: 682280, loss: 0.060245316475629807, data time: 0.01375719706217448\n",
      "step: 682281, loss: 0.05733988806605339, data time: 0.01303495466709137\n",
      "step: 682282, loss: 0.05954635143280029, data time: 0.012387822656070484\n",
      "step: 682283, loss: 0.05635291337966919, data time: 0.011811441845364042\n",
      "step: 682284, loss: 0.06415706872940063, data time: 0.011297188307109633\n",
      "step: 682285, loss: 0.06158643960952759, data time: 0.010845613479614259\n",
      "step: 682286, loss: 0.06630436331033707, data time: 0.010432175227573939\n",
      "step: 682287, loss: 0.06508390605449677, data time: 0.01005442575974898\n",
      "step: 682288, loss: 0.06327827274799347, data time: 0.00970930638520614\n",
      "step: 682289, loss: 0.06581041216850281, data time: 0.00939441720644633\n",
      "step: 682290, loss: 0.06576462090015411, data time: 0.009105815887451171\n",
      "step: 682291, loss: 0.06663580238819122, data time: 0.00883016219505897\n",
      "step: 682292, loss: 0.06823182106018066, data time: 0.008584508189448604\n",
      "step: 682293, loss: 0.06127495318651199, data time: 0.008351436683109828\n",
      "step: 682294, loss: 0.060112446546554565, data time: 0.008141788943060514\n",
      "step: 682295, loss: 0.05994926393032074, data time: 0.007940411567687988\n",
      "step: 682296, loss: 0.062483757734298706, data time: 0.007764731684038716\n",
      "step: 682297, loss: 0.06335119903087616, data time: 0.007593587040901184\n",
      "step: 682298, loss: 0.05989886447787285, data time: 0.007423588723847361\n",
      "step: 682299, loss: 0.06058048829436302, data time: 0.007265055880827063\n",
      "step: 682300, loss: 0.06281176209449768, data time: 0.007112053462437221\n",
      "step: 682301, loss: 0.05785347521305084, data time: 0.006967862447102864\n",
      "step: 682302, loss: 0.06726548075675964, data time: 0.006831072472237252\n",
      "step: 682303, loss: 0.06656812131404877, data time: 0.006708258076717979\n",
      "step: 682304, loss: 0.05987655371427536, data time: 0.00659026855077499\n",
      "step: 682305, loss: 0.051260076463222504, data time: 0.006475758552551269\n",
      "step: 682306, loss: 0.06562518328428268, data time: 0.18296146392822266\n",
      "step: 682307, loss: 0.06435336172580719, data time: 0.09228038787841797\n",
      "step: 682308, loss: 0.06101451441645622, data time: 0.06260045369466145\n",
      "step: 682309, loss: 0.0614130012691021, data time: 0.04733377695083618\n",
      "step: 682310, loss: 0.06500545144081116, data time: 0.03816843032836914\n",
      "step: 682311, loss: 0.05541635677218437, data time: 0.0320507287979126\n",
      "step: 682312, loss: 0.06689302623271942, data time: 0.027839592524937222\n",
      "step: 682313, loss: 0.06258025020360947, data time: 0.024528712034225464\n",
      "step: 682314, loss: 0.061423540115356445, data time: 0.022022406260172527\n",
      "step: 682315, loss: 0.06903261691331863, data time: 0.020019102096557616\n",
      "step: 682316, loss: 0.07206561416387558, data time: 0.01839202100580389\n",
      "step: 682317, loss: 0.060102179646492004, data time: 0.017035822073618572\n",
      "step: 682318, loss: 0.06244026869535446, data time: 0.015892175527719352\n",
      "step: 682319, loss: 0.05767541751265526, data time: 0.014899390084402902\n",
      "step: 682320, loss: 0.06506258994340897, data time: 0.014050785700480144\n",
      "step: 682321, loss: 0.06606829166412354, data time: 0.013303086161613464\n",
      "step: 682322, loss: 0.05581554397940636, data time: 0.01263729263754452\n",
      "step: 682323, loss: 0.05990412086248398, data time: 0.012043158213297525\n",
      "step: 682324, loss: 0.07056745141744614, data time: 0.011520511225650185\n",
      "step: 682325, loss: 0.06556794047355652, data time: 0.011054611206054688\n",
      "step: 682326, loss: 0.05729661509394646, data time: 0.01063137962704613\n",
      "step: 682327, loss: 0.06162063404917717, data time: 0.010243134065107866\n",
      "step: 682328, loss: 0.06550784409046173, data time: 0.009885881258093792\n",
      "step: 682329, loss: 0.05591825768351555, data time: 0.00956195592880249\n",
      "step: 682330, loss: 0.05799279361963272, data time: 0.00926569938659668\n",
      "step: 682331, loss: 0.06599308550357819, data time: 0.008997238599337064\n",
      "step: 682332, loss: 0.05753561109304428, data time: 0.008736115914803964\n",
      "step: 682333, loss: 0.06078220158815384, data time: 0.008495816162654332\n",
      "step: 682334, loss: 0.06410975754261017, data time: 0.008276807850804823\n",
      "step: 682335, loss: 0.06252089142799377, data time: 0.008071605364481609\n",
      "step: 682336, loss: 0.06014091894030571, data time: 0.007879587911790418\n",
      "step: 682337, loss: 0.05840078741312027, data time: 0.007707551121711731\n",
      "step: 682338, loss: 0.06369637697935104, data time: 0.0075399803392814865\n",
      "step: 682339, loss: 0.06285410374403, data time: 0.007378515075234806\n",
      "step: 682340, loss: 0.055328529328107834, data time: 0.007222645623343332\n",
      "step: 682341, loss: 0.0659790188074112, data time: 0.007077468766106499\n",
      "step: 682342, loss: 0.06346006691455841, data time: 0.00693770357080408\n",
      "step: 682343, loss: 0.06402637809515, data time: 0.006808626024346603\n",
      "step: 682344, loss: 0.06508470326662064, data time: 0.0066861311594645185\n",
      "step: 682345, loss: 0.06856818497180939, data time: 0.006570196151733399\n",
      "step: 682346, loss: 0.05909174680709839, data time: 0.19648265838623047\n",
      "step: 682347, loss: 0.05720410868525505, data time: 0.09902167320251465\n",
      "step: 682348, loss: 0.05725352093577385, data time: 0.0665575663248698\n",
      "step: 682349, loss: 0.06531286984682083, data time: 0.05068182945251465\n",
      "step: 682350, loss: 0.060611825436353683, data time: 0.04082884788513184\n",
      "step: 682351, loss: 0.06512773036956787, data time: 0.03425470987955729\n",
      "step: 682352, loss: 0.05770404264330864, data time: 0.029576778411865234\n",
      "step: 682353, loss: 0.05808268114924431, data time: 0.02616247534751892\n",
      "step: 682354, loss: 0.06282307207584381, data time: 0.023412095175849065\n",
      "step: 682355, loss: 0.05898165702819824, data time: 0.021270012855529784\n",
      "step: 682356, loss: 0.06332823634147644, data time: 0.019529884511774235\n",
      "step: 682357, loss: 0.0598275326192379, data time: 0.01808043320973714\n",
      "step: 682358, loss: 0.06255204230546951, data time: 0.01685949472280649\n",
      "step: 682359, loss: 0.0522061362862587, data time: 0.015847103936331614\n",
      "step: 682360, loss: 0.05963623523712158, data time: 0.014946969350179036\n",
      "step: 682361, loss: 0.05805888772010803, data time: 0.014165028929710388\n",
      "step: 682362, loss: 0.061034154146909714, data time: 0.013483187731574564\n",
      "step: 682363, loss: 0.06576154381036758, data time: 0.012864483727349175\n",
      "step: 682364, loss: 0.06397511065006256, data time: 0.01231250010038677\n",
      "step: 682365, loss: 0.05831605941057205, data time: 0.011819112300872802\n",
      "step: 682366, loss: 0.06252899765968323, data time: 0.011374700637090774\n",
      "step: 682367, loss: 0.06223044544458389, data time: 0.010971557010303844\n",
      "step: 682368, loss: 0.06324970722198486, data time: 0.010599177816639776\n",
      "step: 682369, loss: 0.06268072128295898, data time: 0.01025607188542684\n",
      "step: 682370, loss: 0.06022573634982109, data time: 0.009945087432861328\n",
      "step: 682371, loss: 0.062310826033353806, data time: 0.00964509523831881\n",
      "step: 682372, loss: 0.06168222427368164, data time: 0.009361637963189019\n",
      "step: 682373, loss: 0.06124431639909744, data time: 0.009098299912043981\n",
      "step: 682374, loss: 0.06206037849187851, data time: 0.00885890270101613\n",
      "step: 682375, loss: 0.05813948065042496, data time: 0.008638898531595865\n",
      "step: 682376, loss: 0.05931608006358147, data time: 0.008434803255142705\n",
      "step: 682377, loss: 0.06372978538274765, data time: 0.008240804076194763\n",
      "step: 682378, loss: 0.0643087774515152, data time: 0.008048454920450846\n",
      "step: 682379, loss: 0.0638558566570282, data time: 0.007869496065027574\n",
      "step: 682380, loss: 0.0635203868150711, data time: 0.0077021803174700055\n",
      "step: 682381, loss: 0.06057394668459892, data time: 0.007539033889770508\n",
      "step: 682382, loss: 0.06362462788820267, data time: 0.007388166479162268\n",
      "step: 682383, loss: 0.05993124097585678, data time: 0.0072482071424785416\n",
      "step: 682384, loss: 0.06680823862552643, data time: 0.007117283649933644\n",
      "step: 682385, loss: 0.04687303304672241, data time: 0.00699116587638855\n",
      "step: 682386, loss: 0.06571289151906967, data time: 0.19701766967773438\n",
      "step: 682387, loss: 0.0625120997428894, data time: 0.09926223754882812\n",
      "step: 682388, loss: 0.06550276279449463, data time: 0.06668456395467122\n",
      "step: 682389, loss: 0.05615701898932457, data time: 0.050797224044799805\n",
      "step: 682390, loss: 0.07067592442035675, data time: 0.04094676971435547\n",
      "step: 682391, loss: 0.06254974752664566, data time: 0.03439128398895264\n",
      "step: 682392, loss: 0.06112890690565109, data time: 0.02969707761492048\n",
      "step: 682393, loss: 0.062213487923145294, data time: 0.026240557432174683\n",
      "step: 682394, loss: 0.06454899162054062, data time: 0.02347850799560547\n",
      "step: 682395, loss: 0.0606592521071434, data time: 0.02135512828826904\n",
      "step: 682396, loss: 0.06184897944331169, data time: 0.019612810828469017\n",
      "step: 682397, loss: 0.060565631836652756, data time: 0.018168091773986816\n",
      "step: 682398, loss: 0.06025262922048569, data time: 0.01693903482877291\n",
      "step: 682399, loss: 0.0633762925863266, data time: 0.01587378978729248\n",
      "step: 682400, loss: 0.062305402010679245, data time: 0.014960050582885742\n",
      "step: 682401, loss: 0.05813216418027878, data time: 0.014159679412841797\n",
      "step: 682402, loss: 0.0644960030913353, data time: 0.013448925579295438\n",
      "step: 682403, loss: 0.06117066368460655, data time: 0.012815356254577637\n",
      "step: 682404, loss: 0.0597434937953949, data time: 0.012251741007754677\n",
      "step: 682405, loss: 0.06441530585289001, data time: 0.011747896671295166\n",
      "step: 682406, loss: 0.06907281279563904, data time: 0.011304900759742373\n",
      "step: 682407, loss: 0.05985357612371445, data time: 0.010890169577165083\n",
      "step: 682408, loss: 0.06015831232070923, data time: 0.010506598845772121\n",
      "step: 682409, loss: 0.06076415628194809, data time: 0.010161866744359335\n",
      "step: 682410, loss: 0.06384945660829544, data time: 0.00984614372253418\n",
      "step: 682411, loss: 0.06862007826566696, data time: 0.00954594978919396\n",
      "step: 682412, loss: 0.05934375524520874, data time: 0.009263647927178277\n",
      "step: 682413, loss: 0.06106167659163475, data time: 0.009003690310886927\n",
      "step: 682414, loss: 0.057790957391262054, data time: 0.008766971785446694\n",
      "step: 682415, loss: 0.06711004674434662, data time: 0.008547377586364747\n",
      "step: 682416, loss: 0.06276136636734009, data time: 0.008347257491080992\n",
      "step: 682417, loss: 0.05969768390059471, data time: 0.00815793126821518\n",
      "step: 682418, loss: 0.06211360543966293, data time: 0.007971301223292496\n",
      "step: 682419, loss: 0.05735967308282852, data time: 0.00779571252710679\n",
      "step: 682420, loss: 0.06386687606573105, data time: 0.007629939488002232\n",
      "step: 682421, loss: 0.06108894199132919, data time: 0.0074688394864400225\n",
      "step: 682422, loss: 0.06637261062860489, data time: 0.0073189606537690036\n",
      "step: 682423, loss: 0.05782414600253105, data time: 0.007188514659279271\n",
      "step: 682424, loss: 0.060298532247543335, data time: 0.007067631452511518\n",
      "step: 682425, loss: 0.07226492464542389, data time: 0.006944233179092407\n",
      "step: 682426, loss: 0.06449401378631592, data time: 0.19814729690551758\n",
      "step: 682427, loss: 0.0614946112036705, data time: 0.10046148300170898\n",
      "step: 682428, loss: 0.0593884214758873, data time: 0.0674890677134196\n",
      "step: 682429, loss: 0.06259762495756149, data time: 0.051494598388671875\n",
      "step: 682430, loss: 0.06633491814136505, data time: 0.04147782325744629\n",
      "step: 682431, loss: 0.06213199719786644, data time: 0.0348207950592041\n",
      "step: 682432, loss: 0.06047328934073448, data time: 0.030065161841256276\n",
      "step: 682433, loss: 0.06100864335894585, data time: 0.02657473087310791\n",
      "step: 682434, loss: 0.06148593872785568, data time: 0.023771497938368056\n",
      "step: 682435, loss: 0.05889318510890007, data time: 0.02159605026245117\n",
      "step: 682436, loss: 0.0659070685505867, data time: 0.019823984666304154\n",
      "step: 682437, loss: 0.06916987895965576, data time: 0.018350402514139812\n",
      "step: 682438, loss: 0.06530635058879852, data time: 0.017114675962007962\n",
      "step: 682439, loss: 0.06646758317947388, data time: 0.016038741384233748\n",
      "step: 682440, loss: 0.06500586867332458, data time: 0.015112749735514323\n",
      "step: 682441, loss: 0.06426737457513809, data time: 0.014296993613243103\n",
      "step: 682442, loss: 0.060144007205963135, data time: 0.013579985674689798\n",
      "step: 682443, loss: 0.07435731589794159, data time: 0.012932976086934408\n",
      "step: 682444, loss: 0.059898726642131805, data time: 0.01235693379452354\n",
      "step: 682445, loss: 0.06465169787406921, data time: 0.011851942539215088\n",
      "step: 682446, loss: 0.060203827917575836, data time: 0.011408090591430664\n",
      "step: 682447, loss: 0.06320618093013763, data time: 0.010991009798916903\n",
      "step: 682448, loss: 0.06898810714483261, data time: 0.010602422382520594\n",
      "step: 682449, loss: 0.06622719764709473, data time: 0.010252147912979126\n",
      "step: 682450, loss: 0.05864829942584038, data time: 0.009927444458007813\n",
      "step: 682451, loss: 0.06264253705739975, data time: 0.009628846095158504\n",
      "step: 682452, loss: 0.06108572334051132, data time: 0.009345337196632667\n",
      "step: 682453, loss: 0.06681953370571136, data time: 0.009086898394993373\n",
      "step: 682454, loss: 0.06101998686790466, data time: 0.008846817345454776\n",
      "step: 682455, loss: 0.0656820610165596, data time: 0.008621851603190104\n",
      "step: 682456, loss: 0.064220130443573, data time: 0.00841199198076802\n",
      "step: 682457, loss: 0.06616088002920151, data time: 0.00822388380765915\n",
      "step: 682458, loss: 0.060675472021102905, data time: 0.008036114952780983\n",
      "step: 682459, loss: 0.0625462755560875, data time: 0.007860955070046818\n",
      "step: 682460, loss: 0.062425460666418076, data time: 0.007694734845842634\n",
      "step: 682461, loss: 0.05978863686323166, data time: 0.007533742321862115\n",
      "step: 682462, loss: 0.06305234134197235, data time: 0.007381832277452624\n",
      "step: 682463, loss: 0.06189948320388794, data time: 0.007240847537392064\n",
      "step: 682464, loss: 0.06929916143417358, data time: 0.00710756350786258\n",
      "step: 682465, loss: 0.060101449489593506, data time: 0.006980407238006592\n",
      "step: 682466, loss: 0.0622895322740078, data time: 0.18703103065490723\n",
      "step: 682467, loss: 0.06341356039047241, data time: 0.09431517124176025\n",
      "step: 682468, loss: 0.06533733010292053, data time: 0.06381479899088542\n",
      "step: 682469, loss: 0.06078116595745087, data time: 0.04875272512435913\n",
      "step: 682470, loss: 0.060389913618564606, data time: 0.03929305076599121\n",
      "step: 682471, loss: 0.05846099182963371, data time: 0.03298914432525635\n",
      "step: 682472, loss: 0.05490192398428917, data time: 0.02849735532488142\n",
      "step: 682473, loss: 0.0637752115726471, data time: 0.025202274322509766\n",
      "step: 682474, loss: 0.060774095356464386, data time: 0.022553205490112305\n",
      "step: 682475, loss: 0.0632767602801323, data time: 0.02050189971923828\n",
      "step: 682476, loss: 0.06561070680618286, data time: 0.018831036307594994\n",
      "step: 682477, loss: 0.05708525702357292, data time: 0.01744409402211507\n",
      "step: 682478, loss: 0.05754190683364868, data time: 0.016266749455378607\n",
      "step: 682479, loss: 0.07031648606061935, data time: 0.015250018664768763\n",
      "step: 682480, loss: 0.06919028609991074, data time: 0.014386828740437825\n",
      "step: 682481, loss: 0.05283796787261963, data time: 0.013622626662254333\n",
      "step: 682482, loss: 0.055049534887075424, data time: 0.012950518551994772\n",
      "step: 682483, loss: 0.05987502261996269, data time: 0.012343207995096842\n",
      "step: 682484, loss: 0.061124786734580994, data time: 0.011803012145193\n",
      "step: 682485, loss: 0.0661802887916565, data time: 0.011321794986724854\n",
      "step: 682486, loss: 0.06542874127626419, data time: 0.010889359882899694\n",
      "step: 682487, loss: 0.06393074244260788, data time: 0.010501785711808638\n",
      "step: 682488, loss: 0.061259012669324875, data time: 0.010158020517100458\n",
      "step: 682489, loss: 0.06187590956687927, data time: 0.009828706582387289\n",
      "step: 682490, loss: 0.06676220148801804, data time: 0.009524707794189452\n",
      "step: 682491, loss: 0.06995802372694016, data time: 0.009239150927617\n",
      "step: 682492, loss: 0.06854181736707687, data time: 0.008971064179031938\n",
      "step: 682493, loss: 0.05951844900846481, data time: 0.008723267487117223\n",
      "step: 682494, loss: 0.06473574042320251, data time: 0.008499951198183257\n",
      "step: 682495, loss: 0.060159023851156235, data time: 0.008289178212483725\n",
      "step: 682496, loss: 0.0625312402844429, data time: 0.008091019045922064\n",
      "step: 682497, loss: 0.05816718935966492, data time: 0.007909975945949554\n",
      "step: 682498, loss: 0.0648428276181221, data time: 0.007730036070852569\n",
      "step: 682499, loss: 0.06271837651729584, data time: 0.007561915061053108\n",
      "step: 682500, loss: 0.06087794527411461, data time: 0.007403039932250976\n",
      "step: 682501, loss: 0.05940406024456024, data time: 0.007248693042331272\n",
      "step: 682502, loss: 0.0640086680650711, data time: 0.007105994868922878\n",
      "step: 682503, loss: 0.058214813470840454, data time: 0.006973774809586375\n",
      "step: 682504, loss: 0.06270177662372589, data time: 0.006850505486512795\n",
      "step: 682505, loss: 0.052759699523448944, data time: 0.006730479001998901\n",
      "step: 682506, loss: 0.05944273993372917, data time: 0.1897718906402588\n",
      "step: 682507, loss: 0.06019015237689018, data time: 0.09566152095794678\n",
      "step: 682508, loss: 0.06319388002157211, data time: 0.06472325325012207\n",
      "step: 682509, loss: 0.06240040063858032, data time: 0.049312591552734375\n",
      "step: 682510, loss: 0.060857661068439484, data time: 0.03974294662475586\n",
      "step: 682511, loss: 0.0654459148645401, data time: 0.0333861509958903\n",
      "step: 682512, loss: 0.057202812284231186, data time: 0.028830698558262417\n",
      "step: 682513, loss: 0.06563728302717209, data time: 0.025495022535324097\n",
      "step: 682514, loss: 0.06375820934772491, data time: 0.02280865775214301\n",
      "step: 682515, loss: 0.06530043482780457, data time: 0.020728063583374024\n",
      "step: 682516, loss: 0.059497520327568054, data time: 0.019036856564608486\n",
      "step: 682517, loss: 0.056273527443408966, data time: 0.0176277756690979\n",
      "step: 682518, loss: 0.06348003447055817, data time: 0.016437017000638522\n",
      "step: 682519, loss: 0.055619724094867706, data time: 0.015410491398402624\n",
      "step: 682520, loss: 0.0683353990316391, data time: 0.0145233949025472\n",
      "step: 682521, loss: 0.0628211721777916, data time: 0.013748973608016968\n",
      "step: 682522, loss: 0.06301498413085938, data time: 0.013056222130270564\n",
      "step: 682523, loss: 0.06267206370830536, data time: 0.012439899974399142\n",
      "step: 682524, loss: 0.06307527422904968, data time: 0.011892971239591899\n",
      "step: 682525, loss: 0.06385986506938934, data time: 0.011409664154052734\n",
      "step: 682526, loss: 0.06430675089359283, data time: 0.01097001348223005\n",
      "step: 682527, loss: 0.057604461908340454, data time: 0.010569767518477007\n",
      "step: 682528, loss: 0.059944264590740204, data time: 0.010201039521590523\n",
      "step: 682529, loss: 0.06633009016513824, data time: 0.009868015845616659\n",
      "step: 682530, loss: 0.06578560173511505, data time: 0.009561214447021484\n",
      "step: 682531, loss: 0.06252777576446533, data time: 0.009273932530329777\n",
      "step: 682532, loss: 0.06256631761789322, data time: 0.009005723176179108\n",
      "step: 682533, loss: 0.06259362399578094, data time: 0.008761874267033168\n",
      "step: 682534, loss: 0.06049167364835739, data time: 0.008532589879529229\n",
      "step: 682535, loss: 0.06405209749937057, data time: 0.008321809768676757\n",
      "step: 682536, loss: 0.05816064029932022, data time: 0.008123190172256963\n",
      "step: 682537, loss: 0.05790866166353226, data time: 0.007941097021102905\n",
      "step: 682538, loss: 0.06166468560695648, data time: 0.007765098051591353\n",
      "step: 682539, loss: 0.06452743709087372, data time: 0.007596983629114488\n",
      "step: 682540, loss: 0.0588613823056221, data time: 0.007436636516026088\n",
      "step: 682541, loss: 0.06903474777936935, data time: 0.007282601462470161\n",
      "step: 682542, loss: 0.06287035346031189, data time: 0.007139611888576198\n",
      "step: 682543, loss: 0.057743437588214874, data time: 0.007005766818397923\n",
      "step: 682544, loss: 0.061605148017406464, data time: 0.006881573261358799\n",
      "step: 682545, loss: 0.06237306445837021, data time: 0.006760048866271973\n",
      "step: 682546, loss: 0.060768600553274155, data time: 0.1931905746459961\n",
      "step: 682547, loss: 0.06046980619430542, data time: 0.09738397598266602\n",
      "step: 682548, loss: 0.06268328428268433, data time: 0.06607286135355632\n",
      "step: 682549, loss: 0.06591211259365082, data time: 0.05022168159484863\n",
      "step: 682550, loss: 0.06323565542697906, data time: 0.040461397171020506\n",
      "step: 682551, loss: 0.059488579630851746, data time: 0.033974289894104004\n",
      "step: 682552, loss: 0.060445718467235565, data time: 0.029346602303641185\n",
      "step: 682553, loss: 0.06448402255773544, data time: 0.025937765836715698\n",
      "step: 682554, loss: 0.06106042116880417, data time: 0.02321502897474501\n",
      "step: 682555, loss: 0.06407871842384338, data time: 0.021096563339233397\n",
      "step: 682556, loss: 0.060837700963020325, data time: 0.01937491243535822\n",
      "step: 682557, loss: 0.06363783031702042, data time: 0.017939507961273193\n",
      "step: 682558, loss: 0.06351740658283234, data time: 0.01672530174255371\n",
      "step: 682559, loss: 0.05959431827068329, data time: 0.015681096485682895\n",
      "step: 682560, loss: 0.06020168215036392, data time: 0.014777358373006184\n",
      "step: 682561, loss: 0.05358903855085373, data time: 0.013987049460411072\n",
      "step: 682562, loss: 0.05818028375506401, data time: 0.013289732091567096\n",
      "step: 682563, loss: 0.05877828598022461, data time: 0.012664318084716797\n",
      "step: 682564, loss: 0.06982995569705963, data time: 0.012113495876914576\n",
      "step: 682565, loss: 0.060575224459171295, data time: 0.011619460582733155\n",
      "step: 682566, loss: 0.0606866180896759, data time: 0.011170739219302223\n",
      "step: 682567, loss: 0.06207144260406494, data time: 0.010760849172418768\n",
      "step: 682568, loss: 0.06533100455999374, data time: 0.010388218838235607\n",
      "step: 682569, loss: 0.05962406098842621, data time: 0.010046889384587606\n",
      "step: 682570, loss: 0.05718287453055382, data time: 0.00973388671875\n",
      "step: 682571, loss: 0.06441446393728256, data time: 0.009437799453735352\n",
      "step: 682572, loss: 0.06831151247024536, data time: 0.009162134594387479\n",
      "step: 682573, loss: 0.05694996565580368, data time: 0.00890782049724034\n",
      "step: 682574, loss: 0.05627576634287834, data time: 0.008675443715062636\n",
      "step: 682575, loss: 0.06888635456562042, data time: 0.008460052808125814\n",
      "step: 682576, loss: 0.06357300281524658, data time: 0.00825707374080535\n",
      "step: 682577, loss: 0.05698566883802414, data time: 0.00807565450668335\n",
      "step: 682578, loss: 0.060205601155757904, data time: 0.007895093975645123\n",
      "step: 682579, loss: 0.062489792704582214, data time: 0.007723520783817067\n",
      "step: 682580, loss: 0.056518785655498505, data time: 0.007560382570539202\n",
      "step: 682581, loss: 0.06155127286911011, data time: 0.007405625449286567\n",
      "step: 682582, loss: 0.06580164283514023, data time: 0.007257139360582507\n",
      "step: 682583, loss: 0.061185579746961594, data time: 0.0071205214450233865\n",
      "step: 682584, loss: 0.0602242611348629, data time: 0.006990175980788011\n",
      "step: 682585, loss: 0.056236863136291504, data time: 0.006865692138671875\n",
      "step: 682586, loss: 0.06098126992583275, data time: 0.19115591049194336\n",
      "step: 682587, loss: 0.065495066344738, data time: 0.0967099666595459\n",
      "step: 682588, loss: 0.060128383338451385, data time: 0.06537445386250813\n",
      "step: 682589, loss: 0.0586622916162014, data time: 0.049897730350494385\n",
      "step: 682590, loss: 0.05800223350524902, data time: 0.04019894599914551\n",
      "step: 682591, loss: 0.0647958368062973, data time: 0.03375411033630371\n",
      "step: 682592, loss: 0.06091614067554474, data time: 0.029163156236921037\n",
      "step: 682593, loss: 0.054034508764743805, data time: 0.025771677494049072\n",
      "step: 682594, loss: 0.05739164352416992, data time: 0.023075527615017362\n",
      "step: 682595, loss: 0.06677543371915817, data time: 0.020967841148376465\n",
      "step: 682596, loss: 0.062027253210544586, data time: 0.019258455796675247\n",
      "step: 682597, loss: 0.06354217976331711, data time: 0.017828524112701416\n",
      "step: 682598, loss: 0.06109435483813286, data time: 0.016623423649714544\n",
      "step: 682599, loss: 0.05989380180835724, data time: 0.015589594841003418\n",
      "step: 682600, loss: 0.06445127725601196, data time: 0.014692227045694986\n",
      "step: 682601, loss: 0.0637122318148613, data time: 0.01391264796257019\n",
      "step: 682602, loss: 0.058991074562072754, data time: 0.013223774292889763\n",
      "step: 682603, loss: 0.05982547625899315, data time: 0.012602567672729492\n",
      "step: 682604, loss: 0.057394783943891525, data time: 0.012047554317273591\n",
      "step: 682605, loss: 0.061763495206832886, data time: 0.01155155897140503\n",
      "step: 682606, loss: 0.06164299696683884, data time: 0.011104992457798548\n",
      "step: 682607, loss: 0.05784228444099426, data time: 0.010699781504544344\n",
      "step: 682608, loss: 0.06196422874927521, data time: 0.010325079378874405\n",
      "step: 682609, loss: 0.06645353138446808, data time: 0.009988417228062948\n",
      "step: 682610, loss: 0.06537960469722748, data time: 0.009677667617797852\n",
      "step: 682611, loss: 0.06137404963374138, data time: 0.009384879699120155\n",
      "step: 682612, loss: 0.05912235379219055, data time: 0.009109965077152959\n",
      "step: 682613, loss: 0.06408996880054474, data time: 0.008859166077205114\n",
      "step: 682614, loss: 0.06135430932044983, data time: 0.008630094857051455\n",
      "step: 682615, loss: 0.05390950292348862, data time: 0.008415937423706055\n",
      "step: 682616, loss: 0.05970676988363266, data time: 0.008213819996003182\n",
      "step: 682617, loss: 0.06734156608581543, data time: 0.008028298616409302\n",
      "step: 682618, loss: 0.06219923496246338, data time: 0.007846904523444899\n",
      "step: 682619, loss: 0.0566408596932888, data time: 0.007675984326530905\n",
      "step: 682620, loss: 0.061467718333005905, data time: 0.007515491758074079\n",
      "step: 682621, loss: 0.06118598207831383, data time: 0.0073657168282402884\n",
      "step: 682622, loss: 0.06842109560966492, data time: 0.007217433001544024\n",
      "step: 682623, loss: 0.06635677814483643, data time: 0.007084244175961143\n",
      "step: 682624, loss: 0.06057333946228027, data time: 0.006954847238002679\n",
      "step: 682625, loss: 0.06799934804439545, data time: 0.006832706928253174\n",
      "step: 682626, loss: 0.06149263679981232, data time: 0.2040386199951172\n",
      "step: 682627, loss: 0.06269876658916473, data time: 0.1028294563293457\n",
      "step: 682628, loss: 0.06134914606809616, data time: 0.06970945994059245\n",
      "step: 682629, loss: 0.062438882887363434, data time: 0.05296701192855835\n",
      "step: 682630, loss: 0.06402289867401123, data time: 0.04265308380126953\n",
      "step: 682631, loss: 0.06219887733459473, data time: 0.035808444023132324\n",
      "step: 682632, loss: 0.05972663313150406, data time: 0.03091076442173549\n",
      "step: 682633, loss: 0.060585908591747284, data time: 0.027350246906280518\n",
      "step: 682634, loss: 0.06730477511882782, data time: 0.0244954162173801\n",
      "step: 682635, loss: 0.06086950749158859, data time: 0.02228529453277588\n",
      "step: 682636, loss: 0.06046439707279205, data time: 0.02048912915316495\n",
      "step: 682637, loss: 0.0558730773627758, data time: 0.018998881181081135\n",
      "step: 682638, loss: 0.06276345252990723, data time: 0.017732987037071817\n",
      "step: 682639, loss: 0.05608724057674408, data time: 0.016638074602399553\n",
      "step: 682640, loss: 0.061837464570999146, data time: 0.015700229008992515\n",
      "step: 682641, loss: 0.060313425958156586, data time: 0.014884129166603088\n",
      "step: 682642, loss: 0.06015161797404289, data time: 0.01414642614476821\n",
      "step: 682643, loss: 0.05904782935976982, data time: 0.013488835758633085\n",
      "step: 682644, loss: 0.06360195577144623, data time: 0.012908709676642167\n",
      "step: 682645, loss: 0.06064197048544884, data time: 0.012386417388916016\n",
      "step: 682646, loss: 0.06360732018947601, data time: 0.01192028181893485\n",
      "step: 682647, loss: 0.062191545963287354, data time: 0.011493910442699085\n",
      "step: 682648, loss: 0.061206214129924774, data time: 0.011099017184713612\n",
      "step: 682649, loss: 0.05910945683717728, data time: 0.01074512799580892\n",
      "step: 682650, loss: 0.06416331231594086, data time: 0.010416698455810548\n",
      "step: 682651, loss: 0.05699261650443077, data time: 0.010109287041884202\n",
      "step: 682652, loss: 0.06772983074188232, data time: 0.009819869641904478\n",
      "step: 682653, loss: 0.060760416090488434, data time: 0.009554522378104073\n",
      "step: 682654, loss: 0.06258386373519897, data time: 0.009311914443969727\n",
      "step: 682655, loss: 0.06442919373512268, data time: 0.00908515453338623\n",
      "step: 682656, loss: 0.06304296851158142, data time: 0.008873170421969506\n",
      "step: 682657, loss: 0.06534315645694733, data time: 0.008676901459693909\n",
      "step: 682658, loss: 0.06266181915998459, data time: 0.008478475339484938\n",
      "step: 682659, loss: 0.059430427849292755, data time: 0.008291763417861042\n",
      "step: 682660, loss: 0.05827442929148674, data time: 0.008113745280674526\n",
      "step: 682661, loss: 0.06395210325717926, data time: 0.007942067252265083\n",
      "step: 682662, loss: 0.0641048476099968, data time: 0.00778318740226127\n",
      "step: 682663, loss: 0.062014877796173096, data time: 0.007635605962652909\n",
      "step: 682664, loss: 0.06494379043579102, data time: 0.007495757861015124\n",
      "step: 682665, loss: 0.07917013764381409, data time: 0.007362973690032959\n",
      "step: 682666, loss: 0.05868062376976013, data time: 0.20394349098205566\n",
      "step: 682667, loss: 0.06941528618335724, data time: 0.10336446762084961\n",
      "step: 682668, loss: 0.05772947892546654, data time: 0.06981968879699707\n",
      "step: 682669, loss: 0.06518816947937012, data time: 0.05314546823501587\n",
      "step: 682670, loss: 0.06490807980298996, data time: 0.042802286148071286\n",
      "step: 682671, loss: 0.06528552621603012, data time: 0.035932699839274086\n",
      "step: 682672, loss: 0.06697048246860504, data time: 0.03104077066693987\n",
      "step: 682673, loss: 0.06456708908081055, data time: 0.02741888165473938\n",
      "step: 682674, loss: 0.06047997996211052, data time: 0.024533881081475153\n",
      "step: 682675, loss: 0.06038723513484001, data time: 0.02232990264892578\n",
      "step: 682676, loss: 0.06057627126574516, data time: 0.02049311724576083\n",
      "step: 682677, loss: 0.06538456678390503, data time: 0.01896357536315918\n",
      "step: 682678, loss: 0.06925339996814728, data time: 0.017674024288470928\n",
      "step: 682679, loss: 0.05654759332537651, data time: 0.01655834061758859\n",
      "step: 682680, loss: 0.06269025802612305, data time: 0.015602334340413412\n",
      "step: 682681, loss: 0.0606936514377594, data time: 0.01476481556892395\n",
      "step: 682682, loss: 0.05562717467546463, data time: 0.014019082574283375\n",
      "step: 682683, loss: 0.058045607060194016, data time: 0.013364778624640571\n",
      "step: 682684, loss: 0.06468865275382996, data time: 0.012768268585205078\n",
      "step: 682685, loss: 0.0607156865298748, data time: 0.012240958213806153\n",
      "step: 682686, loss: 0.060908716171979904, data time: 0.011758134478614443\n",
      "step: 682687, loss: 0.058237843215465546, data time: 0.011322823437777433\n",
      "step: 682688, loss: 0.06215957924723625, data time: 0.010921270950980808\n",
      "step: 682689, loss: 0.05214356631040573, data time: 0.01055606206258138\n",
      "step: 682690, loss: 0.06008462607860565, data time: 0.010223608016967773\n",
      "step: 682691, loss: 0.05931883305311203, data time: 0.009914425703195425\n",
      "step: 682692, loss: 0.06354767084121704, data time: 0.009630300380565502\n",
      "step: 682693, loss: 0.06309430301189423, data time: 0.009362987109592982\n",
      "step: 682694, loss: 0.06530120223760605, data time: 0.009115465756120354\n",
      "step: 682695, loss: 0.06204017251729965, data time: 0.008883039156595865\n",
      "step: 682696, loss: 0.0597420297563076, data time: 0.00866617694977791\n",
      "step: 682697, loss: 0.05972987413406372, data time: 0.008467338979244232\n",
      "step: 682698, loss: 0.0638180822134018, data time: 0.008272727330525717\n",
      "step: 682699, loss: 0.06262068450450897, data time: 0.008090369841631721\n",
      "step: 682700, loss: 0.062423449009656906, data time: 0.007916055406842913\n",
      "step: 682701, loss: 0.06701379269361496, data time: 0.007749716440836589\n",
      "step: 682702, loss: 0.06205596774816513, data time: 0.007591969258076436\n",
      "step: 682703, loss: 0.06111558526754379, data time: 0.007449175182141755\n",
      "step: 682704, loss: 0.059473633766174316, data time: 0.007312835791172125\n",
      "step: 682705, loss: 0.07008975744247437, data time: 0.007184624671936035\n",
      "step: 682706, loss: 0.06087280437350273, data time: 0.19412517547607422\n",
      "step: 682707, loss: 0.06274031102657318, data time: 0.09843897819519043\n",
      "step: 682708, loss: 0.060669153928756714, data time: 0.06652148564656575\n",
      "step: 682709, loss: 0.06055751070380211, data time: 0.050662457942962646\n",
      "step: 682710, loss: 0.06480275094509125, data time: 0.04081006050109863\n",
      "step: 682711, loss: 0.06301093101501465, data time: 0.03426508108774821\n",
      "step: 682712, loss: 0.06455014646053314, data time: 0.029593093054635183\n",
      "step: 682713, loss: 0.06058977171778679, data time: 0.026166170835494995\n",
      "step: 682714, loss: 0.05508246272802353, data time: 0.02341612180074056\n",
      "step: 682715, loss: 0.06012670323252678, data time: 0.02127397060394287\n",
      "step: 682716, loss: 0.06333496421575546, data time: 0.01954109018499201\n",
      "step: 682717, loss: 0.06011619418859482, data time: 0.018092532952626545\n",
      "step: 682718, loss: 0.05619640275835991, data time: 0.01687145233154297\n",
      "step: 682719, loss: 0.06607311964035034, data time: 0.015810455594744\n",
      "step: 682720, loss: 0.059500228613615036, data time: 0.014901145299275716\n",
      "step: 682721, loss: 0.05483448505401611, data time: 0.014109700918197632\n",
      "step: 682722, loss: 0.06262172013521194, data time: 0.013404271181891947\n",
      "step: 682723, loss: 0.05946695804595947, data time: 0.012778878211975098\n",
      "step: 682724, loss: 0.06638383865356445, data time: 0.012217521667480469\n",
      "step: 682725, loss: 0.058917462825775146, data time: 0.011719143390655518\n",
      "step: 682726, loss: 0.06300555914640427, data time: 0.011274939491635277\n",
      "step: 682727, loss: 0.05812329053878784, data time: 0.010865536603060636\n",
      "step: 682728, loss: 0.06216476857662201, data time: 0.010483731394228727\n",
      "step: 682729, loss: 0.06012833118438721, data time: 0.01014035940170288\n",
      "step: 682730, loss: 0.051363926380872726, data time: 0.00982168197631836\n",
      "step: 682731, loss: 0.06119975447654724, data time: 0.00952307994549091\n",
      "step: 682732, loss: 0.06463363021612167, data time: 0.009246031443277994\n",
      "step: 682733, loss: 0.05496513098478317, data time: 0.008986958435603551\n",
      "step: 682734, loss: 0.059877924621105194, data time: 0.00875593876016551\n",
      "step: 682735, loss: 0.06168143451213837, data time: 0.008538039525349934\n",
      "step: 682736, loss: 0.0623326450586319, data time: 0.00833336768611785\n",
      "step: 682737, loss: 0.06008113548159599, data time: 0.008146658539772034\n",
      "step: 682738, loss: 0.06084063649177551, data time: 0.007963556231874409\n",
      "step: 682739, loss: 0.07026667892932892, data time: 0.007790965192458209\n",
      "step: 682740, loss: 0.05931926891207695, data time: 0.007624830518450055\n",
      "step: 682741, loss: 0.05870116874575615, data time: 0.007466839419470893\n",
      "step: 682742, loss: 0.06534872949123383, data time: 0.0073167826678301835\n",
      "step: 682743, loss: 0.0677974671125412, data time: 0.007178614014073422\n",
      "step: 682744, loss: 0.06244304031133652, data time: 0.007047769350883288\n",
      "step: 682745, loss: 0.05044537037611008, data time: 0.006923079490661621\n",
      "step: 682746, loss: 0.0625031590461731, data time: 0.18743491172790527\n",
      "step: 682747, loss: 0.058794330805540085, data time: 0.09549784660339355\n",
      "step: 682748, loss: 0.06137242913246155, data time: 0.06418601671854655\n",
      "step: 682749, loss: 0.05806756764650345, data time: 0.04893171787261963\n",
      "step: 682750, loss: 0.05877663940191269, data time: 0.03942551612854004\n",
      "step: 682751, loss: 0.06079540774226189, data time: 0.03312516212463379\n",
      "step: 682752, loss: 0.06137443333864212, data time: 0.02860736846923828\n",
      "step: 682753, loss: 0.061491526663303375, data time: 0.025305867195129395\n",
      "step: 682754, loss: 0.05773913115262985, data time: 0.02264947361416287\n",
      "step: 682755, loss: 0.06266182661056519, data time: 0.020584893226623536\n",
      "step: 682756, loss: 0.05916706845164299, data time: 0.018911881880326706\n",
      "step: 682757, loss: 0.06272487342357635, data time: 0.01751643419265747\n",
      "step: 682758, loss: 0.06258231401443481, data time: 0.016340274077195387\n",
      "step: 682759, loss: 0.06075947731733322, data time: 0.015314579010009766\n",
      "step: 682760, loss: 0.06392356753349304, data time: 0.014435927073160807\n",
      "step: 682761, loss: 0.06268943846225739, data time: 0.013672903180122375\n",
      "step: 682762, loss: 0.06169111654162407, data time: 0.012993798536412856\n",
      "step: 682763, loss: 0.06435653567314148, data time: 0.012383182843526205\n",
      "step: 682764, loss: 0.054636500775814056, data time: 0.011839929379914937\n",
      "step: 682765, loss: 0.06567692011594772, data time: 0.011357295513153075\n",
      "step: 682766, loss: 0.06486492604017258, data time: 0.010924248468308221\n",
      "step: 682767, loss: 0.05898192524909973, data time: 0.010524511337280273\n",
      "step: 682768, loss: 0.06020577996969223, data time: 0.010157097940859587\n",
      "step: 682769, loss: 0.06215482950210571, data time: 0.00982441504796346\n",
      "step: 682770, loss: 0.06284280121326447, data time: 0.009517536163330079\n",
      "step: 682771, loss: 0.06089494377374649, data time: 0.009231017186091496\n",
      "step: 682772, loss: 0.06747987866401672, data time: 0.008963540748313622\n",
      "step: 682773, loss: 0.0584932379424572, data time: 0.008718831198556083\n",
      "step: 682774, loss: 0.06947125494480133, data time: 0.008492214926357928\n",
      "step: 682775, loss: 0.06629042327404022, data time: 0.008281485239664713\n",
      "step: 682776, loss: 0.06636430323123932, data time: 0.008084604817052041\n",
      "step: 682777, loss: 0.05708518251776695, data time: 0.007902875542640686\n",
      "step: 682778, loss: 0.059610527008771896, data time: 0.00772496425744259\n",
      "step: 682779, loss: 0.06628790497779846, data time: 0.007558373843922335\n",
      "step: 682780, loss: 0.06090320274233818, data time: 0.007399061747959682\n",
      "step: 682781, loss: 0.06498587876558304, data time: 0.007244937949710422\n",
      "step: 682782, loss: 0.0621701143682003, data time: 0.007100395254186682\n",
      "step: 682783, loss: 0.05373018980026245, data time: 0.006966308543556615\n",
      "step: 682784, loss: 0.06946533918380737, data time: 0.006839091961200421\n",
      "step: 682785, loss: 0.04501361772418022, data time: 0.006719863414764405\n",
      "step: 682786, loss: 0.056645046919584274, data time: 0.20302629470825195\n",
      "step: 682787, loss: 0.06247221678495407, data time: 0.10229504108428955\n",
      "step: 682788, loss: 0.06494729965925217, data time: 0.0687103271484375\n",
      "step: 682789, loss: 0.06603838503360748, data time: 0.05228316783905029\n",
      "step: 682790, loss: 0.0617375522851944, data time: 0.042117738723754884\n",
      "step: 682791, loss: 0.057535119354724884, data time: 0.0353550910949707\n",
      "step: 682792, loss: 0.06556927412748337, data time: 0.030523061752319336\n",
      "step: 682793, loss: 0.062433838844299316, data time: 0.02696022391319275\n",
      "step: 682794, loss: 0.06649782508611679, data time: 0.024119589063856337\n",
      "step: 682795, loss: 0.06658968329429626, data time: 0.021920609474182128\n",
      "step: 682796, loss: 0.05959399789571762, data time: 0.020122311332009056\n",
      "step: 682797, loss: 0.0677800178527832, data time: 0.018639822800954182\n",
      "step: 682798, loss: 0.05889362096786499, data time: 0.01737823853125939\n",
      "step: 682799, loss: 0.05902865156531334, data time: 0.016282643590654646\n",
      "step: 682800, loss: 0.06175510212779045, data time: 0.015342346827189128\n",
      "step: 682801, loss: 0.05837072432041168, data time: 0.01451750099658966\n",
      "step: 682802, loss: 0.06259588897228241, data time: 0.013814631630392635\n",
      "step: 682803, loss: 0.06596152484416962, data time: 0.013157579633924697\n",
      "step: 682804, loss: 0.0617164671421051, data time: 0.012574108023392526\n",
      "step: 682805, loss: 0.05913185700774193, data time: 0.012059509754180908\n",
      "step: 682806, loss: 0.0680544525384903, data time: 0.011598019372849237\n",
      "step: 682807, loss: 0.06323929876089096, data time: 0.011169672012329102\n",
      "step: 682808, loss: 0.05824979394674301, data time: 0.01077376241269319\n",
      "step: 682809, loss: 0.059171564877033234, data time: 0.010414918263753256\n",
      "step: 682810, loss: 0.06249874085187912, data time: 0.010087337493896485\n",
      "step: 682811, loss: 0.06236594170331955, data time: 0.009778866401085487\n",
      "step: 682812, loss: 0.0663512647151947, data time: 0.009489174242372866\n",
      "step: 682813, loss: 0.06180719658732414, data time: 0.009225709097726005\n",
      "step: 682814, loss: 0.0641244500875473, data time: 0.008984171111008217\n",
      "step: 682815, loss: 0.05734248459339142, data time: 0.00875837008158366\n",
      "step: 682816, loss: 0.06303536146879196, data time: 0.008546275477255545\n",
      "step: 682817, loss: 0.05205538496375084, data time: 0.008353181183338165\n",
      "step: 682818, loss: 0.05636686086654663, data time: 0.008163972334428267\n",
      "step: 682819, loss: 0.0606280080974102, data time: 0.007985437617582433\n",
      "step: 682820, loss: 0.06317244470119476, data time: 0.007813419614519392\n",
      "step: 682821, loss: 0.06817138195037842, data time: 0.007647746139102512\n",
      "step: 682822, loss: 0.06417232006788254, data time: 0.0074926775854987065\n",
      "step: 682823, loss: 0.061584651470184326, data time: 0.007349572683635511\n",
      "step: 682824, loss: 0.06264419853687286, data time: 0.007214925228021083\n",
      "step: 682825, loss: 0.05180460214614868, data time: 0.007086598873138427\n",
      "step: 682826, loss: 0.06131267547607422, data time: 0.2020413875579834\n",
      "step: 682827, loss: 0.06192523241043091, data time: 0.10176348686218262\n",
      "step: 682828, loss: 0.06452235579490662, data time: 0.06891735394795735\n",
      "step: 682829, loss: 0.06093832105398178, data time: 0.05237692594528198\n",
      "step: 682830, loss: 0.0620700977742672, data time: 0.04218659400939941\n",
      "step: 682831, loss: 0.06198708340525627, data time: 0.035434842109680176\n",
      "step: 682832, loss: 0.06023011729121208, data time: 0.030611446925571988\n",
      "step: 682833, loss: 0.06533980369567871, data time: 0.027048557996749878\n",
      "step: 682834, loss: 0.06414799392223358, data time: 0.024195353190104168\n",
      "step: 682835, loss: 0.06379608809947968, data time: 0.021981406211853027\n",
      "step: 682836, loss: 0.06461630761623383, data time: 0.02017820965160023\n",
      "step: 682837, loss: 0.06370434165000916, data time: 0.018672784169514973\n",
      "step: 682838, loss: 0.06085186451673508, data time: 0.017410480059110202\n",
      "step: 682839, loss: 0.061913859099149704, data time: 0.01631244591304234\n",
      "step: 682840, loss: 0.06458011269569397, data time: 0.015372991561889648\n",
      "step: 682841, loss: 0.06829164177179337, data time: 0.014554083347320557\n",
      "step: 682842, loss: 0.06431921571493149, data time: 0.013828530031092027\n",
      "step: 682843, loss: 0.0626397356390953, data time: 0.013169487317403158\n",
      "step: 682844, loss: 0.060104481875896454, data time: 0.012585890920538651\n",
      "step: 682845, loss: 0.06014743074774742, data time: 0.012066566944122314\n",
      "step: 682846, loss: 0.05883694440126419, data time: 0.011594023023332869\n",
      "step: 682847, loss: 0.06420990824699402, data time: 0.011168631640347567\n",
      "step: 682848, loss: 0.061840157955884933, data time: 0.010772839836452318\n",
      "step: 682849, loss: 0.06392600387334824, data time: 0.010415236155192057\n",
      "step: 682850, loss: 0.05964052677154541, data time: 0.0100897216796875\n",
      "step: 682851, loss: 0.05801309272646904, data time: 0.009781287266657902\n",
      "step: 682852, loss: 0.058207519352436066, data time: 0.009492644557246455\n",
      "step: 682853, loss: 0.06691203266382217, data time: 0.009226560592651367\n",
      "step: 682854, loss: 0.06538406014442444, data time: 0.008984130004356647\n",
      "step: 682855, loss: 0.06842999160289764, data time: 0.008757241566975911\n",
      "step: 682856, loss: 0.06788411736488342, data time: 0.008546121658817414\n",
      "step: 682857, loss: 0.05925871059298515, data time: 0.008353933691978455\n",
      "step: 682858, loss: 0.06264150887727737, data time: 0.008166450442689838\n",
      "step: 682859, loss: 0.06710782647132874, data time: 0.007990339223076315\n",
      "step: 682860, loss: 0.06031394004821777, data time: 0.007818739754813057\n",
      "step: 682861, loss: 0.06321275234222412, data time: 0.00765452782313029\n",
      "step: 682862, loss: 0.060679417103528976, data time: 0.007499746374181799\n",
      "step: 682863, loss: 0.06146056950092316, data time: 0.007356398984005577\n",
      "step: 682864, loss: 0.06717380881309509, data time: 0.00721897834386581\n",
      "step: 682865, loss: 0.05314376577734947, data time: 0.007089364528656006\n",
      "step: 682866, loss: 0.06656157225370407, data time: 0.2081453800201416\n",
      "step: 682867, loss: 0.06558558344841003, data time: 0.10486865043640137\n",
      "step: 682868, loss: 0.06772586703300476, data time: 0.0704202651977539\n",
      "step: 682869, loss: 0.05523717403411865, data time: 0.053571999073028564\n",
      "step: 682870, loss: 0.05984678119421005, data time: 0.043140602111816403\n",
      "step: 682871, loss: 0.05624906346201897, data time: 0.03619162241617838\n",
      "step: 682872, loss: 0.06619365513324738, data time: 0.031241144452776228\n",
      "step: 682873, loss: 0.05994646996259689, data time: 0.027616232633590698\n",
      "step: 682874, loss: 0.05719427764415741, data time: 0.02470074759589301\n",
      "step: 682875, loss: 0.05721262842416763, data time: 0.022439742088317872\n",
      "step: 682876, loss: 0.06429632008075714, data time: 0.020613930442116478\n",
      "step: 682877, loss: 0.06900891661643982, data time: 0.019072989622751873\n",
      "step: 682878, loss: 0.06463595479726791, data time: 0.017772509501530573\n",
      "step: 682879, loss: 0.05579521879553795, data time: 0.016647730554853166\n",
      "step: 682880, loss: 0.061617206782102585, data time: 0.015682808558146157\n",
      "step: 682881, loss: 0.06132952496409416, data time: 0.014845922589302063\n",
      "step: 682882, loss: 0.06358390301465988, data time: 0.014109204797183765\n",
      "step: 682883, loss: 0.05903414636850357, data time: 0.0134429931640625\n",
      "step: 682884, loss: 0.059468843042850494, data time: 0.012845516204833984\n",
      "step: 682885, loss: 0.060340218245983124, data time: 0.012320125102996826\n",
      "step: 682886, loss: 0.06445884704589844, data time: 0.011835484277634393\n",
      "step: 682887, loss: 0.06385502219200134, data time: 0.011397394266995516\n",
      "step: 682888, loss: 0.06175154447555542, data time: 0.010992558106132175\n",
      "step: 682889, loss: 0.06571256369352341, data time: 0.010621865590413412\n",
      "step: 682890, loss: 0.060292214155197144, data time: 0.010283279418945312\n",
      "step: 682891, loss: 0.056056879460811615, data time: 0.0099715911425077\n",
      "step: 682892, loss: 0.05871089547872543, data time: 0.009683273456714771\n",
      "step: 682893, loss: 0.06465813517570496, data time: 0.009415711675371443\n",
      "step: 682894, loss: 0.06344779580831528, data time: 0.009167301243749159\n",
      "step: 682895, loss: 0.06675242632627487, data time: 0.00893403689066569\n",
      "step: 682896, loss: 0.061655811965465546, data time: 0.008716191014935893\n",
      "step: 682897, loss: 0.0625094398856163, data time: 0.008516825735569\n",
      "step: 682898, loss: 0.059009309858083725, data time: 0.008317405527288263\n",
      "step: 682899, loss: 0.05350977182388306, data time: 0.008135269669925465\n",
      "step: 682900, loss: 0.06579267233610153, data time: 0.00796757425580706\n",
      "step: 682901, loss: 0.06026240438222885, data time: 0.0078020162052578395\n",
      "step: 682902, loss: 0.060922738164663315, data time: 0.007647746318095439\n",
      "step: 682903, loss: 0.06143937632441521, data time: 0.007504676517687346\n",
      "step: 682904, loss: 0.06534911692142487, data time: 0.007368234487680288\n",
      "step: 682905, loss: 0.05270397290587425, data time: 0.007239401340484619\n",
      "step: 682906, loss: 0.06311514973640442, data time: 0.19683361053466797\n",
      "step: 682907, loss: 0.06717076152563095, data time: 0.09926557540893555\n",
      "step: 682908, loss: 0.060084205120801926, data time: 0.06668885548909505\n",
      "step: 682909, loss: 0.05516447126865387, data time: 0.05107182264328003\n",
      "step: 682910, loss: 0.06241397559642792, data time: 0.041213035583496094\n",
      "step: 682911, loss: 0.05928944796323776, data time: 0.034642418225606285\n",
      "step: 682912, loss: 0.06217791885137558, data time: 0.029947451182774136\n",
      "step: 682913, loss: 0.06270833313465118, data time: 0.026508331298828125\n",
      "step: 682914, loss: 0.058567024767398834, data time: 0.02374447716606988\n",
      "step: 682915, loss: 0.0662131980061531, data time: 0.021608352661132812\n",
      "step: 682916, loss: 0.06941540539264679, data time: 0.019877238707108932\n",
      "step: 682917, loss: 0.05470387265086174, data time: 0.018427809079488117\n",
      "step: 682918, loss: 0.06014643609523773, data time: 0.017202469018789437\n",
      "step: 682919, loss: 0.06420972943305969, data time: 0.016144088336399624\n",
      "step: 682920, loss: 0.05709262937307358, data time: 0.015236695607503256\n",
      "step: 682921, loss: 0.06332282721996307, data time: 0.014445871114730835\n",
      "step: 682922, loss: 0.07030009478330612, data time: 0.013742993859683765\n",
      "step: 682923, loss: 0.06550640612840652, data time: 0.013116770320468478\n",
      "step: 682924, loss: 0.06084625422954559, data time: 0.012549977553518195\n",
      "step: 682925, loss: 0.05921562761068344, data time: 0.012047135829925537\n",
      "step: 682926, loss: 0.07124805450439453, data time: 0.011594988050914946\n",
      "step: 682927, loss: 0.05804877728223801, data time: 0.01118415052240545\n",
      "step: 682928, loss: 0.05685454607009888, data time: 0.010802300080009129\n",
      "step: 682929, loss: 0.06516879796981812, data time: 0.010475059350331625\n",
      "step: 682930, loss: 0.058657996356487274, data time: 0.010158500671386718\n",
      "step: 682931, loss: 0.05854741856455803, data time: 0.009862908950218787\n",
      "step: 682932, loss: 0.06393586844205856, data time: 0.009588965663203487\n",
      "step: 682933, loss: 0.06432542949914932, data time: 0.009333278451647078\n",
      "step: 682934, loss: 0.05901414155960083, data time: 0.009100289180360991\n",
      "step: 682935, loss: 0.054413776844739914, data time: 0.00888230800628662\n",
      "step: 682936, loss: 0.0578746572136879, data time: 0.008680020609209615\n",
      "step: 682937, loss: 0.056358516216278076, data time: 0.008489564061164856\n",
      "step: 682938, loss: 0.06532111763954163, data time: 0.008297371141838305\n",
      "step: 682939, loss: 0.06191451475024223, data time: 0.00811606294968549\n",
      "step: 682940, loss: 0.062222838401794434, data time: 0.007944597516741071\n",
      "step: 682941, loss: 0.06310683488845825, data time: 0.0077805254194471575\n",
      "step: 682942, loss: 0.06556525081396103, data time: 0.00762509010933541\n",
      "step: 682943, loss: 0.06812955439090729, data time: 0.007481079352529426\n",
      "step: 682944, loss: 0.062452808022499084, data time: 0.007345217924851637\n",
      "step: 682945, loss: 0.05016174912452698, data time: 0.007215195894241333\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4133279323577881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 682946, loss: 0.06099739670753479, data time: 0.19125080108642578\n",
      "step: 682947, loss: 0.06649458408355713, data time: 0.0971759557723999\n",
      "step: 682948, loss: 0.06346185505390167, data time: 0.06530563036600749\n",
      "step: 682949, loss: 0.05788911506533623, data time: 0.04986530542373657\n",
      "step: 682950, loss: 0.0626404881477356, data time: 0.040194368362426756\n",
      "step: 682951, loss: 0.06165074557065964, data time: 0.03377751509348551\n",
      "step: 682952, loss: 0.05486826226115227, data time: 0.029187985828944614\n",
      "step: 682953, loss: 0.05753042548894882, data time: 0.02583983540534973\n",
      "step: 682954, loss: 0.05907731503248215, data time: 0.023148669136895075\n",
      "step: 682955, loss: 0.06376545131206512, data time: 0.02106454372406006\n",
      "step: 682956, loss: 0.0600569061934948, data time: 0.019386226480657406\n",
      "step: 682957, loss: 0.060527559369802475, data time: 0.017989555994669598\n",
      "step: 682958, loss: 0.054436974227428436, data time: 0.016809885318462666\n",
      "step: 682959, loss: 0.061208516359329224, data time: 0.015781913484845842\n",
      "step: 682960, loss: 0.0636076107621193, data time: 0.014888254801432292\n",
      "step: 682961, loss: 0.05677701532840729, data time: 0.014111921191215515\n",
      "step: 682962, loss: 0.06489905714988708, data time: 0.013424522736493279\n",
      "step: 682963, loss: 0.06394001096487045, data time: 0.012807938787672255\n",
      "step: 682964, loss: 0.0668475329875946, data time: 0.012260424463372482\n",
      "step: 682965, loss: 0.06223910301923752, data time: 0.011770534515380859\n",
      "step: 682966, loss: 0.06137461215257645, data time: 0.011335565930321104\n",
      "step: 682967, loss: 0.06153223663568497, data time: 0.010936509479175915\n",
      "step: 682968, loss: 0.06549309194087982, data time: 0.010562471721483313\n",
      "step: 682969, loss: 0.05733253061771393, data time: 0.01022594173749288\n",
      "step: 682970, loss: 0.06174078583717346, data time: 0.009913673400878906\n",
      "step: 682971, loss: 0.05832753702998161, data time: 0.009627057955815243\n",
      "step: 682972, loss: 0.06324128806591034, data time: 0.009354397102638527\n",
      "step: 682973, loss: 0.06378349661827087, data time: 0.0091083220073155\n",
      "step: 682974, loss: 0.0583646260201931, data time: 0.008881568908691406\n",
      "step: 682975, loss: 0.06019224971532822, data time: 0.00867152214050293\n",
      "step: 682976, loss: 0.07092607766389847, data time: 0.008473826992896295\n",
      "step: 682977, loss: 0.06212320923805237, data time: 0.008295789361000061\n",
      "step: 682978, loss: 0.058995865285396576, data time: 0.008107264836629232\n",
      "step: 682979, loss: 0.07114741206169128, data time: 0.007930215667275822\n",
      "step: 682980, loss: 0.05998273938894272, data time: 0.007762323107038225\n",
      "step: 682981, loss: 0.05928592011332512, data time: 0.007602367136213515\n",
      "step: 682982, loss: 0.05819917842745781, data time: 0.007452520164283547\n",
      "step: 682983, loss: 0.06432968378067017, data time: 0.007312586433009098\n",
      "step: 682984, loss: 0.06687312573194504, data time: 0.007180525706364558\n",
      "step: 682985, loss: 0.057836465537548065, data time: 0.0070546984672546385\n",
      "step: 682986, loss: 0.061425622552633286, data time: 0.20597076416015625\n",
      "step: 682987, loss: 0.060472458600997925, data time: 0.1037818193435669\n",
      "step: 682988, loss: 0.06158408522605896, data time: 0.07062077522277832\n",
      "step: 682989, loss: 0.05802587792277336, data time: 0.05332440137863159\n",
      "step: 682990, loss: 0.06213958188891411, data time: 0.04295072555541992\n",
      "step: 682991, loss: 0.060081467032432556, data time: 0.036028504371643066\n",
      "step: 682992, loss: 0.06899222731590271, data time: 0.031192166464669362\n",
      "step: 682993, loss: 0.06349232792854309, data time: 0.02747476100921631\n",
      "step: 682994, loss: 0.061848342418670654, data time: 0.02457494205898709\n",
      "step: 682995, loss: 0.055847279727458954, data time: 0.0223175048828125\n",
      "step: 682996, loss: 0.06609595566987991, data time: 0.02047974413091486\n",
      "step: 682997, loss: 0.053048115223646164, data time: 0.018955151240030926\n",
      "step: 682998, loss: 0.06484689563512802, data time: 0.01766358889066256\n",
      "step: 682999, loss: 0.05791667848825455, data time: 0.016551494598388672\n",
      "step: 683000, loss: 0.06046895310282707, data time: 0.015589094161987305\n",
      "step: 683001, loss: 0.06168478727340698, data time: 0.01474735140800476\n",
      "step: 683002, loss: 0.053347378969192505, data time: 0.013997484655941235\n",
      "step: 683003, loss: 0.0625188872218132, data time: 0.0133284330368042\n",
      "step: 683004, loss: 0.059312790632247925, data time: 0.012732769313611482\n",
      "step: 683005, loss: 0.062339454889297485, data time: 0.012203562259674072\n",
      "step: 683006, loss: 0.05785459280014038, data time: 0.011724585578555153\n",
      "step: 683007, loss: 0.057447873055934906, data time: 0.01128585772080855\n",
      "step: 683008, loss: 0.0646173432469368, data time: 0.010883455691130264\n",
      "step: 683009, loss: 0.06102126091718674, data time: 0.010515163342157999\n",
      "step: 683010, loss: 0.06286181509494781, data time: 0.010182476043701172\n",
      "step: 683011, loss: 0.06019490212202072, data time: 0.009871492019066444\n",
      "step: 683012, loss: 0.05160343647003174, data time: 0.009580462067215531\n",
      "step: 683013, loss: 0.06496171653270721, data time: 0.0093111480985369\n",
      "step: 683014, loss: 0.060285236686468124, data time: 0.0090642550895954\n",
      "step: 683015, loss: 0.05938901752233505, data time: 0.008832764625549317\n",
      "step: 683016, loss: 0.06727653741836548, data time: 0.008614763136832945\n",
      "step: 683017, loss: 0.06029614061117172, data time: 0.008414886891841888\n",
      "step: 683018, loss: 0.06778170168399811, data time: 0.00822271000255238\n",
      "step: 683019, loss: 0.06473077833652496, data time: 0.008044909028445972\n",
      "step: 683020, loss: 0.06459915637969971, data time: 0.00787416866847447\n",
      "step: 683021, loss: 0.06143559515476227, data time: 0.007711463504367405\n",
      "step: 683022, loss: 0.06254002451896667, data time: 0.007557785188829577\n",
      "step: 683023, loss: 0.06145921349525452, data time: 0.007415376211467542\n",
      "step: 683024, loss: 0.06033630669116974, data time: 0.007280227465507312\n",
      "step: 683025, loss: 0.05524285137653351, data time: 0.00715067982673645\n",
      "step: 683026, loss: 0.05683504045009613, data time: 0.2011125087738037\n",
      "step: 683027, loss: 0.06116178631782532, data time: 0.1023322343826294\n",
      "step: 683028, loss: 0.062264472246170044, data time: 0.06873734792073567\n",
      "step: 683029, loss: 0.05973063036799431, data time: 0.05233347415924072\n",
      "step: 683030, loss: 0.06318836659193039, data time: 0.04215679168701172\n",
      "step: 683031, loss: 0.0575253926217556, data time: 0.035376389821370445\n",
      "step: 683032, loss: 0.0656210333108902, data time: 0.03052476474217006\n",
      "step: 683033, loss: 0.06336557120084763, data time: 0.026983678340911865\n",
      "step: 683034, loss: 0.06450902670621872, data time: 0.02414096726311578\n",
      "step: 683035, loss: 0.05955579876899719, data time: 0.021939992904663086\n",
      "step: 683036, loss: 0.06165074557065964, data time: 0.020139867609197445\n",
      "step: 683037, loss: 0.057577069848775864, data time: 0.018639544645945232\n",
      "step: 683038, loss: 0.05726748704910278, data time: 0.017369196965144232\n",
      "step: 683039, loss: 0.05824369192123413, data time: 0.016275678362165178\n",
      "step: 683040, loss: 0.06291140615940094, data time: 0.015328248341878256\n",
      "step: 683041, loss: 0.05485112592577934, data time: 0.014495015144348145\n",
      "step: 683042, loss: 0.06347319483757019, data time: 0.013767368653241326\n",
      "step: 683043, loss: 0.060519635677337646, data time: 0.013115657700432671\n",
      "step: 683044, loss: 0.06111666560173035, data time: 0.012533401188097502\n",
      "step: 683045, loss: 0.05935928225517273, data time: 0.012014257907867431\n",
      "step: 683046, loss: 0.06462609022855759, data time: 0.011544409252348401\n",
      "step: 683047, loss: 0.06125912070274353, data time: 0.011119907552545721\n",
      "step: 683048, loss: 0.05926908552646637, data time: 0.010728369588437288\n",
      "step: 683049, loss: 0.06147801876068115, data time: 0.01036769151687622\n",
      "step: 683050, loss: 0.06188863515853882, data time: 0.010033683776855469\n",
      "step: 683051, loss: 0.06425008177757263, data time: 0.009731283554664025\n",
      "step: 683052, loss: 0.058048345148563385, data time: 0.009446338370994286\n",
      "step: 683053, loss: 0.06578793376684189, data time: 0.009183296135493688\n",
      "step: 683054, loss: 0.06377573311328888, data time: 0.008941625726634058\n",
      "step: 683055, loss: 0.07136788219213486, data time: 0.008716154098510741\n",
      "step: 683056, loss: 0.056923992931842804, data time: 0.008518895795268397\n",
      "step: 683057, loss: 0.05400742590427399, data time: 0.008324101567268372\n",
      "step: 683058, loss: 0.06002773344516754, data time: 0.008131359562729343\n",
      "step: 683059, loss: 0.06040540337562561, data time: 0.007948833353379193\n",
      "step: 683060, loss: 0.05957486480474472, data time: 0.007779734475272042\n",
      "step: 683061, loss: 0.06342649459838867, data time: 0.007618572976854112\n",
      "step: 683062, loss: 0.06330841034650803, data time: 0.0074672570099701754\n",
      "step: 683063, loss: 0.057220958173274994, data time: 0.007327086047122353\n",
      "step: 683064, loss: 0.06389015913009644, data time: 0.007195815061911559\n",
      "step: 683065, loss: 0.06286794692277908, data time: 0.0070703387260437015\n",
      "step: 683066, loss: 0.05987606570124626, data time: 0.20395898818969727\n",
      "step: 683067, loss: 0.06345117092132568, data time: 0.10368835926055908\n",
      "step: 683068, loss: 0.0666460245847702, data time: 0.06967361768086751\n",
      "step: 683069, loss: 0.06760120391845703, data time: 0.05303525924682617\n",
      "step: 683070, loss: 0.06301670521497726, data time: 0.04272961616516113\n",
      "step: 683071, loss: 0.07090477645397186, data time: 0.03587675094604492\n",
      "step: 683072, loss: 0.05730287730693817, data time: 0.030970198767525808\n",
      "step: 683073, loss: 0.06536198407411575, data time: 0.02735346555709839\n",
      "step: 683074, loss: 0.06349389255046844, data time: 0.024540053473578557\n",
      "step: 683075, loss: 0.05803685635328293, data time: 0.02227940559387207\n",
      "step: 683076, loss: 0.061831243336200714, data time: 0.020449573343450374\n",
      "step: 683077, loss: 0.05907035991549492, data time: 0.018925070762634277\n",
      "step: 683078, loss: 0.060537032783031464, data time: 0.01765297009394719\n",
      "step: 683079, loss: 0.05731021612882614, data time: 0.016540987151009694\n",
      "step: 683080, loss: 0.06405743211507797, data time: 0.015580320358276367\n",
      "step: 683081, loss: 0.06139056012034416, data time: 0.014738470315933228\n",
      "step: 683082, loss: 0.06021353602409363, data time: 0.013995016322416417\n",
      "step: 683083, loss: 0.06333184242248535, data time: 0.013331333796183268\n",
      "step: 683084, loss: 0.060126639902591705, data time: 0.01273303282888312\n",
      "step: 683085, loss: 0.06253369897603989, data time: 0.012210500240325928\n",
      "step: 683086, loss: 0.061374731361866, data time: 0.011735144115629651\n",
      "step: 683087, loss: 0.06607374548912048, data time: 0.011299165812405672\n",
      "step: 683088, loss: 0.06098441779613495, data time: 0.010894474775894829\n",
      "step: 683089, loss: 0.059479594230651855, data time: 0.010530630747477213\n",
      "step: 683090, loss: 0.060139864683151245, data time: 0.010193376541137696\n",
      "step: 683091, loss: 0.06606199592351913, data time: 0.009879332322340745\n",
      "step: 683092, loss: 0.058120742440223694, data time: 0.009593221876356337\n",
      "step: 683093, loss: 0.05915432423353195, data time: 0.009320225034441267\n",
      "step: 683094, loss: 0.05638052150607109, data time: 0.009070996580452755\n",
      "step: 683095, loss: 0.06293409317731857, data time: 0.00883931318918864\n",
      "step: 683096, loss: 0.06611371785402298, data time: 0.008622500204270887\n",
      "step: 683097, loss: 0.0666026622056961, data time: 0.008424639701843262\n",
      "step: 683098, loss: 0.06684942543506622, data time: 0.00823286807898319\n",
      "step: 683099, loss: 0.06179697439074516, data time: 0.008049572215360753\n",
      "step: 683100, loss: 0.06149553880095482, data time: 0.007874768120901925\n",
      "step: 683101, loss: 0.059152256697416306, data time: 0.0077096886105007595\n",
      "step: 683102, loss: 0.05843798443675041, data time: 0.007550974149961729\n",
      "step: 683103, loss: 0.06730466336011887, data time: 0.0074049924549303555\n",
      "step: 683104, loss: 0.07035896182060242, data time: 0.007266827118702424\n",
      "step: 683105, loss: 0.05729668587446213, data time: 0.0071352958679199215\n",
      "step: 683106, loss: 0.06635892391204834, data time: 0.20424818992614746\n",
      "step: 683107, loss: 0.0680030882358551, data time: 0.10347819328308105\n",
      "step: 683108, loss: 0.055639006197452545, data time: 0.07001940409342448\n",
      "step: 683109, loss: 0.05862421914935112, data time: 0.05317145586013794\n",
      "step: 683110, loss: 0.06103373318910599, data time: 0.04281187057495117\n",
      "step: 683111, loss: 0.06266799569129944, data time: 0.03593901793162028\n",
      "step: 683112, loss: 0.059178680181503296, data time: 0.03102850914001465\n",
      "step: 683113, loss: 0.06549011915922165, data time: 0.027404427528381348\n",
      "step: 683114, loss: 0.06352365016937256, data time: 0.02450704574584961\n",
      "step: 683115, loss: 0.059324853122234344, data time: 0.022270941734313966\n",
      "step: 683116, loss: 0.06097321957349777, data time: 0.020449833436445755\n",
      "step: 683117, loss: 0.06114606559276581, data time: 0.01892356077829997\n",
      "step: 683118, loss: 0.05900100991129875, data time: 0.017635455498328574\n",
      "step: 683119, loss: 0.06372755020856857, data time: 0.01652966226850237\n",
      "step: 683120, loss: 0.06285173445940018, data time: 0.015569082895914714\n",
      "step: 683121, loss: 0.05755431577563286, data time: 0.014733240008354187\n",
      "step: 683122, loss: 0.06445436924695969, data time: 0.013988831463982077\n",
      "step: 683123, loss: 0.05777948349714279, data time: 0.013329466183980307\n",
      "step: 683124, loss: 0.060226451605558395, data time: 0.012734538630435341\n",
      "step: 683125, loss: 0.06243587285280228, data time: 0.012205290794372558\n",
      "step: 683126, loss: 0.06622128933668137, data time: 0.011727321715581985\n",
      "step: 683127, loss: 0.06018894165754318, data time: 0.01129072362726385\n",
      "step: 683128, loss: 0.06229468807578087, data time: 0.010888089304384977\n",
      "step: 683129, loss: 0.06198675557971001, data time: 0.010526756445566813\n",
      "step: 683130, loss: 0.06572087854146957, data time: 0.010194883346557618\n",
      "step: 683131, loss: 0.05906885117292404, data time: 0.009878782125619741\n",
      "step: 683132, loss: 0.06190718337893486, data time: 0.009586316567880136\n",
      "step: 683133, loss: 0.05899408459663391, data time: 0.009315294878823417\n",
      "step: 683134, loss: 0.0559316985309124, data time: 0.009066466627449825\n",
      "step: 683135, loss: 0.0649690106511116, data time: 0.008835482597351074\n",
      "step: 683136, loss: 0.06468061357736588, data time: 0.008620800510529549\n",
      "step: 683137, loss: 0.0654982253909111, data time: 0.008423633873462677\n",
      "step: 683138, loss: 0.06873514503240585, data time: 0.00823330156730883\n",
      "step: 683139, loss: 0.05897597596049309, data time: 0.00805037863114301\n",
      "step: 683140, loss: 0.053627729415893555, data time: 0.007876110076904298\n",
      "step: 683141, loss: 0.06908047199249268, data time: 0.007710397243499756\n",
      "step: 683142, loss: 0.06196709722280502, data time: 0.007554022041527\n",
      "step: 683143, loss: 0.055235400795936584, data time: 0.0074093216343929895\n",
      "step: 683144, loss: 0.0664496049284935, data time: 0.007270776308499849\n",
      "step: 683145, loss: 0.06282640993595123, data time: 0.007139438390731811\n",
      "step: 683146, loss: 0.057598039507865906, data time: 0.20287275314331055\n",
      "step: 683147, loss: 0.06489425897598267, data time: 0.10287106037139893\n",
      "step: 683148, loss: 0.06054788827896118, data time: 0.06961536407470703\n",
      "step: 683149, loss: 0.06001675873994827, data time: 0.052894651889801025\n",
      "step: 683150, loss: 0.057841092348098755, data time: 0.042600250244140624\n",
      "step: 683151, loss: 0.0563124418258667, data time: 0.035754640897115074\n",
      "step: 683152, loss: 0.058366939425468445, data time: 0.03086076463971819\n",
      "step: 683153, loss: 0.06263551115989685, data time: 0.027247756719589233\n",
      "step: 683154, loss: 0.05751977860927582, data time: 0.024371888902452257\n",
      "step: 683155, loss: 0.06032765656709671, data time: 0.022146058082580567\n",
      "step: 683156, loss: 0.058454424142837524, data time: 0.020325335589322178\n",
      "step: 683157, loss: 0.06190210580825806, data time: 0.01881641149520874\n",
      "step: 683158, loss: 0.06710118055343628, data time: 0.017540161426250752\n",
      "step: 683159, loss: 0.062289562076330185, data time: 0.016437479427882602\n",
      "step: 683160, loss: 0.06283457577228546, data time: 0.015493043263753255\n",
      "step: 683161, loss: 0.06305262446403503, data time: 0.014656499028205872\n",
      "step: 683162, loss: 0.06179703399538994, data time: 0.013916155871223001\n",
      "step: 683163, loss: 0.05593235418200493, data time: 0.013251264890034994\n",
      "step: 683164, loss: 0.06212257593870163, data time: 0.012664857663606343\n",
      "step: 683165, loss: 0.0605410560965538, data time: 0.012146246433258057\n",
      "step: 683166, loss: 0.05938943848013878, data time: 0.011671497708275205\n",
      "step: 683167, loss: 0.06233956664800644, data time: 0.01124141433022239\n",
      "step: 683168, loss: 0.0614178292453289, data time: 0.010844147723654041\n",
      "step: 683169, loss: 0.0548807755112648, data time: 0.010483205318450928\n",
      "step: 683170, loss: 0.06358782947063446, data time: 0.010149726867675781\n",
      "step: 683171, loss: 0.06556914001703262, data time: 0.00983721476334792\n",
      "step: 683172, loss: 0.056258268654346466, data time: 0.009551083600079571\n",
      "step: 683173, loss: 0.061432134360075, data time: 0.009285492556435722\n",
      "step: 683174, loss: 0.0642632469534874, data time: 0.009039755525260136\n",
      "step: 683175, loss: 0.06881856918334961, data time: 0.008810997009277344\n",
      "step: 683176, loss: 0.06583001464605331, data time: 0.00859729705318328\n",
      "step: 683177, loss: 0.06125102564692497, data time: 0.008399806916713715\n",
      "step: 683178, loss: 0.0605984628200531, data time: 0.008210370034882517\n",
      "step: 683179, loss: 0.06510387361049652, data time: 0.008029993842629826\n",
      "step: 683180, loss: 0.055191099643707275, data time: 0.007856178283691406\n",
      "step: 683181, loss: 0.05648469179868698, data time: 0.007688661416371663\n",
      "step: 683182, loss: 0.05976204201579094, data time: 0.007532905887913059\n",
      "step: 683183, loss: 0.058755770325660706, data time: 0.007399182570608039\n",
      "step: 683184, loss: 0.0639810562133789, data time: 0.007260866654224885\n",
      "step: 683185, loss: 0.03738851100206375, data time: 0.007130485773086548\n",
      "step: 683186, loss: 0.057176657021045685, data time: 0.19785046577453613\n",
      "step: 683187, loss: 0.06303073465824127, data time: 0.09970533847808838\n",
      "step: 683188, loss: 0.060909710824489594, data time: 0.06697750091552734\n",
      "step: 683189, loss: 0.06922142952680588, data time: 0.051200270652770996\n",
      "step: 683190, loss: 0.0578119158744812, data time: 0.04124584197998047\n",
      "step: 683191, loss: 0.06514628231525421, data time: 0.03461599349975586\n",
      "step: 683192, loss: 0.05990780517458916, data time: 0.029880591801234653\n",
      "step: 683193, loss: 0.0638994351029396, data time: 0.026408463716506958\n",
      "step: 683194, loss: 0.061738453805446625, data time: 0.02363239394293891\n",
      "step: 683195, loss: 0.06013529747724533, data time: 0.02148740291595459\n",
      "step: 683196, loss: 0.06450530886650085, data time: 0.019741838628595524\n",
      "step: 683197, loss: 0.062490370124578476, data time: 0.01827476421991984\n",
      "step: 683198, loss: 0.06349308043718338, data time: 0.01703469569866474\n",
      "step: 683199, loss: 0.06800629943609238, data time: 0.015960114342825755\n",
      "step: 683200, loss: 0.059395354241132736, data time: 0.015059248606363932\n",
      "step: 683201, loss: 0.06166378781199455, data time: 0.014276400208473206\n",
      "step: 683202, loss: 0.06139295548200607, data time: 0.0135803082410027\n",
      "step: 683203, loss: 0.056182861328125, data time: 0.01295675171746148\n",
      "step: 683204, loss: 0.06182368844747543, data time: 0.012402258421245375\n",
      "step: 683205, loss: 0.06116163730621338, data time: 0.011909329891204834\n",
      "step: 683206, loss: 0.06648065894842148, data time: 0.011458465031215124\n",
      "step: 683207, loss: 0.06159104034304619, data time: 0.011054689233953302\n",
      "step: 683208, loss: 0.06516595184803009, data time: 0.01068425178527832\n",
      "step: 683209, loss: 0.06506001204252243, data time: 0.010345667600631714\n",
      "step: 683210, loss: 0.05968305096030235, data time: 0.010029983520507813\n",
      "step: 683211, loss: 0.06570975482463837, data time: 0.009741425514221191\n",
      "step: 683212, loss: 0.06836806237697601, data time: 0.009465093965883608\n",
      "step: 683213, loss: 0.0660787969827652, data time: 0.009211361408233643\n",
      "step: 683214, loss: 0.05893434211611748, data time: 0.00898054550433981\n",
      "step: 683215, loss: 0.05670883506536484, data time: 0.008765157063802083\n",
      "step: 683216, loss: 0.058559589087963104, data time: 0.008565372036349389\n",
      "step: 683217, loss: 0.06917991489171982, data time: 0.008379854261875153\n",
      "step: 683218, loss: 0.06457938253879547, data time: 0.008190342874237986\n",
      "step: 683219, loss: 0.06560574471950531, data time: 0.00801243501551011\n",
      "step: 683220, loss: 0.059901706874370575, data time: 0.007843657902308872\n",
      "step: 683221, loss: 0.06067213416099548, data time: 0.007680389616224501\n",
      "step: 683222, loss: 0.06656348705291748, data time: 0.007527377154376055\n",
      "step: 683223, loss: 0.06927262246608734, data time: 0.007388114929199219\n",
      "step: 683224, loss: 0.061981331557035446, data time: 0.007253500131460337\n",
      "step: 683225, loss: 0.05160039663314819, data time: 0.00712699294090271\n",
      "step: 683226, loss: 0.0601472370326519, data time: 0.2073521614074707\n",
      "step: 683227, loss: 0.05903663486242294, data time: 0.10445594787597656\n",
      "step: 683228, loss: 0.061613958328962326, data time: 0.07067553202311198\n",
      "step: 683229, loss: 0.05814788490533829, data time: 0.05356287956237793\n",
      "step: 683230, loss: 0.06396981328725815, data time: 0.043129348754882814\n",
      "step: 683231, loss: 0.05630195885896683, data time: 0.03619241714477539\n",
      "step: 683232, loss: 0.05488504096865654, data time: 0.0314399174281529\n",
      "step: 683233, loss: 0.059282153844833374, data time: 0.027685344219207764\n",
      "step: 683234, loss: 0.061879076063632965, data time: 0.0247650146484375\n",
      "step: 683235, loss: 0.06105070561170578, data time: 0.022501516342163085\n",
      "step: 683236, loss: 0.061321523040533066, data time: 0.02066206932067871\n",
      "step: 683237, loss: 0.06299641728401184, data time: 0.019127408663431805\n",
      "step: 683238, loss: 0.06467238068580627, data time: 0.017823531077458307\n",
      "step: 683239, loss: 0.06451906263828278, data time: 0.016696316855294362\n",
      "step: 683240, loss: 0.060127031058073044, data time: 0.01572761535644531\n",
      "step: 683241, loss: 0.062205396592617035, data time: 0.014882728457450867\n",
      "step: 683242, loss: 0.05827169120311737, data time: 0.014124365413890165\n",
      "step: 683243, loss: 0.06427372992038727, data time: 0.013455933994717069\n",
      "step: 683244, loss: 0.06288842856884003, data time: 0.012856546201204\n",
      "step: 683245, loss: 0.05692606791853905, data time: 0.012323474884033203\n",
      "step: 683246, loss: 0.06087047979235649, data time: 0.011838231767926897\n",
      "step: 683247, loss: 0.06426380574703217, data time: 0.01140241189436479\n",
      "step: 683248, loss: 0.05675240606069565, data time: 0.010994465454764988\n",
      "step: 683249, loss: 0.06028665229678154, data time: 0.01062554121017456\n",
      "step: 683250, loss: 0.06481563299894333, data time: 0.010286827087402344\n",
      "step: 683251, loss: 0.06785990297794342, data time: 0.009969793833219089\n",
      "step: 683252, loss: 0.058407850563526154, data time: 0.009691256063955801\n",
      "step: 683253, loss: 0.062043529003858566, data time: 0.009417576449257987\n",
      "step: 683254, loss: 0.0681881457567215, data time: 0.009167810966228616\n",
      "step: 683255, loss: 0.0640205517411232, data time: 0.008937501907348632\n",
      "step: 683256, loss: 0.06399117410182953, data time: 0.008719667311637633\n",
      "step: 683257, loss: 0.05706661939620972, data time: 0.008518047630786896\n",
      "step: 683258, loss: 0.06784769892692566, data time: 0.008321682612101236\n",
      "step: 683259, loss: 0.06326578557491302, data time: 0.008136686156777775\n",
      "step: 683260, loss: 0.06583695113658905, data time: 0.007958344050816126\n",
      "step: 683261, loss: 0.06535758823156357, data time: 0.0077903734313117135\n",
      "step: 683262, loss: 0.0606527253985405, data time: 0.0076309152551599455\n",
      "step: 683263, loss: 0.06284384429454803, data time: 0.007484410938463713\n",
      "step: 683264, loss: 0.05955193564295769, data time: 0.007344111418112731\n",
      "step: 683265, loss: 0.06815528124570847, data time: 0.00721164345741272\n",
      "step: 683266, loss: 0.06362299621105194, data time: 0.20211338996887207\n",
      "step: 683267, loss: 0.06312981247901917, data time: 0.10182905197143555\n",
      "step: 683268, loss: 0.05951087921857834, data time: 0.06902321179707845\n",
      "step: 683269, loss: 0.06370571255683899, data time: 0.05232560634613037\n",
      "step: 683270, loss: 0.06087416037917137, data time: 0.042139101028442386\n",
      "step: 683271, loss: 0.06907059997320175, data time: 0.03537158171335856\n",
      "step: 683272, loss: 0.07036037743091583, data time: 0.03062401499067034\n",
      "step: 683273, loss: 0.06688734889030457, data time: 0.027063220739364624\n",
      "step: 683274, loss: 0.0633903443813324, data time: 0.024205896589491103\n",
      "step: 683275, loss: 0.06469953060150146, data time: 0.022001028060913086\n",
      "step: 683276, loss: 0.06539003551006317, data time: 0.020197629928588867\n",
      "step: 683277, loss: 0.06312713027000427, data time: 0.018694082895914715\n",
      "step: 683278, loss: 0.060001812875270844, data time: 0.0174257755279541\n",
      "step: 683279, loss: 0.06151185929775238, data time: 0.016326699938092912\n",
      "step: 683280, loss: 0.05764657258987427, data time: 0.015390316645304361\n",
      "step: 683281, loss: 0.06292062252759933, data time: 0.01456083357334137\n",
      "step: 683282, loss: 0.060501985251903534, data time: 0.013820437824024874\n",
      "step: 683283, loss: 0.06723824143409729, data time: 0.0131608247756958\n",
      "step: 683284, loss: 0.06009429320693016, data time: 0.012576216145565635\n",
      "step: 683285, loss: 0.06210922822356224, data time: 0.012057769298553466\n",
      "step: 683286, loss: 0.06314697116613388, data time: 0.011589606602986654\n",
      "step: 683287, loss: 0.0641004666686058, data time: 0.01116069880398837\n",
      "step: 683288, loss: 0.054449137300252914, data time: 0.010762608569601307\n",
      "step: 683289, loss: 0.06038060784339905, data time: 0.010401437679926554\n",
      "step: 683290, loss: 0.060338836163282394, data time: 0.010071439743041992\n",
      "step: 683291, loss: 0.06674488633871078, data time: 0.009763580102186937\n",
      "step: 683292, loss: 0.06397310644388199, data time: 0.009476211335923936\n",
      "step: 683293, loss: 0.06850442290306091, data time: 0.009213720049176897\n",
      "step: 683294, loss: 0.0650167390704155, data time: 0.00897195421416184\n",
      "step: 683295, loss: 0.06293808668851852, data time: 0.008744057019551594\n",
      "step: 683296, loss: 0.05578463897109032, data time: 0.00853045525089387\n",
      "step: 683297, loss: 0.061532553285360336, data time: 0.008334383368492126\n",
      "step: 683298, loss: 0.06275095045566559, data time: 0.008142955375440193\n",
      "step: 683299, loss: 0.05978202074766159, data time: 0.00796346804674934\n",
      "step: 683300, loss: 0.05589141696691513, data time: 0.007793310710362026\n",
      "step: 683301, loss: 0.06307222694158554, data time: 0.0076279375288221575\n",
      "step: 683302, loss: 0.06818878650665283, data time: 0.007473520330480627\n",
      "step: 683303, loss: 0.05994328111410141, data time: 0.0073309697602924545\n",
      "step: 683304, loss: 0.05922132730484009, data time: 0.007197942489232772\n",
      "step: 683305, loss: 0.059734147042036057, data time: 0.007072341442108154\n",
      "step: 683306, loss: 0.06123955175280571, data time: 0.19872021675109863\n",
      "step: 683307, loss: 0.06922359019517899, data time: 0.10074985027313232\n",
      "step: 683308, loss: 0.05799397826194763, data time: 0.06837677955627441\n",
      "step: 683309, loss: 0.05873733386397362, data time: 0.05195862054824829\n",
      "step: 683310, loss: 0.06437426060438156, data time: 0.04185585975646973\n",
      "step: 683311, loss: 0.06165028735995293, data time: 0.03513352076212565\n",
      "step: 683312, loss: 0.05797715112566948, data time: 0.030337367738996233\n",
      "step: 683313, loss: 0.058774687349796295, data time: 0.02681669592857361\n",
      "step: 683314, loss: 0.06567502021789551, data time: 0.023985650804307725\n",
      "step: 683315, loss: 0.058820419013500214, data time: 0.021787810325622558\n",
      "step: 683316, loss: 0.05845427140593529, data time: 0.020001476461237126\n",
      "step: 683317, loss: 0.059771981090307236, data time: 0.01851266622543335\n",
      "step: 683318, loss: 0.06140921637415886, data time: 0.017258112247173604\n",
      "step: 683319, loss: 0.07164770364761353, data time: 0.016172136579241072\n",
      "step: 683320, loss: 0.05688729137182236, data time: 0.015238380432128907\n",
      "step: 683321, loss: 0.06251871585845947, data time: 0.014424845576286316\n",
      "step: 683322, loss: 0.054964300245046616, data time: 0.013693613164565143\n",
      "step: 683323, loss: 0.05802275240421295, data time: 0.013040661811828613\n",
      "step: 683324, loss: 0.063405841588974, data time: 0.012465489538092362\n",
      "step: 683325, loss: 0.06113424152135849, data time: 0.011954224109649659\n",
      "step: 683326, loss: 0.066372349858284, data time: 0.011487869989304315\n",
      "step: 683327, loss: 0.06441881507635117, data time: 0.011064475232904608\n",
      "step: 683328, loss: 0.05671463906764984, data time: 0.010672735131305197\n",
      "step: 683329, loss: 0.06032930314540863, data time: 0.010318895181020101\n",
      "step: 683330, loss: 0.06700481474399567, data time: 0.009991798400878906\n",
      "step: 683331, loss: 0.06105111166834831, data time: 0.009689688682556152\n",
      "step: 683332, loss: 0.05478707700967789, data time: 0.009404350210119176\n",
      "step: 683333, loss: 0.06307999044656754, data time: 0.00913973365511213\n",
      "step: 683334, loss: 0.0572834387421608, data time: 0.008896975681699556\n",
      "step: 683335, loss: 0.06408409774303436, data time: 0.008672070503234864\n",
      "step: 683336, loss: 0.061153169721364975, data time: 0.008463052011305285\n",
      "step: 683337, loss: 0.05890969932079315, data time: 0.00826987624168396\n",
      "step: 683338, loss: 0.06523744761943817, data time: 0.008080561955769857\n",
      "step: 683339, loss: 0.0615403950214386, data time: 0.007902089287252986\n",
      "step: 683340, loss: 0.05632142350077629, data time: 0.007733501706804548\n",
      "step: 683341, loss: 0.06178756058216095, data time: 0.007570882638295491\n",
      "step: 683342, loss: 0.061806172132492065, data time: 0.007417543514354809\n",
      "step: 683343, loss: 0.06310485303401947, data time: 0.007276811097797595\n",
      "step: 683344, loss: 0.06238442659378052, data time: 0.007143124555930113\n",
      "step: 683345, loss: 0.09299260377883911, data time: 0.007020211219787598\n",
      "step: 683346, loss: 0.05769185349345207, data time: 0.19558382034301758\n",
      "step: 683347, loss: 0.06155949831008911, data time: 0.09893620014190674\n",
      "step: 683348, loss: 0.05857327580451965, data time: 0.06713604927062988\n",
      "step: 683349, loss: 0.06183886155486107, data time: 0.05114006996154785\n",
      "step: 683350, loss: 0.06370528787374496, data time: 0.0411898136138916\n",
      "step: 683351, loss: 0.0563662014901638, data time: 0.03458094596862793\n",
      "step: 683352, loss: 0.061562471091747284, data time: 0.029861858912876675\n",
      "step: 683353, loss: 0.058143626898527145, data time: 0.02638569474220276\n",
      "step: 683354, loss: 0.06523700058460236, data time: 0.023623757892184787\n",
      "step: 683355, loss: 0.0642552375793457, data time: 0.02146456241607666\n",
      "step: 683356, loss: 0.062358081340789795, data time: 0.019719947468150745\n",
      "step: 683357, loss: 0.06309574842453003, data time: 0.018258988857269287\n",
      "step: 683358, loss: 0.0626073107123375, data time: 0.017015420473538913\n",
      "step: 683359, loss: 0.0614921972155571, data time: 0.015944787434169223\n",
      "step: 683360, loss: 0.06658601760864258, data time: 0.015028270085652669\n",
      "step: 683361, loss: 0.06163359805941582, data time: 0.014229491353034973\n",
      "step: 683362, loss: 0.05663174018263817, data time: 0.013518543804393095\n",
      "step: 683363, loss: 0.05619116872549057, data time: 0.01288012663523356\n",
      "step: 683364, loss: 0.062399398535490036, data time: 0.01230637650740774\n",
      "step: 683365, loss: 0.057087600231170654, data time: 0.01180117130279541\n",
      "step: 683366, loss: 0.060550183057785034, data time: 0.011342525482177734\n",
      "step: 683367, loss: 0.06722570955753326, data time: 0.01092739538712935\n",
      "step: 683368, loss: 0.06516780704259872, data time: 0.010541532350623089\n",
      "step: 683369, loss: 0.0667935237288475, data time: 0.010192533334096273\n",
      "step: 683370, loss: 0.05854373425245285, data time: 0.009869365692138672\n",
      "step: 683371, loss: 0.06661134213209152, data time: 0.009583830833435059\n",
      "step: 683372, loss: 0.06461815536022186, data time: 0.009315720310917607\n",
      "step: 683373, loss: 0.058640412986278534, data time: 0.009071154253823417\n",
      "step: 683374, loss: 0.056115638464689255, data time: 0.008844877111500707\n",
      "step: 683375, loss: 0.0609755665063858, data time: 0.008634003003438313\n",
      "step: 683376, loss: 0.06141716241836548, data time: 0.008436626003634545\n",
      "step: 683377, loss: 0.06330954283475876, data time: 0.008253432810306549\n",
      "step: 683378, loss: 0.05582930147647858, data time: 0.008069182887221828\n",
      "step: 683379, loss: 0.059958845376968384, data time: 0.007894929717568791\n",
      "step: 683380, loss: 0.06211671233177185, data time: 0.007727452686854771\n",
      "step: 683381, loss: 0.06328599900007248, data time: 0.00756860441631741\n",
      "step: 683382, loss: 0.0657072365283966, data time: 0.007419908368909681\n",
      "step: 683383, loss: 0.062049251049757004, data time: 0.007281372421666195\n",
      "step: 683384, loss: 0.06277723610401154, data time: 0.007149763596363557\n",
      "step: 683385, loss: 0.061829518526792526, data time: 0.007025891542434692\n",
      "step: 683386, loss: 0.05948006734251976, data time: 0.19669461250305176\n",
      "step: 683387, loss: 0.06108745187520981, data time: 0.09911513328552246\n",
      "step: 683388, loss: 0.06382075697183609, data time: 0.06683564186096191\n",
      "step: 683389, loss: 0.05797099694609642, data time: 0.05097222328186035\n",
      "step: 683390, loss: 0.05972412973642349, data time: 0.041068506240844724\n",
      "step: 683391, loss: 0.059712834656238556, data time: 0.03446479638417562\n",
      "step: 683392, loss: 0.06259196251630783, data time: 0.029759883880615234\n",
      "step: 683393, loss: 0.059983428567647934, data time: 0.026302814483642578\n",
      "step: 683394, loss: 0.0622832253575325, data time: 0.02353061570061578\n",
      "step: 683395, loss: 0.06220424175262451, data time: 0.02137300968170166\n",
      "step: 683396, loss: 0.0599808394908905, data time: 0.01962269436229359\n",
      "step: 683397, loss: 0.0630771666765213, data time: 0.018164594968159992\n",
      "step: 683398, loss: 0.06235311180353165, data time: 0.0169370174407959\n",
      "step: 683399, loss: 0.06279294192790985, data time: 0.015872512544904436\n",
      "step: 683400, loss: 0.0635485053062439, data time: 0.014954042434692384\n",
      "step: 683401, loss: 0.05750351399183273, data time: 0.014155849814414978\n",
      "step: 683402, loss: 0.06475906074047089, data time: 0.013446723713594325\n",
      "step: 683403, loss: 0.058233752846717834, data time: 0.012809673945109049\n",
      "step: 683404, loss: 0.06159834563732147, data time: 0.01224182781420256\n",
      "step: 683405, loss: 0.06292355060577393, data time: 0.011736643314361573\n",
      "step: 683406, loss: 0.06594555824995041, data time: 0.011282534826369513\n",
      "step: 683407, loss: 0.06773458421230316, data time: 0.010866208509965376\n",
      "step: 683408, loss: 0.05994871258735657, data time: 0.010482881380164105\n",
      "step: 683409, loss: 0.061067283153533936, data time: 0.010131279627482096\n",
      "step: 683410, loss: 0.06148024648427963, data time: 0.009812183380126953\n",
      "step: 683411, loss: 0.06622123718261719, data time: 0.009521062557513896\n",
      "step: 683412, loss: 0.05346944183111191, data time: 0.009244406664813007\n",
      "step: 683413, loss: 0.06370723247528076, data time: 0.008984829698290144\n",
      "step: 683414, loss: 0.06590591371059418, data time: 0.008747988733752021\n",
      "step: 683415, loss: 0.06627815961837769, data time: 0.008526905377705892\n",
      "step: 683416, loss: 0.06379154324531555, data time: 0.008321523666381836\n",
      "step: 683417, loss: 0.060176681727170944, data time: 0.008132614195346832\n",
      "step: 683418, loss: 0.062063589692115784, data time: 0.007946447892622515\n",
      "step: 683419, loss: 0.06089398264884949, data time: 0.0077716322506175325\n",
      "step: 683420, loss: 0.06343525648117065, data time: 0.007606663022722517\n",
      "step: 683421, loss: 0.06990602612495422, data time: 0.007448401716020372\n",
      "step: 683422, loss: 0.05910184234380722, data time: 0.00729852109342008\n",
      "step: 683423, loss: 0.060913871973752975, data time: 0.007159634640342311\n",
      "step: 683424, loss: 0.061723243445158005, data time: 0.007027375392424755\n",
      "step: 683425, loss: 0.06843279302120209, data time: 0.0069055259227752686\n",
      "step: 683426, loss: 0.06658104062080383, data time: 0.20668578147888184\n",
      "step: 683427, loss: 0.06856365501880646, data time: 0.10410463809967041\n",
      "step: 683428, loss: 0.05939453840255737, data time: 0.07018486658732097\n",
      "step: 683429, loss: 0.06229115650057793, data time: 0.05359697341918945\n",
      "step: 683430, loss: 0.05977001041173935, data time: 0.04316692352294922\n",
      "step: 683431, loss: 0.06278063356876373, data time: 0.036209940910339355\n",
      "step: 683432, loss: 0.06320756673812866, data time: 0.03126249994550433\n",
      "step: 683433, loss: 0.06394802778959274, data time: 0.027631700038909912\n",
      "step: 683434, loss: 0.0627446174621582, data time: 0.024715132183498807\n",
      "step: 683435, loss: 0.05961484834551811, data time: 0.02247664928436279\n",
      "step: 683436, loss: 0.06240362673997879, data time: 0.020636818625710228\n",
      "step: 683437, loss: 0.06803062558174133, data time: 0.019094427426656086\n",
      "step: 683438, loss: 0.06143021583557129, data time: 0.01779000575725849\n",
      "step: 683439, loss: 0.0588076114654541, data time: 0.016670857157026018\n",
      "step: 683440, loss: 0.06086500734090805, data time: 0.015704568227132162\n",
      "step: 683441, loss: 0.062473975121974945, data time: 0.014852523803710938\n",
      "step: 683442, loss: 0.06587737053632736, data time: 0.014115277458639705\n",
      "step: 683443, loss: 0.060981303453445435, data time: 0.013447867499457465\n",
      "step: 683444, loss: 0.06019286438822746, data time: 0.01284714748984889\n",
      "step: 683445, loss: 0.06712429970502853, data time: 0.012314045429229736\n",
      "step: 683446, loss: 0.05959778651595116, data time: 0.01183560916355678\n",
      "step: 683447, loss: 0.06083454191684723, data time: 0.011396093802018599\n",
      "step: 683448, loss: 0.07170488685369492, data time: 0.0109921641971754\n",
      "step: 683449, loss: 0.061572395265102386, data time: 0.010621696710586548\n",
      "step: 683450, loss: 0.06005010008811951, data time: 0.010280780792236328\n",
      "step: 683451, loss: 0.06658913940191269, data time: 0.009972031299884502\n",
      "step: 683452, loss: 0.06507233530282974, data time: 0.009677560241134078\n",
      "step: 683453, loss: 0.06213062256574631, data time: 0.009405936513628279\n",
      "step: 683454, loss: 0.06729666888713837, data time: 0.009156054463879815\n",
      "step: 683455, loss: 0.059426628053188324, data time: 0.008922084172566732\n",
      "step: 683456, loss: 0.05822934955358505, data time: 0.008703923994495023\n",
      "step: 683457, loss: 0.060600996017456055, data time: 0.008506178855895996\n",
      "step: 683458, loss: 0.05956175923347473, data time: 0.008308858582467743\n",
      "step: 683459, loss: 0.06062790006399155, data time: 0.008123292642481187\n",
      "step: 683460, loss: 0.055146150290966034, data time: 0.007951062066214425\n",
      "step: 683461, loss: 0.07051324844360352, data time: 0.0077861083878411185\n",
      "step: 683462, loss: 0.06202521175146103, data time: 0.007627094114148939\n",
      "step: 683463, loss: 0.060778215527534485, data time: 0.007481568738033897\n",
      "step: 683464, loss: 0.0679972842335701, data time: 0.007341446020664313\n",
      "step: 683465, loss: 0.08481986820697784, data time: 0.007218325138092041\n",
      "step: 683466, loss: 0.06374597549438477, data time: 0.20813274383544922\n",
      "step: 683467, loss: 0.06704150140285492, data time: 0.1048269271850586\n",
      "step: 683468, loss: 0.06087389588356018, data time: 0.07039101918538411\n",
      "step: 683469, loss: 0.06209954619407654, data time: 0.05357116460800171\n",
      "step: 683470, loss: 0.058026477694511414, data time: 0.04314632415771484\n",
      "step: 683471, loss: 0.06035185977816582, data time: 0.03620123863220215\n",
      "step: 683472, loss: 0.05764133483171463, data time: 0.03123746599469866\n",
      "step: 683473, loss: 0.06401121616363525, data time: 0.027592241764068604\n",
      "step: 683474, loss: 0.057125821709632874, data time: 0.02468087938096788\n",
      "step: 683475, loss: 0.06428894400596619, data time: 0.022422003746032714\n",
      "step: 683476, loss: 0.06418877094984055, data time: 0.020583174445412376\n",
      "step: 683477, loss: 0.059278588742017746, data time: 0.019043684005737305\n",
      "step: 683478, loss: 0.0669221580028534, data time: 0.017743312395535983\n",
      "step: 683479, loss: 0.057075321674346924, data time: 0.016622373035975864\n",
      "step: 683480, loss: 0.06489643454551697, data time: 0.01566149393717448\n",
      "step: 683481, loss: 0.06276413798332214, data time: 0.0148276686668396\n",
      "step: 683482, loss: 0.05944595858454704, data time: 0.014079276253195369\n",
      "step: 683483, loss: 0.06318119168281555, data time: 0.013404793209499784\n",
      "step: 683484, loss: 0.06128595769405365, data time: 0.012806039107473273\n",
      "step: 683485, loss: 0.058014165610075, data time: 0.01227635145187378\n",
      "step: 683486, loss: 0.06276705116033554, data time: 0.01179754166376023\n",
      "step: 683487, loss: 0.05837523192167282, data time: 0.011357242410833185\n",
      "step: 683488, loss: 0.058730900287628174, data time: 0.010954908702684485\n",
      "step: 683489, loss: 0.061074115335941315, data time: 0.0105894406636556\n",
      "step: 683490, loss: 0.06105930730700493, data time: 0.010251541137695313\n",
      "step: 683491, loss: 0.06491657346487045, data time: 0.009939221235421987\n",
      "step: 683492, loss: 0.06239087134599686, data time: 0.009646318576954029\n",
      "step: 683493, loss: 0.06170312687754631, data time: 0.009373945849282401\n",
      "step: 683494, loss: 0.057722680270671844, data time: 0.009125339573827284\n",
      "step: 683495, loss: 0.06316038966178894, data time: 0.008893926938374838\n",
      "step: 683496, loss: 0.05666354298591614, data time: 0.008677659496184318\n",
      "step: 683497, loss: 0.06328292936086655, data time: 0.008478336036205292\n",
      "step: 683498, loss: 0.06530364602804184, data time: 0.008284185871933445\n",
      "step: 683499, loss: 0.060820065438747406, data time: 0.008101259960847743\n",
      "step: 683500, loss: 0.06318406760692596, data time: 0.007928916386195592\n",
      "step: 683501, loss: 0.05894775688648224, data time: 0.007760935359530979\n",
      "step: 683502, loss: 0.06023507937788963, data time: 0.007602208369487041\n",
      "step: 683503, loss: 0.06371643394231796, data time: 0.007456233626917789\n",
      "step: 683504, loss: 0.06273889541625977, data time: 0.007318246058928661\n",
      "step: 683505, loss: 0.04626256227493286, data time: 0.007187998294830323\n",
      "step: 683506, loss: 0.056650251150131226, data time: 0.19890713691711426\n",
      "step: 683507, loss: 0.059137012809515, data time: 0.10137248039245605\n",
      "step: 683508, loss: 0.05809576064348221, data time: 0.06807978947957356\n",
      "step: 683509, loss: 0.05830907076597214, data time: 0.05188018083572388\n",
      "step: 683510, loss: 0.06724055856466293, data time: 0.041785240173339844\n",
      "step: 683511, loss: 0.06039174646139145, data time: 0.03508555889129639\n",
      "step: 683512, loss: 0.0601230189204216, data time: 0.03029564448765346\n",
      "step: 683513, loss: 0.06277237087488174, data time: 0.026773184537887573\n",
      "step: 683514, loss: 0.06145906448364258, data time: 0.023950815200805664\n",
      "step: 683515, loss: 0.06593619287014008, data time: 0.021754932403564454\n",
      "step: 683516, loss: 0.06856369972229004, data time: 0.019970178604125977\n",
      "step: 683517, loss: 0.06040956825017929, data time: 0.018486241499582928\n",
      "step: 683518, loss: 0.058821141719818115, data time: 0.017258680783785306\n",
      "step: 683519, loss: 0.06706515699625015, data time: 0.016205906867980957\n",
      "step: 683520, loss: 0.06526036560535431, data time: 0.015300146738688151\n",
      "step: 683521, loss: 0.06330263614654541, data time: 0.014500260353088379\n",
      "step: 683522, loss: 0.061897870153188705, data time: 0.013788545832914464\n",
      "step: 683523, loss: 0.05934832617640495, data time: 0.013153963618808322\n",
      "step: 683524, loss: 0.06610041111707687, data time: 0.012594436344347502\n",
      "step: 683525, loss: 0.06127309054136276, data time: 0.012075114250183105\n",
      "step: 683526, loss: 0.056250520050525665, data time: 0.011605058397565569\n",
      "step: 683527, loss: 0.059913381934165955, data time: 0.01117540489543568\n",
      "step: 683528, loss: 0.061613764613866806, data time: 0.010781464369400688\n",
      "step: 683529, loss: 0.06644069403409958, data time: 0.010421196619669596\n",
      "step: 683530, loss: 0.05967540666460991, data time: 0.010089492797851563\n",
      "step: 683531, loss: 0.06566443294286728, data time: 0.009781333116384653\n",
      "step: 683532, loss: 0.06382936239242554, data time: 0.0095010863410102\n",
      "step: 683533, loss: 0.06474904716014862, data time: 0.009264051914215088\n",
      "step: 683534, loss: 0.0648658275604248, data time: 0.009033425100918474\n",
      "step: 683535, loss: 0.057885199785232544, data time: 0.008815248807271322\n",
      "step: 683536, loss: 0.05646809935569763, data time: 0.008610656184534873\n",
      "step: 683537, loss: 0.06033230945467949, data time: 0.008421279489994049\n",
      "step: 683538, loss: 0.057648252695798874, data time: 0.008232615210793236\n",
      "step: 683539, loss: 0.06287839263677597, data time: 0.008054284488453585\n",
      "step: 683540, loss: 0.06440380215644836, data time: 0.007883971078055246\n",
      "step: 683541, loss: 0.06178406625986099, data time: 0.0077210267384847\n",
      "step: 683542, loss: 0.060085833072662354, data time: 0.007564518902752851\n",
      "step: 683543, loss: 0.05846041440963745, data time: 0.007419203457079436\n",
      "step: 683544, loss: 0.06304921954870224, data time: 0.007280429204305013\n",
      "step: 683545, loss: 0.04242068529129028, data time: 0.007150709629058838\n",
      "step: 683546, loss: 0.061475515365600586, data time: 0.1859269142150879\n",
      "step: 683547, loss: 0.059114210307598114, data time: 0.09456360340118408\n",
      "step: 683548, loss: 0.0657612681388855, data time: 0.06355595588684082\n",
      "step: 683549, loss: 0.055311135947704315, data time: 0.04843127727508545\n",
      "step: 683550, loss: 0.06063855439424515, data time: 0.03902316093444824\n",
      "step: 683551, loss: 0.06291741132736206, data time: 0.03276177247365316\n",
      "step: 683552, loss: 0.062072351574897766, data time: 0.028285673686436245\n",
      "step: 683553, loss: 0.05893177539110184, data time: 0.025019079446792603\n",
      "step: 683554, loss: 0.06556403636932373, data time: 0.022386656867133245\n",
      "step: 683555, loss: 0.06166069209575653, data time: 0.020360708236694336\n",
      "step: 683556, loss: 0.061395417898893356, data time: 0.01871301911093972\n",
      "step: 683557, loss: 0.05937023088335991, data time: 0.017344156901041668\n",
      "step: 683558, loss: 0.05920301005244255, data time: 0.016173582810621995\n",
      "step: 683559, loss: 0.0657329261302948, data time: 0.015158721378871374\n",
      "step: 683560, loss: 0.05818033963441849, data time: 0.014296579360961913\n",
      "step: 683561, loss: 0.06394090503454208, data time: 0.013538256287574768\n",
      "step: 683562, loss: 0.07093119621276855, data time: 0.012891208424287684\n",
      "step: 683563, loss: 0.0571124404668808, data time: 0.012283510631985135\n",
      "step: 683564, loss: 0.06194276735186577, data time: 0.011741086056357935\n",
      "step: 683565, loss: 0.06160584092140198, data time: 0.011260282993316651\n",
      "step: 683566, loss: 0.05259165167808533, data time: 0.010826349258422852\n",
      "step: 683567, loss: 0.06419458985328674, data time: 0.01043147390538996\n",
      "step: 683568, loss: 0.05938023328781128, data time: 0.010066861691682236\n",
      "step: 683569, loss: 0.0603410005569458, data time: 0.009738812843958536\n",
      "step: 683570, loss: 0.05442899465560913, data time: 0.009431381225585938\n",
      "step: 683571, loss: 0.06636384129524231, data time: 0.00915067012493427\n",
      "step: 683572, loss: 0.0589752271771431, data time: 0.008952494020815249\n",
      "step: 683573, loss: 0.06102113053202629, data time: 0.008704755987439836\n",
      "step: 683574, loss: 0.06312701106071472, data time: 0.008479734946941507\n",
      "step: 683575, loss: 0.06266507506370544, data time: 0.008269604047139485\n",
      "step: 683576, loss: 0.058282554149627686, data time: 0.00807141488598239\n",
      "step: 683577, loss: 0.06037108227610588, data time: 0.007890433073043823\n",
      "step: 683578, loss: 0.06214015930891037, data time: 0.007711735638705167\n",
      "step: 683579, loss: 0.0578346811234951, data time: 0.007541712592629825\n",
      "step: 683580, loss: 0.06140942871570587, data time: 0.007386037281581334\n",
      "step: 683581, loss: 0.06111640855669975, data time: 0.007233533594343398\n",
      "step: 683582, loss: 0.07021380960941315, data time: 0.007093371571721257\n",
      "step: 683583, loss: 0.06717494130134583, data time: 0.006960479836714895\n",
      "step: 683584, loss: 0.06813465058803558, data time: 0.006834482535337791\n",
      "step: 683585, loss: 0.05449280887842178, data time: 0.006715041399002075\n",
      "step: 683586, loss: 0.06246558576822281, data time: 0.2030642032623291\n",
      "step: 683587, loss: 0.06254616379737854, data time: 0.10306811332702637\n",
      "step: 683588, loss: 0.0577562153339386, data time: 0.06976135571797688\n",
      "step: 683589, loss: 0.06265679001808167, data time: 0.052884459495544434\n",
      "step: 683590, loss: 0.06186004728078842, data time: 0.04258670806884766\n",
      "step: 683591, loss: 0.06280213594436646, data time: 0.03573791186014811\n",
      "step: 683592, loss: 0.06383314728736877, data time: 0.030935151236397878\n",
      "step: 683593, loss: 0.06914632767438889, data time: 0.027323424816131592\n",
      "step: 683594, loss: 0.06308053433895111, data time: 0.02443663279215495\n",
      "step: 683595, loss: 0.06449796259403229, data time: 0.022199082374572753\n",
      "step: 683596, loss: 0.058133333921432495, data time: 0.0203770940954035\n",
      "step: 683597, loss: 0.06952199339866638, data time: 0.01885541280110677\n",
      "step: 683598, loss: 0.06010013818740845, data time: 0.017571504299457256\n",
      "step: 683599, loss: 0.06278076767921448, data time: 0.016460418701171875\n",
      "step: 683600, loss: 0.06520160287618637, data time: 0.015503327051798502\n",
      "step: 683601, loss: 0.05991896986961365, data time: 0.014668896794319153\n",
      "step: 683602, loss: 0.06175466626882553, data time: 0.013932312236112706\n",
      "step: 683603, loss: 0.0638318583369255, data time: 0.013268722428215874\n",
      "step: 683604, loss: 0.06347008049488068, data time: 0.012694195697182104\n",
      "step: 683605, loss: 0.06230616569519043, data time: 0.012186706066131592\n",
      "step: 683606, loss: 0.05737524852156639, data time: 0.011724244980585007\n",
      "step: 683607, loss: 0.06996271014213562, data time: 0.011303587393327192\n",
      "step: 683608, loss: 0.05985420197248459, data time: 0.010919985563858696\n",
      "step: 683609, loss: 0.06474043428897858, data time: 0.010571777820587158\n",
      "step: 683610, loss: 0.06363728642463684, data time: 0.01025193214416504\n",
      "step: 683611, loss: 0.06330592185258865, data time: 0.009951563981863169\n",
      "step: 683612, loss: 0.06003893166780472, data time: 0.009671767552693685\n",
      "step: 683613, loss: 0.059019364416599274, data time: 0.009410858154296875\n",
      "step: 683614, loss: 0.05967099592089653, data time: 0.00917245601785594\n",
      "step: 683615, loss: 0.06168022006750107, data time: 0.008952728907267253\n",
      "step: 683616, loss: 0.06635367125272751, data time: 0.008745893355338805\n",
      "step: 683617, loss: 0.06543586403131485, data time: 0.008561171591281891\n",
      "step: 683618, loss: 0.05258613079786301, data time: 0.008366115165479256\n",
      "step: 683619, loss: 0.0639333724975586, data time: 0.008182588745565975\n",
      "step: 683620, loss: 0.06070215627551079, data time: 0.008006702150617327\n",
      "step: 683621, loss: 0.061513952910900116, data time: 0.00783808363808526\n",
      "step: 683622, loss: 0.06597673892974854, data time: 0.007680371000960066\n",
      "step: 683623, loss: 0.06379304081201553, data time: 0.007534466291728772\n",
      "step: 683624, loss: 0.05835401639342308, data time: 0.007396826377281776\n",
      "step: 683625, loss: 0.0530729703605175, data time: 0.007266867160797119\n",
      "step: 683626, loss: 0.061451178044080734, data time: 0.19261980056762695\n",
      "step: 683627, loss: 0.06270407140254974, data time: 0.0970766544342041\n",
      "step: 683628, loss: 0.056584589183330536, data time: 0.06548905372619629\n",
      "step: 683629, loss: 0.059067558497190475, data time: 0.05003666877746582\n",
      "step: 683630, loss: 0.06079387664794922, data time: 0.04032044410705567\n",
      "step: 683631, loss: 0.05919279903173447, data time: 0.03384943803151449\n",
      "step: 683632, loss: 0.06017401069402695, data time: 0.029241459710257395\n",
      "step: 683633, loss: 0.06760307401418686, data time: 0.025843322277069092\n",
      "step: 683634, loss: 0.05755336210131645, data time: 0.023115873336791992\n",
      "step: 683635, loss: 0.05838197469711304, data time: 0.021015501022338866\n",
      "step: 683636, loss: 0.06438381969928741, data time: 0.01930581439625133\n",
      "step: 683637, loss: 0.062496863305568695, data time: 0.01788012186686198\n",
      "step: 683638, loss: 0.062324997037649155, data time: 0.016667255988487832\n",
      "step: 683639, loss: 0.06411787867546082, data time: 0.01562190055847168\n",
      "step: 683640, loss: 0.05765184760093689, data time: 0.014718961715698243\n",
      "step: 683641, loss: 0.059780463576316833, data time: 0.013936832547187805\n",
      "step: 683642, loss: 0.06363819539546967, data time: 0.013246213688569911\n",
      "step: 683643, loss: 0.05978390574455261, data time: 0.012617972162034776\n",
      "step: 683644, loss: 0.05797041207551956, data time: 0.012065297678897255\n",
      "step: 683645, loss: 0.058099985122680664, data time: 0.011568355560302734\n",
      "step: 683646, loss: 0.061867162585258484, data time: 0.011123066856747582\n",
      "step: 683647, loss: 0.06176726147532463, data time: 0.01071508364243941\n",
      "step: 683648, loss: 0.06141766905784607, data time: 0.010340172311534052\n",
      "step: 683649, loss: 0.05871120095252991, data time: 0.009994695583979288\n",
      "step: 683650, loss: 0.05963566526770592, data time: 0.009678802490234374\n",
      "step: 683651, loss: 0.06322681903839111, data time: 0.009386035112234263\n",
      "step: 683652, loss: 0.06608057022094727, data time: 0.009112631833111798\n",
      "step: 683653, loss: 0.06316915154457092, data time: 0.008862086704799108\n",
      "step: 683654, loss: 0.06284873187541962, data time: 0.00863173090178391\n",
      "step: 683655, loss: 0.05744246765971184, data time: 0.00841505527496338\n",
      "step: 683656, loss: 0.05908132344484329, data time: 0.008215335107618762\n",
      "step: 683657, loss: 0.058349739760160446, data time: 0.008031174540519714\n",
      "step: 683658, loss: 0.06918951869010925, data time: 0.007847814848928741\n",
      "step: 683659, loss: 0.058386217802762985, data time: 0.007676159634309656\n",
      "step: 683660, loss: 0.06109980493783951, data time: 0.007517835072108677\n",
      "step: 683661, loss: 0.06919580698013306, data time: 0.007359822591145833\n",
      "step: 683662, loss: 0.06678937375545502, data time: 0.007214404441214897\n",
      "step: 683663, loss: 0.0616510771214962, data time: 0.007079908722325375\n",
      "step: 683664, loss: 0.06088537722826004, data time: 0.006950341738187349\n",
      "step: 683665, loss: 0.06268700212240219, data time: 0.006826841831207275\n",
      "step: 683666, loss: 0.06245413422584534, data time: 0.18996715545654297\n",
      "step: 683667, loss: 0.06179892644286156, data time: 0.0965954065322876\n",
      "step: 683668, loss: 0.06165765970945358, data time: 0.06489777565002441\n",
      "step: 683669, loss: 0.0646016001701355, data time: 0.04953855276107788\n",
      "step: 683670, loss: 0.06521552801132202, data time: 0.0399566650390625\n",
      "step: 683671, loss: 0.06662578880786896, data time: 0.03355097770690918\n",
      "step: 683672, loss: 0.0590975284576416, data time: 0.028992925371442522\n",
      "step: 683673, loss: 0.06879344582557678, data time: 0.02563193440437317\n",
      "step: 683674, loss: 0.0699612945318222, data time: 0.02295578850640191\n",
      "step: 683675, loss: 0.058621346950531006, data time: 0.020897412300109865\n",
      "step: 683676, loss: 0.06487668305635452, data time: 0.019224925474687057\n",
      "step: 683677, loss: 0.06135892868041992, data time: 0.017831802368164062\n",
      "step: 683678, loss: 0.060257792472839355, data time: 0.016653794508713942\n",
      "step: 683679, loss: 0.057829599827528, data time: 0.015636869839259555\n",
      "step: 683680, loss: 0.06089148670434952, data time: 0.014758046468098958\n",
      "step: 683681, loss: 0.0556911826133728, data time: 0.01399432122707367\n",
      "step: 683682, loss: 0.058140598237514496, data time: 0.013321245417875402\n",
      "step: 683683, loss: 0.06226347014307976, data time: 0.012709723578559028\n",
      "step: 683684, loss: 0.06637357920408249, data time: 0.012168959567421362\n",
      "step: 683685, loss: 0.05955914780497551, data time: 0.011684918403625488\n",
      "step: 683686, loss: 0.06280605494976044, data time: 0.01124898592631022\n",
      "step: 683687, loss: 0.06164446473121643, data time: 0.01085199009288441\n",
      "step: 683688, loss: 0.06024886667728424, data time: 0.010487857072249702\n",
      "step: 683689, loss: 0.06262478977441788, data time: 0.010169694821039835\n",
      "step: 683690, loss: 0.05868715047836304, data time: 0.009865388870239258\n",
      "step: 683691, loss: 0.05777508765459061, data time: 0.009584573599008413\n",
      "step: 683692, loss: 0.060272954404354095, data time: 0.00931611767521611\n",
      "step: 683693, loss: 0.0652046725153923, data time: 0.009070379393441337\n",
      "step: 683694, loss: 0.06138938292860985, data time: 0.008841021307583513\n",
      "step: 683695, loss: 0.06257684528827667, data time: 0.008629965782165527\n",
      "step: 683696, loss: 0.06055155396461487, data time: 0.008432788233603201\n",
      "step: 683697, loss: 0.0590595118701458, data time: 0.0082511305809021\n",
      "step: 683698, loss: 0.061541881412267685, data time: 0.008066220716996626\n",
      "step: 683699, loss: 0.052653875201940536, data time: 0.007892314125509822\n",
      "step: 683700, loss: 0.062271349132061005, data time: 0.007728202002389091\n",
      "step: 683701, loss: 0.06039726734161377, data time: 0.007568617661794026\n",
      "step: 683702, loss: 0.06114464998245239, data time: 0.00741869694477803\n",
      "step: 683703, loss: 0.06139378994703293, data time: 0.0072787623656423465\n",
      "step: 683704, loss: 0.06245408579707146, data time: 0.0071469331398988385\n",
      "step: 683705, loss: 0.05487269163131714, data time: 0.007021409273147583\n",
      "step: 683706, loss: 0.06325557082891464, data time: 0.2108011245727539\n",
      "step: 683707, loss: 0.05459500104188919, data time: 0.10613894462585449\n",
      "step: 683708, loss: 0.0619250051677227, data time: 0.07179347674051921\n",
      "step: 683709, loss: 0.06629322469234467, data time: 0.054612159729003906\n",
      "step: 683710, loss: 0.0617724172770977, data time: 0.043981409072875975\n",
      "step: 683711, loss: 0.06073104590177536, data time: 0.036903818448384605\n",
      "step: 683712, loss: 0.06355166435241699, data time: 0.03186147553580148\n",
      "step: 683713, loss: 0.060010310262441635, data time: 0.02813473343849182\n",
      "step: 683714, loss: 0.060699742287397385, data time: 0.02517003483242459\n",
      "step: 683715, loss: 0.06626197695732117, data time: 0.022855591773986817\n",
      "step: 683716, loss: 0.05697272717952728, data time: 0.020971710031682796\n",
      "step: 683717, loss: 0.05901731178164482, data time: 0.01940383513768514\n",
      "step: 683718, loss: 0.06207915395498276, data time: 0.018082416974581204\n",
      "step: 683719, loss: 0.06243854761123657, data time: 0.016938039234706333\n",
      "step: 683720, loss: 0.06193387508392334, data time: 0.01595028241475423\n",
      "step: 683721, loss: 0.06631544232368469, data time: 0.015085592865943909\n",
      "step: 683722, loss: 0.061727315187454224, data time: 0.014316642985624425\n",
      "step: 683723, loss: 0.06815292686223984, data time: 0.01363152927822537\n",
      "step: 683724, loss: 0.060713570564985275, data time: 0.01302077895716617\n",
      "step: 683725, loss: 0.06222817301750183, data time: 0.012492191791534425\n",
      "step: 683726, loss: 0.06516489386558533, data time: 0.012017147881644112\n",
      "step: 683727, loss: 0.06536083668470383, data time: 0.011582710526206276\n",
      "step: 683728, loss: 0.05951491743326187, data time: 0.011183811270672342\n",
      "step: 683729, loss: 0.06813275814056396, data time: 0.01082142194112142\n",
      "step: 683730, loss: 0.0640646442770958, data time: 0.010492601394653321\n",
      "step: 683731, loss: 0.06233321875333786, data time: 0.010183059252225436\n",
      "step: 683732, loss: 0.05619805306196213, data time: 0.009892551987259477\n",
      "step: 683733, loss: 0.06717656552791595, data time: 0.009624319417136056\n",
      "step: 683734, loss: 0.06060227006673813, data time: 0.009378827851394126\n",
      "step: 683735, loss: 0.06713785976171494, data time: 0.009150203069051106\n",
      "step: 683736, loss: 0.060442786663770676, data time: 0.008936135999618037\n",
      "step: 683737, loss: 0.06319769471883774, data time: 0.008737415075302124\n",
      "step: 683738, loss: 0.06789547204971313, data time: 0.008536757844867128\n",
      "step: 683739, loss: 0.06076899170875549, data time: 0.008347651537726907\n",
      "step: 683740, loss: 0.07194584608078003, data time: 0.008167907169886997\n",
      "step: 683741, loss: 0.07155465334653854, data time: 0.007996678352355957\n",
      "step: 683742, loss: 0.05544459819793701, data time: 0.00783682513881374\n",
      "step: 683743, loss: 0.05250926315784454, data time: 0.007687869824861225\n",
      "step: 683744, loss: 0.05830260366201401, data time: 0.00754554455096905\n",
      "step: 683745, loss: 0.035358235239982605, data time: 0.0074101507663726805\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40895915031433105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 683746, loss: 0.05579938739538193, data time: 0.2030162811279297\n",
      "step: 683747, loss: 0.061610788106918335, data time: 0.10230875015258789\n",
      "step: 683748, loss: 0.06345440447330475, data time: 0.06874489784240723\n",
      "step: 683749, loss: 0.06212472915649414, data time: 0.05242276191711426\n",
      "step: 683750, loss: 0.06524163484573364, data time: 0.04222254753112793\n",
      "step: 683751, loss: 0.06002695485949516, data time: 0.03542105356852213\n",
      "step: 683752, loss: 0.05959886312484741, data time: 0.03055971009390695\n",
      "step: 683753, loss: 0.06761743128299713, data time: 0.027000606060028076\n",
      "step: 683754, loss: 0.05563031882047653, data time: 0.024146768781873915\n",
      "step: 683755, loss: 0.06523447483778, data time: 0.021929454803466798\n",
      "step: 683756, loss: 0.0600910410284996, data time: 0.020129268819635563\n",
      "step: 683757, loss: 0.06739895790815353, data time: 0.018632312615712483\n",
      "step: 683758, loss: 0.06239078938961029, data time: 0.017360944014329176\n",
      "step: 683759, loss: 0.06383152306079865, data time: 0.01626488140651158\n",
      "step: 683760, loss: 0.06011150777339935, data time: 0.015314149856567382\n",
      "step: 683761, loss: 0.05994147062301636, data time: 0.014493033289909363\n",
      "step: 683762, loss: 0.06532980501651764, data time: 0.013765363132252413\n",
      "step: 683763, loss: 0.061915259808301926, data time: 0.013112359576755099\n",
      "step: 683764, loss: 0.06307157874107361, data time: 0.012526399210879677\n",
      "step: 683765, loss: 0.06777024269104004, data time: 0.01201392412185669\n",
      "step: 683766, loss: 0.06353422999382019, data time: 0.011543966474987212\n",
      "step: 683767, loss: 0.06162717938423157, data time: 0.011119950901378284\n",
      "step: 683768, loss: 0.06037069857120514, data time: 0.010722782300866169\n",
      "step: 683769, loss: 0.06307487189769745, data time: 0.010361313819885254\n",
      "step: 683770, loss: 0.06416716426610947, data time: 0.010026702880859375\n",
      "step: 683771, loss: 0.06543921679258347, data time: 0.009718977488004245\n",
      "step: 683772, loss: 0.058269016444683075, data time: 0.009429763864587855\n",
      "step: 683773, loss: 0.05819185823202133, data time: 0.009162570749010359\n",
      "step: 683774, loss: 0.0590771846473217, data time: 0.008921689000622979\n",
      "step: 683775, loss: 0.06321621686220169, data time: 0.008698399861653645\n",
      "step: 683776, loss: 0.05749357491731644, data time: 0.008485486430506553\n",
      "step: 683777, loss: 0.05762133747339249, data time: 0.008291170001029968\n",
      "step: 683778, loss: 0.06225262209773064, data time: 0.008115710634173769\n",
      "step: 683779, loss: 0.061408381909132004, data time: 0.007938967031591079\n",
      "step: 683780, loss: 0.06308054178953171, data time: 0.007772493362426758\n",
      "step: 683781, loss: 0.06226765364408493, data time: 0.007611301210191514\n",
      "step: 683782, loss: 0.06453190743923187, data time: 0.00745988536525417\n",
      "step: 683783, loss: 0.06275929510593414, data time: 0.007326031986035798\n",
      "step: 683784, loss: 0.056605637073516846, data time: 0.007195613323113857\n",
      "step: 683785, loss: 0.08008202910423279, data time: 0.007077348232269287\n",
      "step: 683786, loss: 0.06416285037994385, data time: 0.20402169227600098\n",
      "step: 683787, loss: 0.06728889793157578, data time: 0.10335779190063477\n",
      "step: 683788, loss: 0.06299223750829697, data time: 0.06940793991088867\n",
      "step: 683789, loss: 0.06556475907564163, data time: 0.05292242765426636\n",
      "step: 683790, loss: 0.06290195882320404, data time: 0.042624711990356445\n",
      "step: 683791, loss: 0.062499403953552246, data time: 0.03576246897379557\n",
      "step: 683792, loss: 0.06141997128725052, data time: 0.030861616134643555\n",
      "step: 683793, loss: 0.059149157255887985, data time: 0.027318716049194336\n",
      "step: 683794, loss: 0.06249780207872391, data time: 0.024448527230156794\n",
      "step: 683795, loss: 0.06375721096992493, data time: 0.022242522239685057\n",
      "step: 683796, loss: 0.06898638606071472, data time: 0.020450310273603958\n",
      "step: 683797, loss: 0.06635255366563797, data time: 0.01896442969640096\n",
      "step: 683798, loss: 0.06262372434139252, data time: 0.01769861808189979\n",
      "step: 683799, loss: 0.06333208084106445, data time: 0.016600234167916433\n",
      "step: 683800, loss: 0.06178010627627373, data time: 0.015655899047851564\n",
      "step: 683801, loss: 0.06424829363822937, data time: 0.014827728271484375\n",
      "step: 683802, loss: 0.06520476192235947, data time: 0.0140990509706385\n",
      "step: 683803, loss: 0.05907314643263817, data time: 0.01344344351026747\n",
      "step: 683804, loss: 0.057002123445272446, data time: 0.012861653378135279\n",
      "step: 683805, loss: 0.06993564963340759, data time: 0.012343335151672363\n",
      "step: 683806, loss: 0.05887004733085632, data time: 0.011874709810529436\n",
      "step: 683807, loss: 0.05778318643569946, data time: 0.011455839330499823\n",
      "step: 683808, loss: 0.06235747039318085, data time: 0.011061440343442171\n",
      "step: 683809, loss: 0.059979867190122604, data time: 0.010702967643737793\n",
      "step: 683810, loss: 0.05786282569169998, data time: 0.010372896194458008\n",
      "step: 683811, loss: 0.06092442572116852, data time: 0.010067417071415828\n",
      "step: 683812, loss: 0.057329051196575165, data time: 0.00976652569240994\n",
      "step: 683813, loss: 0.06085502356290817, data time: 0.009488667760576521\n",
      "step: 683814, loss: 0.06344829499721527, data time: 0.00923631109040359\n",
      "step: 683815, loss: 0.059572648257017136, data time: 0.008998664220174153\n",
      "step: 683816, loss: 0.059124212712049484, data time: 0.008792615705920805\n",
      "step: 683817, loss: 0.059764280915260315, data time: 0.00858936458826065\n",
      "step: 683818, loss: 0.06089835986495018, data time: 0.008388056899562027\n",
      "step: 683819, loss: 0.05237352475523949, data time: 0.00819694995880127\n",
      "step: 683820, loss: 0.06345802545547485, data time: 0.008017328807285853\n",
      "step: 683821, loss: 0.061500899493694305, data time: 0.007845229572719999\n",
      "step: 683822, loss: 0.05718745291233063, data time: 0.007684069710808831\n",
      "step: 683823, loss: 0.06592011451721191, data time: 0.007536367366188451\n",
      "step: 683824, loss: 0.0616815909743309, data time: 0.007394845669086163\n",
      "step: 683825, loss: 0.03686708211898804, data time: 0.007262492179870605\n",
      "step: 683826, loss: 0.06399969756603241, data time: 0.20847129821777344\n",
      "step: 683827, loss: 0.062237322330474854, data time: 0.1050567626953125\n",
      "step: 683828, loss: 0.05366508290171623, data time: 0.07053685188293457\n",
      "step: 683829, loss: 0.06342120468616486, data time: 0.05365169048309326\n",
      "step: 683830, loss: 0.056086376309394836, data time: 0.043203115463256836\n",
      "step: 683831, loss: 0.060830287635326385, data time: 0.03624578317006429\n",
      "step: 683832, loss: 0.060543473809957504, data time: 0.03127830369131906\n",
      "step: 683833, loss: 0.06102750822901726, data time: 0.02762681245803833\n",
      "step: 683834, loss: 0.06374920159578323, data time: 0.02470639016893175\n",
      "step: 683835, loss: 0.06316326558589935, data time: 0.022431087493896485\n",
      "step: 683836, loss: 0.05727998539805412, data time: 0.0205867507240989\n",
      "step: 683837, loss: 0.06551054120063782, data time: 0.01904769738515218\n",
      "step: 683838, loss: 0.06424744427204132, data time: 0.01774743887094351\n",
      "step: 683839, loss: 0.0607999786734581, data time: 0.016624246324811662\n",
      "step: 683840, loss: 0.05719407647848129, data time: 0.015649159749348957\n",
      "step: 683841, loss: 0.066049724817276, data time: 0.014799684286117554\n",
      "step: 683842, loss: 0.06242399662733078, data time: 0.014050946516149184\n",
      "step: 683843, loss: 0.07203599810600281, data time: 0.01338085863325331\n",
      "step: 683844, loss: 0.06072775647044182, data time: 0.012780227159198961\n",
      "step: 683845, loss: 0.060172807425260544, data time: 0.012249767780303955\n",
      "step: 683846, loss: 0.06836103647947311, data time: 0.011772973196847098\n",
      "step: 683847, loss: 0.05451490730047226, data time: 0.011341300877657804\n",
      "step: 683848, loss: 0.0700482428073883, data time: 0.01093974320784859\n",
      "step: 683849, loss: 0.059888772666454315, data time: 0.010567545890808105\n",
      "step: 683850, loss: 0.061439625918865204, data time: 0.010248260498046875\n",
      "step: 683851, loss: 0.06387188285589218, data time: 0.009948537899897648\n",
      "step: 683852, loss: 0.06122151017189026, data time: 0.009666425210458261\n",
      "step: 683853, loss: 0.06092839688062668, data time: 0.009404948779514857\n",
      "step: 683854, loss: 0.058957815170288086, data time: 0.009166257134799299\n",
      "step: 683855, loss: 0.06458630412817001, data time: 0.008944829305013021\n",
      "step: 683856, loss: 0.05971178039908409, data time: 0.008740617382910943\n",
      "step: 683857, loss: 0.059739600867033005, data time: 0.008547954261302948\n",
      "step: 683858, loss: 0.0638142079114914, data time: 0.00835143436085094\n",
      "step: 683859, loss: 0.05554580315947533, data time: 0.008167484227348776\n",
      "step: 683860, loss: 0.06439050287008286, data time: 0.007993289402553014\n",
      "step: 683861, loss: 0.05353450030088425, data time: 0.00782637463675605\n",
      "step: 683862, loss: 0.059348784387111664, data time: 0.007667921684883736\n",
      "step: 683863, loss: 0.061343006789684296, data time: 0.007523222973472194\n",
      "step: 683864, loss: 0.06332919001579285, data time: 0.007384642576559996\n",
      "step: 683865, loss: 0.10905848443508148, data time: 0.007254993915557862\n",
      "step: 683866, loss: 0.05926480144262314, data time: 0.19582247734069824\n",
      "step: 683867, loss: 0.0605633482336998, data time: 0.0986793041229248\n",
      "step: 683868, loss: 0.06286434829235077, data time: 0.06684263547261556\n",
      "step: 683869, loss: 0.05911805480718613, data time: 0.05092334747314453\n",
      "step: 683870, loss: 0.06155873462557793, data time: 0.04103121757507324\n",
      "step: 683871, loss: 0.06053406000137329, data time: 0.03444552421569824\n",
      "step: 683872, loss: 0.06520943343639374, data time: 0.029739482062203542\n",
      "step: 683873, loss: 0.06393438577651978, data time: 0.026326030492782593\n",
      "step: 683874, loss: 0.05946410819888115, data time: 0.02358730634053548\n",
      "step: 683875, loss: 0.06260110437870026, data time: 0.021466803550720216\n",
      "step: 683876, loss: 0.05973926931619644, data time: 0.019745068116621536\n",
      "step: 683877, loss: 0.06482657790184021, data time: 0.018311917781829834\n",
      "step: 683878, loss: 0.058534763753414154, data time: 0.01709776658278245\n",
      "step: 683879, loss: 0.0678614154458046, data time: 0.01604715415409633\n",
      "step: 683880, loss: 0.066440649330616, data time: 0.015151341756184896\n",
      "step: 683881, loss: 0.06435131281614304, data time: 0.0143565833568573\n",
      "step: 683882, loss: 0.06318625807762146, data time: 0.01365215638104607\n",
      "step: 683883, loss: 0.06127114221453667, data time: 0.013002104229397245\n",
      "step: 683884, loss: 0.06245216727256775, data time: 0.012424519187525698\n",
      "step: 683885, loss: 0.06263190507888794, data time: 0.011915493011474609\n",
      "step: 683886, loss: 0.07165123522281647, data time: 0.01145376477922712\n",
      "step: 683887, loss: 0.05736234411597252, data time: 0.011029969562183727\n",
      "step: 683888, loss: 0.0551186129450798, data time: 0.010635334512461786\n",
      "step: 683889, loss: 0.07530328631401062, data time: 0.010284900665283203\n",
      "step: 683890, loss: 0.06470373272895813, data time: 0.009960975646972656\n",
      "step: 683891, loss: 0.0583154670894146, data time: 0.009655283047602726\n",
      "step: 683892, loss: 0.06136391684412956, data time: 0.009371413124932183\n",
      "step: 683893, loss: 0.05740063637495041, data time: 0.009109275681631905\n",
      "step: 683894, loss: 0.05882406234741211, data time: 0.008869195806569067\n",
      "step: 683895, loss: 0.05391044542193413, data time: 0.008648316065470377\n",
      "step: 683896, loss: 0.06390116363763809, data time: 0.008440563755650674\n",
      "step: 683897, loss: 0.0689724013209343, data time: 0.008247308433055878\n",
      "step: 683898, loss: 0.06079156696796417, data time: 0.008059046485207298\n",
      "step: 683899, loss: 0.06148161739110947, data time: 0.007884025573730469\n",
      "step: 683900, loss: 0.06428994983434677, data time: 0.00771709850856236\n",
      "step: 683901, loss: 0.0646381601691246, data time: 0.007557133833567302\n",
      "step: 683902, loss: 0.06452860683202744, data time: 0.007407285071708061\n",
      "step: 683903, loss: 0.056817710399627686, data time: 0.00726896838137978\n",
      "step: 683904, loss: 0.06413262337446213, data time: 0.007138973627334986\n",
      "step: 683905, loss: 0.03506705164909363, data time: 0.007013946771621704\n",
      "step: 683906, loss: 0.05676255747675896, data time: 0.19658660888671875\n",
      "step: 683907, loss: 0.061828892678022385, data time: 0.09912848472595215\n",
      "step: 683908, loss: 0.057829003781080246, data time: 0.06701405843098958\n",
      "step: 683909, loss: 0.06556974351406097, data time: 0.05102372169494629\n",
      "step: 683910, loss: 0.06313972175121307, data time: 0.04109702110290527\n",
      "step: 683911, loss: 0.06196029484272003, data time: 0.03449483712514242\n",
      "step: 683912, loss: 0.0687040165066719, data time: 0.029775891985212053\n",
      "step: 683913, loss: 0.05713397264480591, data time: 0.02631184458732605\n",
      "step: 683914, loss: 0.06473135203123093, data time: 0.02354619238111708\n",
      "step: 683915, loss: 0.053590964525938034, data time: 0.02139103412628174\n",
      "step: 683916, loss: 0.06296014040708542, data time: 0.019646644592285156\n",
      "step: 683917, loss: 0.06367626786231995, data time: 0.01818851629892985\n",
      "step: 683918, loss: 0.06695923209190369, data time: 0.016959337087777946\n",
      "step: 683919, loss: 0.06719057261943817, data time: 0.015892914363316128\n",
      "step: 683920, loss: 0.06264106184244156, data time: 0.01497632662455241\n",
      "step: 683921, loss: 0.061518874019384384, data time: 0.01417599618434906\n",
      "step: 683922, loss: 0.062025077641010284, data time: 0.013465713052188648\n",
      "step: 683923, loss: 0.06815530359745026, data time: 0.012822760476006402\n",
      "step: 683924, loss: 0.05725915729999542, data time: 0.012251163783826326\n",
      "step: 683925, loss: 0.06245943158864975, data time: 0.011745798587799072\n",
      "step: 683926, loss: 0.06717713177204132, data time: 0.011289732796805245\n",
      "step: 683927, loss: 0.05584746226668358, data time: 0.010874943299727007\n",
      "step: 683928, loss: 0.06004786118865013, data time: 0.010495517564856487\n",
      "step: 683929, loss: 0.06055138632655144, data time: 0.010145137707392374\n",
      "step: 683930, loss: 0.0693528950214386, data time: 0.009823188781738282\n",
      "step: 683931, loss: 0.060053758323192596, data time: 0.009522181290846605\n",
      "step: 683932, loss: 0.0650363489985466, data time: 0.00924292317143193\n",
      "step: 683933, loss: 0.06041208654642105, data time: 0.008988525186266218\n",
      "step: 683934, loss: 0.058659061789512634, data time: 0.0087548042165822\n",
      "step: 683935, loss: 0.06354425847530365, data time: 0.008534439404805501\n",
      "step: 683936, loss: 0.06561891734600067, data time: 0.008328776205739668\n",
      "step: 683937, loss: 0.058729808777570724, data time: 0.008141584694385529\n",
      "step: 683938, loss: 0.058804284781217575, data time: 0.007955478899406664\n",
      "step: 683939, loss: 0.061454888433218, data time: 0.007779093349681181\n",
      "step: 683940, loss: 0.06351794302463531, data time: 0.007610409600394112\n",
      "step: 683941, loss: 0.06815314292907715, data time: 0.007450693183475071\n",
      "step: 683942, loss: 0.059162478893995285, data time: 0.007300454217034417\n",
      "step: 683943, loss: 0.060106515884399414, data time: 0.007171116377177991\n",
      "step: 683944, loss: 0.06061658263206482, data time: 0.007039155715551131\n",
      "step: 683945, loss: 0.04109915345907211, data time: 0.006916522979736328\n",
      "step: 683946, loss: 0.06269317120313644, data time: 0.19086527824401855\n",
      "step: 683947, loss: 0.059466540813446045, data time: 0.09657943248748779\n",
      "step: 683948, loss: 0.06058165058493614, data time: 0.0655359427134196\n",
      "step: 683949, loss: 0.06639660894870758, data time: 0.04981142282485962\n",
      "step: 683950, loss: 0.06423088908195496, data time: 0.04013118743896484\n",
      "step: 683951, loss: 0.0673380196094513, data time: 0.03370165824890137\n",
      "step: 683952, loss: 0.0640963613986969, data time: 0.029100826808384488\n",
      "step: 683953, loss: 0.06689997017383575, data time: 0.025719672441482544\n",
      "step: 683954, loss: 0.06098725646734238, data time: 0.023015393151177302\n",
      "step: 683955, loss: 0.059836700558662415, data time: 0.020915865898132324\n",
      "step: 683956, loss: 0.06335298717021942, data time: 0.01920628547668457\n",
      "step: 683957, loss: 0.05920719727873802, data time: 0.01778244972229004\n",
      "step: 683958, loss: 0.07028879970312119, data time: 0.01658146197979267\n",
      "step: 683959, loss: 0.06192515045404434, data time: 0.015554070472717285\n",
      "step: 683960, loss: 0.06464125961065292, data time: 0.014652903874715168\n",
      "step: 683961, loss: 0.0629853680729866, data time: 0.013878241181373596\n",
      "step: 683962, loss: 0.060966990888118744, data time: 0.013181770549100988\n",
      "step: 683963, loss: 0.06152806803584099, data time: 0.012558076116773818\n",
      "step: 683964, loss: 0.05955987423658371, data time: 0.012001163081118935\n",
      "step: 683965, loss: 0.0621301531791687, data time: 0.011507856845855712\n",
      "step: 683966, loss: 0.060479722917079926, data time: 0.01106315567379906\n",
      "step: 683967, loss: 0.06100234389305115, data time: 0.010662436485290527\n",
      "step: 683968, loss: 0.06720577925443649, data time: 0.01030105093251104\n",
      "step: 683969, loss: 0.06583874672651291, data time: 0.009975085655848185\n",
      "step: 683970, loss: 0.05741163343191147, data time: 0.009675559997558593\n",
      "step: 683971, loss: 0.06687920540571213, data time: 0.009397405844468337\n",
      "step: 683972, loss: 0.06206329166889191, data time: 0.009128817805537471\n",
      "step: 683973, loss: 0.06577365100383759, data time: 0.008874569620404924\n",
      "step: 683974, loss: 0.058882590383291245, data time: 0.008644136889227506\n",
      "step: 683975, loss: 0.056119706481695175, data time: 0.008428510030110676\n",
      "step: 683976, loss: 0.062211982905864716, data time: 0.008225041051064768\n",
      "step: 683977, loss: 0.06245192140340805, data time: 0.008036978542804718\n",
      "step: 683978, loss: 0.06431034952402115, data time: 0.00785493128227465\n",
      "step: 683979, loss: 0.06156111881136894, data time: 0.007684223792132209\n",
      "step: 683980, loss: 0.0653609037399292, data time: 0.0075190407889229914\n",
      "step: 683981, loss: 0.06487354636192322, data time: 0.007361200120713975\n",
      "step: 683982, loss: 0.06066073104739189, data time: 0.007213772954167546\n",
      "step: 683983, loss: 0.05496517941355705, data time: 0.007077857067710475\n",
      "step: 683984, loss: 0.05781824514269829, data time: 0.006949149645291842\n",
      "step: 683985, loss: 0.07522447407245636, data time: 0.0068253159523010256\n",
      "step: 683986, loss: 0.06480422616004944, data time: 0.20245981216430664\n",
      "step: 683987, loss: 0.06913399696350098, data time: 0.10287189483642578\n",
      "step: 683988, loss: 0.05955211818218231, data time: 0.06910006205240886\n",
      "step: 683989, loss: 0.06230758875608444, data time: 0.052710771560668945\n",
      "step: 683990, loss: 0.05747554823756218, data time: 0.042450237274169925\n",
      "step: 683991, loss: 0.0610622763633728, data time: 0.03562784194946289\n",
      "step: 683992, loss: 0.05728721618652344, data time: 0.03074162346976144\n",
      "step: 683993, loss: 0.060329437255859375, data time: 0.027152955532073975\n",
      "step: 683994, loss: 0.05851532146334648, data time: 0.024281846152411565\n",
      "step: 683995, loss: 0.058368340134620667, data time: 0.02205393314361572\n",
      "step: 683996, loss: 0.057357143610715866, data time: 0.020244728435169567\n",
      "step: 683997, loss: 0.06358043849468231, data time: 0.018735686937967937\n",
      "step: 683998, loss: 0.06267549842596054, data time: 0.017455156032855693\n",
      "step: 683999, loss: 0.06238406151533127, data time: 0.01636026586805071\n",
      "step: 684000, loss: 0.06572592258453369, data time: 0.015411074956258137\n",
      "step: 684001, loss: 0.0632944330573082, data time: 0.014573663473129272\n",
      "step: 684002, loss: 0.06461405009031296, data time: 0.013838347266702092\n",
      "step: 684003, loss: 0.058889903128147125, data time: 0.013186759418911405\n",
      "step: 684004, loss: 0.06377548724412918, data time: 0.012605227922138414\n",
      "step: 684005, loss: 0.06021268293261528, data time: 0.012086308002471924\n",
      "step: 684006, loss: 0.0588839165866375, data time: 0.011613686879475912\n",
      "step: 684007, loss: 0.06075866147875786, data time: 0.011183565313165838\n",
      "step: 684008, loss: 0.05923343822360039, data time: 0.010784263196198837\n",
      "step: 684009, loss: 0.05852959305047989, data time: 0.010426133871078491\n",
      "step: 684010, loss: 0.058705687522888184, data time: 0.010091075897216797\n",
      "step: 684011, loss: 0.06102600321173668, data time: 0.009782057542067308\n",
      "step: 684012, loss: 0.060452502220869064, data time: 0.009493245018853081\n",
      "step: 684013, loss: 0.059189215302467346, data time: 0.009229089532579695\n",
      "step: 684014, loss: 0.06248476728796959, data time: 0.008984894588075835\n",
      "step: 684015, loss: 0.06519117951393127, data time: 0.008757305145263673\n",
      "step: 684016, loss: 0.06917951256036758, data time: 0.008545960149457377\n",
      "step: 684017, loss: 0.05720257759094238, data time: 0.008353598415851593\n",
      "step: 684018, loss: 0.06574279069900513, data time: 0.008162260055541992\n",
      "step: 684019, loss: 0.05831078812479973, data time: 0.007977604866027832\n",
      "step: 684020, loss: 0.06332532316446304, data time: 0.007806852885654994\n",
      "step: 684021, loss: 0.06274078786373138, data time: 0.007642341984642876\n",
      "step: 684022, loss: 0.062484271824359894, data time: 0.00748678800222036\n",
      "step: 684023, loss: 0.061474286019802094, data time: 0.0073428028508236536\n",
      "step: 684024, loss: 0.055500030517578125, data time: 0.007206378838954828\n",
      "step: 684025, loss: 0.06228761002421379, data time: 0.007077157497406006\n",
      "step: 684026, loss: 0.06273403018712997, data time: 0.20422744750976562\n",
      "step: 684027, loss: 0.06456458568572998, data time: 0.1028815507888794\n",
      "step: 684028, loss: 0.06475917994976044, data time: 0.06909608840942383\n",
      "step: 684029, loss: 0.06320127844810486, data time: 0.052694618701934814\n",
      "step: 684030, loss: 0.060865677893161774, data time: 0.04242691993713379\n",
      "step: 684031, loss: 0.06421902030706406, data time: 0.035587588946024575\n",
      "step: 684032, loss: 0.06289561092853546, data time: 0.030721562249319895\n",
      "step: 684033, loss: 0.06532920897006989, data time: 0.027157127857208252\n",
      "step: 684034, loss: 0.06378718465566635, data time: 0.024285157521565754\n",
      "step: 684035, loss: 0.06224663183093071, data time: 0.02205796241760254\n",
      "step: 684036, loss: 0.06202559918165207, data time: 0.020245248621160335\n",
      "step: 684037, loss: 0.05580463632941246, data time: 0.018736859162648518\n",
      "step: 684038, loss: 0.05784344673156738, data time: 0.017462950486403245\n",
      "step: 684039, loss: 0.06168476492166519, data time: 0.016362156186785017\n",
      "step: 684040, loss: 0.055995844304561615, data time: 0.015413236618041993\n",
      "step: 684041, loss: 0.058079056441783905, data time: 0.014583438634872437\n",
      "step: 684042, loss: 0.0608934722840786, data time: 0.01384744924657485\n",
      "step: 684043, loss: 0.05486380308866501, data time: 0.013190759552849663\n",
      "step: 684044, loss: 0.06667481362819672, data time: 0.01260753681785182\n",
      "step: 684045, loss: 0.06065233796834946, data time: 0.012085938453674316\n",
      "step: 684046, loss: 0.06122957170009613, data time: 0.011613879884992327\n",
      "step: 684047, loss: 0.058414410799741745, data time: 0.011182546615600586\n",
      "step: 684048, loss: 0.0629921704530716, data time: 0.01078312293342922\n",
      "step: 684049, loss: 0.058856651186943054, data time: 0.01041753093401591\n",
      "step: 684050, loss: 0.06057000160217285, data time: 0.010086688995361328\n",
      "step: 684051, loss: 0.06287091225385666, data time: 0.009791126618018517\n",
      "step: 684052, loss: 0.06236846745014191, data time: 0.00950641985292788\n",
      "step: 684053, loss: 0.06580683588981628, data time: 0.009238098348890032\n",
      "step: 684054, loss: 0.05242578312754631, data time: 0.008996092039963295\n",
      "step: 684055, loss: 0.06185323745012283, data time: 0.008766500155131023\n",
      "step: 684056, loss: 0.06436972320079803, data time: 0.008553358816331433\n",
      "step: 684057, loss: 0.06125760078430176, data time: 0.008356049656867981\n",
      "step: 684058, loss: 0.06299005448818207, data time: 0.008160800644845673\n",
      "step: 684059, loss: 0.06820406764745712, data time: 0.007980003076441148\n",
      "step: 684060, loss: 0.05857926607131958, data time: 0.007807935987200055\n",
      "step: 684061, loss: 0.06028251349925995, data time: 0.007641745938195122\n",
      "step: 684062, loss: 0.07186964154243469, data time: 0.007486175846409153\n",
      "step: 684063, loss: 0.06135660409927368, data time: 0.0073429283342863385\n",
      "step: 684064, loss: 0.06304812431335449, data time: 0.007206348272470327\n",
      "step: 684065, loss: 0.0695284754037857, data time: 0.007079398632049561\n",
      "step: 684066, loss: 0.06374874711036682, data time: 0.2005608081817627\n",
      "step: 684067, loss: 0.06367982178926468, data time: 0.10105729103088379\n",
      "step: 684068, loss: 0.061894506216049194, data time: 0.06787856419881184\n",
      "step: 684069, loss: 0.06686532497406006, data time: 0.051872074604034424\n",
      "step: 684070, loss: 0.059248700737953186, data time: 0.041791534423828124\n",
      "step: 684071, loss: 0.06132185459136963, data time: 0.03507248560587565\n",
      "step: 684072, loss: 0.06127757951617241, data time: 0.03028832163129534\n",
      "step: 684073, loss: 0.06157419830560684, data time: 0.026772499084472656\n",
      "step: 684074, loss: 0.0631798803806305, data time: 0.02393907970852322\n",
      "step: 684075, loss: 0.06449764221906662, data time: 0.02174978256225586\n",
      "step: 684076, loss: 0.05972057580947876, data time: 0.019976117394187233\n",
      "step: 684077, loss: 0.06635600328445435, data time: 0.018505016962687176\n",
      "step: 684078, loss: 0.0628071278333664, data time: 0.01724831874553974\n",
      "step: 684079, loss: 0.05629337951540947, data time: 0.01616042000906808\n",
      "step: 684080, loss: 0.06670135259628296, data time: 0.015219322840372721\n",
      "step: 684081, loss: 0.06301254779100418, data time: 0.014399215579032898\n",
      "step: 684082, loss: 0.06132839247584343, data time: 0.013696993098539464\n",
      "step: 684083, loss: 0.06595705449581146, data time: 0.013067907757229276\n",
      "step: 684084, loss: 0.06076742708683014, data time: 0.012507777465017219\n",
      "step: 684085, loss: 0.05957777425646782, data time: 0.012015295028686524\n",
      "step: 684086, loss: 0.05649961158633232, data time: 0.011561722982497443\n",
      "step: 684087, loss: 0.057552408427000046, data time: 0.011152560060674494\n",
      "step: 684088, loss: 0.05600806325674057, data time: 0.010769616002621859\n",
      "step: 684089, loss: 0.05980657786130905, data time: 0.01042074958483378\n",
      "step: 684090, loss: 0.061176419258117676, data time: 0.010102081298828124\n",
      "step: 684091, loss: 0.059131622314453125, data time: 0.00980824690598708\n",
      "step: 684092, loss: 0.05710717290639877, data time: 0.009530923984668873\n",
      "step: 684093, loss: 0.06674475967884064, data time: 0.009275027683803014\n",
      "step: 684094, loss: 0.06522589921951294, data time: 0.009041999948435816\n",
      "step: 684095, loss: 0.061126839369535446, data time: 0.008825405438741048\n",
      "step: 684096, loss: 0.06174812465906143, data time: 0.008622892441288117\n",
      "step: 684097, loss: 0.06387675553560257, data time: 0.008433155715465546\n",
      "step: 684098, loss: 0.05898648500442505, data time: 0.008240598620790424\n",
      "step: 684099, loss: 0.05863267183303833, data time: 0.008060918134801528\n",
      "step: 684100, loss: 0.06276316195726395, data time: 0.007890313012259348\n",
      "step: 684101, loss: 0.06588701903820038, data time: 0.007726377911037869\n",
      "step: 684102, loss: 0.061660703271627426, data time: 0.007572451153317014\n",
      "step: 684103, loss: 0.06393277645111084, data time: 0.007430860870762875\n",
      "step: 684104, loss: 0.058369867503643036, data time: 0.007295914185352814\n",
      "step: 684105, loss: 0.05234327167272568, data time: 0.007166558504104614\n",
      "step: 684106, loss: 0.06464311480522156, data time: 0.20052027702331543\n",
      "step: 684107, loss: 0.06536595523357391, data time: 0.1010293960571289\n",
      "step: 684108, loss: 0.060166627168655396, data time: 0.0678540865580241\n",
      "step: 684109, loss: 0.053733836859464645, data time: 0.05174911022186279\n",
      "step: 684110, loss: 0.061352770775556564, data time: 0.04168157577514649\n",
      "step: 684111, loss: 0.058176662772893906, data time: 0.03497906525929769\n",
      "step: 684112, loss: 0.06225806102156639, data time: 0.030195099966866628\n",
      "step: 684113, loss: 0.06357821822166443, data time: 0.026698827743530273\n",
      "step: 684114, loss: 0.06906597316265106, data time: 0.023884349399142794\n",
      "step: 684115, loss: 0.055454693734645844, data time: 0.021697115898132325\n",
      "step: 684116, loss: 0.06438406556844711, data time: 0.019915190610018643\n",
      "step: 684117, loss: 0.061521779745817184, data time: 0.01844114065170288\n",
      "step: 684118, loss: 0.05796664580702782, data time: 0.017197718987098105\n",
      "step: 684119, loss: 0.06234443187713623, data time: 0.01611551216670445\n",
      "step: 684120, loss: 0.05986665189266205, data time: 0.015176153182983399\n",
      "step: 684121, loss: 0.06401710212230682, data time: 0.01436491310596466\n",
      "step: 684122, loss: 0.05700197070837021, data time: 0.013642942204194911\n",
      "step: 684123, loss: 0.057516347616910934, data time: 0.013001481691996256\n",
      "step: 684124, loss: 0.06172932684421539, data time: 0.012422950644242136\n",
      "step: 684125, loss: 0.06486251205205917, data time: 0.011915349960327148\n",
      "step: 684126, loss: 0.0641302615404129, data time: 0.01145156224568685\n",
      "step: 684127, loss: 0.05243980512022972, data time: 0.011030598120255903\n",
      "step: 684128, loss: 0.059739988297224045, data time: 0.010637210763019064\n",
      "step: 684129, loss: 0.0692940354347229, data time: 0.01028640071551005\n",
      "step: 684130, loss: 0.061005692929029465, data time: 0.009965944290161132\n",
      "step: 684131, loss: 0.05507883429527283, data time: 0.009663095841040978\n",
      "step: 684132, loss: 0.060610346496105194, data time: 0.009377223474008066\n",
      "step: 684133, loss: 0.06349866092205048, data time: 0.009113022259303502\n",
      "step: 684134, loss: 0.06589142233133316, data time: 0.008873462677001953\n",
      "step: 684135, loss: 0.06252265721559525, data time: 0.008649118741353353\n",
      "step: 684136, loss: 0.06041219085454941, data time: 0.008443793942851404\n",
      "step: 684137, loss: 0.058330561965703964, data time: 0.008249938488006592\n",
      "step: 684138, loss: 0.06377481669187546, data time: 0.008062138701930191\n",
      "step: 684139, loss: 0.06389308720827103, data time: 0.007883443551905015\n",
      "step: 684140, loss: 0.06192416697740555, data time: 0.007714952741350446\n",
      "step: 684141, loss: 0.05560972914099693, data time: 0.00755200121137831\n",
      "step: 684142, loss: 0.05748648941516876, data time: 0.007400196951788825\n",
      "step: 684143, loss: 0.06361091136932373, data time: 0.007259538299159\n",
      "step: 684144, loss: 0.061322327703237534, data time: 0.007124876364683494\n",
      "step: 684145, loss: 0.042124193161726, data time: 0.006997746229171753\n",
      "step: 684146, loss: 0.06315555423498154, data time: 0.20930862426757812\n",
      "step: 684147, loss: 0.06046247109770775, data time: 0.10651397705078125\n",
      "step: 684148, loss: 0.06280209124088287, data time: 0.07151834170023601\n",
      "step: 684149, loss: 0.05948273092508316, data time: 0.05444377660751343\n",
      "step: 684150, loss: 0.055295731872320175, data time: 0.04383625984191895\n",
      "step: 684151, loss: 0.059907667338848114, data time: 0.036784847577412925\n",
      "step: 684152, loss: 0.0630321204662323, data time: 0.03172833578927176\n",
      "step: 684153, loss: 0.05883464962244034, data time: 0.028028398752212524\n",
      "step: 684154, loss: 0.059192024171352386, data time: 0.02508242925008138\n",
      "step: 684155, loss: 0.062366366386413574, data time: 0.02277185916900635\n",
      "step: 684156, loss: 0.06143100559711456, data time: 0.020908030596646397\n",
      "step: 684157, loss: 0.06617550551891327, data time: 0.019355515638987224\n",
      "step: 684158, loss: 0.0642341673374176, data time: 0.018029524729802057\n",
      "step: 684159, loss: 0.06444022059440613, data time: 0.016885467938014438\n",
      "step: 684160, loss: 0.06441031396389008, data time: 0.015900548299153647\n",
      "step: 684161, loss: 0.06001207232475281, data time: 0.015033349394798279\n",
      "step: 684162, loss: 0.05989627167582512, data time: 0.014278818579281078\n",
      "step: 684163, loss: 0.06293884664773941, data time: 0.013597117529975044\n",
      "step: 684164, loss: 0.05799257382750511, data time: 0.01298765132301732\n",
      "step: 684165, loss: 0.057181216776371, data time: 0.012446296215057374\n",
      "step: 684166, loss: 0.06686070561408997, data time: 0.011961187635149275\n",
      "step: 684167, loss: 0.05908577889204025, data time: 0.011515812440352007\n",
      "step: 684168, loss: 0.064115971326828, data time: 0.011104262393453846\n",
      "step: 684169, loss: 0.06392103433609009, data time: 0.010730713605880737\n",
      "step: 684170, loss: 0.061524130403995514, data time: 0.010382852554321288\n",
      "step: 684171, loss: 0.059093110263347626, data time: 0.010074377059936523\n",
      "step: 684172, loss: 0.07677783071994781, data time: 0.009775815186677155\n",
      "step: 684173, loss: 0.05743276700377464, data time: 0.009500009672982352\n",
      "step: 684174, loss: 0.060978375375270844, data time: 0.00924956387486951\n",
      "step: 684175, loss: 0.0655740275979042, data time: 0.0090133269627889\n",
      "step: 684176, loss: 0.06641562283039093, data time: 0.008791977359402564\n",
      "step: 684177, loss: 0.0634954422712326, data time: 0.00858817994594574\n",
      "step: 684178, loss: 0.0642954558134079, data time: 0.008388714356855913\n",
      "step: 684179, loss: 0.056732177734375, data time: 0.008198955479790182\n",
      "step: 684180, loss: 0.06219760328531265, data time: 0.008022008623395646\n",
      "step: 684181, loss: 0.06128629669547081, data time: 0.00785472657945421\n",
      "step: 684182, loss: 0.05845044180750847, data time: 0.007693335816666887\n",
      "step: 684183, loss: 0.06500059366226196, data time: 0.007545590400695801\n",
      "step: 684184, loss: 0.06176161393523216, data time: 0.0074036977229974205\n",
      "step: 684185, loss: 0.07295512408018112, data time: 0.007270127534866333\n",
      "step: 684186, loss: 0.058597683906555176, data time: 0.19636988639831543\n",
      "step: 684187, loss: 0.06056481599807739, data time: 0.10003876686096191\n",
      "step: 684188, loss: 0.05723228678107262, data time: 0.06719660758972168\n",
      "step: 684189, loss: 0.0630730614066124, data time: 0.05120110511779785\n",
      "step: 684190, loss: 0.0626649409532547, data time: 0.04123811721801758\n",
      "step: 684191, loss: 0.061741720885038376, data time: 0.034623305002848305\n",
      "step: 684192, loss: 0.06643640995025635, data time: 0.02988246509007045\n",
      "step: 684193, loss: 0.05872822925448418, data time: 0.02641424536705017\n",
      "step: 684194, loss: 0.06154807657003403, data time: 0.023627042770385742\n",
      "step: 684195, loss: 0.061301957815885544, data time: 0.021468234062194825\n",
      "step: 684196, loss: 0.05725542828440666, data time: 0.01974498141895641\n",
      "step: 684197, loss: 0.06148005649447441, data time: 0.018283843994140625\n",
      "step: 684198, loss: 0.07040565460920334, data time: 0.017050999861497145\n",
      "step: 684199, loss: 0.06265386193990707, data time: 0.01598567622048514\n",
      "step: 684200, loss: 0.061128467321395874, data time: 0.015062046051025391\n",
      "step: 684201, loss: 0.07162150740623474, data time: 0.014250218868255615\n",
      "step: 684202, loss: 0.06118146702647209, data time: 0.0135346580954159\n",
      "step: 684203, loss: 0.06344793736934662, data time: 0.012892365455627441\n",
      "step: 684204, loss: 0.0613662526011467, data time: 0.01232088239569413\n",
      "step: 684205, loss: 0.06925639510154724, data time: 0.011815261840820313\n",
      "step: 684206, loss: 0.06718268990516663, data time: 0.011359725679670061\n",
      "step: 684207, loss: 0.06397657841444016, data time: 0.010944117199290882\n",
      "step: 684208, loss: 0.06281232088804245, data time: 0.010556086249973463\n",
      "step: 684209, loss: 0.06549035012722015, data time: 0.010204623142878214\n",
      "step: 684210, loss: 0.06283022463321686, data time: 0.009878606796264648\n",
      "step: 684211, loss: 0.06001928448677063, data time: 0.009579117481525127\n",
      "step: 684212, loss: 0.06110043823719025, data time: 0.009296558521412037\n",
      "step: 684213, loss: 0.06451170146465302, data time: 0.009042297090802873\n",
      "step: 684214, loss: 0.06381981074810028, data time: 0.00880649994159567\n",
      "step: 684215, loss: 0.05985134840011597, data time: 0.008583164215087891\n",
      "step: 684216, loss: 0.06557472050189972, data time: 0.00837876719813193\n",
      "step: 684217, loss: 0.05887800455093384, data time: 0.008201107382774353\n",
      "step: 684218, loss: 0.060999881476163864, data time: 0.00801332069165779\n",
      "step: 684219, loss: 0.06128895282745361, data time: 0.007836341857910156\n",
      "step: 684220, loss: 0.05939473956823349, data time: 0.0076698030744280134\n",
      "step: 684221, loss: 0.060588687658309937, data time: 0.00750756926006741\n",
      "step: 684222, loss: 0.061410486698150635, data time: 0.007356005745965081\n",
      "step: 684223, loss: 0.05987895652651787, data time: 0.007215399491159539\n",
      "step: 684224, loss: 0.06534289568662643, data time: 0.007081942680554512\n",
      "step: 684225, loss: 0.0675337165594101, data time: 0.00695611834526062\n",
      "step: 684226, loss: 0.06412717700004578, data time: 0.2204287052154541\n",
      "step: 684227, loss: 0.058140259236097336, data time: 0.11159622669219971\n",
      "step: 684228, loss: 0.05593019723892212, data time: 0.0753316084543864\n",
      "step: 684229, loss: 0.06155966967344284, data time: 0.057270705699920654\n",
      "step: 684230, loss: 0.06568910926580429, data time: 0.04612178802490234\n",
      "step: 684231, loss: 0.06794419884681702, data time: 0.038678646087646484\n",
      "step: 684232, loss: 0.059756290167570114, data time: 0.033368519374302456\n",
      "step: 684233, loss: 0.06348691880702972, data time: 0.029449880123138428\n",
      "step: 684234, loss: 0.0652926117181778, data time: 0.026329755783081055\n",
      "step: 684235, loss: 0.05724792927503586, data time: 0.023896384239196777\n",
      "step: 684236, loss: 0.055263470858335495, data time: 0.021916757930408825\n",
      "step: 684237, loss: 0.059921957552433014, data time: 0.020271042982737224\n",
      "step: 684238, loss: 0.054812245070934296, data time: 0.018879156846266527\n",
      "step: 684239, loss: 0.06129710376262665, data time: 0.017677579607282366\n",
      "step: 684240, loss: 0.06000453233718872, data time: 0.01664722760518392\n",
      "step: 684241, loss: 0.06519416719675064, data time: 0.01574370265007019\n",
      "step: 684242, loss: 0.057512085884809494, data time: 0.014940163668464212\n",
      "step: 684243, loss: 0.06484110653400421, data time: 0.014222674899631076\n",
      "step: 684244, loss: 0.058324944227933884, data time: 0.013580083847045898\n",
      "step: 684245, loss: 0.05687277764081955, data time: 0.01300874948501587\n",
      "step: 684246, loss: 0.058899205178022385, data time: 0.012493564969017393\n",
      "step: 684247, loss: 0.06794236600399017, data time: 0.012024120850996538\n",
      "step: 684248, loss: 0.06214604899287224, data time: 0.011592844258184019\n",
      "step: 684249, loss: 0.06632599234580994, data time: 0.011195520559946695\n",
      "step: 684250, loss: 0.062061648815870285, data time: 0.010834226608276367\n",
      "step: 684251, loss: 0.05922834202647209, data time: 0.010496249565711388\n",
      "step: 684252, loss: 0.06217169016599655, data time: 0.010180649933991608\n",
      "step: 684253, loss: 0.061632052063941956, data time: 0.009893076760428292\n",
      "step: 684254, loss: 0.0669640600681305, data time: 0.00963276008079792\n",
      "step: 684255, loss: 0.06458581984043121, data time: 0.009382327397664389\n",
      "step: 684256, loss: 0.056391894817352295, data time: 0.00914933604578818\n",
      "step: 684257, loss: 0.0645788237452507, data time: 0.008933007717132568\n",
      "step: 684258, loss: 0.0588625892996788, data time: 0.008722507592403528\n",
      "step: 684259, loss: 0.05906609818339348, data time: 0.008525406613069423\n",
      "step: 684260, loss: 0.06828691810369492, data time: 0.008340856007167271\n",
      "step: 684261, loss: 0.06242474541068077, data time: 0.008163538244035509\n",
      "step: 684262, loss: 0.05782356485724449, data time: 0.00799474200686893\n",
      "step: 684263, loss: 0.05957009643316269, data time: 0.007836975549396715\n",
      "step: 684264, loss: 0.060703061521053314, data time: 0.007688369506444686\n",
      "step: 684265, loss: 0.05222772806882858, data time: 0.007547825574874878\n",
      "step: 684266, loss: 0.06055304408073425, data time: 0.21780896186828613\n",
      "step: 684267, loss: 0.06385542452335358, data time: 0.1102747917175293\n",
      "step: 684268, loss: 0.06001369282603264, data time: 0.07441902160644531\n",
      "step: 684269, loss: 0.06439762562513351, data time: 0.05675339698791504\n",
      "step: 684270, loss: 0.06085041165351868, data time: 0.04573259353637695\n",
      "step: 684271, loss: 0.05761614441871643, data time: 0.03839262326558431\n",
      "step: 684272, loss: 0.0637495368719101, data time: 0.03317621776035854\n",
      "step: 684273, loss: 0.057538196444511414, data time: 0.029324054718017578\n",
      "step: 684274, loss: 0.055727533996105194, data time: 0.026249806086222332\n",
      "step: 684275, loss: 0.06626057624816895, data time: 0.02386343479156494\n",
      "step: 684276, loss: 0.06666919589042664, data time: 0.02192620797590776\n",
      "step: 684277, loss: 0.061322152614593506, data time: 0.02030801773071289\n",
      "step: 684278, loss: 0.06505250930786133, data time: 0.018938376353337213\n",
      "step: 684279, loss: 0.06411068141460419, data time: 0.01775440147944859\n",
      "step: 684280, loss: 0.06381671875715256, data time: 0.01673738161722819\n",
      "step: 684281, loss: 0.06339682638645172, data time: 0.015850573778152466\n",
      "step: 684282, loss: 0.061908990144729614, data time: 0.01506178519305061\n",
      "step: 684283, loss: 0.060287248343229294, data time: 0.014356745613945855\n",
      "step: 684284, loss: 0.063801109790802, data time: 0.01372511763321726\n",
      "step: 684285, loss: 0.059108734130859375, data time: 0.013165020942687988\n",
      "step: 684286, loss: 0.0631890818476677, data time: 0.012653339476812454\n",
      "step: 684287, loss: 0.05872581899166107, data time: 0.012195229530334473\n",
      "step: 684288, loss: 0.06448385119438171, data time: 0.01176824777022652\n",
      "step: 684289, loss: 0.06145571172237396, data time: 0.011384755373001099\n",
      "step: 684290, loss: 0.06117401644587517, data time: 0.011028738021850585\n",
      "step: 684291, loss: 0.06185214966535568, data time: 0.010695549157949595\n",
      "step: 684292, loss: 0.05581860989332199, data time: 0.010386890835232206\n",
      "step: 684293, loss: 0.06291237473487854, data time: 0.010104554040091378\n",
      "step: 684294, loss: 0.06937816739082336, data time: 0.009842527323755724\n",
      "step: 684295, loss: 0.05678998678922653, data time: 0.009598342577616374\n",
      "step: 684296, loss: 0.060004353523254395, data time: 0.009370842287617346\n",
      "step: 684297, loss: 0.06538213789463043, data time: 0.009158097207546234\n",
      "step: 684298, loss: 0.061642780900001526, data time: 0.008945241118922378\n",
      "step: 684299, loss: 0.06008695065975189, data time: 0.008745354764601764\n",
      "step: 684300, loss: 0.05841017886996269, data time: 0.008553702490670341\n",
      "step: 684301, loss: 0.061727166175842285, data time: 0.008372253841824003\n",
      "step: 684302, loss: 0.06051861494779587, data time: 0.0082022306081411\n",
      "step: 684303, loss: 0.06135953590273857, data time: 0.00804406718203896\n",
      "step: 684304, loss: 0.06584863364696503, data time: 0.007893727375910832\n",
      "step: 684305, loss: 0.046919893473386765, data time: 0.0077500581741333004\n",
      "step: 684306, loss: 0.06553153693675995, data time: 0.21184921264648438\n",
      "step: 684307, loss: 0.06291984021663666, data time: 0.10704600811004639\n",
      "step: 684308, loss: 0.056861817836761475, data time: 0.07187732060750325\n",
      "step: 684309, loss: 0.05866206809878349, data time: 0.05476433038711548\n",
      "step: 684310, loss: 0.06109301373362541, data time: 0.044095849990844725\n",
      "step: 684311, loss: 0.06429838389158249, data time: 0.0369944175084432\n",
      "step: 684312, loss: 0.06522326171398163, data time: 0.03192431586129325\n",
      "step: 684313, loss: 0.05809406563639641, data time: 0.028203517198562622\n",
      "step: 684314, loss: 0.06834109872579575, data time: 0.025222963756985135\n",
      "step: 684315, loss: 0.06277080625295639, data time: 0.02289712429046631\n",
      "step: 684316, loss: 0.062414877116680145, data time: 0.021010723980990322\n",
      "step: 684317, loss: 0.06274326145648956, data time: 0.019444505373636883\n",
      "step: 684318, loss: 0.061366766691207886, data time: 0.01812131588275616\n",
      "step: 684319, loss: 0.06396618485450745, data time: 0.016968488693237305\n",
      "step: 684320, loss: 0.06382572650909424, data time: 0.015973774592081706\n",
      "step: 684321, loss: 0.06247714161872864, data time: 0.015105664730072021\n",
      "step: 684322, loss: 0.06653884053230286, data time: 0.014344439787023208\n",
      "step: 684323, loss: 0.06321998685598373, data time: 0.013680259386698404\n",
      "step: 684324, loss: 0.06469608098268509, data time: 0.013067584288747687\n",
      "step: 684325, loss: 0.06240876764059067, data time: 0.012520253658294678\n",
      "step: 684326, loss: 0.06001684069633484, data time: 0.01202829678853353\n",
      "step: 684327, loss: 0.06254804134368896, data time: 0.011579708619551226\n",
      "step: 684328, loss: 0.05859338492155075, data time: 0.011166033537491508\n",
      "step: 684329, loss: 0.058733969926834106, data time: 0.010793546835581461\n",
      "step: 684330, loss: 0.06791958212852478, data time: 0.010447673797607422\n",
      "step: 684331, loss: 0.06052359938621521, data time: 0.01012720511509822\n",
      "step: 684332, loss: 0.06844949722290039, data time: 0.00982375498171206\n",
      "step: 684333, loss: 0.0638747364282608, data time: 0.009546092578342982\n",
      "step: 684334, loss: 0.0633644089102745, data time: 0.009291435110157934\n",
      "step: 684335, loss: 0.06560394167900085, data time: 0.009052713712056478\n",
      "step: 684336, loss: 0.0644235834479332, data time: 0.008829678258588236\n",
      "step: 684337, loss: 0.05969318747520447, data time: 0.008627019822597504\n",
      "step: 684338, loss: 0.0584716722369194, data time: 0.008424563841386274\n",
      "step: 684339, loss: 0.06432371586561203, data time: 0.008234977722167969\n",
      "step: 684340, loss: 0.06235882639884949, data time: 0.008058691024780273\n",
      "step: 684341, loss: 0.06019017845392227, data time: 0.007886363400353326\n",
      "step: 684342, loss: 0.061256252229213715, data time: 0.007726753080213392\n",
      "step: 684343, loss: 0.06685681641101837, data time: 0.007578027875799882\n",
      "step: 684344, loss: 0.06069560348987579, data time: 0.007437229156494141\n",
      "step: 684345, loss: 0.09403680264949799, data time: 0.0073030352592468265\n",
      "step: 684346, loss: 0.06395569443702698, data time: 0.2089850902557373\n",
      "step: 684347, loss: 0.061717547476291656, data time: 0.10529696941375732\n",
      "step: 684348, loss: 0.0664500892162323, data time: 0.07071264584859212\n",
      "step: 684349, loss: 0.0681772455573082, data time: 0.054007768630981445\n",
      "step: 684350, loss: 0.061743348836898804, data time: 0.043491029739379884\n",
      "step: 684351, loss: 0.05912631005048752, data time: 0.036473592122395836\n",
      "step: 684352, loss: 0.0641019344329834, data time: 0.03148150444030762\n",
      "step: 684353, loss: 0.06495368480682373, data time: 0.027810871601104736\n",
      "step: 684354, loss: 0.060599833726882935, data time: 0.02488022380405002\n",
      "step: 684355, loss: 0.05958548188209534, data time: 0.022592782974243164\n",
      "step: 684356, loss: 0.0620892196893692, data time: 0.02073170922019265\n",
      "step: 684357, loss: 0.06624903529882431, data time: 0.019180119037628174\n",
      "step: 684358, loss: 0.06039857119321823, data time: 0.0178680419921875\n",
      "step: 684359, loss: 0.060553789138793945, data time: 0.01673603057861328\n",
      "step: 684360, loss: 0.061921365559101105, data time: 0.015761820475260417\n",
      "step: 684361, loss: 0.06416266411542892, data time: 0.014909803867340088\n",
      "step: 684362, loss: 0.0586513951420784, data time: 0.014163367888506721\n",
      "step: 684363, loss: 0.06132576987147331, data time: 0.013489127159118652\n",
      "step: 684364, loss: 0.0563930980861187, data time: 0.012888946031269274\n",
      "step: 684365, loss: 0.06879714131355286, data time: 0.012354087829589844\n",
      "step: 684366, loss: 0.06131681054830551, data time: 0.011870225270589193\n",
      "step: 684367, loss: 0.061721038073301315, data time: 0.011426037008112127\n",
      "step: 684368, loss: 0.06984270364046097, data time: 0.0110208158907683\n",
      "step: 684369, loss: 0.06897275149822235, data time: 0.01064459482828776\n",
      "step: 684370, loss: 0.05980116128921509, data time: 0.01030552864074707\n",
      "step: 684371, loss: 0.06130465120077133, data time: 0.009991315694955679\n",
      "step: 684372, loss: 0.06185121461749077, data time: 0.009696139229668511\n",
      "step: 684373, loss: 0.059495583176612854, data time: 0.009423349584851946\n",
      "step: 684374, loss: 0.06477729976177216, data time: 0.00917673110961914\n",
      "step: 684375, loss: 0.0637400671839714, data time: 0.00894163449605306\n",
      "step: 684376, loss: 0.0610075406730175, data time: 0.008723835791310957\n",
      "step: 684377, loss: 0.05682273954153061, data time: 0.008523769676685333\n",
      "step: 684378, loss: 0.059587061405181885, data time: 0.008324326890887636\n",
      "step: 684379, loss: 0.061379916965961456, data time: 0.008139918832217945\n",
      "step: 684380, loss: 0.06216778606176376, data time: 0.00796504020690918\n",
      "step: 684381, loss: 0.06017448753118515, data time: 0.007799837324354384\n",
      "step: 684382, loss: 0.06371433287858963, data time: 0.00764172785990947\n",
      "step: 684383, loss: 0.06254604458808899, data time: 0.007495936594511333\n",
      "step: 684384, loss: 0.051264237612485886, data time: 0.00735924794123723\n",
      "step: 684385, loss: 0.06036189943552017, data time: 0.007229280471801758\n",
      "step: 684386, loss: 0.057711951434612274, data time: 0.19465947151184082\n",
      "step: 684387, loss: 0.05632374435663223, data time: 0.09948527812957764\n",
      "step: 684388, loss: 0.05928650498390198, data time: 0.06695071856180827\n",
      "step: 684389, loss: 0.05526427552103996, data time: 0.05112731456756592\n",
      "step: 684390, loss: 0.06225039064884186, data time: 0.041251516342163085\n",
      "step: 684391, loss: 0.06265642493963242, data time: 0.034697651863098145\n",
      "step: 684392, loss: 0.058896325528621674, data time: 0.03001577513558524\n",
      "step: 684393, loss: 0.06088022142648697, data time: 0.02657020092010498\n",
      "step: 684394, loss: 0.056894682347774506, data time: 0.02379465103149414\n",
      "step: 684395, loss: 0.0651150494813919, data time: 0.02165219783782959\n",
      "step: 684396, loss: 0.06070124730467796, data time: 0.019913760098544033\n",
      "step: 684397, loss: 0.06465524435043335, data time: 0.01846023400624593\n",
      "step: 684398, loss: 0.05927291512489319, data time: 0.017230840829702523\n",
      "step: 684399, loss: 0.06441634148359299, data time: 0.016171540532793318\n",
      "step: 684400, loss: 0.06672646105289459, data time: 0.01525896390279134\n",
      "step: 684401, loss: 0.06297437846660614, data time: 0.014459848403930664\n",
      "step: 684402, loss: 0.056522950530052185, data time: 0.013751324485330021\n",
      "step: 684403, loss: 0.06476324796676636, data time: 0.013115750418768989\n",
      "step: 684404, loss: 0.06296296417713165, data time: 0.012551709225303248\n",
      "step: 684405, loss: 0.05700116977095604, data time: 0.01204899549484253\n",
      "step: 684406, loss: 0.06669649481773376, data time: 0.011592422212873186\n",
      "step: 684407, loss: 0.061230048537254333, data time: 0.011176781220869585\n",
      "step: 684408, loss: 0.06646860390901566, data time: 0.010795821314272673\n",
      "step: 684409, loss: 0.06659695506095886, data time: 0.010450671116511026\n",
      "step: 684410, loss: 0.05366038531064987, data time: 0.010132207870483398\n",
      "step: 684411, loss: 0.06143611669540405, data time: 0.00983584844149076\n",
      "step: 684412, loss: 0.06639561057090759, data time: 0.009556929270426432\n",
      "step: 684413, loss: 0.06334671378135681, data time: 0.009299814701080322\n",
      "step: 684414, loss: 0.054471950978040695, data time: 0.00906506900129647\n",
      "step: 684415, loss: 0.06218167766928673, data time: 0.008846926689147949\n",
      "step: 684416, loss: 0.06307075917720795, data time: 0.008643896349014775\n",
      "step: 684417, loss: 0.07137854397296906, data time: 0.008454553782939911\n",
      "step: 684418, loss: 0.06225666403770447, data time: 0.008263501253995028\n",
      "step: 684419, loss: 0.06213153153657913, data time: 0.00808281057021197\n",
      "step: 684420, loss: 0.06072039529681206, data time: 0.007909631729125977\n",
      "step: 684421, loss: 0.06760694086551666, data time: 0.007744133472442627\n",
      "step: 684422, loss: 0.06515780091285706, data time: 0.007588599179242109\n",
      "step: 684423, loss: 0.06307177245616913, data time: 0.007444802083467182\n",
      "step: 684424, loss: 0.060368862003088, data time: 0.007308067419590094\n",
      "step: 684425, loss: 0.05472928285598755, data time: 0.007179218530654907\n",
      "step: 684426, loss: 0.059453509747982025, data time: 0.20494961738586426\n",
      "step: 684427, loss: 0.05857696384191513, data time: 0.10364079475402832\n",
      "step: 684428, loss: 0.06962063908576965, data time: 0.07021180788675944\n",
      "step: 684429, loss: 0.0645047053694725, data time: 0.053425610065460205\n",
      "step: 684430, loss: 0.06268005073070526, data time: 0.04302091598510742\n",
      "step: 684431, loss: 0.061073265969753265, data time: 0.03609613577524821\n",
      "step: 684432, loss: 0.06310497224330902, data time: 0.031163624354771206\n",
      "step: 684433, loss: 0.061115067452192307, data time: 0.027525722980499268\n",
      "step: 684434, loss: 0.05863992124795914, data time: 0.024621248245239258\n",
      "step: 684435, loss: 0.058686863631010056, data time: 0.02235751152038574\n",
      "step: 684436, loss: 0.060565799474716187, data time: 0.020520058545199307\n",
      "step: 684437, loss: 0.0656173974275589, data time: 0.018988629182179768\n",
      "step: 684438, loss: 0.05711248517036438, data time: 0.017692052401029147\n",
      "step: 684439, loss: 0.06161773204803467, data time: 0.01657278197152274\n",
      "step: 684440, loss: 0.06086953729391098, data time: 0.015611759821573893\n",
      "step: 684441, loss: 0.05986059829592705, data time: 0.014772817492485046\n",
      "step: 684442, loss: 0.057863831520080566, data time: 0.014028016258688533\n",
      "step: 684443, loss: 0.06800597161054611, data time: 0.01335536109076606\n",
      "step: 684444, loss: 0.05985420197248459, data time: 0.012763575503700659\n",
      "step: 684445, loss: 0.0599185936152935, data time: 0.012231945991516113\n",
      "step: 684446, loss: 0.06664907187223434, data time: 0.01175236701965332\n",
      "step: 684447, loss: 0.06331610679626465, data time: 0.011314522136341442\n",
      "step: 684448, loss: 0.06053968518972397, data time: 0.01091145432513693\n",
      "step: 684449, loss: 0.06021859496831894, data time: 0.010556648174921671\n",
      "step: 684450, loss: 0.0599839985370636, data time: 0.010234804153442382\n",
      "step: 684451, loss: 0.0611807219684124, data time: 0.009917213366581844\n",
      "step: 684452, loss: 0.06513649225234985, data time: 0.009621991051567925\n",
      "step: 684453, loss: 0.06395445764064789, data time: 0.009351577077593123\n",
      "step: 684454, loss: 0.05991996452212334, data time: 0.009103733917762494\n",
      "step: 684455, loss: 0.06259642541408539, data time: 0.008873351414998372\n",
      "step: 684456, loss: 0.06257738918066025, data time: 0.008661170159616777\n",
      "step: 684457, loss: 0.06368009746074677, data time: 0.008462585508823395\n",
      "step: 684458, loss: 0.0683327317237854, data time: 0.008270552664092093\n",
      "step: 684459, loss: 0.05841623991727829, data time: 0.008089801844428568\n",
      "step: 684460, loss: 0.05532187223434448, data time: 0.00791642325265067\n",
      "step: 684461, loss: 0.06457947939634323, data time: 0.007750438319312202\n",
      "step: 684462, loss: 0.05440881475806236, data time: 0.007595513318036054\n",
      "step: 684463, loss: 0.0713220089673996, data time: 0.007452820476732756\n",
      "step: 684464, loss: 0.05553817003965378, data time: 0.007317212911752554\n",
      "step: 684465, loss: 0.08927297592163086, data time: 0.007189375162124634\n",
      "step: 684466, loss: 0.06328507512807846, data time: 0.2096996307373047\n",
      "step: 684467, loss: 0.062147464603185654, data time: 0.10640931129455566\n",
      "step: 684468, loss: 0.06305672228336334, data time: 0.07145015398661296\n",
      "step: 684469, loss: 0.06552867591381073, data time: 0.05448436737060547\n",
      "step: 684470, loss: 0.06279529631137848, data time: 0.04389324188232422\n",
      "step: 684471, loss: 0.061583034694194794, data time: 0.036806305249532066\n",
      "step: 684472, loss: 0.062087349593639374, data time: 0.031768662588936944\n",
      "step: 684473, loss: 0.06019560992717743, data time: 0.028070002794265747\n",
      "step: 684474, loss: 0.056525520980358124, data time: 0.025096072090996638\n",
      "step: 684475, loss: 0.060816168785095215, data time: 0.022787594795227052\n",
      "step: 684476, loss: 0.060319721698760986, data time: 0.020907163619995117\n",
      "step: 684477, loss: 0.061714790761470795, data time: 0.019342978795369465\n",
      "step: 684478, loss: 0.06653442978858948, data time: 0.01801921771122859\n",
      "step: 684479, loss: 0.06087380647659302, data time: 0.016891258103506907\n",
      "step: 684480, loss: 0.057790279388427734, data time: 0.015901947021484376\n",
      "step: 684481, loss: 0.0579143725335598, data time: 0.0150393545627594\n",
      "step: 684482, loss: 0.05652911216020584, data time: 0.014284849166870117\n",
      "step: 684483, loss: 0.06741467118263245, data time: 0.013603316413031684\n",
      "step: 684484, loss: 0.06782680749893188, data time: 0.012993059660259047\n",
      "step: 684485, loss: 0.06265544891357422, data time: 0.012451100349426269\n",
      "step: 684486, loss: 0.06178515404462814, data time: 0.01196024531409854\n",
      "step: 684487, loss: 0.06921891868114471, data time: 0.011514154347506437\n",
      "step: 684488, loss: 0.06501816213130951, data time: 0.01110288371210513\n",
      "step: 684489, loss: 0.06502765417098999, data time: 0.010723968346913656\n",
      "step: 684490, loss: 0.0638476312160492, data time: 0.010388269424438476\n",
      "step: 684491, loss: 0.057655658572912216, data time: 0.01007392773261437\n",
      "step: 684492, loss: 0.06121480464935303, data time: 0.00977325439453125\n",
      "step: 684493, loss: 0.05735281854867935, data time: 0.009495420115334647\n",
      "step: 684494, loss: 0.06065676733851433, data time: 0.009239846262438544\n",
      "step: 684495, loss: 0.060619793832302094, data time: 0.009003607432047527\n",
      "step: 684496, loss: 0.06314122676849365, data time: 0.008783794218494047\n",
      "step: 684497, loss: 0.05781009793281555, data time: 0.008580967783927917\n",
      "step: 684498, loss: 0.05894356966018677, data time: 0.008378939195112749\n",
      "step: 684499, loss: 0.061186790466308594, data time: 0.008194607846877155\n",
      "step: 684500, loss: 0.05531395226716995, data time: 0.008017730712890626\n",
      "step: 684501, loss: 0.06161704286932945, data time: 0.007847448190053305\n",
      "step: 684502, loss: 0.06246946007013321, data time: 0.007687085383647197\n",
      "step: 684503, loss: 0.06700778752565384, data time: 0.0075373084921585886\n",
      "step: 684504, loss: 0.05880413576960564, data time: 0.007396337313529773\n",
      "step: 684505, loss: 0.05489041656255722, data time: 0.007262402772903442\n",
      "step: 684506, loss: 0.06021652743220329, data time: 0.2124464511871338\n",
      "step: 684507, loss: 0.06161484122276306, data time: 0.1069784164428711\n",
      "step: 684508, loss: 0.06484979391098022, data time: 0.07183146476745605\n",
      "step: 684509, loss: 0.05553578585386276, data time: 0.0546567440032959\n",
      "step: 684510, loss: 0.05543079227209091, data time: 0.04400763511657715\n",
      "step: 684511, loss: 0.06628977507352829, data time: 0.03691955407460531\n",
      "step: 684512, loss: 0.06446647644042969, data time: 0.03185950006757464\n",
      "step: 684513, loss: 0.0623897910118103, data time: 0.028145670890808105\n",
      "step: 684514, loss: 0.06279344111680984, data time: 0.0251617431640625\n",
      "step: 684515, loss: 0.06362508982419968, data time: 0.022857165336608885\n",
      "step: 684516, loss: 0.0581372044980526, data time: 0.020986296913840553\n",
      "step: 684517, loss: 0.06175955757498741, data time: 0.019423564275105793\n",
      "step: 684518, loss: 0.058134570717811584, data time: 0.01809862943796011\n",
      "step: 684519, loss: 0.06163087487220764, data time: 0.016953246934073313\n",
      "step: 684520, loss: 0.06076788902282715, data time: 0.015962998072306316\n",
      "step: 684521, loss: 0.0627293810248375, data time: 0.015104368329048157\n",
      "step: 684522, loss: 0.06361246854066849, data time: 0.014341284247005688\n",
      "step: 684523, loss: 0.060239456593990326, data time: 0.01365269554985894\n",
      "step: 684524, loss: 0.06357994675636292, data time: 0.013043968301070364\n",
      "step: 684525, loss: 0.061521708965301514, data time: 0.012502455711364746\n",
      "step: 684526, loss: 0.06045563146471977, data time: 0.012008473986671084\n",
      "step: 684527, loss: 0.05844485014677048, data time: 0.011561545458706942\n",
      "step: 684528, loss: 0.06257445365190506, data time: 0.011148608249166737\n",
      "step: 684529, loss: 0.06163397431373596, data time: 0.010775427023569742\n",
      "step: 684530, loss: 0.07017164677381516, data time: 0.010429468154907227\n",
      "step: 684531, loss: 0.058615535497665405, data time: 0.010111341109642616\n",
      "step: 684532, loss: 0.056065380573272705, data time: 0.009811092306066442\n",
      "step: 684533, loss: 0.06278064846992493, data time: 0.009536317416599818\n",
      "step: 684534, loss: 0.06379548460245132, data time: 0.009281799711030105\n",
      "step: 684535, loss: 0.06354415416717529, data time: 0.009045966466267904\n",
      "step: 684536, loss: 0.059053897857666016, data time: 0.008826455762309412\n",
      "step: 684537, loss: 0.05992215871810913, data time: 0.00862140953540802\n",
      "step: 684538, loss: 0.057561248540878296, data time: 0.008423277826020212\n",
      "step: 684539, loss: 0.06102093309164047, data time: 0.008235538707059972\n",
      "step: 684540, loss: 0.0622483566403389, data time: 0.008059310913085937\n",
      "step: 684541, loss: 0.05885854363441467, data time: 0.007888449562920464\n",
      "step: 684542, loss: 0.06025531142950058, data time: 0.007727249248607739\n",
      "step: 684543, loss: 0.05636271834373474, data time: 0.0075793517263312085\n",
      "step: 684544, loss: 0.06143191084265709, data time: 0.00743702130439954\n",
      "step: 684545, loss: 0.053961485624313354, data time: 0.007302272319793701\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4023749828338623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 684546, loss: 0.05912887305021286, data time: 0.20749878883361816\n",
      "step: 684547, loss: 0.0606495700776577, data time: 0.10449004173278809\n",
      "step: 684548, loss: 0.06153886020183563, data time: 0.07017882664998372\n",
      "step: 684549, loss: 0.05953648313879967, data time: 0.05351895093917847\n",
      "step: 684550, loss: 0.059431515634059906, data time: 0.043076515197753906\n",
      "step: 684551, loss: 0.05983445793390274, data time: 0.03613642851511637\n",
      "step: 684552, loss: 0.06439805030822754, data time: 0.03117537498474121\n",
      "step: 684553, loss: 0.0628361850976944, data time: 0.027529478073120117\n",
      "step: 684554, loss: 0.06276822835206985, data time: 0.024617221620347764\n",
      "step: 684555, loss: 0.06582286208868027, data time: 0.022356843948364256\n",
      "step: 684556, loss: 0.06619718670845032, data time: 0.020571903748945755\n",
      "step: 684557, loss: 0.06479261815547943, data time: 0.01906357208887736\n",
      "step: 684558, loss: 0.06418181210756302, data time: 0.017791069470919095\n",
      "step: 684559, loss: 0.05999530851840973, data time: 0.01668659278324672\n",
      "step: 684560, loss: 0.06075600907206535, data time: 0.015735832850138347\n",
      "step: 684561, loss: 0.06408964097499847, data time: 0.014882683753967285\n",
      "step: 684562, loss: 0.06397902965545654, data time: 0.014135893653420842\n",
      "step: 684563, loss: 0.054784055799245834, data time: 0.013455973731146919\n",
      "step: 684564, loss: 0.06376437842845917, data time: 0.012853283631174188\n",
      "step: 684565, loss: 0.06309499591588974, data time: 0.012319993972778321\n",
      "step: 684566, loss: 0.06457602232694626, data time: 0.011835041500273206\n",
      "step: 684567, loss: 0.06621144711971283, data time: 0.011393590406938032\n",
      "step: 684568, loss: 0.06080019101500511, data time: 0.010983550030252209\n",
      "step: 684569, loss: 0.05895250290632248, data time: 0.010614226261774698\n",
      "step: 684570, loss: 0.06509623676538467, data time: 0.010271339416503907\n",
      "step: 684571, loss: 0.060011282563209534, data time: 0.00995486516218919\n",
      "step: 684572, loss: 0.0610307976603508, data time: 0.009672571111608434\n",
      "step: 684573, loss: 0.058894287794828415, data time: 0.009409495762416295\n",
      "step: 684574, loss: 0.06175052374601364, data time: 0.009169257920363853\n",
      "step: 684575, loss: 0.0637572705745697, data time: 0.008948850631713866\n",
      "step: 684576, loss: 0.058941494673490524, data time: 0.008744732026130922\n",
      "step: 684577, loss: 0.06531904637813568, data time: 0.008560702204704285\n",
      "step: 684578, loss: 0.06363755464553833, data time: 0.00836008967775287\n",
      "step: 684579, loss: 0.06454125046730042, data time: 0.008171705638661104\n",
      "step: 684580, loss: 0.06315232813358307, data time: 0.007994100025721958\n",
      "step: 684581, loss: 0.06394915282726288, data time: 0.007821758588155111\n",
      "step: 684582, loss: 0.06127762049436569, data time: 0.0076605564839131125\n",
      "step: 684583, loss: 0.061008892953395844, data time: 0.007513209393149928\n",
      "step: 684584, loss: 0.06082499027252197, data time: 0.007373131238497221\n",
      "step: 684585, loss: 0.07499483227729797, data time: 0.00724027156829834\n",
      "step: 684586, loss: 0.07073254883289337, data time: 0.21306514739990234\n",
      "step: 684587, loss: 0.06606566905975342, data time: 0.10729968547821045\n",
      "step: 684588, loss: 0.0662170872092247, data time: 0.07254886627197266\n",
      "step: 684589, loss: 0.057494498789310455, data time: 0.05497103929519653\n",
      "step: 684590, loss: 0.06036592647433281, data time: 0.04425630569458008\n",
      "step: 684591, loss: 0.06197771430015564, data time: 0.03715221087137858\n",
      "step: 684592, loss: 0.059821322560310364, data time: 0.03219750949314663\n",
      "step: 684593, loss: 0.05453988164663315, data time: 0.028473973274230957\n",
      "step: 684594, loss: 0.06149625778198242, data time: 0.02548519770304362\n",
      "step: 684595, loss: 0.06147874519228935, data time: 0.023178720474243165\n",
      "step: 684596, loss: 0.06899555772542953, data time: 0.02129897204312411\n",
      "step: 684597, loss: 0.06505237519741058, data time: 0.01973724365234375\n",
      "step: 684598, loss: 0.0642460286617279, data time: 0.01841310354379507\n",
      "step: 684599, loss: 0.061333149671554565, data time: 0.017273238727024624\n",
      "step: 684600, loss: 0.057190828025341034, data time: 0.016284767786661783\n",
      "step: 684601, loss: 0.057789843529462814, data time: 0.015421673655509949\n",
      "step: 684602, loss: 0.06515677273273468, data time: 0.014651691212373622\n",
      "step: 684603, loss: 0.06658466905355453, data time: 0.01396958033243815\n",
      "step: 684604, loss: 0.058852531015872955, data time: 0.013359370984529195\n",
      "step: 684605, loss: 0.06136211007833481, data time: 0.012816143035888673\n",
      "step: 684606, loss: 0.06159164756536484, data time: 0.012327773230416434\n",
      "step: 684607, loss: 0.06157493591308594, data time: 0.01188306374983354\n",
      "step: 684608, loss: 0.06400085240602493, data time: 0.011469073917554773\n",
      "step: 684609, loss: 0.06329214572906494, data time: 0.011089116334915161\n",
      "step: 684610, loss: 0.06946645677089691, data time: 0.010742292404174805\n",
      "step: 684611, loss: 0.062338970601558685, data time: 0.010419882260836087\n",
      "step: 684612, loss: 0.05935582146048546, data time: 0.010118228417855722\n",
      "step: 684613, loss: 0.06216954439878464, data time: 0.009844771453312464\n",
      "step: 684614, loss: 0.05645742267370224, data time: 0.009593478564558357\n",
      "step: 684615, loss: 0.06338204443454742, data time: 0.00935677687327067\n",
      "step: 684616, loss: 0.0623614601790905, data time: 0.009135246276855469\n",
      "step: 684617, loss: 0.0624680332839489, data time: 0.008930966258049011\n",
      "step: 684618, loss: 0.0619964562356472, data time: 0.00872274601098263\n",
      "step: 684619, loss: 0.06262922286987305, data time: 0.00852954387664795\n",
      "step: 684620, loss: 0.06231893226504326, data time: 0.008343138013567243\n",
      "step: 684621, loss: 0.05840735137462616, data time: 0.00816638602150811\n",
      "step: 684622, loss: 0.06016172096133232, data time: 0.008000683140110326\n",
      "step: 684623, loss: 0.06329326331615448, data time: 0.007847246370817485\n",
      "step: 684624, loss: 0.0656072124838829, data time: 0.007702148877657377\n",
      "step: 684625, loss: 0.053719088435173035, data time: 0.007562553882598877\n",
      "step: 684626, loss: 0.05771810933947563, data time: 0.22208642959594727\n",
      "step: 684627, loss: 0.05651349574327469, data time: 0.11181414127349854\n",
      "step: 684628, loss: 0.05890949070453644, data time: 0.0754249890645345\n",
      "step: 684629, loss: 0.06577342748641968, data time: 0.05732464790344238\n",
      "step: 684630, loss: 0.05264049395918846, data time: 0.04614715576171875\n",
      "step: 684631, loss: 0.06476974487304688, data time: 0.03869756062825521\n",
      "step: 684632, loss: 0.05927043408155441, data time: 0.03338701384408133\n",
      "step: 684633, loss: 0.05672983452677727, data time: 0.02947533130645752\n",
      "step: 684634, loss: 0.06137656420469284, data time: 0.026349915398491755\n",
      "step: 684635, loss: 0.06264131516218185, data time: 0.023941683769226074\n",
      "step: 684636, loss: 0.06127617508172989, data time: 0.022027340802279385\n",
      "step: 684637, loss: 0.06328217685222626, data time: 0.020404199759165447\n",
      "step: 684638, loss: 0.05916594713926315, data time: 0.019026572887714092\n",
      "step: 684639, loss: 0.06352060288190842, data time: 0.01783907413482666\n",
      "step: 684640, loss: 0.06712930649518967, data time: 0.0168091615041097\n",
      "step: 684641, loss: 0.05791453644633293, data time: 0.01591789722442627\n",
      "step: 684642, loss: 0.06492534279823303, data time: 0.015122105093563305\n",
      "step: 684643, loss: 0.07237967848777771, data time: 0.014412853452894423\n",
      "step: 684644, loss: 0.06288665533065796, data time: 0.013778034009431539\n",
      "step: 684645, loss: 0.06315232813358307, data time: 0.013212835788726807\n",
      "step: 684646, loss: 0.05927976593375206, data time: 0.012703645797002883\n",
      "step: 684647, loss: 0.0671079158782959, data time: 0.012239781293002043\n",
      "step: 684648, loss: 0.0674886554479599, data time: 0.011810966159986414\n",
      "step: 684649, loss: 0.06614940613508224, data time: 0.0114192267258962\n",
      "step: 684650, loss: 0.06620799005031586, data time: 0.011063308715820312\n",
      "step: 684651, loss: 0.06397911161184311, data time: 0.010728863569406362\n",
      "step: 684652, loss: 0.060420405119657516, data time: 0.010419015531186704\n",
      "step: 684653, loss: 0.06442289799451828, data time: 0.01013190405709403\n",
      "step: 684654, loss: 0.05921611189842224, data time: 0.00986987146837958\n",
      "step: 684655, loss: 0.059009164571762085, data time: 0.009624616305033366\n",
      "step: 684656, loss: 0.060434140264987946, data time: 0.009394191926525484\n",
      "step: 684657, loss: 0.06732931733131409, data time: 0.009182974696159363\n",
      "step: 684658, loss: 0.060483332723379135, data time: 0.008967861984715317\n",
      "step: 684659, loss: 0.05986091494560242, data time: 0.008771370438968433\n",
      "step: 684660, loss: 0.06637271493673325, data time: 0.008579765047345842\n",
      "step: 684661, loss: 0.057379212230443954, data time: 0.00839633411831326\n",
      "step: 684662, loss: 0.06167880818247795, data time: 0.008222547737327782\n",
      "step: 684663, loss: 0.05980971083045006, data time: 0.008062136800665604\n",
      "step: 684664, loss: 0.06549270451068878, data time: 0.007909725873898238\n",
      "step: 684665, loss: 0.04228180646896362, data time: 0.007764583826065064\n",
      "step: 684666, loss: 0.05972211807966232, data time: 0.19866633415222168\n",
      "step: 684667, loss: 0.06116547808051109, data time: 0.1004495620727539\n",
      "step: 684668, loss: 0.06842949986457825, data time: 0.06808145840962727\n",
      "step: 684669, loss: 0.0604582354426384, data time: 0.051867008209228516\n",
      "step: 684670, loss: 0.06113938242197037, data time: 0.04177746772766113\n",
      "step: 684671, loss: 0.06599442660808563, data time: 0.03506151835123698\n",
      "step: 684672, loss: 0.06047753244638443, data time: 0.03028716359819685\n",
      "step: 684673, loss: 0.06535608321428299, data time: 0.026755958795547485\n",
      "step: 684674, loss: 0.06829226016998291, data time: 0.023933384153578017\n",
      "step: 684675, loss: 0.058798715472221375, data time: 0.021734213829040526\n",
      "step: 684676, loss: 0.06147254630923271, data time: 0.019958517768166283\n",
      "step: 684677, loss: 0.058658283203840256, data time: 0.018478035926818848\n",
      "step: 684678, loss: 0.058617547154426575, data time: 0.017222844637357272\n",
      "step: 684679, loss: 0.06876217573881149, data time: 0.016136220523289273\n",
      "step: 684680, loss: 0.06955336034297943, data time: 0.015200408299763997\n",
      "step: 684681, loss: 0.057149142026901245, data time: 0.014382615685462952\n",
      "step: 684682, loss: 0.06041873246431351, data time: 0.01366421755622415\n",
      "step: 684683, loss: 0.06641298532485962, data time: 0.013022051917182075\n",
      "step: 684684, loss: 0.06167226284742355, data time: 0.01244270174126876\n",
      "step: 684685, loss: 0.06596994400024414, data time: 0.011928415298461914\n",
      "step: 684686, loss: 0.06403355300426483, data time: 0.011463142576671782\n",
      "step: 684687, loss: 0.060843706130981445, data time: 0.011039322072809393\n",
      "step: 684688, loss: 0.06395493447780609, data time: 0.010646944460661514\n",
      "step: 684689, loss: 0.06921010464429855, data time: 0.010290513435999552\n",
      "step: 684690, loss: 0.061669766902923584, data time: 0.009963674545288086\n",
      "step: 684691, loss: 0.06383481621742249, data time: 0.009658510868365947\n",
      "step: 684692, loss: 0.06343013048171997, data time: 0.009374159353750723\n",
      "step: 684693, loss: 0.05978342145681381, data time: 0.009110876492091588\n",
      "step: 684694, loss: 0.061071962118148804, data time: 0.008870338571482691\n",
      "step: 684695, loss: 0.06103545427322388, data time: 0.008644962310791015\n",
      "step: 684696, loss: 0.060030482709407806, data time: 0.008434972455424647\n",
      "step: 684697, loss: 0.059464067220687866, data time: 0.008243396878242493\n",
      "step: 684698, loss: 0.060720961540937424, data time: 0.008054596005064068\n",
      "step: 684699, loss: 0.06105993688106537, data time: 0.00787969196543974\n",
      "step: 684700, loss: 0.061618294566869736, data time: 0.0077094214303152905\n",
      "step: 684701, loss: 0.06004908308386803, data time: 0.007547616958618164\n",
      "step: 684702, loss: 0.06291835755109787, data time: 0.007394307368510478\n",
      "step: 684703, loss: 0.06640544533729553, data time: 0.007253100997523258\n",
      "step: 684704, loss: 0.06478792428970337, data time: 0.007129180125701122\n",
      "step: 684705, loss: 0.0694037601351738, data time: 0.007001525163650513\n",
      "step: 684706, loss: 0.05876537784934044, data time: 0.2075023651123047\n",
      "step: 684707, loss: 0.06563596427440643, data time: 0.10451376438140869\n",
      "step: 684708, loss: 0.061623506247997284, data time: 0.0701905886332194\n",
      "step: 684709, loss: 0.06063221022486687, data time: 0.053420841693878174\n",
      "step: 684710, loss: 0.06338011473417282, data time: 0.04301586151123047\n",
      "step: 684711, loss: 0.06407313793897629, data time: 0.036104281743367515\n",
      "step: 684712, loss: 0.05690249800682068, data time: 0.031140599931989397\n",
      "step: 684713, loss: 0.060839779675006866, data time: 0.027507007122039795\n",
      "step: 684714, loss: 0.06819843500852585, data time: 0.024606254365709092\n",
      "step: 684715, loss: 0.05658460780978203, data time: 0.0223480224609375\n",
      "step: 684716, loss: 0.05622066184878349, data time: 0.020515420220114967\n",
      "step: 684717, loss: 0.062335699796676636, data time: 0.018991331259409588\n",
      "step: 684718, loss: 0.0641486719250679, data time: 0.017701222346379206\n",
      "step: 684719, loss: 0.06709650903940201, data time: 0.01657698835645403\n",
      "step: 684720, loss: 0.058553650975227356, data time: 0.015611902872721354\n",
      "step: 684721, loss: 0.06399698555469513, data time: 0.014760732650756836\n",
      "step: 684722, loss: 0.06860378384590149, data time: 0.01401371114394244\n",
      "step: 684723, loss: 0.05723510682582855, data time: 0.013370090060763888\n",
      "step: 684724, loss: 0.06728100776672363, data time: 0.012791683799342105\n",
      "step: 684725, loss: 0.06547443568706512, data time: 0.012262213230133056\n",
      "step: 684726, loss: 0.066001757979393, data time: 0.011785836446852911\n",
      "step: 684727, loss: 0.06210753321647644, data time: 0.01134757562117143\n",
      "step: 684728, loss: 0.06561285257339478, data time: 0.010942241419916567\n",
      "step: 684729, loss: 0.06328117102384567, data time: 0.010573784510294596\n",
      "step: 684730, loss: 0.06337551027536392, data time: 0.010230684280395507\n",
      "step: 684731, loss: 0.059523191303014755, data time: 0.009914334003741924\n",
      "step: 684732, loss: 0.06769322603940964, data time: 0.0096197834721318\n",
      "step: 684733, loss: 0.0551297664642334, data time: 0.009348247732434953\n",
      "step: 684734, loss: 0.05911637097597122, data time: 0.009098751791592303\n",
      "step: 684735, loss: 0.05613517388701439, data time: 0.00886816183725993\n",
      "step: 684736, loss: 0.06322287768125534, data time: 0.008650510541854365\n",
      "step: 684737, loss: 0.06253187358379364, data time: 0.008450888097286224\n",
      "step: 684738, loss: 0.05726510286331177, data time: 0.008255481719970703\n",
      "step: 684739, loss: 0.06265552341938019, data time: 0.008068323135375977\n",
      "step: 684740, loss: 0.058195680379867554, data time: 0.007893439701625279\n",
      "step: 684741, loss: 0.061541877686977386, data time: 0.007725510332319472\n",
      "step: 684742, loss: 0.05616087466478348, data time: 0.0075674379194105\n",
      "step: 684743, loss: 0.05660247802734375, data time: 0.007431657690750926\n",
      "step: 684744, loss: 0.05960169434547424, data time: 0.007292857536902795\n",
      "step: 684745, loss: 0.05320502445101738, data time: 0.007160961627960205\n",
      "step: 684746, loss: 0.05803072080016136, data time: 0.19598603248596191\n",
      "step: 684747, loss: 0.06342842429876328, data time: 0.09933710098266602\n",
      "step: 684748, loss: 0.06584616750478745, data time: 0.06726392110188802\n",
      "step: 684749, loss: 0.06633585691452026, data time: 0.05125319957733154\n",
      "step: 684750, loss: 0.06199566647410393, data time: 0.04133310317993164\n",
      "step: 684751, loss: 0.06544126570224762, data time: 0.034740289052327476\n",
      "step: 684752, loss: 0.062099337577819824, data time: 0.03003277097429548\n",
      "step: 684753, loss: 0.05916297435760498, data time: 0.02657473087310791\n",
      "step: 684754, loss: 0.05829283222556114, data time: 0.023795339796278212\n",
      "step: 684755, loss: 0.05825920030474663, data time: 0.02164921760559082\n",
      "step: 684756, loss: 0.06011093780398369, data time: 0.019909880378029564\n",
      "step: 684757, loss: 0.05873027443885803, data time: 0.018464903036753338\n",
      "step: 684758, loss: 0.06583397090435028, data time: 0.017242871798001803\n",
      "step: 684759, loss: 0.06344839930534363, data time: 0.016183461461748396\n",
      "step: 684760, loss: 0.056296251714229584, data time: 0.015271441141764323\n",
      "step: 684761, loss: 0.057880550622940063, data time: 0.014468759298324585\n",
      "step: 684762, loss: 0.0626654326915741, data time: 0.01376041244058048\n",
      "step: 684763, loss: 0.06136082857847214, data time: 0.013128691249423556\n",
      "step: 684764, loss: 0.06353630125522614, data time: 0.012566152371858296\n",
      "step: 684765, loss: 0.057243864983320236, data time: 0.012061631679534912\n",
      "step: 684766, loss: 0.06786908209323883, data time: 0.011606454849243164\n",
      "step: 684767, loss: 0.06033749505877495, data time: 0.011189894242720171\n",
      "step: 684768, loss: 0.0633024051785469, data time: 0.010810219723245373\n",
      "step: 684769, loss: 0.06335316598415375, data time: 0.010466734568277994\n",
      "step: 684770, loss: 0.05893176421523094, data time: 0.010144758224487304\n",
      "step: 684771, loss: 0.06673386693000793, data time: 0.009847448422358586\n",
      "step: 684772, loss: 0.05931442230939865, data time: 0.009568479326036241\n",
      "step: 684773, loss: 0.061667799949645996, data time: 0.0093111480985369\n",
      "step: 684774, loss: 0.05682995170354843, data time: 0.009075551197446626\n",
      "step: 684775, loss: 0.058561839163303375, data time: 0.008858060836791993\n",
      "step: 684776, loss: 0.06253168731927872, data time: 0.008652225617439516\n",
      "step: 684777, loss: 0.060583680868148804, data time: 0.008462004363536835\n",
      "step: 684778, loss: 0.06112883612513542, data time: 0.008270704385006067\n",
      "step: 684779, loss: 0.06440182030200958, data time: 0.008089128662558162\n",
      "step: 684780, loss: 0.06082986295223236, data time: 0.007915633065359933\n",
      "step: 684781, loss: 0.06126663088798523, data time: 0.007751074102189805\n",
      "step: 684782, loss: 0.06876260042190552, data time: 0.0075965120985701275\n",
      "step: 684783, loss: 0.06355665624141693, data time: 0.007453196927120811\n",
      "step: 684784, loss: 0.0563398152589798, data time: 0.007316870567126152\n",
      "step: 684785, loss: 0.06083697825670242, data time: 0.007187658548355102\n",
      "step: 684786, loss: 0.06917394697666168, data time: 0.21268844604492188\n",
      "step: 684787, loss: 0.05517340451478958, data time: 0.1074833869934082\n",
      "step: 684788, loss: 0.0654180645942688, data time: 0.07296180725097656\n",
      "step: 684789, loss: 0.06367351859807968, data time: 0.055101871490478516\n",
      "step: 684790, loss: 0.06016215682029724, data time: 0.044388771057128906\n",
      "step: 684791, loss: 0.056619059294462204, data time: 0.03724026679992676\n",
      "step: 684792, loss: 0.0657227635383606, data time: 0.032202141625540595\n",
      "step: 684793, loss: 0.0639030858874321, data time: 0.02834644913673401\n",
      "step: 684794, loss: 0.0555507056415081, data time: 0.025427129533555772\n",
      "step: 684795, loss: 0.0601842924952507, data time: 0.02308001518249512\n",
      "step: 684796, loss: 0.058338847011327744, data time: 0.021177920428189365\n",
      "step: 684797, loss: 0.06250699609518051, data time: 0.019595801830291748\n",
      "step: 684798, loss: 0.05985536798834801, data time: 0.01825110728924091\n",
      "step: 684799, loss: 0.0626879334449768, data time: 0.01710641384124756\n",
      "step: 684800, loss: 0.0605013333261013, data time: 0.016109275817871093\n",
      "step: 684801, loss: 0.0615868866443634, data time: 0.01522834599018097\n",
      "step: 684802, loss: 0.06055378168821335, data time: 0.014448193942799288\n",
      "step: 684803, loss: 0.06846213340759277, data time: 0.013758619626363119\n",
      "step: 684804, loss: 0.06663881242275238, data time: 0.013135495938752828\n",
      "step: 684805, loss: 0.06372462958097458, data time: 0.012590622901916504\n",
      "step: 684806, loss: 0.06066083163022995, data time: 0.012094281968616304\n",
      "step: 684807, loss: 0.057488493621349335, data time: 0.011646509170532227\n",
      "step: 684808, loss: 0.06149817630648613, data time: 0.01123121510381284\n",
      "step: 684809, loss: 0.05864466354250908, data time: 0.010863343874613443\n",
      "step: 684810, loss: 0.06276541948318481, data time: 0.010508317947387696\n",
      "step: 684811, loss: 0.05985807627439499, data time: 0.010181225263155423\n",
      "step: 684812, loss: 0.05835220217704773, data time: 0.009877619919953522\n",
      "step: 684813, loss: 0.06425727158784866, data time: 0.009594167981828963\n",
      "step: 684814, loss: 0.05780985206365585, data time: 0.009336964837436018\n",
      "step: 684815, loss: 0.0605737641453743, data time: 0.00909566084543864\n",
      "step: 684816, loss: 0.06035447120666504, data time: 0.008869894089237336\n",
      "step: 684817, loss: 0.05684158205986023, data time: 0.008666597306728363\n",
      "step: 684818, loss: 0.05909193679690361, data time: 0.008465990875706528\n",
      "step: 684819, loss: 0.06446870416402817, data time: 0.00827333506415872\n",
      "step: 684820, loss: 0.06456711143255234, data time: 0.008091824395315987\n",
      "step: 684821, loss: 0.06369932740926743, data time: 0.00792151689529419\n",
      "step: 684822, loss: 0.07000388205051422, data time: 0.0077568002649255705\n",
      "step: 684823, loss: 0.05407927557826042, data time: 0.0076067886854472916\n",
      "step: 684824, loss: 0.06184734031558037, data time: 0.007464158229338817\n",
      "step: 684825, loss: 0.07527817785739899, data time: 0.0073292076587677\n",
      "step: 684826, loss: 0.05446271225810051, data time: 0.2243821620941162\n",
      "step: 684827, loss: 0.062481969594955444, data time: 0.11298072338104248\n",
      "step: 684828, loss: 0.05521037429571152, data time: 0.07625412940979004\n",
      "step: 684829, loss: 0.06299687176942825, data time: 0.057849347591400146\n",
      "step: 684830, loss: 0.05874209851026535, data time: 0.04656796455383301\n",
      "step: 684831, loss: 0.06100829690694809, data time: 0.03906655311584473\n",
      "step: 684832, loss: 0.05924701318144798, data time: 0.033876282828194756\n",
      "step: 684833, loss: 0.061537645757198334, data time: 0.029822349548339844\n",
      "step: 684834, loss: 0.06142600625753403, data time: 0.026741080813937716\n",
      "step: 684835, loss: 0.06614585965871811, data time: 0.02427031993865967\n",
      "step: 684836, loss: 0.056308381259441376, data time: 0.022270007566972214\n",
      "step: 684837, loss: 0.06546106934547424, data time: 0.020602881908416748\n",
      "step: 684838, loss: 0.06598936766386032, data time: 0.01919104502751277\n",
      "step: 684839, loss: 0.06450169533491135, data time: 0.01796553816114153\n",
      "step: 684840, loss: 0.05916387587785721, data time: 0.016908740997314452\n",
      "step: 684841, loss: 0.06385517120361328, data time: 0.015984997153282166\n",
      "step: 684842, loss: 0.06134995445609093, data time: 0.015181611565982593\n",
      "step: 684843, loss: 0.05996596813201904, data time: 0.014471689860026041\n",
      "step: 684844, loss: 0.06340281665325165, data time: 0.013842419574135229\n",
      "step: 684845, loss: 0.06861820816993713, data time: 0.013274908065795898\n",
      "step: 684846, loss: 0.06621053069829941, data time: 0.01276170639764695\n",
      "step: 684847, loss: 0.0635489970445633, data time: 0.012301163239912554\n",
      "step: 684848, loss: 0.06276095658540726, data time: 0.011871835459833559\n",
      "step: 684849, loss: 0.06569263339042664, data time: 0.011477818091710409\n",
      "step: 684850, loss: 0.06860813498497009, data time: 0.0111181640625\n",
      "step: 684851, loss: 0.05886160582304001, data time: 0.010782370200523963\n",
      "step: 684852, loss: 0.06237819045782089, data time: 0.01047327783372667\n",
      "step: 684853, loss: 0.06453277170658112, data time: 0.010182585035051619\n",
      "step: 684854, loss: 0.0635722428560257, data time: 0.009918681506452889\n",
      "step: 684855, loss: 0.06339727342128754, data time: 0.009670472145080567\n",
      "step: 684856, loss: 0.06620974838733673, data time: 0.00944066047668457\n",
      "step: 684857, loss: 0.06105469912290573, data time: 0.009226605296134949\n",
      "step: 684858, loss: 0.0599224716424942, data time: 0.0090108568018133\n",
      "step: 684859, loss: 0.06878803670406342, data time: 0.008809349116157083\n",
      "step: 684860, loss: 0.06016603112220764, data time: 0.008615943363734654\n",
      "step: 684861, loss: 0.05723342299461365, data time: 0.008433732721540663\n",
      "step: 684862, loss: 0.06453734636306763, data time: 0.008260391853951119\n",
      "step: 684863, loss: 0.056457094848155975, data time: 0.0081003653375726\n",
      "step: 684864, loss: 0.06676337122917175, data time: 0.007947878959851388\n",
      "step: 684865, loss: 0.06629350781440735, data time: 0.007802712917327881\n",
      "step: 684866, loss: 0.06323637813329697, data time: 0.22357559204101562\n",
      "step: 684867, loss: 0.0653795525431633, data time: 0.11256682872772217\n",
      "step: 684868, loss: 0.058069583028554916, data time: 0.0760793685913086\n",
      "step: 684869, loss: 0.06694263219833374, data time: 0.0578540563583374\n",
      "step: 684870, loss: 0.06408701837062836, data time: 0.0465660572052002\n",
      "step: 684871, loss: 0.06151648983359337, data time: 0.03906067212422689\n",
      "step: 684872, loss: 0.058323901146650314, data time: 0.03369518688746861\n",
      "step: 684873, loss: 0.0627596378326416, data time: 0.029733806848526\n",
      "step: 684874, loss: 0.05890354514122009, data time: 0.026579194598727755\n",
      "step: 684875, loss: 0.06193387508392334, data time: 0.02412431240081787\n",
      "step: 684876, loss: 0.05479522794485092, data time: 0.02214824069630016\n",
      "step: 684877, loss: 0.0613873228430748, data time: 0.020477791627248127\n",
      "step: 684878, loss: 0.06813590228557587, data time: 0.01906930483304537\n",
      "step: 684879, loss: 0.06188900023698807, data time: 0.017853413309369768\n",
      "step: 684880, loss: 0.05636148899793625, data time: 0.016813151041666665\n",
      "step: 684881, loss: 0.06177076697349548, data time: 0.015896886587142944\n",
      "step: 684882, loss: 0.06380710750818253, data time: 0.01510291941025678\n",
      "step: 684883, loss: 0.06716273725032806, data time: 0.014399290084838867\n",
      "step: 684884, loss: 0.06271295994520187, data time: 0.013765184502852591\n",
      "step: 684885, loss: 0.06955845654010773, data time: 0.013203907012939452\n",
      "step: 684886, loss: 0.055158697068691254, data time: 0.01269455183120001\n",
      "step: 684887, loss: 0.06094103679060936, data time: 0.012230266224254261\n",
      "step: 684888, loss: 0.06710673868656158, data time: 0.011804124583368715\n",
      "step: 684889, loss: 0.055526942014694214, data time: 0.011416494846343994\n",
      "step: 684890, loss: 0.07154479622840881, data time: 0.011060438156127929\n",
      "step: 684891, loss: 0.06963813304901123, data time: 0.01072849677159236\n",
      "step: 684892, loss: 0.06657277047634125, data time: 0.010418618166888202\n",
      "step: 684893, loss: 0.060332588851451874, data time: 0.010132355349404472\n",
      "step: 684894, loss: 0.059212617576122284, data time: 0.00985852603254647\n",
      "step: 684895, loss: 0.06476675719022751, data time: 0.009600400924682617\n",
      "step: 684896, loss: 0.063571035861969, data time: 0.00935952894149288\n",
      "step: 684897, loss: 0.05955415591597557, data time: 0.009140066802501678\n",
      "step: 684898, loss: 0.06174667924642563, data time: 0.00892577026829575\n",
      "step: 684899, loss: 0.06177555024623871, data time: 0.008725615108714384\n",
      "step: 684900, loss: 0.05797684192657471, data time: 0.008531509126935686\n",
      "step: 684901, loss: 0.06030082702636719, data time: 0.008349292808108859\n",
      "step: 684902, loss: 0.06138312816619873, data time: 0.008178955799824483\n",
      "step: 684903, loss: 0.06357717514038086, data time: 0.008017383123698988\n",
      "step: 684904, loss: 0.0657418817281723, data time: 0.007865019333668245\n",
      "step: 684905, loss: 0.04432591050863266, data time: 0.0077189445495605465\n",
      "step: 684906, loss: 0.06415415555238724, data time: 0.21684694290161133\n",
      "step: 684907, loss: 0.06112228333950043, data time: 0.1095801591873169\n",
      "step: 684908, loss: 0.056645553559064865, data time: 0.0742025375366211\n",
      "step: 684909, loss: 0.058931827545166016, data time: 0.05631524324417114\n",
      "step: 684910, loss: 0.05904592201113701, data time: 0.04536275863647461\n",
      "step: 684911, loss: 0.06185392290353775, data time: 0.038056413332621254\n",
      "step: 684912, loss: 0.06600142270326614, data time: 0.032842465809413364\n",
      "step: 684913, loss: 0.06767793744802475, data time: 0.029010653495788574\n",
      "step: 684914, loss: 0.058428775519132614, data time: 0.02593676249186198\n",
      "step: 684915, loss: 0.060634396970272064, data time: 0.023542141914367674\n",
      "step: 684916, loss: 0.05796777456998825, data time: 0.02159417759288441\n",
      "step: 684917, loss: 0.0565958134829998, data time: 0.019979357719421387\n",
      "step: 684918, loss: 0.06002318486571312, data time: 0.018605635716364935\n",
      "step: 684919, loss: 0.06045647710561752, data time: 0.01742720603942871\n",
      "step: 684920, loss: 0.06751015782356262, data time: 0.016416184107462563\n",
      "step: 684921, loss: 0.06566070020198822, data time: 0.01552724838256836\n",
      "step: 684922, loss: 0.06024625897407532, data time: 0.014736273709465475\n",
      "step: 684923, loss: 0.057208701968193054, data time: 0.01402845647599962\n",
      "step: 684924, loss: 0.06485156714916229, data time: 0.013395861575478002\n",
      "step: 684925, loss: 0.05800759047269821, data time: 0.01283423900604248\n",
      "step: 684926, loss: 0.06738622486591339, data time: 0.012324253718058268\n",
      "step: 684927, loss: 0.060389768332242966, data time: 0.011862624775279652\n",
      "step: 684928, loss: 0.0589815191924572, data time: 0.011449720548546833\n",
      "step: 684929, loss: 0.06537225842475891, data time: 0.011078665653864542\n",
      "step: 684930, loss: 0.05961134284734726, data time: 0.01072047233581543\n",
      "step: 684931, loss: 0.0679105818271637, data time: 0.010385687534625713\n",
      "step: 684932, loss: 0.0626809224486351, data time: 0.010080381676002784\n",
      "step: 684933, loss: 0.05846337601542473, data time: 0.0097922682762146\n",
      "step: 684934, loss: 0.06670116633176804, data time: 0.009528077881911705\n",
      "step: 684935, loss: 0.06430398672819138, data time: 0.00928182601928711\n",
      "step: 684936, loss: 0.06272029876708984, data time: 0.009050938390916394\n",
      "step: 684937, loss: 0.06618845462799072, data time: 0.008839920163154602\n",
      "step: 684938, loss: 0.05650646984577179, data time: 0.00863283330743963\n",
      "step: 684939, loss: 0.06630607694387436, data time: 0.008438446942497702\n",
      "step: 684940, loss: 0.0671064555644989, data time: 0.008251196997506278\n",
      "step: 684941, loss: 0.06616660952568054, data time: 0.008077005545298258\n",
      "step: 684942, loss: 0.05505213141441345, data time: 0.00791036116110312\n",
      "step: 684943, loss: 0.060240067541599274, data time: 0.007755411298651444\n",
      "step: 684944, loss: 0.06089279055595398, data time: 0.0076080224452874596\n",
      "step: 684945, loss: 0.06346705555915833, data time: 0.007468748092651367\n",
      "step: 684946, loss: 0.06293191015720367, data time: 0.21503710746765137\n",
      "step: 684947, loss: 0.057346902787685394, data time: 0.10920679569244385\n",
      "step: 684948, loss: 0.06952272355556488, data time: 0.07331784566243489\n",
      "step: 684949, loss: 0.05913148075342178, data time: 0.05587369203567505\n",
      "step: 684950, loss: 0.06174018234014511, data time: 0.044994783401489255\n",
      "step: 684951, loss: 0.0646899864077568, data time: 0.03775076071421305\n",
      "step: 684952, loss: 0.06992057710886002, data time: 0.03256419726780483\n",
      "step: 684953, loss: 0.06294729560613632, data time: 0.02874886989593506\n",
      "step: 684954, loss: 0.06262382864952087, data time: 0.025702423519558378\n",
      "step: 684955, loss: 0.0631832405924797, data time: 0.02334141731262207\n",
      "step: 684956, loss: 0.05991581082344055, data time: 0.021449370817704635\n",
      "step: 684957, loss: 0.05869561433792114, data time: 0.019878347714742024\n",
      "step: 684958, loss: 0.06282847374677658, data time: 0.018543004989624023\n",
      "step: 684959, loss: 0.060688894242048264, data time: 0.017390847206115723\n",
      "step: 684960, loss: 0.06517219543457031, data time: 0.016400782267252605\n",
      "step: 684961, loss: 0.06595098972320557, data time: 0.015535056591033936\n",
      "step: 684962, loss: 0.06047111004590988, data time: 0.014760634478400736\n",
      "step: 684963, loss: 0.0698554515838623, data time: 0.014069517453511557\n",
      "step: 684964, loss: 0.06353757530450821, data time: 0.013452831067537007\n",
      "step: 684965, loss: 0.05968865007162094, data time: 0.01288745403289795\n",
      "step: 684966, loss: 0.06721749901771545, data time: 0.012376705805460611\n",
      "step: 684967, loss: 0.05984501540660858, data time: 0.011913646351207386\n",
      "step: 684968, loss: 0.057249002158641815, data time: 0.011489723039710003\n",
      "step: 684969, loss: 0.061890698969364166, data time: 0.011110126972198486\n",
      "step: 684970, loss: 0.05935095250606537, data time: 0.010754947662353515\n",
      "step: 684971, loss: 0.058555684983730316, data time: 0.010424063755915714\n",
      "step: 684972, loss: 0.06287669390439987, data time: 0.010114855236477323\n",
      "step: 684973, loss: 0.06252145767211914, data time: 0.009825621332441057\n",
      "step: 684974, loss: 0.06365850567817688, data time: 0.009561012531148976\n",
      "step: 684975, loss: 0.06147312372922897, data time: 0.009312637646993001\n",
      "step: 684976, loss: 0.06374408304691315, data time: 0.00908056382210024\n",
      "step: 684977, loss: 0.05931774526834488, data time: 0.008868768811225891\n",
      "step: 684978, loss: 0.06081389635801315, data time: 0.008663206389456085\n",
      "step: 684979, loss: 0.06381604820489883, data time: 0.008467239492079792\n",
      "step: 684980, loss: 0.061998989433050156, data time: 0.008280876704624721\n",
      "step: 684981, loss: 0.058872759342193604, data time: 0.008102271291944716\n",
      "step: 684982, loss: 0.0587807223200798, data time: 0.007935685080450934\n",
      "step: 684983, loss: 0.058067914098501205, data time: 0.007786487278185393\n",
      "step: 684984, loss: 0.06129222363233566, data time: 0.007640465711935973\n",
      "step: 684985, loss: 0.05976484343409538, data time: 0.007501184940338135\n",
      "step: 684986, loss: 0.059176914393901825, data time: 0.2084040641784668\n",
      "step: 684987, loss: 0.057351790368556976, data time: 0.10533750057220459\n",
      "step: 684988, loss: 0.059675540775060654, data time: 0.07111541430155437\n",
      "step: 684989, loss: 0.0607018768787384, data time: 0.05421262979507446\n",
      "step: 684990, loss: 0.07064302265644073, data time: 0.04366059303283691\n",
      "step: 684991, loss: 0.06190541386604309, data time: 0.03663543860117594\n",
      "step: 684992, loss: 0.06460621953010559, data time: 0.03163763454982212\n",
      "step: 684993, loss: 0.06075572967529297, data time: 0.027936160564422607\n",
      "step: 684994, loss: 0.06314771622419357, data time: 0.024976465437147353\n",
      "step: 684995, loss: 0.05897098034620285, data time: 0.022679781913757323\n",
      "step: 684996, loss: 0.05950922518968582, data time: 0.020815004001964222\n",
      "step: 684997, loss: 0.06465505808591843, data time: 0.019259134928385418\n",
      "step: 684998, loss: 0.062315016984939575, data time: 0.01795337750361516\n",
      "step: 684999, loss: 0.05904427170753479, data time: 0.016826646668570384\n",
      "step: 685000, loss: 0.0635777935385704, data time: 0.015858983993530272\n",
      "step: 685001, loss: 0.06105775386095047, data time: 0.015011638402938843\n",
      "step: 685002, loss: 0.06252162158489227, data time: 0.014255846247953527\n",
      "step: 685003, loss: 0.05895479768514633, data time: 0.013577606942918565\n",
      "step: 685004, loss: 0.060998350381851196, data time: 0.01297106240925036\n",
      "step: 685005, loss: 0.060816165059804916, data time: 0.012428641319274902\n",
      "step: 685006, loss: 0.061643749475479126, data time: 0.0119585877373105\n",
      "step: 685007, loss: 0.0612473301589489, data time: 0.011533282019875267\n",
      "step: 685008, loss: 0.058610688894987106, data time: 0.011136386705481487\n",
      "step: 685009, loss: 0.06431043148040771, data time: 0.010776956876118978\n",
      "step: 685010, loss: 0.06550704687833786, data time: 0.010450658798217773\n",
      "step: 685011, loss: 0.06563834846019745, data time: 0.010145920973557692\n",
      "step: 685012, loss: 0.06165463849902153, data time: 0.009856488969590928\n",
      "step: 685013, loss: 0.066236212849617, data time: 0.0095890930720738\n",
      "step: 685014, loss: 0.061726443469524384, data time: 0.00934504640513453\n",
      "step: 685015, loss: 0.05678882822394371, data time: 0.009118978182474773\n",
      "step: 685016, loss: 0.05708380788564682, data time: 0.008907225824171496\n",
      "step: 685017, loss: 0.06220707297325134, data time: 0.008709698915481567\n",
      "step: 685018, loss: 0.06605517119169235, data time: 0.008512511397853043\n",
      "step: 685019, loss: 0.0632723793387413, data time: 0.00832509994506836\n",
      "step: 685020, loss: 0.05773889273405075, data time: 0.008145087105887277\n",
      "step: 685021, loss: 0.05803999304771423, data time: 0.007972803380754259\n",
      "step: 685022, loss: 0.05524076148867607, data time: 0.007811662313100454\n",
      "step: 685023, loss: 0.060179390013217926, data time: 0.007662578632957057\n",
      "step: 685024, loss: 0.06309662014245987, data time: 0.007521342008541792\n",
      "step: 685025, loss: 0.05337192118167877, data time: 0.007387173175811767\n",
      "step: 685026, loss: 0.06323771178722382, data time: 0.22102808952331543\n",
      "step: 685027, loss: 0.058596402406692505, data time: 0.11192595958709717\n",
      "step: 685028, loss: 0.06655412912368774, data time: 0.07552814483642578\n",
      "step: 685029, loss: 0.054808370769023895, data time: 0.05719602108001709\n",
      "step: 685030, loss: 0.055310361087322235, data time: 0.046042251586914065\n",
      "step: 685031, loss: 0.06546738743782043, data time: 0.03861717383066813\n",
      "step: 685032, loss: 0.06636963784694672, data time: 0.03339999062674386\n",
      "step: 685033, loss: 0.05431957170367241, data time: 0.02947983145713806\n",
      "step: 685034, loss: 0.05747745558619499, data time: 0.026429626676771376\n",
      "step: 685035, loss: 0.060435812920331955, data time: 0.02398207187652588\n",
      "step: 685036, loss: 0.0627833753824234, data time: 0.021996389735828747\n",
      "step: 685037, loss: 0.059670209884643555, data time: 0.020341992378234863\n",
      "step: 685038, loss: 0.05833921954035759, data time: 0.018944171758798454\n",
      "step: 685039, loss: 0.05858447402715683, data time: 0.017735293933323452\n",
      "step: 685040, loss: 0.06697199493646622, data time: 0.016698710123697915\n",
      "step: 685041, loss: 0.059933096170425415, data time: 0.01578553020954132\n",
      "step: 685042, loss: 0.06508561223745346, data time: 0.014978955773746265\n",
      "step: 685043, loss: 0.06031227856874466, data time: 0.014258967505560981\n",
      "step: 685044, loss: 0.061862170696258545, data time: 0.0136124334837261\n",
      "step: 685045, loss: 0.07306984812021255, data time: 0.013036847114562988\n",
      "step: 685046, loss: 0.06397157907485962, data time: 0.012519938605172294\n",
      "step: 685047, loss: 0.05841278284788132, data time: 0.0120475942438299\n",
      "step: 685048, loss: 0.058030903339385986, data time: 0.011611482371454653\n",
      "step: 685049, loss: 0.06375377625226974, data time: 0.0112169881661733\n",
      "step: 685050, loss: 0.05937696620821953, data time: 0.010852909088134766\n",
      "step: 685051, loss: 0.059521518647670746, data time: 0.010513103925264798\n",
      "step: 685052, loss: 0.06598837673664093, data time: 0.010199546813964844\n",
      "step: 685053, loss: 0.055602118372917175, data time: 0.009905108383723668\n",
      "step: 685054, loss: 0.06528826057910919, data time: 0.009638630110642006\n",
      "step: 685055, loss: 0.06302698701620102, data time: 0.00939161777496338\n",
      "step: 685056, loss: 0.05886354297399521, data time: 0.009158119078605406\n",
      "step: 685057, loss: 0.05791902169585228, data time: 0.008941061794757843\n",
      "step: 685058, loss: 0.06715049594640732, data time: 0.008733315901322798\n",
      "step: 685059, loss: 0.062364619225263596, data time: 0.008536051301395191\n",
      "step: 685060, loss: 0.06246417388319969, data time: 0.008346966334751673\n",
      "step: 685061, loss: 0.06218843162059784, data time: 0.008168511920505099\n",
      "step: 685062, loss: 0.06124312803149223, data time: 0.007997396829965952\n",
      "step: 685063, loss: 0.058843016624450684, data time: 0.0078411855195698\n",
      "step: 685064, loss: 0.06056191027164459, data time: 0.00769206193777231\n",
      "step: 685065, loss: 0.06379158794879913, data time: 0.007551324367523193\n",
      "step: 685066, loss: 0.06499943137168884, data time: 0.20278048515319824\n",
      "step: 685067, loss: 0.06312236189842224, data time: 0.10297834873199463\n",
      "step: 685068, loss: 0.0606137290596962, data time: 0.06957356135050456\n",
      "step: 685069, loss: 0.061281781643629074, data time: 0.05288499593734741\n",
      "step: 685070, loss: 0.061878204345703125, data time: 0.04261822700500488\n",
      "step: 685071, loss: 0.0611451081931591, data time: 0.035776217778523765\n",
      "step: 685072, loss: 0.06275956332683563, data time: 0.03087803295680455\n",
      "step: 685073, loss: 0.06444521993398666, data time: 0.02728334069252014\n",
      "step: 685074, loss: 0.06682411581277847, data time: 0.024397426181369357\n",
      "step: 685075, loss: 0.057337962090969086, data time: 0.02216918468475342\n",
      "step: 685076, loss: 0.062432900071144104, data time: 0.02034737847068093\n",
      "step: 685077, loss: 0.06114678829908371, data time: 0.018826961517333984\n",
      "step: 685078, loss: 0.05905642360448837, data time: 0.017548285997830905\n",
      "step: 685079, loss: 0.05984572693705559, data time: 0.016441941261291504\n",
      "step: 685080, loss: 0.05973506718873978, data time: 0.01549542744954427\n",
      "step: 685081, loss: 0.06062949448823929, data time: 0.014661893248558044\n",
      "step: 685082, loss: 0.05833872780203819, data time: 0.013927277396706975\n",
      "step: 685083, loss: 0.06026359647512436, data time: 0.013306617736816406\n",
      "step: 685084, loss: 0.0647493302822113, data time: 0.012717661104704204\n",
      "step: 685085, loss: 0.06040782853960991, data time: 0.012187588214874267\n",
      "step: 685086, loss: 0.06454826891422272, data time: 0.011712153752644857\n",
      "step: 685087, loss: 0.06516796350479126, data time: 0.011278217489069159\n",
      "step: 685088, loss: 0.05700599402189255, data time: 0.010876033617102581\n",
      "step: 685089, loss: 0.06549127399921417, data time: 0.010511259237925211\n",
      "step: 685090, loss: 0.061456840485334396, data time: 0.0101910400390625\n",
      "step: 685091, loss: 0.06596231460571289, data time: 0.009892537043644832\n",
      "step: 685092, loss: 0.06100652739405632, data time: 0.009613858328925239\n",
      "step: 685093, loss: 0.06770744919776917, data time: 0.009355851582118444\n",
      "step: 685094, loss: 0.06976929306983948, data time: 0.009121952385737979\n",
      "step: 685095, loss: 0.06205945461988449, data time: 0.008902851740519207\n",
      "step: 685096, loss: 0.06263957917690277, data time: 0.008697909693564139\n",
      "step: 685097, loss: 0.06109188124537468, data time: 0.008506134152412415\n",
      "step: 685098, loss: 0.05862577259540558, data time: 0.008314652876420454\n",
      "step: 685099, loss: 0.06557390093803406, data time: 0.008133572690627155\n",
      "step: 685100, loss: 0.0645555704832077, data time: 0.007959188733782088\n",
      "step: 685101, loss: 0.06048571690917015, data time: 0.007792856958177354\n",
      "step: 685102, loss: 0.06643040478229523, data time: 0.007637030369526631\n",
      "step: 685103, loss: 0.053514741361141205, data time: 0.007493426925257633\n",
      "step: 685104, loss: 0.060747236013412476, data time: 0.007357646257449419\n",
      "step: 685105, loss: 0.06179158017039299, data time: 0.007227516174316407\n",
      "step: 685106, loss: 0.061695851385593414, data time: 0.20824146270751953\n",
      "step: 685107, loss: 0.06513501703739166, data time: 0.104897141456604\n",
      "step: 685108, loss: 0.061681635677814484, data time: 0.07043965657552083\n",
      "step: 685109, loss: 0.06374804675579071, data time: 0.05363309383392334\n",
      "step: 685110, loss: 0.06171981617808342, data time: 0.043218803405761716\n",
      "step: 685111, loss: 0.05717149376869202, data time: 0.03626441955566406\n",
      "step: 685112, loss: 0.05856657773256302, data time: 0.031294516154697964\n",
      "step: 685113, loss: 0.06794922053813934, data time: 0.027651548385620117\n",
      "step: 685114, loss: 0.06156466528773308, data time: 0.024726602766248915\n",
      "step: 685115, loss: 0.06677562743425369, data time: 0.02245306968688965\n",
      "step: 685116, loss: 0.0572834312915802, data time: 0.020600427280772816\n",
      "step: 685117, loss: 0.06710267066955566, data time: 0.01906283696492513\n",
      "step: 685118, loss: 0.059756092727184296, data time: 0.017764825087327223\n",
      "step: 685119, loss: 0.061136435717344284, data time: 0.01665675640106201\n",
      "step: 685120, loss: 0.06149101257324219, data time: 0.01568458875020345\n",
      "step: 685121, loss: 0.06415187567472458, data time: 0.014837607741355896\n",
      "step: 685122, loss: 0.06844916194677353, data time: 0.01410220651065602\n",
      "step: 685123, loss: 0.0633280798792839, data time: 0.013432886865403917\n",
      "step: 685124, loss: 0.061935633420944214, data time: 0.012834097209729646\n",
      "step: 685125, loss: 0.05910549685359001, data time: 0.01229935884475708\n",
      "step: 685126, loss: 0.06007909029722214, data time: 0.011814719154721215\n",
      "step: 685127, loss: 0.06126546859741211, data time: 0.011377009478482332\n",
      "step: 685128, loss: 0.06022266671061516, data time: 0.010969918707142706\n",
      "step: 685129, loss: 0.06333305686712265, data time: 0.010596762100855509\n",
      "step: 685130, loss: 0.06216584891080856, data time: 0.010264816284179688\n",
      "step: 685131, loss: 0.060539186000823975, data time: 0.009954544214101939\n",
      "step: 685132, loss: 0.05782713741064072, data time: 0.00966095041345667\n",
      "step: 685133, loss: 0.06418921798467636, data time: 0.009392355169568743\n",
      "step: 685134, loss: 0.05907305330038071, data time: 0.009144174641576307\n",
      "step: 685135, loss: 0.0698845386505127, data time: 0.008916052182515462\n",
      "step: 685136, loss: 0.06669074296951294, data time: 0.008699878569572203\n",
      "step: 685137, loss: 0.057960063219070435, data time: 0.008498616516590118\n",
      "step: 685138, loss: 0.0664372593164444, data time: 0.008299199017611418\n",
      "step: 685139, loss: 0.06385060399770737, data time: 0.008114092490252326\n",
      "step: 685140, loss: 0.06930829584598541, data time: 0.007941280092511858\n",
      "step: 685141, loss: 0.0668434351682663, data time: 0.007773545053270128\n",
      "step: 685142, loss: 0.06664766371250153, data time: 0.007617048315099768\n",
      "step: 685143, loss: 0.061669185757637024, data time: 0.007470237581353439\n",
      "step: 685144, loss: 0.06186102330684662, data time: 0.007331053415934245\n",
      "step: 685145, loss: 0.06064140051603317, data time: 0.00719802975654602\n",
      "step: 685146, loss: 0.057881854474544525, data time: 0.2259080410003662\n",
      "step: 685147, loss: 0.06661957502365112, data time: 0.11369311809539795\n",
      "step: 685148, loss: 0.058140646666288376, data time: 0.0767051378885905\n",
      "step: 685149, loss: 0.06260097026824951, data time: 0.058319926261901855\n",
      "step: 685150, loss: 0.05793914198875427, data time: 0.0469449520111084\n",
      "step: 685151, loss: 0.06556257605552673, data time: 0.039370457331339516\n",
      "step: 685152, loss: 0.060443587601184845, data time: 0.03396075112479074\n",
      "step: 685153, loss: 0.05801781639456749, data time: 0.0299663245677948\n",
      "step: 685154, loss: 0.060469187796115875, data time: 0.02678028742472331\n",
      "step: 685155, loss: 0.06455495208501816, data time: 0.024306106567382812\n",
      "step: 685156, loss: 0.06550466269254684, data time: 0.022318905050104313\n",
      "step: 685157, loss: 0.07080467790365219, data time: 0.02064518133799235\n",
      "step: 685158, loss: 0.0623592846095562, data time: 0.019222332881047174\n",
      "step: 685159, loss: 0.06014181300997734, data time: 0.017995987619672502\n",
      "step: 685160, loss: 0.06468375027179718, data time: 0.01693860689798991\n",
      "step: 685161, loss: 0.06282871961593628, data time: 0.016044899821281433\n",
      "step: 685162, loss: 0.06032545492053032, data time: 0.01521912743063534\n",
      "step: 685163, loss: 0.06478612869977951, data time: 0.01448561085595025\n",
      "step: 685164, loss: 0.062279339879751205, data time: 0.013830799805490594\n",
      "step: 685165, loss: 0.06281431764364243, data time: 0.013248836994171143\n",
      "step: 685166, loss: 0.0643695592880249, data time: 0.012721390951247443\n",
      "step: 685167, loss: 0.06313914060592651, data time: 0.012243563478643244\n",
      "step: 685168, loss: 0.06476397067308426, data time: 0.011801481246948242\n",
      "step: 685169, loss: 0.06607309728860855, data time: 0.011399755875269571\n",
      "step: 685170, loss: 0.06088297814130783, data time: 0.011033926010131836\n",
      "step: 685171, loss: 0.06276367604732513, data time: 0.010689863791832557\n",
      "step: 685172, loss: 0.05975675582885742, data time: 0.010366784201727973\n",
      "step: 685173, loss: 0.062458209693431854, data time: 0.010069217000688826\n",
      "step: 685174, loss: 0.062109898775815964, data time: 0.009796109692803744\n",
      "step: 685175, loss: 0.05661243200302124, data time: 0.009553853670756023\n",
      "step: 685176, loss: 0.06254012882709503, data time: 0.009328342253162016\n",
      "step: 685177, loss: 0.059084586799144745, data time: 0.009111717343330383\n",
      "step: 685178, loss: 0.06634234637022018, data time: 0.008896343635790276\n",
      "step: 685179, loss: 0.06691988557577133, data time: 0.008693351465113023\n",
      "step: 685180, loss: 0.055806588381528854, data time: 0.00850020136151995\n",
      "step: 685181, loss: 0.06419473886489868, data time: 0.008314821455213759\n",
      "step: 685182, loss: 0.062404051423072815, data time: 0.008141420982979439\n",
      "step: 685183, loss: 0.05954360216856003, data time: 0.007982222657454642\n",
      "step: 685184, loss: 0.05303855985403061, data time: 0.007831420653905624\n",
      "step: 685185, loss: 0.06921599805355072, data time: 0.007687270641326904\n",
      "step: 685186, loss: 0.0610739141702652, data time: 0.2163848876953125\n",
      "step: 685187, loss: 0.058720748871564865, data time: 0.10895347595214844\n",
      "step: 685188, loss: 0.06121554225683212, data time: 0.07339660326639812\n",
      "step: 685189, loss: 0.06211131811141968, data time: 0.05600404739379883\n",
      "step: 685190, loss: 0.06770478188991547, data time: 0.04508051872253418\n",
      "step: 685191, loss: 0.053577132523059845, data time: 0.03782955805460612\n",
      "step: 685192, loss: 0.05755361169576645, data time: 0.0326422963823591\n",
      "step: 685193, loss: 0.062962606549263, data time: 0.028821974992752075\n",
      "step: 685194, loss: 0.056263625621795654, data time: 0.02577792273627387\n",
      "step: 685195, loss: 0.059438519179821014, data time: 0.02341282367706299\n",
      "step: 685196, loss: 0.06636923551559448, data time: 0.02148814634843306\n",
      "step: 685197, loss: 0.05818668007850647, data time: 0.019878268241882324\n",
      "step: 685198, loss: 0.06185012310743332, data time: 0.018526132290179912\n",
      "step: 685199, loss: 0.0664203017950058, data time: 0.017353364399501255\n",
      "step: 685200, loss: 0.061650585383176804, data time: 0.016335090001424152\n",
      "step: 685201, loss: 0.06059853360056877, data time: 0.015444278717041016\n",
      "step: 685202, loss: 0.058366529643535614, data time: 0.014661017586203182\n",
      "step: 685203, loss: 0.05967436358332634, data time: 0.013953314887152778\n",
      "step: 685204, loss: 0.06490983814001083, data time: 0.013324336001747534\n",
      "step: 685205, loss: 0.06375545263290405, data time: 0.012764084339141845\n",
      "step: 685206, loss: 0.05827697739005089, data time: 0.012259937468029204\n",
      "step: 685207, loss: 0.06352934241294861, data time: 0.011801925572482023\n",
      "step: 685208, loss: 0.05776378512382507, data time: 0.011377604111381199\n",
      "step: 685209, loss: 0.06005232781171799, data time: 0.010990659395853678\n",
      "step: 685210, loss: 0.06225522980093956, data time: 0.010651111602783203\n",
      "step: 685211, loss: 0.06008067727088928, data time: 0.010320736811711239\n",
      "step: 685212, loss: 0.06095327064394951, data time: 0.01001119613647461\n",
      "step: 685213, loss: 0.06163504719734192, data time: 0.009728074073791504\n",
      "step: 685214, loss: 0.0567057840526104, data time: 0.009465842411435884\n",
      "step: 685215, loss: 0.06089057773351669, data time: 0.009221760431925456\n",
      "step: 685216, loss: 0.058123450726270676, data time: 0.008994963861280871\n",
      "step: 685217, loss: 0.07030699402093887, data time: 0.00878511369228363\n",
      "step: 685218, loss: 0.06282378733158112, data time: 0.008584275390162613\n",
      "step: 685219, loss: 0.06099414825439453, data time: 0.008391478482414694\n",
      "step: 685220, loss: 0.06074770912528038, data time: 0.008208669934953962\n",
      "step: 685221, loss: 0.060440074652433395, data time: 0.008036944601270888\n",
      "step: 685222, loss: 0.06439059972763062, data time: 0.00787565514848039\n",
      "step: 685223, loss: 0.06076328083872795, data time: 0.007725163509971217\n",
      "step: 685224, loss: 0.0639578104019165, data time: 0.007582774529090295\n",
      "step: 685225, loss: 0.05076558515429497, data time: 0.007448160648345947\n",
      "step: 685226, loss: 0.06320982426404953, data time: 0.20742392539978027\n",
      "step: 685227, loss: 0.0631973147392273, data time: 0.10447025299072266\n",
      "step: 685228, loss: 0.06088713929057121, data time: 0.07038052876790364\n",
      "step: 685229, loss: 0.06256800889968872, data time: 0.05365192890167236\n",
      "step: 685230, loss: 0.06268005073070526, data time: 0.04320745468139649\n",
      "step: 685231, loss: 0.06364764273166656, data time: 0.036246935526529946\n",
      "step: 685232, loss: 0.055074542760849, data time: 0.03129461833408901\n",
      "step: 685233, loss: 0.06526773422956467, data time: 0.027646303176879883\n",
      "step: 685234, loss: 0.056124694645404816, data time: 0.02472792731391059\n",
      "step: 685235, loss: 0.05653684586286545, data time: 0.022457647323608398\n",
      "step: 685236, loss: 0.0577961802482605, data time: 0.02060968225652521\n",
      "step: 685237, loss: 0.0637807846069336, data time: 0.019076526165008545\n",
      "step: 685238, loss: 0.0654306411743164, data time: 0.017788446866548978\n",
      "step: 685239, loss: 0.06820699572563171, data time: 0.016669273376464844\n",
      "step: 685240, loss: 0.06853961944580078, data time: 0.01570898691813151\n",
      "step: 685241, loss: 0.05880756303668022, data time: 0.014867246150970459\n",
      "step: 685242, loss: 0.06227622181177139, data time: 0.014115978689754711\n",
      "step: 685243, loss: 0.0649806559085846, data time: 0.013442039489746094\n",
      "step: 685244, loss: 0.06321272253990173, data time: 0.012840032577514648\n",
      "step: 685245, loss: 0.06177850067615509, data time: 0.012303960323333741\n",
      "step: 685246, loss: 0.0586080364882946, data time: 0.011818329493204752\n",
      "step: 685247, loss: 0.061101995408535004, data time: 0.01138216798955744\n",
      "step: 685248, loss: 0.061955928802490234, data time: 0.010975464530613111\n",
      "step: 685249, loss: 0.0646151602268219, data time: 0.010608216126759848\n",
      "step: 685250, loss: 0.062557652592659, data time: 0.010271902084350587\n",
      "step: 685251, loss: 0.06071503087878227, data time: 0.009957625315739559\n",
      "step: 685252, loss: 0.06443632394075394, data time: 0.009660862110279224\n",
      "step: 685253, loss: 0.06559406220912933, data time: 0.009386335100446428\n",
      "step: 685254, loss: 0.05886080116033554, data time: 0.009136216393832502\n",
      "step: 685255, loss: 0.06371799111366272, data time: 0.008903090159098308\n",
      "step: 685256, loss: 0.0657632127404213, data time: 0.008686157964891004\n",
      "step: 685257, loss: 0.061822108924388885, data time: 0.008492238819599152\n",
      "step: 685258, loss: 0.059941232204437256, data time: 0.00829611402569395\n",
      "step: 685259, loss: 0.056812893599271774, data time: 0.008112409535576315\n",
      "step: 685260, loss: 0.06172209233045578, data time: 0.007939761025565012\n",
      "step: 685261, loss: 0.06597854197025299, data time: 0.007771942350599501\n",
      "step: 685262, loss: 0.06062514707446098, data time: 0.007613394711468671\n",
      "step: 685263, loss: 0.06011256203055382, data time: 0.007467414203443025\n",
      "step: 685264, loss: 0.0612168125808239, data time: 0.007327850048358624\n",
      "step: 685265, loss: 0.0766710489988327, data time: 0.00719560980796814\n",
      "step: 685266, loss: 0.06496255844831467, data time: 0.22270941734313965\n",
      "step: 685267, loss: 0.06313508003950119, data time: 0.1121208667755127\n",
      "step: 685268, loss: 0.06341573596000671, data time: 0.07574907938639323\n",
      "step: 685269, loss: 0.06232396513223648, data time: 0.057473719120025635\n",
      "step: 685270, loss: 0.06407241523265839, data time: 0.046262073516845706\n",
      "step: 685271, loss: 0.06084468215703964, data time: 0.0388110876083374\n",
      "step: 685272, loss: 0.06268470734357834, data time: 0.03347808974129813\n",
      "step: 685273, loss: 0.06565085053443909, data time: 0.029548078775405884\n",
      "step: 685274, loss: 0.06129505857825279, data time: 0.026414288414849177\n",
      "step: 685275, loss: 0.05930939316749573, data time: 0.023979735374450684\n",
      "step: 685276, loss: 0.059548765420913696, data time: 0.021991751410744408\n",
      "step: 685277, loss: 0.06254696100950241, data time: 0.02033952871958415\n",
      "step: 685278, loss: 0.059198491275310516, data time: 0.018941897612351637\n",
      "step: 685279, loss: 0.06284560263156891, data time: 0.017735958099365234\n",
      "step: 685280, loss: 0.064353808760643, data time: 0.016698137919108073\n",
      "step: 685281, loss: 0.05746014043688774, data time: 0.01579366624355316\n",
      "step: 685282, loss: 0.06716288626194, data time: 0.014986753463745117\n",
      "step: 685283, loss: 0.06258591264486313, data time: 0.01426700750986735\n",
      "step: 685284, loss: 0.06191907078027725, data time: 0.013625182603534899\n",
      "step: 685285, loss: 0.06621702015399933, data time: 0.013050436973571777\n",
      "step: 685286, loss: 0.06190689653158188, data time: 0.012530463082449777\n",
      "step: 685287, loss: 0.06794699281454086, data time: 0.012058388103138317\n",
      "step: 685288, loss: 0.0602046363055706, data time: 0.011623859405517578\n",
      "step: 685289, loss: 0.06223772093653679, data time: 0.011230905850728353\n",
      "step: 685290, loss: 0.06912240386009216, data time: 0.010870885848999024\n",
      "step: 685291, loss: 0.06594031304121017, data time: 0.010531040338369517\n",
      "step: 685292, loss: 0.06526350975036621, data time: 0.010218487845526801\n",
      "step: 685293, loss: 0.06670644879341125, data time: 0.009929750646863664\n",
      "step: 685294, loss: 0.06422772258520126, data time: 0.009661025014416924\n",
      "step: 685295, loss: 0.05722923204302788, data time: 0.009415197372436523\n",
      "step: 685296, loss: 0.06829758733510971, data time: 0.009181445644747826\n",
      "step: 685297, loss: 0.058922313153743744, data time: 0.008967071771621704\n",
      "step: 685298, loss: 0.06535946577787399, data time: 0.008757591247558594\n",
      "step: 685299, loss: 0.06625419110059738, data time: 0.008559619679170497\n",
      "step: 685300, loss: 0.05850015580654144, data time: 0.008369452612740653\n",
      "step: 685301, loss: 0.060772232711315155, data time: 0.00818698936038547\n",
      "step: 685302, loss: 0.06319680064916611, data time: 0.00801706314086914\n",
      "step: 685303, loss: 0.057608649134635925, data time: 0.007860102151569567\n",
      "step: 685304, loss: 0.062477171421051025, data time: 0.00771027956253443\n",
      "step: 685305, loss: 0.07093870639801025, data time: 0.007569813728332519\n",
      "step: 685306, loss: 0.059487294405698776, data time: 0.21682453155517578\n",
      "step: 685307, loss: 0.06513393670320511, data time: 0.10925912857055664\n",
      "step: 685308, loss: 0.06056626886129379, data time: 0.07335011164347331\n",
      "step: 685309, loss: 0.060102105140686035, data time: 0.05576968193054199\n",
      "step: 685310, loss: 0.05999794229865074, data time: 0.04489841461181641\n",
      "step: 685311, loss: 0.06321503221988678, data time: 0.037667036056518555\n",
      "step: 685312, loss: 0.06937579810619354, data time: 0.0325049672807966\n",
      "step: 685313, loss: 0.06345107406377792, data time: 0.02871844172477722\n",
      "step: 685314, loss: 0.06768285483121872, data time: 0.025684595108032227\n",
      "step: 685315, loss: 0.06890709698200226, data time: 0.023318219184875488\n",
      "step: 685316, loss: 0.058578409254550934, data time: 0.02139550989324396\n",
      "step: 685317, loss: 0.058036427944898605, data time: 0.019788821538289387\n",
      "step: 685318, loss: 0.0624026358127594, data time: 0.018431388414823092\n",
      "step: 685319, loss: 0.06368684023618698, data time: 0.017262697219848633\n",
      "step: 685320, loss: 0.056375276297330856, data time: 0.016254615783691407\n",
      "step: 685321, loss: 0.06586113572120667, data time: 0.015370428562164307\n",
      "step: 685322, loss: 0.06401681900024414, data time: 0.01459801898283117\n",
      "step: 685323, loss: 0.0578019917011261, data time: 0.013899021678500705\n",
      "step: 685324, loss: 0.05942168086767197, data time: 0.013278772956446597\n",
      "step: 685325, loss: 0.06361367553472519, data time: 0.012718784809112548\n",
      "step: 685326, loss: 0.0607379674911499, data time: 0.012217896325247628\n",
      "step: 685327, loss: 0.060158394277095795, data time: 0.011761719530278986\n",
      "step: 685328, loss: 0.06223855912685394, data time: 0.01134212120719578\n",
      "step: 685329, loss: 0.06828708946704865, data time: 0.010956476132074991\n",
      "step: 685330, loss: 0.06616061925888062, data time: 0.010601491928100585\n",
      "step: 685331, loss: 0.062284138053655624, data time: 0.010279031900259165\n",
      "step: 685332, loss: 0.06120991334319115, data time: 0.009974532657199435\n",
      "step: 685333, loss: 0.07154510915279388, data time: 0.009693094662257604\n",
      "step: 685334, loss: 0.06851057708263397, data time: 0.00943292420485924\n",
      "step: 685335, loss: 0.06172352284193039, data time: 0.009191123644510905\n",
      "step: 685336, loss: 0.06521110981702805, data time: 0.008964046355216734\n",
      "step: 685337, loss: 0.06339788436889648, data time: 0.008764892816543579\n",
      "step: 685338, loss: 0.06340251863002777, data time: 0.008559219764940666\n",
      "step: 685339, loss: 0.06094689294695854, data time: 0.008365553968092975\n",
      "step: 685340, loss: 0.053497910499572754, data time: 0.008185018811907087\n",
      "step: 685341, loss: 0.05842671915888786, data time: 0.008010142379336886\n",
      "step: 685342, loss: 0.061636414378881454, data time: 0.007845749726166596\n",
      "step: 685343, loss: 0.05747738108038902, data time: 0.00769562470285516\n",
      "step: 685344, loss: 0.05998803302645683, data time: 0.007550306809254182\n",
      "step: 685345, loss: 0.09561887383460999, data time: 0.007414436340332032\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4124581813812256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 685346, loss: 0.07034081965684891, data time: 0.21740388870239258\n",
      "step: 685347, loss: 0.06420508027076721, data time: 0.11013758182525635\n",
      "step: 685348, loss: 0.06064146012067795, data time: 0.07447600364685059\n",
      "step: 685349, loss: 0.06499484181404114, data time: 0.05649513006210327\n",
      "step: 685350, loss: 0.06794893741607666, data time: 0.045512056350708006\n",
      "step: 685351, loss: 0.061786405742168427, data time: 0.03819835186004639\n",
      "step: 685352, loss: 0.05256550759077072, data time: 0.03299491746085031\n",
      "step: 685353, loss: 0.07382657378911972, data time: 0.029161721467971802\n",
      "step: 685354, loss: 0.0636337473988533, data time: 0.02609162860446506\n",
      "step: 685355, loss: 0.06548301875591278, data time: 0.023716259002685546\n",
      "step: 685356, loss: 0.059893399477005005, data time: 0.021793235432017933\n",
      "step: 685357, loss: 0.0641925260424614, data time: 0.02019015947977702\n",
      "step: 685358, loss: 0.0678928792476654, data time: 0.018829217323890098\n",
      "step: 685359, loss: 0.06491158902645111, data time: 0.017648969377790178\n",
      "step: 685360, loss: 0.06224464625120163, data time: 0.016630125045776368\n",
      "step: 685361, loss: 0.054995324462652206, data time: 0.015742197632789612\n",
      "step: 685362, loss: 0.05395476520061493, data time: 0.0149534309611601\n",
      "step: 685363, loss: 0.06319426000118256, data time: 0.01425125863817003\n",
      "step: 685364, loss: 0.05712127685546875, data time: 0.013622986642937911\n",
      "step: 685365, loss: 0.05845224857330322, data time: 0.013067495822906495\n",
      "step: 685366, loss: 0.05556907877326012, data time: 0.012568371636526925\n",
      "step: 685367, loss: 0.06623273342847824, data time: 0.012104998935352673\n",
      "step: 685368, loss: 0.06561698019504547, data time: 0.011681981708692469\n",
      "step: 685369, loss: 0.06171637400984764, data time: 0.011294305324554443\n",
      "step: 685370, loss: 0.05910172313451767, data time: 0.010939769744873047\n",
      "step: 685371, loss: 0.06320527195930481, data time: 0.010611873406630296\n",
      "step: 685372, loss: 0.05650978535413742, data time: 0.0103029763257062\n",
      "step: 685373, loss: 0.056532423943281174, data time: 0.010021184171949114\n",
      "step: 685374, loss: 0.05404753237962723, data time: 0.00976145678553088\n",
      "step: 685375, loss: 0.06247477978467941, data time: 0.009507457415262857\n",
      "step: 685376, loss: 0.05715339630842209, data time: 0.009270098901564074\n",
      "step: 685377, loss: 0.06460483372211456, data time: 0.00906255841255188\n",
      "step: 685378, loss: 0.06181295961141586, data time: 0.008848732168024237\n",
      "step: 685379, loss: 0.0672142505645752, data time: 0.008647504974814021\n",
      "step: 685380, loss: 0.06382189691066742, data time: 0.008457422256469727\n",
      "step: 685381, loss: 0.06206333637237549, data time: 0.008276071813371446\n",
      "step: 685382, loss: 0.06534303724765778, data time: 0.008105445552516627\n",
      "step: 685383, loss: 0.0580214187502861, data time: 0.00794794057544909\n",
      "step: 685384, loss: 0.06385160982608795, data time: 0.0078002244998247195\n",
      "step: 685385, loss: 0.08404334634542465, data time: 0.0076570272445678714\n",
      "step: 685386, loss: 0.056060366332530975, data time: 0.2252211570739746\n",
      "step: 685387, loss: 0.06727685779333115, data time: 0.11340820789337158\n",
      "step: 685388, loss: 0.05676520615816116, data time: 0.07615431149800618\n",
      "step: 685389, loss: 0.054485298693180084, data time: 0.057862043380737305\n",
      "step: 685390, loss: 0.05816791579127312, data time: 0.046564579010009766\n",
      "step: 685391, loss: 0.05628025531768799, data time: 0.03903472423553467\n",
      "step: 685392, loss: 0.06490983814001083, data time: 0.033665009907313755\n",
      "step: 685393, loss: 0.06309637427330017, data time: 0.029716074466705322\n",
      "step: 685394, loss: 0.06080690026283264, data time: 0.02657098240322537\n",
      "step: 685395, loss: 0.0644567608833313, data time: 0.02411491870880127\n",
      "step: 685396, loss: 0.0603981539607048, data time: 0.022117007862437855\n",
      "step: 685397, loss: 0.058003850281238556, data time: 0.020452141761779785\n",
      "step: 685398, loss: 0.06322480738162994, data time: 0.019048507396991435\n",
      "step: 685399, loss: 0.0637086033821106, data time: 0.017832670892987932\n",
      "step: 685400, loss: 0.05857250094413757, data time: 0.016785415013631184\n",
      "step: 685401, loss: 0.0657220333814621, data time: 0.015873461961746216\n",
      "step: 685402, loss: 0.062400203198194504, data time: 0.01506477243760053\n",
      "step: 685403, loss: 0.06275001168251038, data time: 0.01433702309926351\n",
      "step: 685404, loss: 0.06145496293902397, data time: 0.013691462968525133\n",
      "step: 685405, loss: 0.06406810134649277, data time: 0.013116908073425294\n",
      "step: 685406, loss: 0.06649850308895111, data time: 0.012596107664562407\n",
      "step: 685407, loss: 0.060681939125061035, data time: 0.01212155818939209\n",
      "step: 685408, loss: 0.06413567066192627, data time: 0.01168569274570631\n",
      "step: 685409, loss: 0.06679581105709076, data time: 0.011286209026972452\n",
      "step: 685410, loss: 0.062147002667188644, data time: 0.010915260314941406\n",
      "step: 685411, loss: 0.06449934840202332, data time: 0.010574891017033504\n",
      "step: 685412, loss: 0.05775165557861328, data time: 0.0102592044406467\n",
      "step: 685413, loss: 0.05929521471261978, data time: 0.009962993008749825\n",
      "step: 685414, loss: 0.05920853465795517, data time: 0.00969486400998872\n",
      "step: 685415, loss: 0.06384998559951782, data time: 0.009443958600362143\n",
      "step: 685416, loss: 0.0575813353061676, data time: 0.009209217563752205\n",
      "step: 685417, loss: 0.05950647592544556, data time: 0.008992962539196014\n",
      "step: 685418, loss: 0.06299638748168945, data time: 0.008778405911994703\n",
      "step: 685419, loss: 0.06178548187017441, data time: 0.008581196560579188\n",
      "step: 685420, loss: 0.05693608522415161, data time: 0.008392231804983956\n",
      "step: 685421, loss: 0.06374713778495789, data time: 0.008209897412194146\n",
      "step: 685422, loss: 0.06296789646148682, data time: 0.008039906218245223\n",
      "step: 685423, loss: 0.056352756917476654, data time: 0.007883027980202123\n",
      "step: 685424, loss: 0.0643882006406784, data time: 0.00773314940623748\n",
      "step: 685425, loss: 0.053566962480545044, data time: 0.007592058181762696\n",
      "step: 685426, loss: 0.06358405202627182, data time: 0.20787620544433594\n",
      "step: 685427, loss: 0.06712962687015533, data time: 0.10472309589385986\n",
      "step: 685428, loss: 0.05742686241865158, data time: 0.07060750325520833\n",
      "step: 685429, loss: 0.05991317331790924, data time: 0.05380433797836304\n",
      "step: 685430, loss: 0.06858260929584503, data time: 0.043316221237182616\n",
      "step: 685431, loss: 0.06745214760303497, data time: 0.036340673764546715\n",
      "step: 685432, loss: 0.05956956744194031, data time: 0.03135255404881069\n",
      "step: 685433, loss: 0.06062261760234833, data time: 0.027688711881637573\n",
      "step: 685434, loss: 0.06633510440587997, data time: 0.024753199683295354\n",
      "step: 685435, loss: 0.06510688364505768, data time: 0.022478151321411132\n",
      "step: 685436, loss: 0.06427201628684998, data time: 0.020629189231178978\n",
      "step: 685437, loss: 0.06362469494342804, data time: 0.019096672534942627\n",
      "step: 685438, loss: 0.06428998708724976, data time: 0.01779312353867751\n",
      "step: 685439, loss: 0.06629752367734909, data time: 0.01667365006038121\n",
      "step: 685440, loss: 0.0629187524318695, data time: 0.015702438354492188\n",
      "step: 685441, loss: 0.059853922575712204, data time: 0.014847919344902039\n",
      "step: 685442, loss: 0.05867544561624527, data time: 0.01409292221069336\n",
      "step: 685443, loss: 0.06086195260286331, data time: 0.013420899709065756\n",
      "step: 685444, loss: 0.06887130439281464, data time: 0.012820657930876079\n",
      "step: 685445, loss: 0.06344786286354065, data time: 0.012289512157440185\n",
      "step: 685446, loss: 0.058227356523275375, data time: 0.011812516621180944\n",
      "step: 685447, loss: 0.06471025198698044, data time: 0.011376825245943937\n",
      "step: 685448, loss: 0.06230245158076286, data time: 0.010969410771908968\n",
      "step: 685449, loss: 0.06333260983228683, data time: 0.010596821705500284\n",
      "step: 685450, loss: 0.061056237667798996, data time: 0.010271921157836914\n",
      "step: 685451, loss: 0.0573231503367424, data time: 0.009969555414639987\n",
      "step: 685452, loss: 0.05983923003077507, data time: 0.009686346407289858\n",
      "step: 685453, loss: 0.061523765325546265, data time: 0.009425657136099679\n",
      "step: 685454, loss: 0.06611470878124237, data time: 0.009186761132602033\n",
      "step: 685455, loss: 0.05533631145954132, data time: 0.008963688214619955\n",
      "step: 685456, loss: 0.06313430517911911, data time: 0.008756106899630639\n",
      "step: 685457, loss: 0.05756216496229172, data time: 0.008562915027141571\n",
      "step: 685458, loss: 0.06096264719963074, data time: 0.008365486607407078\n",
      "step: 685459, loss: 0.06130557507276535, data time: 0.008179804858039407\n",
      "step: 685460, loss: 0.06002572178840637, data time: 0.008004992348807199\n",
      "step: 685461, loss: 0.058360349386930466, data time: 0.007836745844946967\n",
      "step: 685462, loss: 0.06570256501436234, data time: 0.007679249789263751\n",
      "step: 685463, loss: 0.06384902447462082, data time: 0.007535263111716823\n",
      "step: 685464, loss: 0.06478393822908401, data time: 0.007406992790026543\n",
      "step: 685465, loss: 0.05774868652224541, data time: 0.007276266813278198\n",
      "step: 685466, loss: 0.059664733707904816, data time: 0.21097135543823242\n",
      "step: 685467, loss: 0.060969896614551544, data time: 0.1073446273803711\n",
      "step: 685468, loss: 0.05598589777946472, data time: 0.07210334142049153\n",
      "step: 685469, loss: 0.062379851937294006, data time: 0.05486094951629639\n",
      "step: 685470, loss: 0.06537005305290222, data time: 0.04416079521179199\n",
      "step: 685471, loss: 0.06698048859834671, data time: 0.03703868389129639\n",
      "step: 685472, loss: 0.05714336037635803, data time: 0.031963620867047994\n",
      "step: 685473, loss: 0.05735502392053604, data time: 0.028237104415893555\n",
      "step: 685474, loss: 0.05953409522771835, data time: 0.0252483950720893\n",
      "step: 685475, loss: 0.05994480103254318, data time: 0.022924184799194336\n",
      "step: 685476, loss: 0.057340413331985474, data time: 0.021038900722156872\n",
      "step: 685477, loss: 0.05559428781270981, data time: 0.019469698270161945\n",
      "step: 685478, loss: 0.061221785843372345, data time: 0.018138335301325872\n",
      "step: 685479, loss: 0.059601377695798874, data time: 0.0169827938079834\n",
      "step: 685480, loss: 0.06479260325431824, data time: 0.015990479787190755\n",
      "step: 685481, loss: 0.06705327332019806, data time: 0.015120580792427063\n",
      "step: 685482, loss: 0.06696823239326477, data time: 0.014365939533009249\n",
      "step: 685483, loss: 0.06389319896697998, data time: 0.01367807388305664\n",
      "step: 685484, loss: 0.057526104152202606, data time: 0.013072327563637182\n",
      "step: 685485, loss: 0.054408468306064606, data time: 0.012525629997253419\n",
      "step: 685486, loss: 0.06442072242498398, data time: 0.01203095345270066\n",
      "step: 685487, loss: 0.06491618603467941, data time: 0.011581041596152565\n",
      "step: 685488, loss: 0.05981547757983208, data time: 0.01116285116776176\n",
      "step: 685489, loss: 0.05623211711645126, data time: 0.010785301526387533\n",
      "step: 685490, loss: 0.0706329494714737, data time: 0.010442237854003906\n",
      "step: 685491, loss: 0.06270143389701843, data time: 0.010123638006357046\n",
      "step: 685492, loss: 0.06056613475084305, data time: 0.009820611388595015\n",
      "step: 685493, loss: 0.05830177664756775, data time: 0.009541664804731096\n",
      "step: 685494, loss: 0.06033072620630264, data time: 0.009286231008069268\n",
      "step: 685495, loss: 0.05491712689399719, data time: 0.009047158559163411\n",
      "step: 685496, loss: 0.0648370087146759, data time: 0.008824048503752678\n",
      "step: 685497, loss: 0.058261092752218246, data time: 0.008617311716079712\n",
      "step: 685498, loss: 0.06386959552764893, data time: 0.008416125268647165\n",
      "step: 685499, loss: 0.0631818175315857, data time: 0.008224830907933852\n",
      "step: 685500, loss: 0.06568437814712524, data time: 0.008048990794590542\n",
      "step: 685501, loss: 0.05981282889842987, data time: 0.007875137858920626\n",
      "step: 685502, loss: 0.06059864163398743, data time: 0.0077139622456318625\n",
      "step: 685503, loss: 0.059511613100767136, data time: 0.007564074114749306\n",
      "step: 685504, loss: 0.06214985251426697, data time: 0.007421970367431641\n",
      "step: 685505, loss: 0.03990987688302994, data time: 0.007287603616714477\n",
      "step: 685506, loss: 0.05867975577712059, data time: 0.20804262161254883\n",
      "step: 685507, loss: 0.06011125445365906, data time: 0.10516631603240967\n",
      "step: 685508, loss: 0.06148528307676315, data time: 0.07114330927530925\n",
      "step: 685509, loss: 0.06272855401039124, data time: 0.05414783954620361\n",
      "step: 685510, loss: 0.06254395097494125, data time: 0.04358968734741211\n",
      "step: 685511, loss: 0.06332702934741974, data time: 0.036588708559672035\n",
      "step: 685512, loss: 0.062200259417295456, data time: 0.03157472610473633\n",
      "step: 685513, loss: 0.06843847036361694, data time: 0.027904361486434937\n",
      "step: 685514, loss: 0.06451990455389023, data time: 0.02495090166727702\n",
      "step: 685515, loss: 0.06498192250728607, data time: 0.022677969932556153\n",
      "step: 685516, loss: 0.056062184274196625, data time: 0.020815155722878197\n",
      "step: 685517, loss: 0.062159083783626556, data time: 0.01925977071126302\n",
      "step: 685518, loss: 0.057735420763492584, data time: 0.017940777998704176\n",
      "step: 685519, loss: 0.05930357798933983, data time: 0.016798990113394602\n",
      "step: 685520, loss: 0.06451781839132309, data time: 0.015819438298543296\n",
      "step: 685521, loss: 0.06033451110124588, data time: 0.01496325433254242\n",
      "step: 685522, loss: 0.06467505544424057, data time: 0.014204894795137294\n",
      "step: 685523, loss: 0.05948423966765404, data time: 0.013526545630560981\n",
      "step: 685524, loss: 0.061224859207868576, data time: 0.012926816940307617\n",
      "step: 685525, loss: 0.054589785635471344, data time: 0.01239156723022461\n",
      "step: 685526, loss: 0.06429656594991684, data time: 0.011908156531197684\n",
      "step: 685527, loss: 0.0669107660651207, data time: 0.011467359282753685\n",
      "step: 685528, loss: 0.06650647521018982, data time: 0.011054349982220194\n",
      "step: 685529, loss: 0.0637572631239891, data time: 0.010681827863057455\n",
      "step: 685530, loss: 0.06051059067249298, data time: 0.010337953567504882\n",
      "step: 685531, loss: 0.06326683610677719, data time: 0.010017908536470853\n",
      "step: 685532, loss: 0.0680655986070633, data time: 0.00971877133404767\n",
      "step: 685533, loss: 0.06426742672920227, data time: 0.009445726871490479\n",
      "step: 685534, loss: 0.05767364427447319, data time: 0.00919476870832772\n",
      "step: 685535, loss: 0.061909019947052, data time: 0.008958848317464192\n",
      "step: 685536, loss: 0.06585654616355896, data time: 0.008740255909581338\n",
      "step: 685537, loss: 0.05854504555463791, data time: 0.008536294102668762\n",
      "step: 685538, loss: 0.05952684208750725, data time: 0.00833995414502693\n",
      "step: 685539, loss: 0.06260450929403305, data time: 0.00815593495088465\n",
      "step: 685540, loss: 0.06166648864746094, data time: 0.007977090563092913\n",
      "step: 685541, loss: 0.06429620832204819, data time: 0.007808519734276665\n",
      "step: 685542, loss: 0.05638517066836357, data time: 0.007651812321430928\n",
      "step: 685543, loss: 0.05910605192184448, data time: 0.007503302473770945\n",
      "step: 685544, loss: 0.060307037085294724, data time: 0.007362017264732948\n",
      "step: 685545, loss: 0.0651417076587677, data time: 0.007228457927703857\n",
      "step: 685546, loss: 0.05605431646108627, data time: 0.21097826957702637\n",
      "step: 685547, loss: 0.058677710592746735, data time: 0.10625994205474854\n",
      "step: 685548, loss: 0.059970658272504807, data time: 0.07137791315714519\n",
      "step: 685549, loss: 0.05688050389289856, data time: 0.054398179054260254\n",
      "step: 685550, loss: 0.06422219425439835, data time: 0.04378819465637207\n",
      "step: 685551, loss: 0.059344153851270676, data time: 0.036736130714416504\n",
      "step: 685552, loss: 0.059770554304122925, data time: 0.031697443553379605\n",
      "step: 685553, loss: 0.05881718918681145, data time: 0.02803856134414673\n",
      "step: 685554, loss: 0.07078933715820312, data time: 0.025095091925726995\n",
      "step: 685555, loss: 0.0668078362941742, data time: 0.02282242774963379\n",
      "step: 685556, loss: 0.05770617723464966, data time: 0.020977605472911488\n",
      "step: 685557, loss: 0.05955404043197632, data time: 0.019446969032287598\n",
      "step: 685558, loss: 0.06590995192527771, data time: 0.01814875235924354\n",
      "step: 685559, loss: 0.06349554657936096, data time: 0.01701690469469343\n",
      "step: 685560, loss: 0.05985298007726669, data time: 0.016041978200276693\n",
      "step: 685561, loss: 0.06671285629272461, data time: 0.015194788575172424\n",
      "step: 685562, loss: 0.06166669726371765, data time: 0.01443941452923943\n",
      "step: 685563, loss: 0.06756675988435745, data time: 0.013764129744635688\n",
      "step: 685564, loss: 0.06879107654094696, data time: 0.013162173722919664\n",
      "step: 685565, loss: 0.0627589002251625, data time: 0.012629497051239013\n",
      "step: 685566, loss: 0.06968911737203598, data time: 0.012151014237176804\n",
      "step: 685567, loss: 0.06328818202018738, data time: 0.011713851581920277\n",
      "step: 685568, loss: 0.06031356379389763, data time: 0.011307529781175696\n",
      "step: 685569, loss: 0.05647045373916626, data time: 0.010938604672749838\n",
      "step: 685570, loss: 0.06510281562805176, data time: 0.010601158142089845\n",
      "step: 685571, loss: 0.06388066709041595, data time: 0.010286542085500864\n",
      "step: 685572, loss: 0.058401137590408325, data time: 0.009993182288275825\n",
      "step: 685573, loss: 0.06020834296941757, data time: 0.009723177977970668\n",
      "step: 685574, loss: 0.06331777572631836, data time: 0.009476505476852944\n",
      "step: 685575, loss: 0.059203945100307465, data time: 0.00924371083577474\n",
      "step: 685576, loss: 0.06425610929727554, data time: 0.00902691195088048\n",
      "step: 685577, loss: 0.06680964678525925, data time: 0.008824162185192108\n",
      "step: 685578, loss: 0.05705171078443527, data time: 0.008617841836177942\n",
      "step: 685579, loss: 0.0595739483833313, data time: 0.008426883641411276\n",
      "step: 685580, loss: 0.06485171616077423, data time: 0.008245754241943359\n",
      "step: 685581, loss: 0.056074053049087524, data time: 0.008070462279849581\n",
      "step: 685582, loss: 0.06294594705104828, data time: 0.007907867431640625\n",
      "step: 685583, loss: 0.058974843472242355, data time: 0.007757130422090229\n",
      "step: 685584, loss: 0.06858783960342407, data time: 0.0076141968751565\n",
      "step: 685585, loss: 0.03433069586753845, data time: 0.007478851079940796\n",
      "step: 685586, loss: 0.05848211795091629, data time: 0.222564697265625\n",
      "step: 685587, loss: 0.06080543249845505, data time: 0.11206591129302979\n",
      "step: 685588, loss: 0.06422977149486542, data time: 0.07598002751668294\n",
      "step: 685589, loss: 0.05955621227622032, data time: 0.0576704740524292\n",
      "step: 685590, loss: 0.06394489854574203, data time: 0.04640851020812988\n",
      "step: 685591, loss: 0.05732686445116997, data time: 0.03892529010772705\n",
      "step: 685592, loss: 0.0700160562992096, data time: 0.03358040537152972\n",
      "step: 685593, loss: 0.057293668389320374, data time: 0.029640614986419678\n",
      "step: 685594, loss: 0.0575832836329937, data time: 0.02650618553161621\n",
      "step: 685595, loss: 0.06135844439268112, data time: 0.024065852165222168\n",
      "step: 685596, loss: 0.058783821761608124, data time: 0.02208189530806108\n",
      "step: 685597, loss: 0.06546226888895035, data time: 0.020438929398854572\n",
      "step: 685598, loss: 0.06209806725382805, data time: 0.019032808450552132\n",
      "step: 685599, loss: 0.06434335559606552, data time: 0.01781971114022391\n",
      "step: 685600, loss: 0.061042144894599915, data time: 0.016776275634765626\n",
      "step: 685601, loss: 0.06213584542274475, data time: 0.015854820609092712\n",
      "step: 685602, loss: 0.056622158735990524, data time: 0.015046119689941406\n",
      "step: 685603, loss: 0.06297557801008224, data time: 0.014319380124409994\n",
      "step: 685604, loss: 0.058762893080711365, data time: 0.01367991848995811\n",
      "step: 685605, loss: 0.059358831495046616, data time: 0.013108408451080323\n",
      "step: 685606, loss: 0.06641034781932831, data time: 0.012587592715308779\n",
      "step: 685607, loss: 0.06107483431696892, data time: 0.01211757009679621\n",
      "step: 685608, loss: 0.06458660960197449, data time: 0.01167607307434082\n",
      "step: 685609, loss: 0.05919948220252991, data time: 0.011280794938405355\n",
      "step: 685610, loss: 0.06822469830513, data time: 0.01091228485107422\n",
      "step: 685611, loss: 0.06151285767555237, data time: 0.010578733224135179\n",
      "step: 685612, loss: 0.06019260734319687, data time: 0.01026179172374584\n",
      "step: 685613, loss: 0.06259322166442871, data time: 0.009968314852033342\n",
      "step: 685614, loss: 0.06184899061918259, data time: 0.009701161549009126\n",
      "step: 685615, loss: 0.06030962988734245, data time: 0.009448806444803873\n",
      "step: 685616, loss: 0.06731919944286346, data time: 0.00921364753477035\n",
      "step: 685617, loss: 0.05797076225280762, data time: 0.008997887372970581\n",
      "step: 685618, loss: 0.05958070605993271, data time: 0.008788007678407612\n",
      "step: 685619, loss: 0.06119523569941521, data time: 0.008586378658519071\n",
      "step: 685620, loss: 0.062161169946193695, data time: 0.00839595113481794\n",
      "step: 685621, loss: 0.061354830861091614, data time: 0.008213301499684652\n",
      "step: 685622, loss: 0.06253259629011154, data time: 0.008047213425507417\n",
      "step: 685623, loss: 0.06645042449235916, data time: 0.00788851160752146\n",
      "step: 685624, loss: 0.06216727942228317, data time: 0.007737508186927209\n",
      "step: 685625, loss: 0.04439164325594902, data time: 0.007594197988510132\n",
      "step: 685626, loss: 0.060728829354047775, data time: 0.20132899284362793\n",
      "step: 685627, loss: 0.05627277493476868, data time: 0.10203874111175537\n",
      "step: 685628, loss: 0.0641363337635994, data time: 0.06895915667215984\n",
      "step: 685629, loss: 0.06163923442363739, data time: 0.05249541997909546\n",
      "step: 685630, loss: 0.07027401030063629, data time: 0.04225964546203613\n",
      "step: 685631, loss: 0.061619944870471954, data time: 0.03548971811930338\n",
      "step: 685632, loss: 0.06315553188323975, data time: 0.03062966891697475\n",
      "step: 685633, loss: 0.06040932983160019, data time: 0.02705952525138855\n",
      "step: 685634, loss: 0.05995966121554375, data time: 0.02419998910692003\n",
      "step: 685635, loss: 0.06047412008047104, data time: 0.02198188304901123\n",
      "step: 685636, loss: 0.060608647763729095, data time: 0.02018484202298251\n",
      "step: 685637, loss: 0.059673428535461426, data time: 0.01868647336959839\n",
      "step: 685638, loss: 0.05834924429655075, data time: 0.01741957664489746\n",
      "step: 685639, loss: 0.05835345387458801, data time: 0.01631374018532889\n",
      "step: 685640, loss: 0.060298752039670944, data time: 0.015368890762329102\n",
      "step: 685641, loss: 0.058688580989837646, data time: 0.014538690447807312\n",
      "step: 685642, loss: 0.056247320026159286, data time: 0.013802584479836857\n",
      "step: 685643, loss: 0.061039458960294724, data time: 0.013144559330410428\n",
      "step: 685644, loss: 0.06464263051748276, data time: 0.012561045194927015\n",
      "step: 685645, loss: 0.0631556436419487, data time: 0.012044668197631836\n",
      "step: 685646, loss: 0.056054212152957916, data time: 0.011576913651965913\n",
      "step: 685647, loss: 0.06453469395637512, data time: 0.011147249828685413\n",
      "step: 685648, loss: 0.05676376819610596, data time: 0.010748313820880392\n",
      "step: 685649, loss: 0.06298630684614182, data time: 0.010388622681299845\n",
      "step: 685650, loss: 0.06798205524682999, data time: 0.010053834915161132\n",
      "step: 685651, loss: 0.060904424637556076, data time: 0.00974417649782621\n",
      "step: 685652, loss: 0.05724626034498215, data time: 0.009457199661820024\n",
      "step: 685653, loss: 0.06773006916046143, data time: 0.009191461971827916\n",
      "step: 685654, loss: 0.05644975230097771, data time: 0.008950767845943057\n",
      "step: 685655, loss: 0.06190896034240723, data time: 0.00872337023417155\n",
      "step: 685656, loss: 0.05963820964097977, data time: 0.008512127783990676\n",
      "step: 685657, loss: 0.06399890780448914, data time: 0.008314408361911774\n",
      "step: 685658, loss: 0.06157489866018295, data time: 0.008122725920243696\n",
      "step: 685659, loss: 0.06056893989443779, data time: 0.007940124062930836\n",
      "step: 685660, loss: 0.060611434280872345, data time: 0.007767629623413086\n",
      "step: 685661, loss: 0.062237560749053955, data time: 0.007602532704671224\n",
      "step: 685662, loss: 0.06293746083974838, data time: 0.007448776348217114\n",
      "step: 685663, loss: 0.0585259310901165, data time: 0.007306349904913651\n",
      "step: 685664, loss: 0.05400116741657257, data time: 0.0071713007413423974\n",
      "step: 685665, loss: 0.06385542452335358, data time: 0.007042789459228515\n",
      "step: 685666, loss: 0.06045497581362724, data time: 0.22846412658691406\n",
      "step: 685667, loss: 0.0625632256269455, data time: 0.11500084400177002\n",
      "step: 685668, loss: 0.06210532784461975, data time: 0.07757528622945149\n",
      "step: 685669, loss: 0.07053567469120026, data time: 0.05892711877822876\n",
      "step: 685670, loss: 0.06159697100520134, data time: 0.04742164611816406\n",
      "step: 685671, loss: 0.06213746964931488, data time: 0.0397493839263916\n",
      "step: 685672, loss: 0.06182495504617691, data time: 0.034289394106183736\n",
      "step: 685673, loss: 0.0571819469332695, data time: 0.030256927013397217\n",
      "step: 685674, loss: 0.0599999725818634, data time: 0.027038282818264432\n",
      "step: 685675, loss: 0.06764020025730133, data time: 0.02453434467315674\n",
      "step: 685676, loss: 0.06819602847099304, data time: 0.022494294426657936\n",
      "step: 685677, loss: 0.06754680722951889, data time: 0.02079719305038452\n",
      "step: 685678, loss: 0.0623721107840538, data time: 0.019361275892991286\n",
      "step: 685679, loss: 0.055092256516218185, data time: 0.018124171665736606\n",
      "step: 685680, loss: 0.06046750769019127, data time: 0.017055447896321616\n",
      "step: 685681, loss: 0.06228707358241081, data time: 0.01612231135368347\n",
      "step: 685682, loss: 0.05863369628787041, data time: 0.01531679490033318\n",
      "step: 685683, loss: 0.0618773028254509, data time: 0.014592104487948947\n",
      "step: 685684, loss: 0.06735454499721527, data time: 0.013942580474050422\n",
      "step: 685685, loss: 0.06409844756126404, data time: 0.013373315334320068\n",
      "step: 685686, loss: 0.06551217287778854, data time: 0.012853577023460752\n",
      "step: 685687, loss: 0.062294505536556244, data time: 0.012384761463512074\n",
      "step: 685688, loss: 0.06418411433696747, data time: 0.01195116665052331\n",
      "step: 685689, loss: 0.0663185864686966, data time: 0.011551042397816976\n",
      "step: 685690, loss: 0.06468591839075089, data time: 0.011189432144165038\n",
      "step: 685691, loss: 0.060810528695583344, data time: 0.010851511588463416\n",
      "step: 685692, loss: 0.06387683749198914, data time: 0.010533165048669886\n",
      "step: 685693, loss: 0.06267152726650238, data time: 0.010243960789271764\n",
      "step: 685694, loss: 0.06582401692867279, data time: 0.009977348919572502\n",
      "step: 685695, loss: 0.06156197190284729, data time: 0.00972903569539388\n",
      "step: 685696, loss: 0.05659594386816025, data time: 0.009496734988304877\n",
      "step: 685697, loss: 0.05893317982554436, data time: 0.009282052516937256\n",
      "step: 685698, loss: 0.06277488172054291, data time: 0.009063388362075344\n",
      "step: 685699, loss: 0.06899002194404602, data time: 0.008860258495106417\n",
      "step: 685700, loss: 0.0631718561053276, data time: 0.008665541240147182\n",
      "step: 685701, loss: 0.058634933084249496, data time: 0.008479250801934136\n",
      "step: 685702, loss: 0.05986512824892998, data time: 0.008303564948004645\n",
      "step: 685703, loss: 0.058054279536008835, data time: 0.00814040083634226\n",
      "step: 685704, loss: 0.05393671244382858, data time: 0.007986025932507638\n",
      "step: 685705, loss: 0.08986672013998032, data time: 0.00783931016921997\n",
      "step: 685706, loss: 0.06266647577285767, data time: 0.20206594467163086\n",
      "step: 685707, loss: 0.05898711085319519, data time: 0.10180819034576416\n",
      "step: 685708, loss: 0.06597781181335449, data time: 0.06902098655700684\n",
      "step: 685709, loss: 0.06556357443332672, data time: 0.05231153964996338\n",
      "step: 685710, loss: 0.059792257845401764, data time: 0.04213213920593262\n",
      "step: 685711, loss: 0.056539714336395264, data time: 0.03537217775980631\n",
      "step: 685712, loss: 0.06817713379859924, data time: 0.030679259981427873\n",
      "step: 685713, loss: 0.06675300002098083, data time: 0.027010440826416016\n",
      "step: 685714, loss: 0.05800606310367584, data time: 0.02415911356608073\n",
      "step: 685715, loss: 0.0684778243303299, data time: 0.021978116035461424\n",
      "step: 685716, loss: 0.05835587531328201, data time: 0.02017454667524858\n",
      "step: 685717, loss: 0.06318104267120361, data time: 0.018670777479807537\n",
      "step: 685718, loss: 0.06084929406642914, data time: 0.017395973205566406\n",
      "step: 685719, loss: 0.06207394599914551, data time: 0.016312479972839355\n",
      "step: 685720, loss: 0.054195843636989594, data time: 0.015367428461710611\n",
      "step: 685721, loss: 0.0634412094950676, data time: 0.014538779854774475\n",
      "step: 685722, loss: 0.06372563540935516, data time: 0.013801883248721851\n",
      "step: 685723, loss: 0.063236765563488, data time: 0.013144784503512912\n",
      "step: 685724, loss: 0.06116226315498352, data time: 0.012560480519344932\n",
      "step: 685725, loss: 0.06136402487754822, data time: 0.012040328979492188\n",
      "step: 685726, loss: 0.05934786796569824, data time: 0.011569386436825707\n",
      "step: 685727, loss: 0.06026814505457878, data time: 0.011137680573896929\n",
      "step: 685728, loss: 0.05380667746067047, data time: 0.010743711305701214\n",
      "step: 685729, loss: 0.06142566725611687, data time: 0.01038399338722229\n",
      "step: 685730, loss: 0.06186332553625107, data time: 0.010052766799926758\n",
      "step: 685731, loss: 0.06379611790180206, data time: 0.009743442902198205\n",
      "step: 685732, loss: 0.06141841411590576, data time: 0.0094554689195421\n",
      "step: 685733, loss: 0.062464915215969086, data time: 0.009189120360783168\n",
      "step: 685734, loss: 0.057455144822597504, data time: 0.008944675840180496\n",
      "step: 685735, loss: 0.0683695450425148, data time: 0.008716789881388347\n",
      "step: 685736, loss: 0.06823407858610153, data time: 0.008506851811562815\n",
      "step: 685737, loss: 0.06536777317523956, data time: 0.008315786719322205\n",
      "step: 685738, loss: 0.06289906054735184, data time: 0.008125695315274324\n",
      "step: 685739, loss: 0.06628783047199249, data time: 0.007943896686329562\n",
      "step: 685740, loss: 0.05322265625, data time: 0.007770803996494838\n",
      "step: 685741, loss: 0.055895041674375534, data time: 0.0076070427894592285\n",
      "step: 685742, loss: 0.05962521955370903, data time: 0.007455767811955632\n",
      "step: 685743, loss: 0.0628180205821991, data time: 0.007311858628925525\n",
      "step: 685744, loss: 0.0641435906291008, data time: 0.007175745108188727\n",
      "step: 685745, loss: 0.05404457449913025, data time: 0.0070459902286529544\n",
      "step: 685746, loss: 0.06513014435768127, data time: 0.21282505989074707\n",
      "step: 685747, loss: 0.06324172019958496, data time: 0.10715067386627197\n",
      "step: 685748, loss: 0.06501355767250061, data time: 0.07260600725809734\n",
      "step: 685749, loss: 0.06481656432151794, data time: 0.0551108717918396\n",
      "step: 685750, loss: 0.06938008964061737, data time: 0.04437088966369629\n",
      "step: 685751, loss: 0.0683693140745163, data time: 0.03724209467569987\n",
      "step: 685752, loss: 0.061931025236845016, data time: 0.032134294509887695\n",
      "step: 685753, loss: 0.057812899351119995, data time: 0.028377026319503784\n",
      "step: 685754, loss: 0.05663887783885002, data time: 0.025369273291693792\n",
      "step: 685755, loss: 0.06382519006729126, data time: 0.02303471565246582\n",
      "step: 685756, loss: 0.05887603759765625, data time: 0.02113760601390492\n",
      "step: 685757, loss: 0.060443516820669174, data time: 0.019563059012095135\n",
      "step: 685758, loss: 0.05648171529173851, data time: 0.018218150505652793\n",
      "step: 685759, loss: 0.06519846618175507, data time: 0.017098631177629744\n",
      "step: 685760, loss: 0.06138933449983597, data time: 0.01612394650777181\n",
      "step: 685761, loss: 0.06347937881946564, data time: 0.015270829200744629\n",
      "step: 685762, loss: 0.056538455188274384, data time: 0.014516634099623737\n",
      "step: 685763, loss: 0.05546761676669121, data time: 0.013826423221164279\n",
      "step: 685764, loss: 0.062401752918958664, data time: 0.013206619965402703\n",
      "step: 685765, loss: 0.057833604514598846, data time: 0.01266152858734131\n",
      "step: 685766, loss: 0.05923261493444443, data time: 0.012162117731003534\n",
      "step: 685767, loss: 0.0642080008983612, data time: 0.011705181815407494\n",
      "step: 685768, loss: 0.06069245934486389, data time: 0.011285450147545856\n",
      "step: 685769, loss: 0.060843728482723236, data time: 0.010904004176457724\n",
      "step: 685770, loss: 0.06331604719161987, data time: 0.010552482604980469\n",
      "step: 685771, loss: 0.06628729403018951, data time: 0.010225286850562463\n",
      "step: 685772, loss: 0.060294173657894135, data time: 0.009925612696894893\n",
      "step: 685773, loss: 0.06587044894695282, data time: 0.009645061833517892\n",
      "step: 685774, loss: 0.06441197544336319, data time: 0.009388315266576307\n",
      "step: 685775, loss: 0.06048006936907768, data time: 0.009149805704752604\n",
      "step: 685776, loss: 0.05707889795303345, data time: 0.008922130830826299\n",
      "step: 685777, loss: 0.066227488219738, data time: 0.008713029325008392\n",
      "step: 685778, loss: 0.06253045797348022, data time: 0.008509989940758907\n",
      "step: 685779, loss: 0.06102561205625534, data time: 0.00831901325899012\n",
      "step: 685780, loss: 0.0545940101146698, data time: 0.008137845993041992\n",
      "step: 685781, loss: 0.06291429698467255, data time: 0.00796304808722602\n",
      "step: 685782, loss: 0.06799983978271484, data time: 0.007801062351948506\n",
      "step: 685783, loss: 0.0590779148042202, data time: 0.0076515298140676395\n",
      "step: 685784, loss: 0.06800393760204315, data time: 0.007506547830043695\n",
      "step: 685785, loss: 0.06691392511129379, data time: 0.007370150089263916\n",
      "step: 685786, loss: 0.05880354344844818, data time: 0.21464014053344727\n",
      "step: 685787, loss: 0.058971889317035675, data time: 0.10805225372314453\n",
      "step: 685788, loss: 0.060857053846120834, data time: 0.072539488474528\n",
      "step: 685789, loss: 0.05819254741072655, data time: 0.055177927017211914\n",
      "step: 685790, loss: 0.06125050410628319, data time: 0.04441866874694824\n",
      "step: 685791, loss: 0.06344278901815414, data time: 0.0372535785039266\n",
      "step: 685792, loss: 0.06849540770053864, data time: 0.032134022031511576\n",
      "step: 685793, loss: 0.062147073447704315, data time: 0.028386950492858887\n",
      "step: 685794, loss: 0.060463838279247284, data time: 0.025376372867160372\n",
      "step: 685795, loss: 0.06009037420153618, data time: 0.023049545288085938\n",
      "step: 685796, loss: 0.0651579424738884, data time: 0.021156246011907406\n",
      "step: 685797, loss: 0.06119300425052643, data time: 0.019575874010721844\n",
      "step: 685798, loss: 0.06532569974660873, data time: 0.01823867284334623\n",
      "step: 685799, loss: 0.058781784027814865, data time: 0.017081550189426968\n",
      "step: 685800, loss: 0.05421983823180199, data time: 0.01608309745788574\n",
      "step: 685801, loss: 0.0603007897734642, data time: 0.01520530879497528\n",
      "step: 685802, loss: 0.06162385269999504, data time: 0.01443773157456342\n",
      "step: 685803, loss: 0.059984173625707626, data time: 0.013743837674458822\n",
      "step: 685804, loss: 0.059906601905822754, data time: 0.013131593403063323\n",
      "step: 685805, loss: 0.06481488794088364, data time: 0.012587380409240723\n",
      "step: 685806, loss: 0.05635339021682739, data time: 0.012089207058861143\n",
      "step: 685807, loss: 0.0631239041686058, data time: 0.011635953729802912\n",
      "step: 685808, loss: 0.06762667745351791, data time: 0.011219377103059189\n",
      "step: 685809, loss: 0.05894196778535843, data time: 0.01083997885386149\n",
      "step: 685810, loss: 0.05944046378135681, data time: 0.01049210548400879\n",
      "step: 685811, loss: 0.0628419890999794, data time: 0.010171165833106408\n",
      "step: 685812, loss: 0.06490863859653473, data time: 0.009867403242323134\n",
      "step: 685813, loss: 0.05997944995760918, data time: 0.00958738156727382\n",
      "step: 685814, loss: 0.05942890793085098, data time: 0.009332492433745286\n",
      "step: 685815, loss: 0.061277106404304504, data time: 0.009091448783874512\n",
      "step: 685816, loss: 0.06478489935398102, data time: 0.008868448195918914\n",
      "step: 685817, loss: 0.06603772938251495, data time: 0.008662834763526917\n",
      "step: 685818, loss: 0.0614812970161438, data time: 0.008460341077862364\n",
      "step: 685819, loss: 0.0576988160610199, data time: 0.008269688662360697\n",
      "step: 685820, loss: 0.062222130596637726, data time: 0.008090346200125558\n",
      "step: 685821, loss: 0.06211468204855919, data time: 0.007918649249606662\n",
      "step: 685822, loss: 0.06262006610631943, data time: 0.007755350422214817\n",
      "step: 685823, loss: 0.06020359322428703, data time: 0.007603883743286133\n",
      "step: 685824, loss: 0.06252764910459518, data time: 0.007460691989996495\n",
      "step: 685825, loss: 0.07546082139015198, data time: 0.0073238790035247804\n",
      "step: 685826, loss: 0.06260398030281067, data time: 0.2140488624572754\n",
      "step: 685827, loss: 0.06165570020675659, data time: 0.1078190803527832\n",
      "step: 685828, loss: 0.062359776347875595, data time: 0.07277719179789226\n",
      "step: 685829, loss: 0.05942143127322197, data time: 0.055439114570617676\n",
      "step: 685830, loss: 0.0684279203414917, data time: 0.04463028907775879\n",
      "step: 685831, loss: 0.0639720931649208, data time: 0.03743894894917806\n",
      "step: 685832, loss: 0.06389504671096802, data time: 0.03230994088309152\n",
      "step: 685833, loss: 0.06169261783361435, data time: 0.028539448976516724\n",
      "step: 685834, loss: 0.06207443028688431, data time: 0.025548564063178167\n",
      "step: 685835, loss: 0.06746034324169159, data time: 0.02323298454284668\n",
      "step: 685836, loss: 0.059936683624982834, data time: 0.02134847640991211\n",
      "step: 685837, loss: 0.05629923939704895, data time: 0.0197792649269104\n",
      "step: 685838, loss: 0.06179484724998474, data time: 0.01845075533940242\n",
      "step: 685839, loss: 0.06265544891357422, data time: 0.017309461321149553\n",
      "step: 685840, loss: 0.0655282661318779, data time: 0.01632350285847982\n",
      "step: 685841, loss: 0.06569235026836395, data time: 0.01546032726764679\n",
      "step: 685842, loss: 0.06286109983921051, data time: 0.014694564482745002\n",
      "step: 685843, loss: 0.06435199826955795, data time: 0.013992084397210015\n",
      "step: 685844, loss: 0.06263791769742966, data time: 0.01336242023267244\n",
      "step: 685845, loss: 0.06223147362470627, data time: 0.012802803516387939\n",
      "step: 685846, loss: 0.06202242895960808, data time: 0.012303965432303292\n",
      "step: 685847, loss: 0.0737379714846611, data time: 0.011844006451693449\n",
      "step: 685848, loss: 0.06323742121458054, data time: 0.011420716410097868\n",
      "step: 685849, loss: 0.057662397623062134, data time: 0.011031885941823324\n",
      "step: 685850, loss: 0.06213381513953209, data time: 0.0106756591796875\n",
      "step: 685851, loss: 0.06674829125404358, data time: 0.010342744680551382\n",
      "step: 685852, loss: 0.06422029435634613, data time: 0.010032150480482314\n",
      "step: 685853, loss: 0.05692410096526146, data time: 0.009749957493373327\n",
      "step: 685854, loss: 0.06375031173229218, data time: 0.009487966011310446\n",
      "step: 685855, loss: 0.06675088405609131, data time: 0.009243210156758627\n",
      "step: 685856, loss: 0.06305783987045288, data time: 0.009015044858378748\n",
      "step: 685857, loss: 0.06717882305383682, data time: 0.008803755044937134\n",
      "step: 685858, loss: 0.06363633275032043, data time: 0.008597850799560547\n",
      "step: 685859, loss: 0.06183391809463501, data time: 0.008403504596037023\n",
      "step: 685860, loss: 0.05986738204956055, data time: 0.008218601771763394\n",
      "step: 685861, loss: 0.05955295264720917, data time: 0.00804158714082506\n",
      "step: 685862, loss: 0.060875654220581055, data time: 0.00787783313441921\n",
      "step: 685863, loss: 0.06324851512908936, data time: 0.007725119590759277\n",
      "step: 685864, loss: 0.06192551180720329, data time: 0.007578904812152569\n",
      "step: 685865, loss: 0.08217556029558182, data time: 0.007439213991165161\n",
      "step: 685866, loss: 0.0630597174167633, data time: 0.22452139854431152\n",
      "step: 685867, loss: 0.06257785856723785, data time: 0.1130138635635376\n",
      "step: 685868, loss: 0.06637158989906311, data time: 0.07639877001444499\n",
      "step: 685869, loss: 0.07022812962532043, data time: 0.058225154876708984\n",
      "step: 685870, loss: 0.06270015239715576, data time: 0.04691157341003418\n",
      "step: 685871, loss: 0.06197644770145416, data time: 0.03939104080200195\n",
      "step: 685872, loss: 0.060890089720487595, data time: 0.03400342805044992\n",
      "step: 685873, loss: 0.060189973562955856, data time: 0.03005501627922058\n",
      "step: 685874, loss: 0.0668082982301712, data time: 0.026902967029147677\n",
      "step: 685875, loss: 0.06430761516094208, data time: 0.024451947212219237\n",
      "step: 685876, loss: 0.06117204949259758, data time: 0.022458163174715908\n",
      "step: 685877, loss: 0.06311997026205063, data time: 0.02079927921295166\n",
      "step: 685878, loss: 0.058002036064863205, data time: 0.019391738451444186\n",
      "step: 685879, loss: 0.0644577145576477, data time: 0.018178037234715054\n",
      "step: 685880, loss: 0.06002933904528618, data time: 0.017135254542032876\n",
      "step: 685881, loss: 0.06257828325033188, data time: 0.016222849488258362\n",
      "step: 685882, loss: 0.05804302915930748, data time: 0.015409259235157687\n",
      "step: 685883, loss: 0.06058472394943237, data time: 0.014684226777818467\n",
      "step: 685884, loss: 0.06021355092525482, data time: 0.014038914128353721\n",
      "step: 685885, loss: 0.061963342130184174, data time: 0.013463783264160156\n",
      "step: 685886, loss: 0.05505559593439102, data time: 0.012940679277692522\n",
      "step: 685887, loss: 0.06287036091089249, data time: 0.012466820803555574\n",
      "step: 685888, loss: 0.06693259626626968, data time: 0.012032249699468199\n",
      "step: 685889, loss: 0.06515699625015259, data time: 0.011634121338526407\n",
      "step: 685890, loss: 0.0626845434308052, data time: 0.011265783309936524\n",
      "step: 685891, loss: 0.06513403356075287, data time: 0.010925549727219801\n",
      "step: 685892, loss: 0.06504851579666138, data time: 0.010609176423814561\n",
      "step: 685893, loss: 0.0629035159945488, data time: 0.010315392698560442\n",
      "step: 685894, loss: 0.05810588225722313, data time: 0.010048183901556608\n",
      "step: 685895, loss: 0.065163753926754, data time: 0.009796047210693359\n",
      "step: 685896, loss: 0.05642774701118469, data time: 0.009560608094738375\n",
      "step: 685897, loss: 0.06791380792856216, data time: 0.00934351235628128\n",
      "step: 685898, loss: 0.05249905586242676, data time: 0.00912567340966427\n",
      "step: 685899, loss: 0.0630171000957489, data time: 0.008918067988227396\n",
      "step: 685900, loss: 0.0664503201842308, data time: 0.008722461972917829\n",
      "step: 685901, loss: 0.058014124631881714, data time: 0.008537915017869737\n",
      "step: 685902, loss: 0.0673191249370575, data time: 0.008362576768204972\n",
      "step: 685903, loss: 0.055163245648145676, data time: 0.008198612614681846\n",
      "step: 685904, loss: 0.06914519518613815, data time: 0.00804453629713792\n",
      "step: 685905, loss: 0.05633537098765373, data time: 0.007898056507110595\n",
      "step: 685906, loss: 0.06072602793574333, data time: 0.21649718284606934\n",
      "step: 685907, loss: 0.06484442949295044, data time: 0.10900318622589111\n",
      "step: 685908, loss: 0.06271983683109283, data time: 0.07369629542032878\n",
      "step: 685909, loss: 0.06541857123374939, data time: 0.05604952573776245\n",
      "step: 685910, loss: 0.0625394880771637, data time: 0.04512114524841308\n",
      "step: 685911, loss: 0.06243526563048363, data time: 0.03785566488901774\n",
      "step: 685912, loss: 0.0604260191321373, data time: 0.03266150610787528\n",
      "step: 685913, loss: 0.059936922043561935, data time: 0.02883964776992798\n",
      "step: 685914, loss: 0.0633251965045929, data time: 0.02578327390882704\n",
      "step: 685915, loss: 0.05583576112985611, data time: 0.023405027389526368\n",
      "step: 685916, loss: 0.06165039539337158, data time: 0.02147442644292658\n",
      "step: 685917, loss: 0.06132008880376816, data time: 0.019877652327219646\n",
      "step: 685918, loss: 0.05380228906869888, data time: 0.01851650384756235\n",
      "step: 685919, loss: 0.05922050401568413, data time: 0.017339536121913364\n",
      "step: 685920, loss: 0.06346800923347473, data time: 0.01633137067159017\n",
      "step: 685921, loss: 0.06489726901054382, data time: 0.015446215867996216\n",
      "step: 685922, loss: 0.05737081170082092, data time: 0.014655239441815545\n",
      "step: 685923, loss: 0.06372588872909546, data time: 0.013949301507737901\n",
      "step: 685924, loss: 0.06940077245235443, data time: 0.013321600462261\n",
      "step: 685925, loss: 0.06445290148258209, data time: 0.012763631343841553\n",
      "step: 685926, loss: 0.06018473953008652, data time: 0.012273288908458892\n",
      "step: 685927, loss: 0.06310896575450897, data time: 0.011831348592584784\n",
      "step: 685928, loss: 0.05928521603345871, data time: 0.01142226094784944\n",
      "step: 685929, loss: 0.06670226156711578, data time: 0.011051317056020101\n",
      "step: 685930, loss: 0.05711108446121216, data time: 0.01070612907409668\n",
      "step: 685931, loss: 0.061906762421131134, data time: 0.010385531645554762\n",
      "step: 685932, loss: 0.05971686169505119, data time: 0.010085618054425274\n",
      "step: 685933, loss: 0.05971255898475647, data time: 0.009810583932059152\n",
      "step: 685934, loss: 0.06432711333036423, data time: 0.009557362260489628\n",
      "step: 685935, loss: 0.06732472777366638, data time: 0.009320727984110514\n",
      "step: 685936, loss: 0.060605525970458984, data time: 0.009101267783872543\n",
      "step: 685937, loss: 0.07310628890991211, data time: 0.008898898959159851\n",
      "step: 685938, loss: 0.05919565260410309, data time: 0.008695183378277403\n",
      "step: 685939, loss: 0.06009781360626221, data time: 0.00850216781391817\n",
      "step: 685940, loss: 0.06487972289323807, data time: 0.008317463738577707\n",
      "step: 685941, loss: 0.06735168397426605, data time: 0.008141577243804932\n",
      "step: 685942, loss: 0.056205011904239655, data time: 0.007975720070503853\n",
      "step: 685943, loss: 0.06917053461074829, data time: 0.007822250065050627\n",
      "step: 685944, loss: 0.06639059633016586, data time: 0.007675990080222106\n",
      "step: 685945, loss: 0.05547270178794861, data time: 0.007537013292312622\n",
      "step: 685946, loss: 0.05776836723089218, data time: 0.20822405815124512\n",
      "step: 685947, loss: 0.06315302103757858, data time: 0.10548031330108643\n",
      "step: 685948, loss: 0.0591505728662014, data time: 0.07125298182169597\n",
      "step: 685949, loss: 0.059674352407455444, data time: 0.054204583168029785\n",
      "step: 685950, loss: 0.06240474805235863, data time: 0.043653440475463864\n",
      "step: 685951, loss: 0.05810629948973656, data time: 0.03662276268005371\n",
      "step: 685952, loss: 0.06161756068468094, data time: 0.031607253210885186\n",
      "step: 685953, loss: 0.06549312174320221, data time: 0.027904152870178223\n",
      "step: 685954, loss: 0.05899667367339134, data time: 0.0249527825249566\n",
      "step: 685955, loss: 0.06645230948925018, data time: 0.022659993171691893\n",
      "step: 685956, loss: 0.05920543521642685, data time: 0.020795822143554688\n",
      "step: 685957, loss: 0.057638101279735565, data time: 0.019241074721018474\n",
      "step: 685958, loss: 0.0599234402179718, data time: 0.017934799194335938\n",
      "step: 685959, loss: 0.06072750687599182, data time: 0.016795107296534946\n",
      "step: 685960, loss: 0.06142163276672363, data time: 0.015815226236979167\n",
      "step: 685961, loss: 0.06024919077754021, data time: 0.014959052205085754\n",
      "step: 685962, loss: 0.05938805267214775, data time: 0.014204095391666187\n",
      "step: 685963, loss: 0.0644766092300415, data time: 0.013529221216837565\n",
      "step: 685964, loss: 0.06427964568138123, data time: 0.012922588147615132\n",
      "step: 685965, loss: 0.06431828439235687, data time: 0.012385129928588867\n",
      "step: 685966, loss: 0.06293194741010666, data time: 0.011897711526779901\n",
      "step: 685967, loss: 0.06113710254430771, data time: 0.011453693563287909\n",
      "step: 685968, loss: 0.06334277242422104, data time: 0.01104345528975777\n",
      "step: 685969, loss: 0.06681694090366364, data time: 0.010690997044245401\n",
      "step: 685970, loss: 0.06246968358755112, data time: 0.010350751876831054\n",
      "step: 685971, loss: 0.06227951496839523, data time: 0.01003263546870305\n",
      "step: 685972, loss: 0.06076325103640556, data time: 0.009731681258590133\n",
      "step: 685973, loss: 0.0625634491443634, data time: 0.009455527578081404\n",
      "step: 685974, loss: 0.06242026016116142, data time: 0.009202299446895205\n",
      "step: 685975, loss: 0.05793466418981552, data time: 0.008967781066894531\n",
      "step: 685976, loss: 0.06357824802398682, data time: 0.008749323506509104\n",
      "step: 685977, loss: 0.0601879321038723, data time: 0.008547782897949219\n",
      "step: 685978, loss: 0.0602431520819664, data time: 0.008348696159593987\n",
      "step: 685979, loss: 0.0602123960852623, data time: 0.008162421338698444\n",
      "step: 685980, loss: 0.07482177019119263, data time: 0.007983834402901785\n",
      "step: 685981, loss: 0.06165778636932373, data time: 0.007813215255737305\n",
      "step: 685982, loss: 0.06337589770555496, data time: 0.007652765995747334\n",
      "step: 685983, loss: 0.06801608204841614, data time: 0.007505749401293303\n",
      "step: 685984, loss: 0.06302101910114288, data time: 0.007365997021014874\n",
      "step: 685985, loss: 0.09753306955099106, data time: 0.007233506441116333\n",
      "step: 685986, loss: 0.07017656415700912, data time: 0.21717429161071777\n",
      "step: 685987, loss: 0.06539963185787201, data time: 0.11011767387390137\n",
      "step: 685988, loss: 0.062824547290802, data time: 0.07392342885335286\n",
      "step: 685989, loss: 0.062129877507686615, data time: 0.0561070442199707\n",
      "step: 685990, loss: 0.061494290828704834, data time: 0.04516620635986328\n",
      "step: 685991, loss: 0.06438057869672775, data time: 0.03787426153818766\n",
      "step: 685992, loss: 0.06190147250890732, data time: 0.03266208512442453\n",
      "step: 685993, loss: 0.06270402669906616, data time: 0.02886006236076355\n",
      "step: 685994, loss: 0.06510704010725021, data time: 0.025797976387871638\n",
      "step: 685995, loss: 0.058743201196193695, data time: 0.023414206504821778\n",
      "step: 685996, loss: 0.053287722170352936, data time: 0.021480907093394886\n",
      "step: 685997, loss: 0.059575241059064865, data time: 0.019871830940246582\n",
      "step: 685998, loss: 0.06368355453014374, data time: 0.01851813609783466\n",
      "step: 685999, loss: 0.0645972415804863, data time: 0.017337918281555176\n",
      "step: 686000, loss: 0.062121450901031494, data time: 0.016318845748901366\n",
      "step: 686001, loss: 0.06675130128860474, data time: 0.01542818546295166\n",
      "step: 686002, loss: 0.05990005284547806, data time: 0.014649475322050206\n",
      "step: 686003, loss: 0.06042209267616272, data time: 0.013945606019761827\n",
      "step: 686004, loss: 0.06039073318243027, data time: 0.013319103341353567\n",
      "step: 686005, loss: 0.06301985681056976, data time: 0.012768137454986572\n",
      "step: 686006, loss: 0.062182605266571045, data time: 0.012262866610572451\n",
      "step: 686007, loss: 0.05991284176707268, data time: 0.011805848641829058\n",
      "step: 686008, loss: 0.06385774910449982, data time: 0.01138069318688434\n",
      "step: 686009, loss: 0.05806022882461548, data time: 0.010992735624313354\n",
      "step: 686010, loss: 0.05734604597091675, data time: 0.010634603500366211\n",
      "step: 686011, loss: 0.058777324855327606, data time: 0.01031195200406588\n",
      "step: 686012, loss: 0.06633205711841583, data time: 0.010003699196709527\n",
      "step: 686013, loss: 0.060328900814056396, data time: 0.009719465460096086\n",
      "step: 686014, loss: 0.06111789494752884, data time: 0.009456420766896215\n",
      "step: 686015, loss: 0.0649787038564682, data time: 0.009212668736775715\n",
      "step: 686016, loss: 0.06091911345720291, data time: 0.008985257917834867\n",
      "step: 686017, loss: 0.06635245680809021, data time: 0.008777908980846405\n",
      "step: 686018, loss: 0.06351426243782043, data time: 0.008572455608483517\n",
      "step: 686019, loss: 0.05846647545695305, data time: 0.008381955763872932\n",
      "step: 686020, loss: 0.06790957599878311, data time: 0.00820096560886928\n",
      "step: 686021, loss: 0.06041347607970238, data time: 0.008025050163269043\n",
      "step: 686022, loss: 0.06166350841522217, data time: 0.007859468460083008\n",
      "step: 686023, loss: 0.06299343705177307, data time: 0.007707727582831132\n",
      "step: 686024, loss: 0.06217622756958008, data time: 0.007561420783018455\n",
      "step: 686025, loss: 0.06956125795841217, data time: 0.007423955202102661\n",
      "step: 686026, loss: 0.06303562968969345, data time: 0.22098374366760254\n",
      "step: 686027, loss: 0.06704160571098328, data time: 0.11223971843719482\n",
      "step: 686028, loss: 0.06875337660312653, data time: 0.07532906532287598\n",
      "step: 686029, loss: 0.06925535947084427, data time: 0.057268619537353516\n",
      "step: 686030, loss: 0.062291041016578674, data time: 0.04609427452087402\n",
      "step: 686031, loss: 0.05724257230758667, data time: 0.03866199652353922\n",
      "step: 686032, loss: 0.06011710688471794, data time: 0.03335121699741909\n",
      "step: 686033, loss: 0.059335049241781235, data time: 0.029434889554977417\n",
      "step: 686034, loss: 0.06016257405281067, data time: 0.026308218638102215\n",
      "step: 686035, loss: 0.06485795974731445, data time: 0.023876452445983888\n",
      "step: 686036, loss: 0.06339965015649796, data time: 0.021903103048151188\n",
      "step: 686037, loss: 0.06257417798042297, data time: 0.020256916681925457\n",
      "step: 686038, loss: 0.06756016612052917, data time: 0.018866575681246243\n",
      "step: 686039, loss: 0.06212589144706726, data time: 0.017659272466387068\n",
      "step: 686040, loss: 0.060856372117996216, data time: 0.0166258176167806\n",
      "step: 686041, loss: 0.06372992694377899, data time: 0.015726104378700256\n",
      "step: 686042, loss: 0.06060698628425598, data time: 0.014923488392549403\n",
      "step: 686043, loss: 0.060936205089092255, data time: 0.01420442263285319\n",
      "step: 686044, loss: 0.05964573472738266, data time: 0.013562076970150596\n",
      "step: 686045, loss: 0.05789834260940552, data time: 0.012991297245025634\n",
      "step: 686046, loss: 0.06282608211040497, data time: 0.012477238972981771\n",
      "step: 686047, loss: 0.06270518898963928, data time: 0.012008406899192116\n",
      "step: 686048, loss: 0.060297101736068726, data time: 0.011577305586441704\n",
      "step: 686049, loss: 0.0548010990023613, data time: 0.011185407638549805\n",
      "step: 686050, loss: 0.06159444898366928, data time: 0.010820178985595704\n",
      "step: 686051, loss: 0.0616074800491333, data time: 0.010482292908888597\n",
      "step: 686052, loss: 0.05995979905128479, data time: 0.010167907785486293\n",
      "step: 686053, loss: 0.05875161290168762, data time: 0.009877562522888184\n",
      "step: 686054, loss: 0.05997658893465996, data time: 0.009610874899502459\n",
      "step: 686055, loss: 0.05985753983259201, data time: 0.009361855189005534\n",
      "step: 686056, loss: 0.06503231078386307, data time: 0.00912992415889617\n",
      "step: 686057, loss: 0.060492854565382004, data time: 0.008914388716220856\n",
      "step: 686058, loss: 0.06859880685806274, data time: 0.00870826750090628\n",
      "step: 686059, loss: 0.06103486567735672, data time: 0.008510203922496122\n",
      "step: 686060, loss: 0.06210441514849663, data time: 0.008321230752127512\n",
      "step: 686061, loss: 0.06669547408819199, data time: 0.008141775925954184\n",
      "step: 686062, loss: 0.06170432269573212, data time: 0.007973645184491132\n",
      "step: 686063, loss: 0.07023207098245621, data time: 0.007817011130483527\n",
      "step: 686064, loss: 0.06789422035217285, data time: 0.007668226193159054\n",
      "step: 686065, loss: 0.05516444891691208, data time: 0.007528090476989746\n",
      "step: 686066, loss: 0.06436331570148468, data time: 0.2195451259613037\n",
      "step: 686067, loss: 0.058687739074230194, data time: 0.11056661605834961\n",
      "step: 686068, loss: 0.06363335996866226, data time: 0.07421485582987468\n",
      "step: 686069, loss: 0.06433633714914322, data time: 0.05651497840881348\n",
      "step: 686070, loss: 0.06596744060516357, data time: 0.045494794845581055\n",
      "step: 686071, loss: 0.0645170733332634, data time: 0.038161913553873696\n",
      "step: 686072, loss: 0.06087992712855339, data time: 0.03293262209211077\n",
      "step: 686073, loss: 0.06400924921035767, data time: 0.02907434105873108\n",
      "step: 686074, loss: 0.06548146903514862, data time: 0.025990592108832464\n",
      "step: 686075, loss: 0.06321372091770172, data time: 0.023593902587890625\n",
      "step: 686076, loss: 0.06405056267976761, data time: 0.021657943725585938\n",
      "step: 686077, loss: 0.05949532240629196, data time: 0.02003667751948039\n",
      "step: 686078, loss: 0.06351660937070847, data time: 0.018667936325073242\n",
      "step: 686079, loss: 0.05836756154894829, data time: 0.01747684819357736\n",
      "step: 686080, loss: 0.05852087587118149, data time: 0.016451517740885418\n",
      "step: 686081, loss: 0.06533512473106384, data time: 0.015552625060081482\n",
      "step: 686082, loss: 0.058901868760585785, data time: 0.014761377783382641\n",
      "step: 686083, loss: 0.06206908077001572, data time: 0.01405428515540229\n",
      "step: 686084, loss: 0.06137103959918022, data time: 0.013421924490677683\n",
      "step: 686085, loss: 0.059547096490859985, data time: 0.012866246700286865\n",
      "step: 686086, loss: 0.06885679066181183, data time: 0.012356406166439965\n",
      "step: 686087, loss: 0.06613720953464508, data time: 0.011899222027171742\n",
      "step: 686088, loss: 0.058899085968732834, data time: 0.011470597723255987\n",
      "step: 686089, loss: 0.05798996239900589, data time: 0.011081695556640625\n",
      "step: 686090, loss: 0.06186750531196594, data time: 0.010723180770874023\n",
      "step: 686091, loss: 0.06315653026103973, data time: 0.010393463648282565\n",
      "step: 686092, loss: 0.05887806415557861, data time: 0.010080205069647895\n",
      "step: 686093, loss: 0.06435481458902359, data time: 0.009795708315713065\n",
      "step: 686094, loss: 0.06654022634029388, data time: 0.009534227436986464\n",
      "step: 686095, loss: 0.058432042598724365, data time: 0.009300303459167481\n",
      "step: 686096, loss: 0.06591032445430756, data time: 0.00909267702410298\n",
      "step: 686097, loss: 0.05826934799551964, data time: 0.008890777826309204\n",
      "step: 686098, loss: 0.05847370624542236, data time: 0.008685632185502485\n",
      "step: 686099, loss: 0.06448005884885788, data time: 0.008493612794315113\n",
      "step: 686100, loss: 0.06366267055273056, data time: 0.008310937881469726\n",
      "step: 686101, loss: 0.05755419656634331, data time: 0.008134398195478652\n",
      "step: 686102, loss: 0.06543116271495819, data time: 0.007970229999439136\n",
      "step: 686103, loss: 0.06325866281986237, data time: 0.007818887108250669\n",
      "step: 686104, loss: 0.06191188097000122, data time: 0.007673281889695387\n",
      "step: 686105, loss: 0.06001810356974602, data time: 0.007534408569335937\n",
      "step: 686106, loss: 0.06436710059642792, data time: 0.22444653511047363\n",
      "step: 686107, loss: 0.06844870746135712, data time: 0.11299777030944824\n",
      "step: 686108, loss: 0.05991018936038017, data time: 0.07592368125915527\n",
      "step: 686109, loss: 0.06137235090136528, data time: 0.05785030126571655\n",
      "step: 686110, loss: 0.06375430524349213, data time: 0.04660739898681641\n",
      "step: 686111, loss: 0.06111101433634758, data time: 0.039127071698506675\n",
      "step: 686112, loss: 0.05792643874883652, data time: 0.033800874437604635\n",
      "step: 686113, loss: 0.06243191286921501, data time: 0.029890447854995728\n",
      "step: 686114, loss: 0.06528680771589279, data time: 0.02675697538587782\n",
      "step: 686115, loss: 0.06402929127216339, data time: 0.024321103096008302\n",
      "step: 686116, loss: 0.06173919886350632, data time: 0.02233834700150923\n",
      "step: 686117, loss: 0.05915438383817673, data time: 0.02069278558095296\n",
      "step: 686118, loss: 0.06711988896131516, data time: 0.019294115213247445\n",
      "step: 686119, loss: 0.06364849209785461, data time: 0.018089004925319126\n",
      "step: 686120, loss: 0.06484467536211014, data time: 0.017048056920369467\n",
      "step: 686121, loss: 0.060225456953048706, data time: 0.016109228134155273\n",
      "step: 686122, loss: 0.057321079075336456, data time: 0.015291115816901712\n",
      "step: 686123, loss: 0.06240726634860039, data time: 0.01455239454905192\n",
      "step: 686124, loss: 0.05995863676071167, data time: 0.01389497204830772\n",
      "step: 686125, loss: 0.06058133766055107, data time: 0.013308393955230712\n",
      "step: 686126, loss: 0.062432192265987396, data time: 0.0127781799861363\n",
      "step: 686127, loss: 0.05584416911005974, data time: 0.012296741658991034\n",
      "step: 686128, loss: 0.06149303913116455, data time: 0.011850719866545303\n",
      "step: 686129, loss: 0.05800431966781616, data time: 0.011444419622421265\n",
      "step: 686130, loss: 0.05709577724337578, data time: 0.011072111129760743\n",
      "step: 686131, loss: 0.06143203377723694, data time: 0.010730954316946177\n",
      "step: 686132, loss: 0.06261375546455383, data time: 0.01040551397535536\n",
      "step: 686133, loss: 0.06028255820274353, data time: 0.010105516229357039\n",
      "step: 686134, loss: 0.05840907618403435, data time: 0.009835818718219626\n",
      "step: 686135, loss: 0.0645928606390953, data time: 0.009580604235331218\n",
      "step: 686136, loss: 0.06919306516647339, data time: 0.009341455275012601\n",
      "step: 686137, loss: 0.06138881295919418, data time: 0.009121537208557129\n",
      "step: 686138, loss: 0.0606258362531662, data time: 0.008905540813099255\n",
      "step: 686139, loss: 0.06340619176626205, data time: 0.008701415622935575\n",
      "step: 686140, loss: 0.05886216089129448, data time: 0.008512394768851144\n",
      "step: 686141, loss: 0.06204911321401596, data time: 0.008328133159213595\n",
      "step: 686142, loss: 0.05918760225176811, data time: 0.008155088166932802\n",
      "step: 686143, loss: 0.06106985732913017, data time: 0.00799368557177092\n",
      "step: 686144, loss: 0.060725025832653046, data time: 0.007843787853534404\n",
      "step: 686145, loss: 0.06356234848499298, data time: 0.007698458433151245\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40871238708496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 686146, loss: 0.06325143575668335, data time: 0.21282267570495605\n",
      "step: 686147, loss: 0.05992567539215088, data time: 0.10770547389984131\n",
      "step: 686148, loss: 0.0599033385515213, data time: 0.07270479202270508\n",
      "step: 686149, loss: 0.05858250707387924, data time: 0.05534946918487549\n",
      "step: 686150, loss: 0.06016971915960312, data time: 0.044531822204589844\n",
      "step: 686151, loss: 0.05924045294523239, data time: 0.03736642996470133\n",
      "step: 686152, loss: 0.0649334043264389, data time: 0.032225234167916436\n",
      "step: 686153, loss: 0.06587221473455429, data time: 0.0284574031829834\n",
      "step: 686154, loss: 0.06831112504005432, data time: 0.025436984168158636\n",
      "step: 686155, loss: 0.06007577106356621, data time: 0.023090291023254394\n",
      "step: 686156, loss: 0.06380385160446167, data time: 0.021187002008611507\n",
      "step: 686157, loss: 0.0645853653550148, data time: 0.0196113387743632\n",
      "step: 686158, loss: 0.0618862584233284, data time: 0.018259030122023363\n",
      "step: 686159, loss: 0.062626913189888, data time: 0.017092006547110423\n",
      "step: 686160, loss: 0.059419866651296616, data time: 0.016086244583129884\n",
      "step: 686161, loss: 0.059041768312454224, data time: 0.015207603573799133\n",
      "step: 686162, loss: 0.06422233581542969, data time: 0.01443625898922191\n",
      "step: 686163, loss: 0.06043427810072899, data time: 0.013746261596679688\n",
      "step: 686164, loss: 0.05566658824682236, data time: 0.013124202427111174\n",
      "step: 686165, loss: 0.06275759637355804, data time: 0.012576282024383545\n",
      "step: 686166, loss: 0.06254994124174118, data time: 0.012081305185953775\n",
      "step: 686167, loss: 0.06179840862751007, data time: 0.01162232052196156\n",
      "step: 686168, loss: 0.06429678201675415, data time: 0.01120038654493249\n",
      "step: 686169, loss: 0.0655738115310669, data time: 0.010820498069127401\n",
      "step: 686170, loss: 0.06445689499378204, data time: 0.010475320816040039\n",
      "step: 686171, loss: 0.06267669796943665, data time: 0.010150542626014123\n",
      "step: 686172, loss: 0.06274199485778809, data time: 0.009844294300785771\n",
      "step: 686173, loss: 0.05143210291862488, data time: 0.009564246450151716\n",
      "step: 686174, loss: 0.06334634870290756, data time: 0.009307129629727068\n",
      "step: 686175, loss: 0.06063725799322128, data time: 0.009069426854451498\n",
      "step: 686176, loss: 0.05917967110872269, data time: 0.008845367739277502\n",
      "step: 686177, loss: 0.06361852586269379, data time: 0.008637428283691406\n",
      "step: 686178, loss: 0.061704885214567184, data time: 0.008434916987563625\n",
      "step: 686179, loss: 0.06500756740570068, data time: 0.008247964522417854\n",
      "step: 686180, loss: 0.06787383556365967, data time: 0.008067240033830915\n",
      "step: 686181, loss: 0.05559677630662918, data time: 0.007892747720082601\n",
      "step: 686182, loss: 0.05856899172067642, data time: 0.007730574221224399\n",
      "step: 686183, loss: 0.06432846188545227, data time: 0.007580995559692383\n",
      "step: 686184, loss: 0.06114845722913742, data time: 0.007438781933906751\n",
      "step: 686185, loss: 0.05318135768175125, data time: 0.0073015689849853516\n",
      "step: 686186, loss: 0.06527327001094818, data time: 0.22394180297851562\n",
      "step: 686187, loss: 0.0640011802315712, data time: 0.11333739757537842\n",
      "step: 686188, loss: 0.07312940806150436, data time: 0.07672937711079915\n",
      "step: 686189, loss: 0.0539470873773098, data time: 0.05820763111114502\n",
      "step: 686190, loss: 0.059469036757946014, data time: 0.04691462516784668\n",
      "step: 686191, loss: 0.0660819560289383, data time: 0.0393826961517334\n",
      "step: 686192, loss: 0.06084208935499191, data time: 0.03409491266523089\n",
      "step: 686193, loss: 0.06218213960528374, data time: 0.03012329339981079\n",
      "step: 686194, loss: 0.06177816540002823, data time: 0.02694604131910536\n",
      "step: 686195, loss: 0.057262688875198364, data time: 0.024494147300720213\n",
      "step: 686196, loss: 0.06195715069770813, data time: 0.02249975637956099\n",
      "step: 686197, loss: 0.061985939741134644, data time: 0.02083555857340495\n",
      "step: 686198, loss: 0.06775112450122833, data time: 0.019427134440495417\n",
      "step: 686199, loss: 0.05517081171274185, data time: 0.01820863996233259\n",
      "step: 686200, loss: 0.06657350063323975, data time: 0.01715861956278483\n",
      "step: 686201, loss: 0.058011021465063095, data time: 0.016238883137702942\n",
      "step: 686202, loss: 0.05940692871809006, data time: 0.015421067967134364\n",
      "step: 686203, loss: 0.06039030849933624, data time: 0.014695525169372559\n",
      "step: 686204, loss: 0.06383606791496277, data time: 0.014052064795243112\n",
      "step: 686205, loss: 0.0576133094727993, data time: 0.013474845886230468\n",
      "step: 686206, loss: 0.06085934489965439, data time: 0.012936921346755255\n",
      "step: 686207, loss: 0.0582658052444458, data time: 0.012463363734158602\n",
      "step: 686208, loss: 0.06037358567118645, data time: 0.012030912482220194\n",
      "step: 686209, loss: 0.061015717685222626, data time: 0.011628756920496622\n",
      "step: 686210, loss: 0.06120333820581436, data time: 0.011260128021240235\n",
      "step: 686211, loss: 0.060252368450164795, data time: 0.010919304994436411\n",
      "step: 686212, loss: 0.06580652296543121, data time: 0.010603551511411314\n",
      "step: 686213, loss: 0.06741112470626831, data time: 0.010308606284005302\n",
      "step: 686214, loss: 0.06660749018192291, data time: 0.010030680689318427\n",
      "step: 686215, loss: 0.06268813461065292, data time: 0.009769415855407715\n",
      "step: 686216, loss: 0.06359024345874786, data time: 0.009523314814413748\n",
      "step: 686217, loss: 0.058559563010931015, data time: 0.009297013282775879\n",
      "step: 686218, loss: 0.06951162219047546, data time: 0.009074023275664358\n",
      "step: 686219, loss: 0.057872842997312546, data time: 0.008862397250007181\n",
      "step: 686220, loss: 0.05942359194159508, data time: 0.008661563055855886\n",
      "step: 686221, loss: 0.06357388943433762, data time: 0.008474005593193902\n",
      "step: 686222, loss: 0.06037341058254242, data time: 0.008296470384340029\n",
      "step: 686223, loss: 0.06167854368686676, data time: 0.008145577029178017\n",
      "step: 686224, loss: 0.05744270980358124, data time: 0.007991949717203775\n",
      "step: 686225, loss: 0.07320460677146912, data time: 0.007846593856811523\n",
      "step: 686226, loss: 0.06172415241599083, data time: 0.2128300666809082\n",
      "step: 686227, loss: 0.05639201030135155, data time: 0.10787200927734375\n",
      "step: 686228, loss: 0.0646677315235138, data time: 0.07281835873921712\n",
      "step: 686229, loss: 0.06178612262010574, data time: 0.05537289381027222\n",
      "step: 686230, loss: 0.05737939476966858, data time: 0.0445866584777832\n",
      "step: 686231, loss: 0.06483301520347595, data time: 0.03738896052042643\n",
      "step: 686232, loss: 0.059377413243055344, data time: 0.032249586922781806\n",
      "step: 686233, loss: 0.06217467039823532, data time: 0.028470247983932495\n",
      "step: 686234, loss: 0.058007072657346725, data time: 0.025458362367418077\n",
      "step: 686235, loss: 0.060601912438869476, data time: 0.023110318183898925\n",
      "step: 686236, loss: 0.0634186714887619, data time: 0.021203344518488102\n",
      "step: 686237, loss: 0.0610710084438324, data time: 0.0196151336034139\n",
      "step: 686238, loss: 0.056396156549453735, data time: 0.018270455873929538\n",
      "step: 686239, loss: 0.05896401405334473, data time: 0.01711062022617885\n",
      "step: 686240, loss: 0.06346410512924194, data time: 0.01611162821451823\n",
      "step: 686241, loss: 0.06433146446943283, data time: 0.015229463577270508\n",
      "step: 686242, loss: 0.06023285910487175, data time: 0.01445307451135972\n",
      "step: 686243, loss: 0.06112247705459595, data time: 0.013754937383863661\n",
      "step: 686244, loss: 0.0627427026629448, data time: 0.013139185152555766\n",
      "step: 686245, loss: 0.06519963592290878, data time: 0.012595748901367188\n",
      "step: 686246, loss: 0.05625605210661888, data time: 0.012101366406395322\n",
      "step: 686247, loss: 0.058952122926712036, data time: 0.01164741949601607\n",
      "step: 686248, loss: 0.06109890341758728, data time: 0.011232303536456564\n",
      "step: 686249, loss: 0.06493555009365082, data time: 0.010848095019658407\n",
      "step: 686250, loss: 0.06718724966049194, data time: 0.010496044158935547\n",
      "step: 686251, loss: 0.06446157395839691, data time: 0.01017151429102971\n",
      "step: 686252, loss: 0.0665515661239624, data time: 0.009875200412891529\n",
      "step: 686253, loss: 0.06105237454175949, data time: 0.009595896516527449\n",
      "step: 686254, loss: 0.06157168000936508, data time: 0.009338757087444437\n",
      "step: 686255, loss: 0.06137950345873833, data time: 0.009097456932067871\n",
      "step: 686256, loss: 0.06633090227842331, data time: 0.008874947024929908\n",
      "step: 686257, loss: 0.05636654794216156, data time: 0.008668385446071625\n",
      "step: 686258, loss: 0.05934363603591919, data time: 0.00846711794535319\n",
      "step: 686259, loss: 0.061431508511304855, data time: 0.008274499107809627\n",
      "step: 686260, loss: 0.06384420394897461, data time: 0.008094712666102817\n",
      "step: 686261, loss: 0.0616353414952755, data time: 0.007920258575015597\n",
      "step: 686262, loss: 0.06010618433356285, data time: 0.0077567680461986645\n",
      "step: 686263, loss: 0.0661260187625885, data time: 0.007607710988898026\n",
      "step: 686264, loss: 0.06133989244699478, data time: 0.007464219362307818\n",
      "step: 686265, loss: 0.07307552546262741, data time: 0.007328623533248901\n",
      "step: 686266, loss: 0.05608603358268738, data time: 0.2144927978515625\n",
      "step: 686267, loss: 0.061237528920173645, data time: 0.10837399959564209\n",
      "step: 686268, loss: 0.06675875186920166, data time: 0.07315786679585774\n",
      "step: 686269, loss: 0.055593013763427734, data time: 0.05572003126144409\n",
      "step: 686270, loss: 0.06113898381590843, data time: 0.044850397109985354\n",
      "step: 686271, loss: 0.05850563570857048, data time: 0.03762670358022054\n",
      "step: 686272, loss: 0.06218463182449341, data time: 0.032461745398385186\n",
      "step: 686273, loss: 0.0589706115424633, data time: 0.02865925431251526\n",
      "step: 686274, loss: 0.06113661453127861, data time: 0.02562183803982205\n",
      "step: 686275, loss: 0.06226928159594536, data time: 0.023260879516601562\n",
      "step: 686276, loss: 0.0617026686668396, data time: 0.021342190829190342\n",
      "step: 686277, loss: 0.0637224093079567, data time: 0.019750793774922688\n",
      "step: 686278, loss: 0.06162562593817711, data time: 0.018398413291344277\n",
      "step: 686279, loss: 0.05926227569580078, data time: 0.017229557037353516\n",
      "step: 686280, loss: 0.06470014154911041, data time: 0.016221237182617188\n",
      "step: 686281, loss: 0.05787030607461929, data time: 0.015336230397224426\n",
      "step: 686282, loss: 0.06037510558962822, data time: 0.014559058582081515\n",
      "step: 686283, loss: 0.07076558470726013, data time: 0.013860026995340982\n",
      "step: 686284, loss: 0.061514224857091904, data time: 0.013237012060065018\n",
      "step: 686285, loss: 0.06003883108496666, data time: 0.012682473659515381\n",
      "step: 686286, loss: 0.05831688642501831, data time: 0.012183552696591332\n",
      "step: 686287, loss: 0.06478743255138397, data time: 0.011728091673417524\n",
      "step: 686288, loss: 0.06832467764616013, data time: 0.01130539437998896\n",
      "step: 686289, loss: 0.06162744760513306, data time: 0.010927567879358927\n",
      "step: 686290, loss: 0.0667620524764061, data time: 0.010575838088989258\n",
      "step: 686291, loss: 0.06940920650959015, data time: 0.010247065470768856\n",
      "step: 686292, loss: 0.06345060467720032, data time: 0.009939820678145797\n",
      "step: 686293, loss: 0.056818991899490356, data time: 0.00965888159615653\n",
      "step: 686294, loss: 0.06113670766353607, data time: 0.00940097611525963\n",
      "step: 686295, loss: 0.06012279912829399, data time: 0.009158698717753093\n",
      "step: 686296, loss: 0.06464873999357224, data time: 0.008931321482504568\n",
      "step: 686297, loss: 0.05908995866775513, data time: 0.008721880614757538\n",
      "step: 686298, loss: 0.06559959053993225, data time: 0.008515646963408499\n",
      "step: 686299, loss: 0.05784628912806511, data time: 0.008321986478917739\n",
      "step: 686300, loss: 0.06146221607923508, data time: 0.008138268334524973\n",
      "step: 686301, loss: 0.06712650507688522, data time: 0.00796300835079617\n",
      "step: 686302, loss: 0.06779445707798004, data time: 0.007801745388958906\n",
      "step: 686303, loss: 0.0626833364367485, data time: 0.007648624871906482\n",
      "step: 686304, loss: 0.06070096790790558, data time: 0.007505367963741987\n",
      "step: 686305, loss: 0.0743984803557396, data time: 0.0073682904243469235\n",
      "step: 686306, loss: 0.05858805030584335, data time: 0.21300339698791504\n",
      "step: 686307, loss: 0.06381921470165253, data time: 0.10816001892089844\n",
      "step: 686308, loss: 0.06290443241596222, data time: 0.07261411348978679\n",
      "step: 686309, loss: 0.06387661397457123, data time: 0.05531919002532959\n",
      "step: 686310, loss: 0.06132927164435387, data time: 0.044541358947753906\n",
      "step: 686311, loss: 0.058082401752471924, data time: 0.037356932957967125\n",
      "step: 686312, loss: 0.06413097679615021, data time: 0.03221699169703892\n",
      "step: 686313, loss: 0.06320037692785263, data time: 0.02844187617301941\n",
      "step: 686314, loss: 0.061817847192287445, data time: 0.025425937440660264\n",
      "step: 686315, loss: 0.06078469008207321, data time: 0.023085474967956543\n",
      "step: 686316, loss: 0.06540141999721527, data time: 0.02118418433449485\n",
      "step: 686317, loss: 0.06021970510482788, data time: 0.01961416006088257\n",
      "step: 686318, loss: 0.05798535794019699, data time: 0.01827870882474459\n",
      "step: 686319, loss: 0.0599968284368515, data time: 0.017113668578011648\n",
      "step: 686320, loss: 0.06443628668785095, data time: 0.016111342112223308\n",
      "step: 686321, loss: 0.05590834468603134, data time: 0.015230163931846619\n",
      "step: 686322, loss: 0.06288978457450867, data time: 0.01445613187902114\n",
      "step: 686323, loss: 0.06142699718475342, data time: 0.013766394721137153\n",
      "step: 686324, loss: 0.06234313175082207, data time: 0.013148985410991468\n",
      "step: 686325, loss: 0.0681505799293518, data time: 0.012603342533111572\n",
      "step: 686326, loss: 0.06360042840242386, data time: 0.012104954038347517\n",
      "step: 686327, loss: 0.057269662618637085, data time: 0.011655493216081099\n",
      "step: 686328, loss: 0.058730147778987885, data time: 0.011235548102337381\n",
      "step: 686329, loss: 0.05852678045630455, data time: 0.010851611693700155\n",
      "step: 686330, loss: 0.06934436410665512, data time: 0.01049793243408203\n",
      "step: 686331, loss: 0.06301984190940857, data time: 0.010171936108515812\n",
      "step: 686332, loss: 0.06249801442027092, data time: 0.00986694406580042\n",
      "step: 686333, loss: 0.06528661400079727, data time: 0.009591000420706612\n",
      "step: 686334, loss: 0.05846940726041794, data time: 0.009335994720458984\n",
      "step: 686335, loss: 0.06066431105136871, data time: 0.00909581184387207\n",
      "step: 686336, loss: 0.059859465807676315, data time: 0.008870001762144027\n",
      "step: 686337, loss: 0.06514940410852432, data time: 0.008662104606628418\n",
      "step: 686338, loss: 0.05648258328437805, data time: 0.008457812395962801\n",
      "step: 686339, loss: 0.055210262537002563, data time: 0.008265383103314568\n",
      "step: 686340, loss: 0.0630568414926529, data time: 0.00808544158935547\n",
      "step: 686341, loss: 0.06705031543970108, data time: 0.007914688852098253\n",
      "step: 686342, loss: 0.0634380578994751, data time: 0.007752218761959592\n",
      "step: 686343, loss: 0.06086854636669159, data time: 0.007603168487548828\n",
      "step: 686344, loss: 0.06594464182853699, data time: 0.0074640420766977165\n",
      "step: 686345, loss: 0.057214364409446716, data time: 0.0073309183120727536\n",
      "step: 686346, loss: 0.0627361536026001, data time: 0.21730995178222656\n",
      "step: 686347, loss: 0.059587299823760986, data time: 0.1106560230255127\n",
      "step: 686348, loss: 0.06190350279211998, data time: 0.07428534825642903\n",
      "step: 686349, loss: 0.06753268092870712, data time: 0.05646204948425293\n",
      "step: 686350, loss: 0.05786494165658951, data time: 0.04544782638549805\n",
      "step: 686351, loss: 0.06567831337451935, data time: 0.03811991214752197\n",
      "step: 686352, loss: 0.06451839208602905, data time: 0.03288418906075614\n",
      "step: 686353, loss: 0.06273485720157623, data time: 0.029027432203292847\n",
      "step: 686354, loss: 0.06549140810966492, data time: 0.02595218022664388\n",
      "step: 686355, loss: 0.06832185387611389, data time: 0.02355501651763916\n",
      "step: 686356, loss: 0.06033074110746384, data time: 0.021615028381347656\n",
      "step: 686357, loss: 0.0593753382563591, data time: 0.0199890931447347\n",
      "step: 686358, loss: 0.061564527451992035, data time: 0.018617556645320013\n",
      "step: 686359, loss: 0.05945247411727905, data time: 0.01743279184613909\n",
      "step: 686360, loss: 0.064104363322258, data time: 0.016411320368448893\n",
      "step: 686361, loss: 0.05950021743774414, data time: 0.0155104398727417\n",
      "step: 686362, loss: 0.06246664747595787, data time: 0.014714844086590935\n",
      "step: 686363, loss: 0.05625687539577484, data time: 0.014004535145229764\n",
      "step: 686364, loss: 0.059243302792310715, data time: 0.01337390196950812\n",
      "step: 686365, loss: 0.06585114449262619, data time: 0.012814593315124512\n",
      "step: 686366, loss: 0.06132262945175171, data time: 0.012312503088088263\n",
      "step: 686367, loss: 0.05895267054438591, data time: 0.011848731474442915\n",
      "step: 686368, loss: 0.06221510469913483, data time: 0.011422457902327827\n",
      "step: 686369, loss: 0.06215120479464531, data time: 0.011033048232396444\n",
      "step: 686370, loss: 0.055155448615550995, data time: 0.010673170089721679\n",
      "step: 686371, loss: 0.06409703195095062, data time: 0.010344276061424842\n",
      "step: 686372, loss: 0.0594976469874382, data time: 0.010032936378761573\n",
      "step: 686373, loss: 0.05458557605743408, data time: 0.009745138032095773\n",
      "step: 686374, loss: 0.062094178050756454, data time: 0.009495784496438914\n",
      "step: 686375, loss: 0.057743024080991745, data time: 0.009263555208841959\n",
      "step: 686376, loss: 0.06457570195198059, data time: 0.009045085599345545\n",
      "step: 686377, loss: 0.06060951203107834, data time: 0.008843965828418732\n",
      "step: 686378, loss: 0.057702403515577316, data time: 0.00864086729107481\n",
      "step: 686379, loss: 0.06935179978609085, data time: 0.008447415688458611\n",
      "step: 686380, loss: 0.05881192907691002, data time: 0.008265066146850585\n",
      "step: 686381, loss: 0.05767089128494263, data time: 0.008090025848812528\n",
      "step: 686382, loss: 0.06519626826047897, data time: 0.007926032349870011\n",
      "step: 686383, loss: 0.05971545726060867, data time: 0.0077742902856124075\n",
      "step: 686384, loss: 0.0673396959900856, data time: 0.007629877481705103\n",
      "step: 686385, loss: 0.050758153200149536, data time: 0.007492852210998535\n",
      "step: 686386, loss: 0.06430016458034515, data time: 0.2212679386138916\n",
      "step: 686387, loss: 0.06581985950469971, data time: 0.11194431781768799\n",
      "step: 686388, loss: 0.06491537392139435, data time: 0.07565808296203613\n",
      "step: 686389, loss: 0.07401890307664871, data time: 0.05732119083404541\n",
      "step: 686390, loss: 0.06779994070529938, data time: 0.04614276885986328\n",
      "step: 686391, loss: 0.06363421678543091, data time: 0.03870213031768799\n",
      "step: 686392, loss: 0.0718759149312973, data time: 0.033475399017333984\n",
      "step: 686393, loss: 0.06431147456169128, data time: 0.029542505741119385\n",
      "step: 686394, loss: 0.059055861085653305, data time: 0.026404990090264216\n",
      "step: 686395, loss: 0.06743379682302475, data time: 0.02396101951599121\n",
      "step: 686396, loss: 0.06185763329267502, data time: 0.021977793086658825\n",
      "step: 686397, loss: 0.0639861524105072, data time: 0.020333488782246906\n",
      "step: 686398, loss: 0.061754774302244186, data time: 0.018931058736947868\n",
      "step: 686399, loss: 0.059579964727163315, data time: 0.017727783748081753\n",
      "step: 686400, loss: 0.06444010883569717, data time: 0.01668694814046224\n",
      "step: 686401, loss: 0.06421865522861481, data time: 0.015777021646499634\n",
      "step: 686402, loss: 0.06149020045995712, data time: 0.014968142789952895\n",
      "step: 686403, loss: 0.0575222373008728, data time: 0.01424513922797309\n",
      "step: 686404, loss: 0.06025753542780876, data time: 0.013608267432764956\n",
      "step: 686405, loss: 0.06577436625957489, data time: 0.013033390045166016\n",
      "step: 686406, loss: 0.0663742870092392, data time: 0.012513694309052966\n",
      "step: 686407, loss: 0.06644250452518463, data time: 0.012043075128035112\n",
      "step: 686408, loss: 0.06706050038337708, data time: 0.011607242667156717\n",
      "step: 686409, loss: 0.05693656951189041, data time: 0.011210699876149496\n",
      "step: 686410, loss: 0.05722393840551376, data time: 0.010842294692993163\n",
      "step: 686411, loss: 0.0651642233133316, data time: 0.01050268686734713\n",
      "step: 686412, loss: 0.058019302785396576, data time: 0.010188217516298647\n",
      "step: 686413, loss: 0.06119098514318466, data time: 0.009899428912571498\n",
      "step: 686414, loss: 0.060219064354896545, data time: 0.009632406563594424\n",
      "step: 686415, loss: 0.06576443463563919, data time: 0.009381969769795736\n",
      "step: 686416, loss: 0.06396376341581345, data time: 0.00915058966605894\n",
      "step: 686417, loss: 0.060529083013534546, data time: 0.008934937417507172\n",
      "step: 686418, loss: 0.06287702172994614, data time: 0.008726539033831972\n",
      "step: 686419, loss: 0.062280409038066864, data time: 0.008527131641612333\n",
      "step: 686420, loss: 0.06192752718925476, data time: 0.008340958186558315\n",
      "step: 686421, loss: 0.06528715789318085, data time: 0.008159663942125108\n",
      "step: 686422, loss: 0.061689794063568115, data time: 0.007990340928773623\n",
      "step: 686423, loss: 0.06396618485450745, data time: 0.007834603911951968\n",
      "step: 686424, loss: 0.06022767350077629, data time: 0.007685123345790765\n",
      "step: 686425, loss: 0.05126224085688591, data time: 0.007545191049575806\n",
      "step: 686426, loss: 0.0637815073132515, data time: 0.2230057716369629\n",
      "step: 686427, loss: 0.06473284959793091, data time: 0.11312007904052734\n",
      "step: 686428, loss: 0.06672826409339905, data time: 0.0760347048441569\n",
      "step: 686429, loss: 0.06166922673583031, data time: 0.05795729160308838\n",
      "step: 686430, loss: 0.06584063917398453, data time: 0.0466982364654541\n",
      "step: 686431, loss: 0.06225816532969475, data time: 0.03920527299245199\n",
      "step: 686432, loss: 0.05741272494196892, data time: 0.033839055470057895\n",
      "step: 686433, loss: 0.06320298463106155, data time: 0.029909402132034302\n",
      "step: 686434, loss: 0.05870497226715088, data time: 0.026771068572998047\n",
      "step: 686435, loss: 0.06210744380950928, data time: 0.024329733848571778\n",
      "step: 686436, loss: 0.056473519653081894, data time: 0.022345087744972923\n",
      "step: 686437, loss: 0.05847161263227463, data time: 0.020693262418111164\n",
      "step: 686438, loss: 0.06471594423055649, data time: 0.019300222396850586\n",
      "step: 686439, loss: 0.061665914952754974, data time: 0.018093058041163852\n",
      "step: 686440, loss: 0.06477101147174835, data time: 0.017049566904703776\n",
      "step: 686441, loss: 0.06373903900384903, data time: 0.01613442599773407\n",
      "step: 686442, loss: 0.05660921335220337, data time: 0.015324508442598231\n",
      "step: 686443, loss: 0.06081513687968254, data time: 0.014605734083387587\n",
      "step: 686444, loss: 0.056130301207304, data time: 0.013961754347148695\n",
      "step: 686445, loss: 0.06017794460058212, data time: 0.013390064239501953\n",
      "step: 686446, loss: 0.061476193368434906, data time: 0.012870936166672479\n",
      "step: 686447, loss: 0.05953747034072876, data time: 0.012395728718150745\n",
      "step: 686448, loss: 0.06292200088500977, data time: 0.011962807696798573\n",
      "step: 686449, loss: 0.05986713990569115, data time: 0.011565585931142172\n",
      "step: 686450, loss: 0.06375396251678467, data time: 0.011198196411132812\n",
      "step: 686451, loss: 0.06002722680568695, data time: 0.010847091674804688\n",
      "step: 686452, loss: 0.06174805760383606, data time: 0.010518312454223633\n",
      "step: 686453, loss: 0.05815133452415466, data time: 0.010216431958334786\n",
      "step: 686454, loss: 0.061779122799634933, data time: 0.009936941081079943\n",
      "step: 686455, loss: 0.06625421345233917, data time: 0.009677449862162272\n",
      "step: 686456, loss: 0.06294306367635727, data time: 0.009440468203636908\n",
      "step: 686457, loss: 0.06573711335659027, data time: 0.0092196986079216\n",
      "step: 686458, loss: 0.060823358595371246, data time: 0.008999419934821852\n",
      "step: 686459, loss: 0.06337469816207886, data time: 0.008791804313659668\n",
      "step: 686460, loss: 0.061482347548007965, data time: 0.008597046988351004\n",
      "step: 686461, loss: 0.06620942801237106, data time: 0.008409029907650419\n",
      "step: 686462, loss: 0.06494446098804474, data time: 0.008234520216245909\n",
      "step: 686463, loss: 0.06478038430213928, data time: 0.008071121416593852\n",
      "step: 686464, loss: 0.054273687303066254, data time: 0.007916725598848783\n",
      "step: 686465, loss: 0.06516865640878677, data time: 0.007771521806716919\n",
      "step: 686466, loss: 0.06426757574081421, data time: 0.22036027908325195\n",
      "step: 686467, loss: 0.0625540018081665, data time: 0.11094379425048828\n",
      "step: 686468, loss: 0.0604388490319252, data time: 0.07486883799235027\n",
      "step: 686469, loss: 0.0687670111656189, data time: 0.056937456130981445\n",
      "step: 686470, loss: 0.06363897025585175, data time: 0.04583616256713867\n",
      "step: 686471, loss: 0.06331518292427063, data time: 0.0384523868560791\n",
      "step: 686472, loss: 0.06260383129119873, data time: 0.03317744391305106\n",
      "step: 686473, loss: 0.06311090290546417, data time: 0.029342710971832275\n",
      "step: 686474, loss: 0.05912790447473526, data time: 0.026228692796495225\n",
      "step: 686475, loss: 0.06490160524845123, data time: 0.023810577392578126\n",
      "step: 686476, loss: 0.06222374364733696, data time: 0.021840008822354404\n",
      "step: 686477, loss: 0.06291347742080688, data time: 0.02019949754079183\n",
      "step: 686478, loss: 0.06427952647209167, data time: 0.018811666048490085\n",
      "step: 686479, loss: 0.057389598339796066, data time: 0.017610958644321988\n",
      "step: 686480, loss: 0.06253518164157867, data time: 0.016572268803914388\n",
      "step: 686481, loss: 0.06385406106710434, data time: 0.01567050814628601\n",
      "step: 686482, loss: 0.05962715670466423, data time: 0.014871400945326862\n",
      "step: 686483, loss: 0.06288491934537888, data time: 0.01415562629699707\n",
      "step: 686484, loss: 0.06346332281827927, data time: 0.013519174174258583\n",
      "step: 686485, loss: 0.06854834407567978, data time: 0.01295616626739502\n",
      "step: 686486, loss: 0.06170661374926567, data time: 0.01244354248046875\n",
      "step: 686487, loss: 0.059563469141721725, data time: 0.011976415460759943\n",
      "step: 686488, loss: 0.06119418144226074, data time: 0.011544611143029255\n",
      "step: 686489, loss: 0.06314370036125183, data time: 0.01114952564239502\n",
      "step: 686490, loss: 0.060804735869169235, data time: 0.010790023803710937\n",
      "step: 686491, loss: 0.06177182123064995, data time: 0.010451628611637996\n",
      "step: 686492, loss: 0.062338173389434814, data time: 0.010138317390724464\n",
      "step: 686493, loss: 0.06406335532665253, data time: 0.009849028927939278\n",
      "step: 686494, loss: 0.06222628057003021, data time: 0.009596693104711073\n",
      "step: 686495, loss: 0.06373830139636993, data time: 0.00936125119527181\n",
      "step: 686496, loss: 0.062171272933483124, data time: 0.009140268448860414\n",
      "step: 686497, loss: 0.06237783282995224, data time: 0.00893692672252655\n",
      "step: 686498, loss: 0.0637298971414566, data time: 0.008727557731397224\n",
      "step: 686499, loss: 0.060011278837919235, data time: 0.008532902773688822\n",
      "step: 686500, loss: 0.05493612587451935, data time: 0.008347020830426898\n",
      "step: 686501, loss: 0.06620483100414276, data time: 0.008170081509484185\n",
      "step: 686502, loss: 0.05999842286109924, data time: 0.008004549387338999\n",
      "step: 686503, loss: 0.059030890464782715, data time: 0.007850891665408486\n",
      "step: 686504, loss: 0.06771011650562286, data time: 0.007702154990954277\n",
      "step: 686505, loss: 0.0642591342329979, data time: 0.00756259560585022\n",
      "step: 686506, loss: 0.05662345141172409, data time: 0.22212958335876465\n",
      "step: 686507, loss: 0.06400035321712494, data time: 0.11182761192321777\n",
      "step: 686508, loss: 0.06850744783878326, data time: 0.0750900109608968\n",
      "step: 686509, loss: 0.0677228793501854, data time: 0.05718284845352173\n",
      "step: 686510, loss: 0.062171973288059235, data time: 0.046099281311035155\n",
      "step: 686511, loss: 0.06261612474918365, data time: 0.03871166706085205\n",
      "step: 686512, loss: 0.06268161535263062, data time: 0.03343054226466587\n",
      "step: 686513, loss: 0.06475497782230377, data time: 0.029569536447525024\n",
      "step: 686514, loss: 0.06180483102798462, data time: 0.026456885867648654\n",
      "step: 686515, loss: 0.06298284232616425, data time: 0.024052739143371582\n",
      "step: 686516, loss: 0.06004814803600311, data time: 0.022102204236117275\n",
      "step: 686517, loss: 0.06392137706279755, data time: 0.020468016465504963\n",
      "step: 686518, loss: 0.061184853315353394, data time: 0.019086434290959284\n",
      "step: 686519, loss: 0.05674763023853302, data time: 0.017896107264927456\n",
      "step: 686520, loss: 0.056444957852363586, data time: 0.016864744822184245\n",
      "step: 686521, loss: 0.06492605805397034, data time: 0.015959888696670532\n",
      "step: 686522, loss: 0.06480418145656586, data time: 0.01516123378978056\n",
      "step: 686523, loss: 0.05741272494196892, data time: 0.014444510142008463\n",
      "step: 686524, loss: 0.05941849946975708, data time: 0.013813207024022153\n",
      "step: 686525, loss: 0.06349125504493713, data time: 0.01324899196624756\n",
      "step: 686526, loss: 0.05852149426937103, data time: 0.012738625208536783\n",
      "step: 686527, loss: 0.05949712544679642, data time: 0.012278275056318804\n",
      "step: 686528, loss: 0.05678775534033775, data time: 0.011849475943523905\n",
      "step: 686529, loss: 0.05669034644961357, data time: 0.011453340450922648\n",
      "step: 686530, loss: 0.06104287505149841, data time: 0.011089887619018555\n",
      "step: 686531, loss: 0.06045392155647278, data time: 0.010759518696711613\n",
      "step: 686532, loss: 0.06164653226733208, data time: 0.010445709581728335\n",
      "step: 686533, loss: 0.07043813169002533, data time: 0.01014587708881923\n",
      "step: 686534, loss: 0.06448186933994293, data time: 0.009874590511979729\n",
      "step: 686535, loss: 0.060469359159469604, data time: 0.009618330001831054\n",
      "step: 686536, loss: 0.06324891746044159, data time: 0.009379740684263168\n",
      "step: 686537, loss: 0.06493651121854782, data time: 0.009159877896308899\n",
      "step: 686538, loss: 0.06691515445709229, data time: 0.008941383072824188\n",
      "step: 686539, loss: 0.06415067613124847, data time: 0.008734955507166246\n",
      "step: 686540, loss: 0.0656694769859314, data time: 0.008542742047991072\n",
      "step: 686541, loss: 0.05716785788536072, data time: 0.008356425497266982\n",
      "step: 686542, loss: 0.05842439457774162, data time: 0.008183305327956742\n",
      "step: 686543, loss: 0.0661579892039299, data time: 0.00802029434003328\n",
      "step: 686544, loss: 0.05642176419496536, data time: 0.007866951135488657\n",
      "step: 686545, loss: 0.03993656858801842, data time: 0.007723498344421387\n",
      "step: 686546, loss: 0.060308657586574554, data time: 0.21149682998657227\n",
      "step: 686547, loss: 0.06213982030749321, data time: 0.10688090324401855\n",
      "step: 686548, loss: 0.0669783502817154, data time: 0.07259766260782878\n",
      "step: 686549, loss: 0.0592661127448082, data time: 0.05524212121963501\n",
      "step: 686550, loss: 0.055493514984846115, data time: 0.04452247619628906\n",
      "step: 686551, loss: 0.0589902363717556, data time: 0.03739098707834879\n",
      "step: 686552, loss: 0.05789266526699066, data time: 0.032292263848440986\n",
      "step: 686553, loss: 0.061515554785728455, data time: 0.02855551242828369\n",
      "step: 686554, loss: 0.0593646764755249, data time: 0.025556405385335285\n",
      "step: 686555, loss: 0.05857483670115471, data time: 0.02324366569519043\n",
      "step: 686556, loss: 0.06002148240804672, data time: 0.021358923478560013\n",
      "step: 686557, loss: 0.05789012834429741, data time: 0.019793609778086346\n",
      "step: 686558, loss: 0.0656512975692749, data time: 0.018466454285841722\n",
      "step: 686559, loss: 0.06510333716869354, data time: 0.01731588159288679\n",
      "step: 686560, loss: 0.07157634943723679, data time: 0.016327730814615884\n",
      "step: 686561, loss: 0.06416209042072296, data time: 0.01545974612236023\n",
      "step: 686562, loss: 0.05969654768705368, data time: 0.01468657044803395\n",
      "step: 686563, loss: 0.059824615716934204, data time: 0.013977766036987305\n",
      "step: 686564, loss: 0.06309707462787628, data time: 0.013352356458965101\n",
      "step: 686565, loss: 0.06555049121379852, data time: 0.012794685363769532\n",
      "step: 686566, loss: 0.06448028981685638, data time: 0.012286072685605004\n",
      "step: 686567, loss: 0.0659780204296112, data time: 0.011823079802773216\n",
      "step: 686568, loss: 0.06203937157988548, data time: 0.011397962984831436\n",
      "step: 686569, loss: 0.05892074108123779, data time: 0.01100898782412211\n",
      "step: 686570, loss: 0.06519335508346558, data time: 0.010653505325317383\n",
      "step: 686571, loss: 0.06339167803525925, data time: 0.010321901394770695\n",
      "step: 686572, loss: 0.06318192183971405, data time: 0.010014233765778717\n",
      "step: 686573, loss: 0.06614688038825989, data time: 0.009730637073516846\n",
      "step: 686574, loss: 0.06658117473125458, data time: 0.009467560669471478\n",
      "step: 686575, loss: 0.06441239267587662, data time: 0.00922082265218099\n",
      "step: 686576, loss: 0.06289520859718323, data time: 0.008991133782171434\n",
      "step: 686577, loss: 0.05723201483488083, data time: 0.00878317654132843\n",
      "step: 686578, loss: 0.0631628930568695, data time: 0.008576891639015892\n",
      "step: 686579, loss: 0.0620550811290741, data time: 0.008385139353135052\n",
      "step: 686580, loss: 0.07051827013492584, data time: 0.008201313018798829\n",
      "step: 686581, loss: 0.06301447004079819, data time: 0.008023560047149658\n",
      "step: 686582, loss: 0.06186354160308838, data time: 0.007858282810932881\n",
      "step: 686583, loss: 0.05855933949351311, data time: 0.007705136349326686\n",
      "step: 686584, loss: 0.06072946637868881, data time: 0.0075604854485927485\n",
      "step: 686585, loss: 0.05500364303588867, data time: 0.007421761751174927\n",
      "step: 686586, loss: 0.060416869819164276, data time: 0.2247462272644043\n",
      "step: 686587, loss: 0.05899989604949951, data time: 0.11316442489624023\n",
      "step: 686588, loss: 0.05993548780679703, data time: 0.07635124524434407\n",
      "step: 686589, loss: 0.05748404562473297, data time: 0.058023810386657715\n",
      "step: 686590, loss: 0.06229739636182785, data time: 0.04672226905822754\n",
      "step: 686591, loss: 0.06231117993593216, data time: 0.03918691476186117\n",
      "step: 686592, loss: 0.05861086770892143, data time: 0.03380189623151507\n",
      "step: 686593, loss: 0.059996768832206726, data time: 0.02983209490776062\n",
      "step: 686594, loss: 0.06497689336538315, data time: 0.0266623232099745\n",
      "step: 686595, loss: 0.05535323917865753, data time: 0.024193406105041504\n",
      "step: 686596, loss: 0.06112666428089142, data time: 0.02218562906438654\n",
      "step: 686597, loss: 0.06604649126529694, data time: 0.020525018374125164\n",
      "step: 686598, loss: 0.06074900925159454, data time: 0.019111193143404447\n",
      "step: 686599, loss: 0.06313304603099823, data time: 0.01789372307913644\n",
      "step: 686600, loss: 0.06583636999130249, data time: 0.01688078244527181\n",
      "step: 686601, loss: 0.05292116105556488, data time: 0.01595860719680786\n",
      "step: 686602, loss: 0.0675993487238884, data time: 0.015141823712517233\n",
      "step: 686603, loss: 0.062450241297483444, data time: 0.014410932858784994\n",
      "step: 686604, loss: 0.06507980078458786, data time: 0.013763578314530222\n",
      "step: 686605, loss: 0.06142919510602951, data time: 0.013183069229125977\n",
      "step: 686606, loss: 0.05705694854259491, data time: 0.012659254528227307\n",
      "step: 686607, loss: 0.062445443123579025, data time: 0.012181195345791903\n",
      "step: 686608, loss: 0.06045615300536156, data time: 0.011740715607352879\n",
      "step: 686609, loss: 0.06394663453102112, data time: 0.011341979106267294\n",
      "step: 686610, loss: 0.06224658340215683, data time: 0.010971565246582032\n",
      "step: 686611, loss: 0.06412622332572937, data time: 0.010628489347604604\n",
      "step: 686612, loss: 0.061314553022384644, data time: 0.01030784183078342\n",
      "step: 686613, loss: 0.05819092318415642, data time: 0.010013137544904436\n",
      "step: 686614, loss: 0.06189460679888725, data time: 0.00974352606411638\n",
      "step: 686615, loss: 0.06075255572795868, data time: 0.009491952260335286\n",
      "step: 686616, loss: 0.06446684896945953, data time: 0.009255555368238879\n",
      "step: 686617, loss: 0.06005547568202019, data time: 0.009040139615535736\n",
      "step: 686618, loss: 0.05728079006075859, data time: 0.008827924728393555\n",
      "step: 686619, loss: 0.06313484162092209, data time: 0.008629637606003705\n",
      "step: 686620, loss: 0.05819527804851532, data time: 0.008441502707345144\n",
      "step: 686621, loss: 0.06519556045532227, data time: 0.008261011706458198\n",
      "step: 686622, loss: 0.0619296059012413, data time: 0.008092532286772857\n",
      "step: 686623, loss: 0.062339555472135544, data time: 0.007938460299843237\n",
      "step: 686624, loss: 0.0639207661151886, data time: 0.00778969128926595\n",
      "step: 686625, loss: 0.06324069947004318, data time: 0.0076469480991363525\n",
      "step: 686626, loss: 0.06326264142990112, data time: 0.220672607421875\n",
      "step: 686627, loss: 0.06035391613841057, data time: 0.11110484600067139\n",
      "step: 686628, loss: 0.0626753643155098, data time: 0.07482632001241048\n",
      "step: 686629, loss: 0.06892459839582443, data time: 0.05696803331375122\n",
      "step: 686630, loss: 0.05901694297790527, data time: 0.04585819244384766\n",
      "step: 686631, loss: 0.06206886097788811, data time: 0.03844594955444336\n",
      "step: 686632, loss: 0.06232106313109398, data time: 0.0331660338810512\n",
      "step: 686633, loss: 0.06020950525999069, data time: 0.02927824854850769\n",
      "step: 686634, loss: 0.0666344165802002, data time: 0.026174174414740667\n",
      "step: 686635, loss: 0.06624817103147507, data time: 0.023765754699707032\n",
      "step: 686636, loss: 0.0640452653169632, data time: 0.02180103822187944\n",
      "step: 686637, loss: 0.057411134243011475, data time: 0.020168026288350422\n",
      "step: 686638, loss: 0.061185069382190704, data time: 0.01878499984741211\n",
      "step: 686639, loss: 0.06254348158836365, data time: 0.01759128911154611\n",
      "step: 686640, loss: 0.06071730703115463, data time: 0.016556278864542643\n",
      "step: 686641, loss: 0.07168897241353989, data time: 0.015657052397727966\n",
      "step: 686642, loss: 0.060886796563863754, data time: 0.01486506181604722\n",
      "step: 686643, loss: 0.06323058903217316, data time: 0.014149626096089682\n",
      "step: 686644, loss: 0.0667661726474762, data time: 0.013510177009984067\n",
      "step: 686645, loss: 0.06352634727954865, data time: 0.012945973873138427\n",
      "step: 686646, loss: 0.06532412022352219, data time: 0.012432257334391275\n",
      "step: 686647, loss: 0.06222514808177948, data time: 0.011965697461908514\n",
      "step: 686648, loss: 0.06187004968523979, data time: 0.01153360242428987\n",
      "step: 686649, loss: 0.057731132954359055, data time: 0.01114774743715922\n",
      "step: 686650, loss: 0.07368122041225433, data time: 0.01078542709350586\n",
      "step: 686651, loss: 0.06076878309249878, data time: 0.010451637781583346\n",
      "step: 686652, loss: 0.06136588379740715, data time: 0.010137284243548358\n",
      "step: 686653, loss: 0.06345629692077637, data time: 0.009848747934613909\n",
      "step: 686654, loss: 0.06405076384544373, data time: 0.009582248227349642\n",
      "step: 686655, loss: 0.05828721821308136, data time: 0.009334301948547364\n",
      "step: 686656, loss: 0.06336238980293274, data time: 0.009104982499153383\n",
      "step: 686657, loss: 0.06364434957504272, data time: 0.008895210921764374\n",
      "step: 686658, loss: 0.06344255059957504, data time: 0.008682836185802113\n",
      "step: 686659, loss: 0.060694143176078796, data time: 0.008487610255970675\n",
      "step: 686660, loss: 0.058856792747974396, data time: 0.008300917489188058\n",
      "step: 686661, loss: 0.06603744626045227, data time: 0.008121000395880805\n",
      "step: 686662, loss: 0.05683824419975281, data time: 0.007953592248865077\n",
      "step: 686663, loss: 0.0628671795129776, data time: 0.007798753286662854\n",
      "step: 686664, loss: 0.06439593434333801, data time: 0.007650607671493139\n",
      "step: 686665, loss: 0.07255154103040695, data time: 0.007509869337081909\n",
      "step: 686666, loss: 0.06261942535638809, data time: 0.23751068115234375\n",
      "step: 686667, loss: 0.057386886328458786, data time: 0.11953198909759521\n",
      "step: 686668, loss: 0.05999046564102173, data time: 0.08094231287638347\n",
      "step: 686669, loss: 0.06552663445472717, data time: 0.06163585186004639\n",
      "step: 686670, loss: 0.05992016941308975, data time: 0.049658203125\n",
      "step: 686671, loss: 0.06283776462078094, data time: 0.041666229565938316\n",
      "step: 686672, loss: 0.05673741549253464, data time: 0.03596602167401995\n",
      "step: 686673, loss: 0.061516307294368744, data time: 0.0317709743976593\n",
      "step: 686674, loss: 0.05982175096869469, data time: 0.02842280599806044\n",
      "step: 686675, loss: 0.06163744255900383, data time: 0.02581939697265625\n",
      "step: 686676, loss: 0.0638909563422203, data time: 0.023701797832142223\n",
      "step: 686677, loss: 0.061461448669433594, data time: 0.0219385027885437\n",
      "step: 686678, loss: 0.06483376026153564, data time: 0.020450830459594727\n",
      "step: 686679, loss: 0.0611623078584671, data time: 0.01916231427873884\n",
      "step: 686680, loss: 0.058893539011478424, data time: 0.0180423895517985\n",
      "step: 686681, loss: 0.057380497455596924, data time: 0.017073452472686768\n",
      "step: 686682, loss: 0.0602090060710907, data time: 0.01620779317968032\n",
      "step: 686683, loss: 0.05861120671033859, data time: 0.015436026785108779\n",
      "step: 686684, loss: 0.0624605230987072, data time: 0.014754270252428557\n",
      "step: 686685, loss: 0.06075301393866539, data time: 0.014141976833343506\n",
      "step: 686686, loss: 0.05665411800146103, data time: 0.01358772459484282\n",
      "step: 686687, loss: 0.06824545562267303, data time: 0.013081225481900301\n",
      "step: 686688, loss: 0.05813691020011902, data time: 0.0126211228577987\n",
      "step: 686689, loss: 0.0629325658082962, data time: 0.012196709712346395\n",
      "step: 686690, loss: 0.05812516063451767, data time: 0.011797065734863282\n",
      "step: 686691, loss: 0.061447642743587494, data time: 0.01142279001382681\n",
      "step: 686692, loss: 0.06629925966262817, data time: 0.011071222799795645\n",
      "step: 686693, loss: 0.06104110926389694, data time: 0.010752524648393904\n",
      "step: 686694, loss: 0.06346770375967026, data time: 0.01045487666952199\n",
      "step: 686695, loss: 0.062075622379779816, data time: 0.010177318255106609\n",
      "step: 686696, loss: 0.06859613209962845, data time: 0.009919628020255797\n",
      "step: 686697, loss: 0.05915910750627518, data time: 0.009681880474090576\n",
      "step: 686698, loss: 0.06332635879516602, data time: 0.00944874503395774\n",
      "step: 686699, loss: 0.05736256763339043, data time: 0.009230178945204792\n",
      "step: 686700, loss: 0.06424327939748764, data time: 0.00902094841003418\n",
      "step: 686701, loss: 0.058889538049697876, data time: 0.008821553654140897\n",
      "step: 686702, loss: 0.06201435625553131, data time: 0.008634477048306851\n",
      "step: 686703, loss: 0.06054377555847168, data time: 0.008460427585401033\n",
      "step: 686704, loss: 0.06370095908641815, data time: 0.008295199809930263\n",
      "step: 686705, loss: 0.0543537437915802, data time: 0.00813826322555542\n",
      "step: 686706, loss: 0.06262537837028503, data time: 0.22731995582580566\n",
      "step: 686707, loss: 0.0584033839404583, data time: 0.11444902420043945\n",
      "step: 686708, loss: 0.06113797426223755, data time: 0.07681592305501302\n",
      "step: 686709, loss: 0.06302487105131149, data time: 0.05838024616241455\n",
      "step: 686710, loss: 0.05752262473106384, data time: 0.04698576927185059\n",
      "step: 686711, loss: 0.061255451291799545, data time: 0.03939064343770345\n",
      "step: 686712, loss: 0.055323850363492966, data time: 0.03397253581455776\n",
      "step: 686713, loss: 0.062420498579740524, data time: 0.03002128005027771\n",
      "step: 686714, loss: 0.059225037693977356, data time: 0.026836633682250977\n",
      "step: 686715, loss: 0.06830859184265137, data time: 0.024352383613586426\n",
      "step: 686716, loss: 0.0626472681760788, data time: 0.022339018908413975\n",
      "step: 686717, loss: 0.05839724466204643, data time: 0.020653148492177326\n",
      "step: 686718, loss: 0.059652552008628845, data time: 0.019231539506178636\n",
      "step: 686719, loss: 0.06602734327316284, data time: 0.018005473273141042\n",
      "step: 686720, loss: 0.06185968965291977, data time: 0.016941372553507486\n",
      "step: 686721, loss: 0.05985904857516289, data time: 0.016011416912078857\n",
      "step: 686722, loss: 0.06490696966648102, data time: 0.015197375241447897\n",
      "step: 686723, loss: 0.057097598910331726, data time: 0.014462987581888834\n",
      "step: 686724, loss: 0.06661777198314667, data time: 0.013808413555747584\n",
      "step: 686725, loss: 0.060821808874607086, data time: 0.013224911689758301\n",
      "step: 686726, loss: 0.060468219220638275, data time: 0.012698196229480561\n",
      "step: 686727, loss: 0.059940602630376816, data time: 0.012245492501692339\n",
      "step: 686728, loss: 0.06247984990477562, data time: 0.011805555094843325\n",
      "step: 686729, loss: 0.06282594799995422, data time: 0.011397719383239746\n",
      "step: 686730, loss: 0.06096109002828598, data time: 0.011024112701416016\n",
      "step: 686731, loss: 0.0640869289636612, data time: 0.010681830919705905\n",
      "step: 686732, loss: 0.06098317354917526, data time: 0.010364488319114403\n",
      "step: 686733, loss: 0.06086943671107292, data time: 0.010067726884569441\n",
      "step: 686734, loss: 0.06359536945819855, data time: 0.009795517757021147\n",
      "step: 686735, loss: 0.0625772699713707, data time: 0.00954422950744629\n",
      "step: 686736, loss: 0.06349062919616699, data time: 0.009305900143038842\n",
      "step: 686737, loss: 0.0580814927816391, data time: 0.009085424244403839\n",
      "step: 686738, loss: 0.06469152867794037, data time: 0.008868636506976503\n",
      "step: 686739, loss: 0.06359386444091797, data time: 0.008664250373840332\n",
      "step: 686740, loss: 0.05948711186647415, data time: 0.008476829528808594\n",
      "step: 686741, loss: 0.06346065551042557, data time: 0.008293059137132432\n",
      "step: 686742, loss: 0.056313153356313705, data time: 0.008120652791616079\n",
      "step: 686743, loss: 0.05893678218126297, data time: 0.00796260959223697\n",
      "step: 686744, loss: 0.06207934021949768, data time: 0.007812909590892302\n",
      "step: 686745, loss: 0.057405661791563034, data time: 0.007668709754943848\n",
      "step: 686746, loss: 0.06720702350139618, data time: 0.2290191650390625\n",
      "step: 686747, loss: 0.061034612357616425, data time: 0.11586475372314453\n",
      "step: 686748, loss: 0.0618392750620842, data time: 0.07773621877034505\n",
      "step: 686749, loss: 0.05816594883799553, data time: 0.05925363302230835\n",
      "step: 686750, loss: 0.05757413059473038, data time: 0.047690820693969724\n",
      "step: 686751, loss: 0.06206474453210831, data time: 0.03999896844228109\n",
      "step: 686752, loss: 0.05662478134036064, data time: 0.03448666845049177\n",
      "step: 686753, loss: 0.061769336462020874, data time: 0.03042188286781311\n",
      "step: 686754, loss: 0.05621975660324097, data time: 0.027189678615993924\n",
      "step: 686755, loss: 0.06834031641483307, data time: 0.02467656135559082\n",
      "step: 686756, loss: 0.06612640619277954, data time: 0.02263428948142312\n",
      "step: 686757, loss: 0.055632513016462326, data time: 0.020928104718526203\n",
      "step: 686758, loss: 0.06275726854801178, data time: 0.01948404312133789\n",
      "step: 686759, loss: 0.05725393816828728, data time: 0.01823878288269043\n",
      "step: 686760, loss: 0.06047924607992172, data time: 0.0171633243560791\n",
      "step: 686761, loss: 0.06286926567554474, data time: 0.01621946692466736\n",
      "step: 686762, loss: 0.06737824529409409, data time: 0.015386735691743739\n",
      "step: 686763, loss: 0.06267865002155304, data time: 0.01464529832204183\n",
      "step: 686764, loss: 0.06312873214483261, data time: 0.013979184000115646\n",
      "step: 686765, loss: 0.06134551018476486, data time: 0.013389408588409424\n",
      "step: 686766, loss: 0.06538670510053635, data time: 0.01285379273550851\n",
      "step: 686767, loss: 0.062193404883146286, data time: 0.012372970581054688\n",
      "step: 686768, loss: 0.06172655150294304, data time: 0.01192958458610203\n",
      "step: 686769, loss: 0.06578443944454193, data time: 0.011521538098653158\n",
      "step: 686770, loss: 0.0641494020819664, data time: 0.011143379211425782\n",
      "step: 686771, loss: 0.06168542429804802, data time: 0.010791650185218224\n",
      "step: 686772, loss: 0.062483400106430054, data time: 0.010463317235310873\n",
      "step: 686773, loss: 0.06421712040901184, data time: 0.010161025183541434\n",
      "step: 686774, loss: 0.06506447494029999, data time: 0.009885459110654634\n",
      "step: 686775, loss: 0.05823205038905144, data time: 0.009630282719930014\n",
      "step: 686776, loss: 0.06499406695365906, data time: 0.009389000554238596\n",
      "step: 686777, loss: 0.06275447458028793, data time: 0.00916597992181778\n",
      "step: 686778, loss: 0.06087764352560043, data time: 0.008949012467355438\n",
      "step: 686779, loss: 0.0605015866458416, data time: 0.008745572146247415\n",
      "step: 686780, loss: 0.060350701212882996, data time: 0.008549785614013672\n",
      "step: 686781, loss: 0.06087283417582512, data time: 0.008362935649024116\n",
      "step: 686782, loss: 0.06679457426071167, data time: 0.008189252904943517\n",
      "step: 686783, loss: 0.06522837281227112, data time: 0.008026857125131707\n",
      "step: 686784, loss: 0.06060699746012688, data time: 0.007872942166450696\n",
      "step: 686785, loss: 0.04510924965143204, data time: 0.007727271318435669\n",
      "step: 686786, loss: 0.07208985835313797, data time: 0.22911810874938965\n",
      "step: 686787, loss: 0.06456442922353745, data time: 0.11532318592071533\n",
      "step: 686788, loss: 0.07002846896648407, data time: 0.07767581939697266\n",
      "step: 686789, loss: 0.06097167730331421, data time: 0.05913698673248291\n",
      "step: 686790, loss: 0.06119714304804802, data time: 0.04761161804199219\n",
      "step: 686791, loss: 0.06150157377123833, data time: 0.03991647561391195\n",
      "step: 686792, loss: 0.06096575781702995, data time: 0.03442978858947754\n",
      "step: 686793, loss: 0.05879892036318779, data time: 0.030398279428482056\n",
      "step: 686794, loss: 0.0646960437297821, data time: 0.027172459496392146\n",
      "step: 686795, loss: 0.06173667311668396, data time: 0.024660444259643553\n",
      "step: 686796, loss: 0.06634853780269623, data time: 0.02261053432117809\n",
      "step: 686797, loss: 0.06845460832118988, data time: 0.020907044410705566\n",
      "step: 686798, loss: 0.0638561099767685, data time: 0.01946566655085637\n",
      "step: 686799, loss: 0.062351226806640625, data time: 0.018220952578953335\n",
      "step: 686800, loss: 0.062324684113264084, data time: 0.01714488665262858\n",
      "step: 686801, loss: 0.06217455491423607, data time: 0.01620808243751526\n",
      "step: 686802, loss: 0.05649548023939133, data time: 0.015378320918363683\n",
      "step: 686803, loss: 0.06319104880094528, data time: 0.014635642369588217\n",
      "step: 686804, loss: 0.0683669000864029, data time: 0.013968354777285927\n",
      "step: 686805, loss: 0.05925796553492546, data time: 0.01337825059890747\n",
      "step: 686806, loss: 0.06125013157725334, data time: 0.012845345905848913\n",
      "step: 686807, loss: 0.05726990848779678, data time: 0.012366522442210804\n",
      "step: 686808, loss: 0.062072694301605225, data time: 0.011925604032433552\n",
      "step: 686809, loss: 0.06491323560476303, data time: 0.0115184485912323\n",
      "step: 686810, loss: 0.06078493595123291, data time: 0.01114455223083496\n",
      "step: 686811, loss: 0.06143999099731445, data time: 0.010794410338768592\n",
      "step: 686812, loss: 0.0611003041267395, data time: 0.01046627539175528\n",
      "step: 686813, loss: 0.06242596358060837, data time: 0.010169650827135359\n",
      "step: 686814, loss: 0.0557357482612133, data time: 0.009900759006368703\n",
      "step: 686815, loss: 0.06295471638441086, data time: 0.009644508361816406\n",
      "step: 686816, loss: 0.06134149059653282, data time: 0.0094018982302758\n",
      "step: 686817, loss: 0.05829247832298279, data time: 0.009181186556816101\n",
      "step: 686818, loss: 0.06181657314300537, data time: 0.008961460807106712\n",
      "step: 686819, loss: 0.06008082628250122, data time: 0.008756476290085736\n",
      "step: 686820, loss: 0.06310414522886276, data time: 0.008562558037894113\n",
      "step: 686821, loss: 0.06082260608673096, data time: 0.008376167880164253\n",
      "step: 686822, loss: 0.05929870530962944, data time: 0.008202849207697687\n",
      "step: 686823, loss: 0.0616438090801239, data time: 0.008041444577668843\n",
      "step: 686824, loss: 0.05929045006632805, data time: 0.007886990522726988\n",
      "step: 686825, loss: 0.08113470673561096, data time: 0.0077409505844116214\n",
      "step: 686826, loss: 0.05737888067960739, data time: 0.23655295372009277\n",
      "step: 686827, loss: 0.06838856637477875, data time: 0.11901438236236572\n",
      "step: 686828, loss: 0.0531734973192215, data time: 0.08043233553568523\n",
      "step: 686829, loss: 0.06063837930560112, data time: 0.06097990274429321\n",
      "step: 686830, loss: 0.05926021933555603, data time: 0.049082374572753905\n",
      "step: 686831, loss: 0.060358546674251556, data time: 0.04114663600921631\n",
      "step: 686832, loss: 0.0605861060321331, data time: 0.03548189571925572\n",
      "step: 686833, loss: 0.06551770865917206, data time: 0.03130549192428589\n",
      "step: 686834, loss: 0.05768841505050659, data time: 0.02798220846388075\n",
      "step: 686835, loss: 0.07023082673549652, data time: 0.02539017200469971\n",
      "step: 686836, loss: 0.06549708545207977, data time: 0.023276437412608753\n",
      "step: 686837, loss: 0.06326789408922195, data time: 0.02151175340016683\n",
      "step: 686838, loss: 0.061424992978572845, data time: 0.020015973311204176\n",
      "step: 686839, loss: 0.05989110469818115, data time: 0.01872755799974714\n",
      "step: 686840, loss: 0.06271333992481232, data time: 0.017620007197062176\n",
      "step: 686841, loss: 0.05607287213206291, data time: 0.016663506627082825\n",
      "step: 686842, loss: 0.05645466968417168, data time: 0.015804192599128273\n",
      "step: 686843, loss: 0.06295577436685562, data time: 0.01504079500834147\n",
      "step: 686844, loss: 0.06299248337745667, data time: 0.014353124718917044\n",
      "step: 686845, loss: 0.0587492398917675, data time: 0.013739836215972901\n",
      "step: 686846, loss: 0.0674622654914856, data time: 0.013192244938441686\n",
      "step: 686847, loss: 0.06687413901090622, data time: 0.01269330761649392\n",
      "step: 686848, loss: 0.060036301612854004, data time: 0.012230074923971424\n",
      "step: 686849, loss: 0.06018262729048729, data time: 0.011810094118118286\n",
      "step: 686850, loss: 0.06415998935699463, data time: 0.01142329216003418\n",
      "step: 686851, loss: 0.05963315814733505, data time: 0.011061466657198392\n",
      "step: 686852, loss: 0.06743764132261276, data time: 0.01072606333979854\n",
      "step: 686853, loss: 0.05495581030845642, data time: 0.010414506707872664\n",
      "step: 686854, loss: 0.06156506389379501, data time: 0.010143600661179116\n",
      "step: 686855, loss: 0.06450053304433823, data time: 0.009878937403361003\n",
      "step: 686856, loss: 0.06328790634870529, data time: 0.009628019025248865\n",
      "step: 686857, loss: 0.07079477608203888, data time: 0.009398601949214935\n",
      "step: 686858, loss: 0.056198280304670334, data time: 0.00917772813276811\n",
      "step: 686859, loss: 0.06171287223696709, data time: 0.008968100828282973\n",
      "step: 686860, loss: 0.06703931093215942, data time: 0.008767448152814593\n",
      "step: 686861, loss: 0.0605202279984951, data time: 0.0085779693391588\n",
      "step: 686862, loss: 0.06581733375787735, data time: 0.008398436211250923\n",
      "step: 686863, loss: 0.06357261538505554, data time: 0.008230999896400854\n",
      "step: 686864, loss: 0.06255777180194855, data time: 0.0080720583597819\n",
      "step: 686865, loss: 0.0515906922519207, data time: 0.007920259237289428\n",
      "step: 686866, loss: 0.06529934704303741, data time: 0.22024130821228027\n",
      "step: 686867, loss: 0.06269217282533646, data time: 0.1112823486328125\n",
      "step: 686868, loss: 0.0631171241402626, data time: 0.07533685366312663\n",
      "step: 686869, loss: 0.059039946645498276, data time: 0.05731147527694702\n",
      "step: 686870, loss: 0.06256043910980225, data time: 0.04613218307495117\n",
      "step: 686871, loss: 0.060439497232437134, data time: 0.03870932261149088\n",
      "step: 686872, loss: 0.06044438108801842, data time: 0.03338827405657087\n",
      "step: 686873, loss: 0.06388205289840698, data time: 0.029476702213287354\n",
      "step: 686874, loss: 0.058287542313337326, data time: 0.026347557703653973\n",
      "step: 686875, loss: 0.06344036757946014, data time: 0.02391343116760254\n",
      "step: 686876, loss: 0.06498107314109802, data time: 0.021946473555131393\n",
      "step: 686877, loss: 0.06754162162542343, data time: 0.02030034859975179\n",
      "step: 686878, loss: 0.057524196803569794, data time: 0.01891215030963604\n",
      "step: 686879, loss: 0.06167728826403618, data time: 0.017702409199305942\n",
      "step: 686880, loss: 0.057577960193157196, data time: 0.01666599909464518\n",
      "step: 686881, loss: 0.06211160868406296, data time: 0.015771478414535522\n",
      "step: 686882, loss: 0.05822274833917618, data time: 0.014962476842543659\n",
      "step: 686883, loss: 0.05929739028215408, data time: 0.014240317874484591\n",
      "step: 686884, loss: 0.0642608180642128, data time: 0.013593937221326326\n",
      "step: 686885, loss: 0.06002787873148918, data time: 0.01302196979522705\n",
      "step: 686886, loss: 0.055065955966711044, data time: 0.01250473658243815\n",
      "step: 686887, loss: 0.05800539255142212, data time: 0.012034849687056108\n",
      "step: 686888, loss: 0.05880633741617203, data time: 0.011600691339244013\n",
      "step: 686889, loss: 0.06431468576192856, data time: 0.011209577322006226\n",
      "step: 686890, loss: 0.06601491570472717, data time: 0.010843925476074219\n",
      "step: 686891, loss: 0.05910608172416687, data time: 0.010506923382098857\n",
      "step: 686892, loss: 0.06465698778629303, data time: 0.010190389774463795\n",
      "step: 686893, loss: 0.05940118059515953, data time: 0.009898722171783447\n",
      "step: 686894, loss: 0.061751510947942734, data time: 0.009630227911061254\n",
      "step: 686895, loss: 0.06601108610630035, data time: 0.009380507469177245\n",
      "step: 686896, loss: 0.06765875220298767, data time: 0.009147021078294324\n",
      "step: 686897, loss: 0.05765646696090698, data time: 0.008935950696468353\n",
      "step: 686898, loss: 0.06068366393446922, data time: 0.008727391560872396\n",
      "step: 686899, loss: 0.06143493205308914, data time: 0.008528940817889045\n",
      "step: 686900, loss: 0.05521330237388611, data time: 0.00833960941859654\n",
      "step: 686901, loss: 0.06002151221036911, data time: 0.008159173859490288\n",
      "step: 686902, loss: 0.060986604541540146, data time: 0.00798970299798089\n",
      "step: 686903, loss: 0.05778191611170769, data time: 0.007832044049313194\n",
      "step: 686904, loss: 0.06280438601970673, data time: 0.007683466642330854\n",
      "step: 686905, loss: 0.06018915772438049, data time: 0.007541698217391968\n",
      "step: 686906, loss: 0.06674069911241531, data time: 0.22216296195983887\n",
      "step: 686907, loss: 0.0652923434972763, data time: 0.11249840259552002\n",
      "step: 686908, loss: 0.06212857365608215, data time: 0.07590572039286296\n",
      "step: 686909, loss: 0.059438809752464294, data time: 0.05770450830459595\n",
      "step: 686910, loss: 0.06298119574785233, data time: 0.04644646644592285\n",
      "step: 686911, loss: 0.05972180888056755, data time: 0.0389548142751058\n",
      "step: 686912, loss: 0.05732651799917221, data time: 0.03361201286315918\n",
      "step: 686913, loss: 0.05445685237646103, data time: 0.02966904640197754\n",
      "step: 686914, loss: 0.05871468782424927, data time: 0.026519219080607098\n",
      "step: 686915, loss: 0.06483206152915955, data time: 0.024071812629699707\n",
      "step: 686916, loss: 0.060168858617544174, data time: 0.022075024518099697\n",
      "step: 686917, loss: 0.06382878124713898, data time: 0.020415306091308594\n",
      "step: 686918, loss: 0.06571779400110245, data time: 0.019010525483351488\n",
      "step: 686919, loss: 0.06252408027648926, data time: 0.017809373991830007\n",
      "step: 686920, loss: 0.06141158938407898, data time: 0.016763893763224284\n",
      "step: 686921, loss: 0.06055423617362976, data time: 0.015857383608818054\n",
      "step: 686922, loss: 0.06264964491128922, data time: 0.015043118420769186\n",
      "step: 686923, loss: 0.05807463079690933, data time: 0.014314081933763292\n",
      "step: 686924, loss: 0.06117071956396103, data time: 0.01366659214622096\n",
      "step: 686925, loss: 0.06941789388656616, data time: 0.013092637062072754\n",
      "step: 686926, loss: 0.06250204890966415, data time: 0.012577420189267113\n",
      "step: 686927, loss: 0.058138564229011536, data time: 0.012104630470275879\n",
      "step: 686928, loss: 0.06598630547523499, data time: 0.011667054632435675\n",
      "step: 686929, loss: 0.05777720734477043, data time: 0.011271357536315918\n",
      "step: 686930, loss: 0.06247090548276901, data time: 0.0109075927734375\n",
      "step: 686931, loss: 0.06191229820251465, data time: 0.010568031897911659\n",
      "step: 686932, loss: 0.06306011974811554, data time: 0.01024990611606174\n",
      "step: 686933, loss: 0.06432618200778961, data time: 0.00995635986328125\n",
      "step: 686934, loss: 0.06566061824560165, data time: 0.00968672489297801\n",
      "step: 686935, loss: 0.06452181935310364, data time: 0.009435311953226725\n",
      "step: 686936, loss: 0.058233700692653656, data time: 0.009199057855913717\n",
      "step: 686937, loss: 0.06106744706630707, data time: 0.008981525897979736\n",
      "step: 686938, loss: 0.07117073982954025, data time: 0.008770913788766571\n",
      "step: 686939, loss: 0.06335955113172531, data time: 0.008577543146470013\n",
      "step: 686940, loss: 0.062157124280929565, data time: 0.008390787669590541\n",
      "step: 686941, loss: 0.06157311797142029, data time: 0.008213327990637885\n",
      "step: 686942, loss: 0.06494464725255966, data time: 0.008047593606484903\n",
      "step: 686943, loss: 0.06843823194503784, data time: 0.007891855741802015\n",
      "step: 686944, loss: 0.05701379105448723, data time: 0.007744538478362255\n",
      "step: 686945, loss: 0.06668397039175034, data time: 0.007605147361755371\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40911054611206055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 686946, loss: 0.05823011323809624, data time: 0.22661066055297852\n",
      "step: 686947, loss: 0.0630728155374527, data time: 0.11404836177825928\n",
      "step: 686948, loss: 0.06292938441038132, data time: 0.0768124262491862\n",
      "step: 686949, loss: 0.06193699687719345, data time: 0.05847960710525513\n",
      "step: 686950, loss: 0.059832774102687836, data time: 0.047057151794433594\n",
      "step: 686951, loss: 0.060240745544433594, data time: 0.03945342699686686\n",
      "step: 686952, loss: 0.06821621209383011, data time: 0.034019333975655694\n",
      "step: 686953, loss: 0.055741842836141586, data time: 0.030015498399734497\n",
      "step: 686954, loss: 0.06864960491657257, data time: 0.026820394727918837\n",
      "step: 686955, loss: 0.06355404853820801, data time: 0.0243699312210083\n",
      "step: 686956, loss: 0.06430663168430328, data time: 0.022345326163552025\n",
      "step: 686957, loss: 0.06765174865722656, data time: 0.020660380522410076\n",
      "step: 686958, loss: 0.061275482177734375, data time: 0.019231356107271634\n",
      "step: 686959, loss: 0.060031913220882416, data time: 0.017997707639421736\n",
      "step: 686960, loss: 0.06520421802997589, data time: 0.016939957936604817\n",
      "step: 686961, loss: 0.06032589077949524, data time: 0.016005203127861023\n",
      "step: 686962, loss: 0.05983434244990349, data time: 0.015182186575496899\n",
      "step: 686963, loss: 0.0618547648191452, data time: 0.014445397588941786\n",
      "step: 686964, loss: 0.06817024946212769, data time: 0.013787545655903063\n",
      "step: 686965, loss: 0.059674959629774094, data time: 0.013207125663757324\n",
      "step: 686966, loss: 0.06205028295516968, data time: 0.012684288479032971\n",
      "step: 686967, loss: 0.061485420912504196, data time: 0.01220382343639027\n",
      "step: 686968, loss: 0.05873357877135277, data time: 0.011759177498195482\n",
      "step: 686969, loss: 0.06332769989967346, data time: 0.011352558930714926\n",
      "step: 686970, loss: 0.06192940101027489, data time: 0.010981988906860352\n",
      "step: 686971, loss: 0.058077242225408554, data time: 0.010638860555795522\n",
      "step: 686972, loss: 0.06365576386451721, data time: 0.010316548524079499\n",
      "step: 686973, loss: 0.0665416270494461, data time: 0.010026429380689348\n",
      "step: 686974, loss: 0.05818040296435356, data time: 0.009767203495420259\n",
      "step: 686975, loss: 0.0570230633020401, data time: 0.009525275230407715\n",
      "step: 686976, loss: 0.06351682543754578, data time: 0.009296394163562406\n",
      "step: 686977, loss: 0.06024641543626785, data time: 0.009085409343242645\n",
      "step: 686978, loss: 0.056619711220264435, data time: 0.008872819669318922\n",
      "step: 686979, loss: 0.06117301806807518, data time: 0.008671220611123479\n",
      "step: 686980, loss: 0.06765257567167282, data time: 0.008482749121529715\n",
      "step: 686981, loss: 0.060047995299100876, data time: 0.008311748504638672\n",
      "step: 686982, loss: 0.060161709785461426, data time: 0.008140944145821236\n",
      "step: 686983, loss: 0.06790461391210556, data time: 0.007983678265621788\n",
      "step: 686984, loss: 0.06343622505664825, data time: 0.007835180331499148\n",
      "step: 686985, loss: 0.07127327471971512, data time: 0.007692962884902954\n",
      "step: 686986, loss: 0.060707733035087585, data time: 0.22968316078186035\n",
      "step: 686987, loss: 0.06724047660827637, data time: 0.11559224128723145\n",
      "step: 686988, loss: 0.062238067388534546, data time: 0.0775894323984782\n",
      "step: 686989, loss: 0.05948827415704727, data time: 0.05905282497406006\n",
      "step: 686990, loss: 0.0691288411617279, data time: 0.04752159118652344\n",
      "step: 686991, loss: 0.06331682205200195, data time: 0.039841016133626304\n",
      "step: 686992, loss: 0.05911839008331299, data time: 0.03434617178780692\n",
      "step: 686993, loss: 0.06279071420431137, data time: 0.030300617218017578\n",
      "step: 686994, loss: 0.05911686643958092, data time: 0.02708501285976834\n",
      "step: 686995, loss: 0.06186423823237419, data time: 0.024574875831604004\n",
      "step: 686996, loss: 0.06120780110359192, data time: 0.022541154514659534\n",
      "step: 686997, loss: 0.06334372609853745, data time: 0.02084501584370931\n",
      "step: 686998, loss: 0.057483769953250885, data time: 0.01940774917602539\n",
      "step: 686999, loss: 0.0593193881213665, data time: 0.018163510731288364\n",
      "step: 687000, loss: 0.05998115986585617, data time: 0.017087841033935548\n",
      "step: 687001, loss: 0.06509533524513245, data time: 0.016145452857017517\n",
      "step: 687002, loss: 0.06486324965953827, data time: 0.015314901576322667\n",
      "step: 687003, loss: 0.05709237977862358, data time: 0.014574898613823785\n",
      "step: 687004, loss: 0.06057112663984299, data time: 0.013911435478611997\n",
      "step: 687005, loss: 0.05546816438436508, data time: 0.013322460651397704\n",
      "step: 687006, loss: 0.05870768427848816, data time: 0.012789033708118257\n",
      "step: 687007, loss: 0.06463322043418884, data time: 0.012305508960377087\n",
      "step: 687008, loss: 0.061909448355436325, data time: 0.01185750961303711\n",
      "step: 687009, loss: 0.05623449385166168, data time: 0.011449257532755533\n",
      "step: 687010, loss: 0.06693420559167862, data time: 0.011072654724121094\n",
      "step: 687011, loss: 0.065260149538517, data time: 0.010724617884709286\n",
      "step: 687012, loss: 0.06360359489917755, data time: 0.010398184811627423\n",
      "step: 687013, loss: 0.06475543975830078, data time: 0.010096694741930281\n",
      "step: 687014, loss: 0.0610862672328949, data time: 0.009822886565635944\n",
      "step: 687015, loss: 0.06350677460432053, data time: 0.009569382667541504\n",
      "step: 687016, loss: 0.06064338609576225, data time: 0.00933397969891948\n",
      "step: 687017, loss: 0.06540700793266296, data time: 0.009113192558288574\n",
      "step: 687018, loss: 0.0639394149184227, data time: 0.008897174488414417\n",
      "step: 687019, loss: 0.06289342045783997, data time: 0.008690469405230354\n",
      "step: 687020, loss: 0.060298722237348557, data time: 0.008495875767299108\n",
      "step: 687021, loss: 0.05845654010772705, data time: 0.008311072985331217\n",
      "step: 687022, loss: 0.058462344110012054, data time: 0.008138121785344305\n",
      "step: 687023, loss: 0.061705201864242554, data time: 0.007978865974827817\n",
      "step: 687024, loss: 0.06004801020026207, data time: 0.007828095020392003\n",
      "step: 687025, loss: 0.05574437975883484, data time: 0.007683169841766357\n",
      "step: 687026, loss: 0.05998304486274719, data time: 0.21517324447631836\n",
      "step: 687027, loss: 0.060177139937877655, data time: 0.10871875286102295\n",
      "step: 687028, loss: 0.06466808915138245, data time: 0.07361904780069987\n",
      "step: 687029, loss: 0.05585780739784241, data time: 0.05591714382171631\n",
      "step: 687030, loss: 0.0619206577539444, data time: 0.04503102302551269\n",
      "step: 687031, loss: 0.0681503638625145, data time: 0.03778278827667236\n",
      "step: 687032, loss: 0.058854322880506516, data time: 0.03258255549839565\n",
      "step: 687033, loss: 0.06730607151985168, data time: 0.02875545620918274\n",
      "step: 687034, loss: 0.05800453945994377, data time: 0.02570297982957628\n",
      "step: 687035, loss: 0.052653394639492035, data time: 0.02333528995513916\n",
      "step: 687036, loss: 0.06825689971446991, data time: 0.021407799287275833\n",
      "step: 687037, loss: 0.05883566662669182, data time: 0.019818464914957683\n",
      "step: 687038, loss: 0.06124424189329147, data time: 0.018465684010432318\n",
      "step: 687039, loss: 0.05690277740359306, data time: 0.017290166446140835\n",
      "step: 687040, loss: 0.06034993380308151, data time: 0.016273657480875652\n",
      "step: 687041, loss: 0.06649982929229736, data time: 0.015385150909423828\n",
      "step: 687042, loss: 0.06595195829868317, data time: 0.01459483539356905\n",
      "step: 687043, loss: 0.05827825516462326, data time: 0.013891802893744575\n",
      "step: 687044, loss: 0.0639839619398117, data time: 0.013270227532637747\n",
      "step: 687045, loss: 0.0618211030960083, data time: 0.012713229656219483\n",
      "step: 687046, loss: 0.057515840977430344, data time: 0.01221080053420294\n",
      "step: 687047, loss: 0.059702590107917786, data time: 0.011760570786216042\n",
      "step: 687048, loss: 0.06479312479496002, data time: 0.011337041854858398\n",
      "step: 687049, loss: 0.06305298954248428, data time: 0.010949671268463135\n",
      "step: 687050, loss: 0.06126614660024643, data time: 0.010591421127319336\n",
      "step: 687051, loss: 0.0560760498046875, data time: 0.010258362843440129\n",
      "step: 687052, loss: 0.060081660747528076, data time: 0.009949622330842194\n",
      "step: 687053, loss: 0.06418749690055847, data time: 0.009668733392442976\n",
      "step: 687054, loss: 0.05979589745402336, data time: 0.009410545743744949\n",
      "step: 687055, loss: 0.0589904859662056, data time: 0.009174656867980958\n",
      "step: 687056, loss: 0.0658918023109436, data time: 0.00894751856403966\n",
      "step: 687057, loss: 0.06385686248540878, data time: 0.008738100528717041\n",
      "step: 687058, loss: 0.06103233993053436, data time: 0.008531592108986595\n",
      "step: 687059, loss: 0.06656347960233688, data time: 0.008336740381577435\n",
      "step: 687060, loss: 0.0675601065158844, data time: 0.008151238305228098\n",
      "step: 687061, loss: 0.05919879302382469, data time: 0.007981558640797934\n",
      "step: 687062, loss: 0.061541326344013214, data time: 0.007821411699862094\n",
      "step: 687063, loss: 0.06124222278594971, data time: 0.0076709922991300885\n",
      "step: 687064, loss: 0.055935055017471313, data time: 0.007529827264639048\n",
      "step: 687065, loss: 0.09548449516296387, data time: 0.00739600658416748\n",
      "step: 687066, loss: 0.06553655117750168, data time: 0.22797083854675293\n",
      "step: 687067, loss: 0.05887695401906967, data time: 0.11474728584289551\n",
      "step: 687068, loss: 0.06403892487287521, data time: 0.0770260492960612\n",
      "step: 687069, loss: 0.057128146290779114, data time: 0.058618009090423584\n",
      "step: 687070, loss: 0.05959586799144745, data time: 0.047173929214477536\n",
      "step: 687071, loss: 0.06023240089416504, data time: 0.039546966552734375\n",
      "step: 687072, loss: 0.055947285145521164, data time: 0.034092835017613\n",
      "step: 687073, loss: 0.06214107200503349, data time: 0.030092567205429077\n",
      "step: 687074, loss: 0.06486968696117401, data time: 0.026903920703464083\n",
      "step: 687075, loss: 0.05818721652030945, data time: 0.024410200119018555\n",
      "step: 687076, loss: 0.06856332719326019, data time: 0.02239968559958718\n",
      "step: 687077, loss: 0.06340284645557404, data time: 0.02071450153986613\n",
      "step: 687078, loss: 0.06504425406455994, data time: 0.01928883332472581\n",
      "step: 687079, loss: 0.05965419113636017, data time: 0.01805525166647775\n",
      "step: 687080, loss: 0.06389736384153366, data time: 0.016992568969726562\n",
      "step: 687081, loss: 0.06323497742414474, data time: 0.016061440110206604\n",
      "step: 687082, loss: 0.056913744658231735, data time: 0.015244582120110007\n",
      "step: 687083, loss: 0.0632355660200119, data time: 0.014510220951504178\n",
      "step: 687084, loss: 0.05421124026179314, data time: 0.013850061517012747\n",
      "step: 687085, loss: 0.059550873935222626, data time: 0.013265836238861083\n",
      "step: 687086, loss: 0.06160496920347214, data time: 0.012736865452357702\n",
      "step: 687087, loss: 0.05926027521491051, data time: 0.012260491197759455\n",
      "step: 687088, loss: 0.06577295064926147, data time: 0.011815371720687202\n",
      "step: 687089, loss: 0.05851499363780022, data time: 0.01141202449798584\n",
      "step: 687090, loss: 0.06332149356603622, data time: 0.011037931442260743\n",
      "step: 687091, loss: 0.06339661777019501, data time: 0.01069331169128418\n",
      "step: 687092, loss: 0.0644010603427887, data time: 0.010370263346919307\n",
      "step: 687093, loss: 0.06311634182929993, data time: 0.010070349488939558\n",
      "step: 687094, loss: 0.06515108048915863, data time: 0.009798329451988483\n",
      "step: 687095, loss: 0.06040220707654953, data time: 0.009548099835713704\n",
      "step: 687096, loss: 0.060626205056905746, data time: 0.009309430276193926\n",
      "step: 687097, loss: 0.06294746696949005, data time: 0.009097635746002197\n",
      "step: 687098, loss: 0.058206282556056976, data time: 0.008880420164628462\n",
      "step: 687099, loss: 0.05727703124284744, data time: 0.008675224640790154\n",
      "step: 687100, loss: 0.06090449541807175, data time: 0.008483389445713588\n",
      "step: 687101, loss: 0.06362505257129669, data time: 0.008299218283759223\n",
      "step: 687102, loss: 0.06331665068864822, data time: 0.008126426387477565\n",
      "step: 687103, loss: 0.06061612814664841, data time: 0.007967057980989156\n",
      "step: 687104, loss: 0.06273793429136276, data time: 0.00781564223460662\n",
      "step: 687105, loss: 0.03665990009903908, data time: 0.007672125101089477\n",
      "step: 687106, loss: 0.06360342353582382, data time: 0.22594380378723145\n",
      "step: 687107, loss: 0.0671168640255928, data time: 0.11371934413909912\n",
      "step: 687108, loss: 0.06347239017486572, data time: 0.07631882031758626\n",
      "step: 687109, loss: 0.057993084192276, data time: 0.058306992053985596\n",
      "step: 687110, loss: 0.06448819488286972, data time: 0.046975898742675784\n",
      "step: 687111, loss: 0.05994860455393791, data time: 0.03944361209869385\n",
      "step: 687112, loss: 0.05737326666712761, data time: 0.03405560765947614\n",
      "step: 687113, loss: 0.06238694116473198, data time: 0.030102670192718506\n",
      "step: 687114, loss: 0.06189405918121338, data time: 0.026930120256212022\n",
      "step: 687115, loss: 0.06897392123937607, data time: 0.024478745460510255\n",
      "step: 687116, loss: 0.05917530879378319, data time: 0.022483630613847214\n",
      "step: 687117, loss: 0.05520922690629959, data time: 0.020815908908843994\n",
      "step: 687118, loss: 0.0609770342707634, data time: 0.019411288774930514\n",
      "step: 687119, loss: 0.06046126037836075, data time: 0.01819157600402832\n",
      "step: 687120, loss: 0.06334191560745239, data time: 0.017142057418823242\n",
      "step: 687121, loss: 0.05937547609210014, data time: 0.01622864603996277\n",
      "step: 687122, loss: 0.06774650514125824, data time: 0.015414167852962719\n",
      "step: 687123, loss: 0.06211477518081665, data time: 0.014687286482916938\n",
      "step: 687124, loss: 0.061897408217191696, data time: 0.0140446989159835\n",
      "step: 687125, loss: 0.053259432315826416, data time: 0.01346745491027832\n",
      "step: 687126, loss: 0.06502845138311386, data time: 0.012947445824032738\n",
      "step: 687127, loss: 0.05745997279882431, data time: 0.01247174089605158\n",
      "step: 687128, loss: 0.0550326332449913, data time: 0.012032083843065344\n",
      "step: 687129, loss: 0.06092998385429382, data time: 0.011633813381195068\n",
      "step: 687130, loss: 0.0641985759139061, data time: 0.011266088485717774\n",
      "step: 687131, loss: 0.060482751578092575, data time: 0.010924531863285946\n",
      "step: 687132, loss: 0.06257189810276031, data time: 0.010605255762736002\n",
      "step: 687133, loss: 0.06009257957339287, data time: 0.010312148502894811\n",
      "step: 687134, loss: 0.06255068629980087, data time: 0.010043341538001752\n",
      "step: 687135, loss: 0.05968516319990158, data time: 0.00979320208231608\n",
      "step: 687136, loss: 0.05665285885334015, data time: 0.009558777655324629\n",
      "step: 687137, loss: 0.05887695401906967, data time: 0.009339794516563416\n",
      "step: 687138, loss: 0.0631488561630249, data time: 0.00911823186007413\n",
      "step: 687139, loss: 0.06470145285129547, data time: 0.0089111608617446\n",
      "step: 687140, loss: 0.0680631548166275, data time: 0.0087144102369036\n",
      "step: 687141, loss: 0.06615124642848969, data time: 0.00852611329820421\n",
      "step: 687142, loss: 0.06195564568042755, data time: 0.008350301433253932\n",
      "step: 687143, loss: 0.056832265108823776, data time: 0.008187306554693925\n",
      "step: 687144, loss: 0.060072265565395355, data time: 0.008033990859985352\n",
      "step: 687145, loss: 0.05661835893988609, data time: 0.007888638973236084\n",
      "step: 687146, loss: 0.0517301969230175, data time: 0.23552417755126953\n",
      "step: 687147, loss: 0.06238561123609543, data time: 0.11912894248962402\n",
      "step: 687148, loss: 0.06243350729346275, data time: 0.08033108711242676\n",
      "step: 687149, loss: 0.0650082677602768, data time: 0.0610317587852478\n",
      "step: 687150, loss: 0.06490721553564072, data time: 0.04911365509033203\n",
      "step: 687151, loss: 0.06801776587963104, data time: 0.0411832332611084\n",
      "step: 687152, loss: 0.06276491284370422, data time: 0.035492897033691406\n",
      "step: 687153, loss: 0.06160437688231468, data time: 0.03131002187728882\n",
      "step: 687154, loss: 0.060802798718214035, data time: 0.02797828780280219\n",
      "step: 687155, loss: 0.06627794355154037, data time: 0.025388598442077637\n",
      "step: 687156, loss: 0.06731769442558289, data time: 0.02329035238786177\n",
      "step: 687157, loss: 0.06064596027135849, data time: 0.02153170108795166\n",
      "step: 687158, loss: 0.06771482527256012, data time: 0.020041209000807542\n",
      "step: 687159, loss: 0.06386202573776245, data time: 0.018760391644069126\n",
      "step: 687160, loss: 0.06292609125375748, data time: 0.017648744583129882\n",
      "step: 687161, loss: 0.056398577988147736, data time: 0.01667456328868866\n",
      "step: 687162, loss: 0.05824577435851097, data time: 0.015816253774306354\n",
      "step: 687163, loss: 0.06266961991786957, data time: 0.01504871580335829\n",
      "step: 687164, loss: 0.06115657091140747, data time: 0.014361770529496042\n",
      "step: 687165, loss: 0.06545620411634445, data time: 0.013754022121429444\n",
      "step: 687166, loss: 0.05902896448969841, data time: 0.013202678589593796\n",
      "step: 687167, loss: 0.06219300255179405, data time: 0.012697989290410822\n",
      "step: 687168, loss: 0.06412860751152039, data time: 0.012236843938412874\n",
      "step: 687169, loss: 0.06096299737691879, data time: 0.011819332838058472\n",
      "step: 687170, loss: 0.0647595226764679, data time: 0.011432952880859375\n",
      "step: 687171, loss: 0.06353592127561569, data time: 0.011070654942439152\n",
      "step: 687172, loss: 0.06556083261966705, data time: 0.01073349846733941\n",
      "step: 687173, loss: 0.06054089590907097, data time: 0.010422255311693465\n",
      "step: 687174, loss: 0.061810802668333054, data time: 0.010136464546466696\n",
      "step: 687175, loss: 0.06398078054189682, data time: 0.009873247146606446\n",
      "step: 687176, loss: 0.06012621521949768, data time: 0.009630426283805602\n",
      "step: 687177, loss: 0.06149589270353317, data time: 0.009402163326740265\n",
      "step: 687178, loss: 0.06138373538851738, data time: 0.009179216442686138\n",
      "step: 687179, loss: 0.0641547366976738, data time: 0.008964762968175551\n",
      "step: 687180, loss: 0.06556273996829987, data time: 0.008764157976422991\n",
      "step: 687181, loss: 0.06075334548950195, data time: 0.008571366469065348\n",
      "step: 687182, loss: 0.06155683845281601, data time: 0.008391464078748549\n",
      "step: 687183, loss: 0.06549681723117828, data time: 0.008224775916651675\n",
      "step: 687184, loss: 0.06765009462833405, data time: 0.008066018422444662\n",
      "step: 687185, loss: 0.05270588397979736, data time: 0.007914876937866211\n",
      "step: 687186, loss: 0.05934842675924301, data time: 0.2213587760925293\n",
      "step: 687187, loss: 0.0570334792137146, data time: 0.112296462059021\n",
      "step: 687188, loss: 0.0663246363401413, data time: 0.07537325223286946\n",
      "step: 687189, loss: 0.06513484567403793, data time: 0.057414233684539795\n",
      "step: 687190, loss: 0.06075933575630188, data time: 0.046212291717529295\n",
      "step: 687191, loss: 0.06016191840171814, data time: 0.03876880804697672\n",
      "step: 687192, loss: 0.06108994409441948, data time: 0.03342829431806292\n",
      "step: 687193, loss: 0.057878151535987854, data time: 0.0295160710811615\n",
      "step: 687194, loss: 0.06439729034900665, data time: 0.026382605234781902\n",
      "step: 687195, loss: 0.06245875358581543, data time: 0.02394397258758545\n",
      "step: 687196, loss: 0.06300137937068939, data time: 0.021961840716275303\n",
      "step: 687197, loss: 0.06284020096063614, data time: 0.02031618356704712\n",
      "step: 687198, loss: 0.06177888065576553, data time: 0.018927720876840446\n",
      "step: 687199, loss: 0.057645611464977264, data time: 0.017722879137311662\n",
      "step: 687200, loss: 0.06111612170934677, data time: 0.016687726974487303\n",
      "step: 687201, loss: 0.06011020764708519, data time: 0.015771105885505676\n",
      "step: 687202, loss: 0.057348765432834625, data time: 0.014975730110617244\n",
      "step: 687203, loss: 0.06563133001327515, data time: 0.01425525877210829\n",
      "step: 687204, loss: 0.05893540382385254, data time: 0.013612094678376851\n",
      "step: 687205, loss: 0.060916684567928314, data time: 0.013040256500244141\n",
      "step: 687206, loss: 0.05978962033987045, data time: 0.012525456292288644\n",
      "step: 687207, loss: 0.05919719859957695, data time: 0.012053944847800514\n",
      "step: 687208, loss: 0.062330082058906555, data time: 0.011616489161615786\n",
      "step: 687209, loss: 0.0631265938282013, data time: 0.01122249166170756\n",
      "step: 687210, loss: 0.06390484422445297, data time: 0.010852766036987305\n",
      "step: 687211, loss: 0.0564604327082634, data time: 0.010516010797940768\n",
      "step: 687212, loss: 0.05797497183084488, data time: 0.01020822701630769\n",
      "step: 687213, loss: 0.06111125275492668, data time: 0.009915922369275774\n",
      "step: 687214, loss: 0.062344666570425034, data time: 0.009649210962755927\n",
      "step: 687215, loss: 0.061158835887908936, data time: 0.009398587544759114\n",
      "step: 687216, loss: 0.06083406135439873, data time: 0.009163579633159022\n",
      "step: 687217, loss: 0.0691080093383789, data time: 0.008947610855102539\n",
      "step: 687218, loss: 0.06021348387002945, data time: 0.008736964428063595\n",
      "step: 687219, loss: 0.06862135231494904, data time: 0.008536990951089299\n",
      "step: 687220, loss: 0.0595795139670372, data time: 0.008350276947021484\n",
      "step: 687221, loss: 0.06455772370100021, data time: 0.008170339796278212\n",
      "step: 687222, loss: 0.06379830837249756, data time: 0.00800213942656646\n",
      "step: 687223, loss: 0.06562075018882751, data time: 0.007844956297623483\n",
      "step: 687224, loss: 0.067479208111763, data time: 0.007695607649974334\n",
      "step: 687225, loss: 0.048899583518505096, data time: 0.007553869485855102\n",
      "step: 687226, loss: 0.05788447707891464, data time: 0.22399210929870605\n",
      "step: 687227, loss: 0.06726523488759995, data time: 0.11274302005767822\n",
      "step: 687228, loss: 0.06001787632703781, data time: 0.0761423110961914\n",
      "step: 687229, loss: 0.061465393751859665, data time: 0.057880520820617676\n",
      "step: 687230, loss: 0.06568125635385513, data time: 0.04657812118530273\n",
      "step: 687231, loss: 0.05721330642700195, data time: 0.03904648621877035\n",
      "step: 687232, loss: 0.05187243968248367, data time: 0.0336782591683524\n",
      "step: 687233, loss: 0.06305639445781708, data time: 0.0297698974609375\n",
      "step: 687234, loss: 0.06655068695545197, data time: 0.02660928832160102\n",
      "step: 687235, loss: 0.0644490122795105, data time: 0.024149036407470702\n",
      "step: 687236, loss: 0.05603783577680588, data time: 0.02215764739296653\n",
      "step: 687237, loss: 0.06381524354219437, data time: 0.020489195982615154\n",
      "step: 687238, loss: 0.06126081198453903, data time: 0.019087186226477988\n",
      "step: 687239, loss: 0.05915072560310364, data time: 0.017866270882742747\n",
      "step: 687240, loss: 0.06404800713062286, data time: 0.016809399922688803\n",
      "step: 687241, loss: 0.05280415341258049, data time: 0.01589179039001465\n",
      "step: 687242, loss: 0.06337814033031464, data time: 0.015075122608857997\n",
      "step: 687243, loss: 0.06273149698972702, data time: 0.014348546663920084\n",
      "step: 687244, loss: 0.06334706395864487, data time: 0.013697548916465357\n",
      "step: 687245, loss: 0.060016240924596786, data time: 0.013122260570526123\n",
      "step: 687246, loss: 0.0602298378944397, data time: 0.012601670764741443\n",
      "step: 687247, loss: 0.06257016956806183, data time: 0.0121265324679288\n",
      "step: 687248, loss: 0.05888668820261955, data time: 0.01168754826421323\n",
      "step: 687249, loss: 0.06534507125616074, data time: 0.011284957329432169\n",
      "step: 687250, loss: 0.06277692317962646, data time: 0.01091963768005371\n",
      "step: 687251, loss: 0.058799754828214645, data time: 0.01057885243342473\n",
      "step: 687252, loss: 0.061833761632442474, data time: 0.010259681277804904\n",
      "step: 687253, loss: 0.0634370818734169, data time: 0.009964185101645333\n",
      "step: 687254, loss: 0.06485678255558014, data time: 0.009694658476730874\n",
      "step: 687255, loss: 0.057807933539152145, data time: 0.009443720181783041\n",
      "step: 687256, loss: 0.06266934424638748, data time: 0.009209640564457063\n",
      "step: 687257, loss: 0.060763947665691376, data time: 0.008992046117782593\n",
      "step: 687258, loss: 0.05367989465594292, data time: 0.008777538935343424\n",
      "step: 687259, loss: 0.060512930154800415, data time: 0.008578209316029269\n",
      "step: 687260, loss: 0.060761820524930954, data time: 0.008389472961425781\n",
      "step: 687261, loss: 0.058091044425964355, data time: 0.008207996686299643\n",
      "step: 687262, loss: 0.0647595226764679, data time: 0.008038321056881466\n",
      "step: 687263, loss: 0.05984995886683464, data time: 0.007881221018339457\n",
      "step: 687264, loss: 0.06820674985647202, data time: 0.007731425456511669\n",
      "step: 687265, loss: 0.04437355324625969, data time: 0.007589477300643921\n",
      "step: 687266, loss: 0.05721474811434746, data time: 0.22292661666870117\n",
      "step: 687267, loss: 0.061968546360731125, data time: 0.11267805099487305\n",
      "step: 687268, loss: 0.06292552500963211, data time: 0.0756210486094157\n",
      "step: 687269, loss: 0.055307190865278244, data time: 0.05768418312072754\n",
      "step: 687270, loss: 0.06463606655597687, data time: 0.046425247192382814\n",
      "step: 687271, loss: 0.06332963705062866, data time: 0.03892199198404948\n",
      "step: 687272, loss: 0.06068689748644829, data time: 0.03356238773890904\n",
      "step: 687273, loss: 0.06042465567588806, data time: 0.02964150905609131\n",
      "step: 687274, loss: 0.05834221467375755, data time: 0.026491244633992512\n",
      "step: 687275, loss: 0.062219660729169846, data time: 0.024058151245117187\n",
      "step: 687276, loss: 0.06343632191419601, data time: 0.02206954089078036\n",
      "step: 687277, loss: 0.06490569561719894, data time: 0.020412544409434002\n",
      "step: 687278, loss: 0.06174846738576889, data time: 0.019007297662588265\n",
      "step: 687279, loss: 0.05431085824966431, data time: 0.01779001099722726\n",
      "step: 687280, loss: 0.057682693004608154, data time: 0.0167385737101237\n",
      "step: 687281, loss: 0.060541145503520966, data time: 0.01582266390323639\n",
      "step: 687282, loss: 0.058103740215301514, data time: 0.015024072983685662\n",
      "step: 687283, loss: 0.05482834205031395, data time: 0.014297339651319716\n",
      "step: 687284, loss: 0.0628138929605484, data time: 0.013655461763080797\n",
      "step: 687285, loss: 0.06384493410587311, data time: 0.013078224658966065\n",
      "step: 687286, loss: 0.0633152648806572, data time: 0.012558244523547944\n",
      "step: 687287, loss: 0.061199527233839035, data time: 0.012084776704961603\n",
      "step: 687288, loss: 0.06239887326955795, data time: 0.011651795843373175\n",
      "step: 687289, loss: 0.0659896582365036, data time: 0.011256436506907145\n",
      "step: 687290, loss: 0.0616031214594841, data time: 0.010893449783325196\n",
      "step: 687291, loss: 0.057585716247558594, data time: 0.010556294367863582\n",
      "step: 687292, loss: 0.060741424560546875, data time: 0.010237552501537182\n",
      "step: 687293, loss: 0.06763891875743866, data time: 0.009947478771209717\n",
      "step: 687294, loss: 0.05986017733812332, data time: 0.009680558895242625\n",
      "step: 687295, loss: 0.05806514248251915, data time: 0.009431958198547363\n",
      "step: 687296, loss: 0.0649946928024292, data time: 0.009197850381174395\n",
      "step: 687297, loss: 0.06203769892454147, data time: 0.008980892598628998\n",
      "step: 687298, loss: 0.06231405586004257, data time: 0.008766773975256718\n",
      "step: 687299, loss: 0.06248344108462334, data time: 0.008563960299772374\n",
      "step: 687300, loss: 0.055380403995513916, data time: 0.008379037039620535\n",
      "step: 687301, loss: 0.058160196989774704, data time: 0.008196804258558486\n",
      "step: 687302, loss: 0.06939631700515747, data time: 0.00802692851504764\n",
      "step: 687303, loss: 0.06275276839733124, data time: 0.00787235561170076\n",
      "step: 687304, loss: 0.05810501426458359, data time: 0.007726076321724134\n",
      "step: 687305, loss: 0.07881169021129608, data time: 0.0075874865055084225\n",
      "step: 687306, loss: 0.06766438484191895, data time: 0.22754740715026855\n",
      "step: 687307, loss: 0.06103751063346863, data time: 0.11458301544189453\n",
      "step: 687308, loss: 0.06449773162603378, data time: 0.07689364751180013\n",
      "step: 687309, loss: 0.061870019882917404, data time: 0.058489859104156494\n",
      "step: 687310, loss: 0.06145632266998291, data time: 0.04707646369934082\n",
      "step: 687311, loss: 0.06032624840736389, data time: 0.03946908315022787\n",
      "step: 687312, loss: 0.05697622895240784, data time: 0.03403520584106445\n",
      "step: 687313, loss: 0.06319817155599594, data time: 0.030042380094528198\n",
      "step: 687314, loss: 0.05793263763189316, data time: 0.02685403823852539\n",
      "step: 687315, loss: 0.06220581755042076, data time: 0.024364542961120606\n",
      "step: 687316, loss: 0.05712982267141342, data time: 0.02234385230324485\n",
      "step: 687317, loss: 0.06968915462493896, data time: 0.02066085735956828\n",
      "step: 687318, loss: 0.06123960018157959, data time: 0.019235867720383864\n",
      "step: 687319, loss: 0.0588759109377861, data time: 0.018004332269941057\n",
      "step: 687320, loss: 0.06320735812187195, data time: 0.016938416163126628\n",
      "step: 687321, loss: 0.0651489794254303, data time: 0.016003847122192383\n",
      "step: 687322, loss: 0.06068209558725357, data time: 0.015186506159165326\n",
      "step: 687323, loss: 0.0638379454612732, data time: 0.014461874961853027\n",
      "step: 687324, loss: 0.05552249401807785, data time: 0.013811186740272924\n",
      "step: 687325, loss: 0.05904463678598404, data time: 0.01322767734527588\n",
      "step: 687326, loss: 0.07035794854164124, data time: 0.01269825299580892\n",
      "step: 687327, loss: 0.06581445783376694, data time: 0.012219797481190075\n",
      "step: 687328, loss: 0.06532257050275803, data time: 0.011777224748030952\n",
      "step: 687329, loss: 0.06271792948246002, data time: 0.011373112599054972\n",
      "step: 687330, loss: 0.06882541626691818, data time: 0.011002798080444336\n",
      "step: 687331, loss: 0.06513893604278564, data time: 0.010666232842665452\n",
      "step: 687332, loss: 0.058182746171951294, data time: 0.010343189592714663\n",
      "step: 687333, loss: 0.06266909837722778, data time: 0.010043093136378698\n",
      "step: 687334, loss: 0.06875044852495193, data time: 0.009771963645671976\n",
      "step: 687335, loss: 0.056618642061948776, data time: 0.009519561131795248\n",
      "step: 687336, loss: 0.067427858710289, data time: 0.0092838502699329\n",
      "step: 687337, loss: 0.0626494437456131, data time: 0.009064115583896637\n",
      "step: 687338, loss: 0.062015634030103683, data time: 0.008846730896920868\n",
      "step: 687339, loss: 0.06153581663966179, data time: 0.008641663719626033\n",
      "step: 687340, loss: 0.057593218982219696, data time: 0.008451877321515764\n",
      "step: 687341, loss: 0.05869830399751663, data time: 0.008268482155270047\n",
      "step: 687342, loss: 0.05903809890151024, data time: 0.008098956700917837\n",
      "step: 687343, loss: 0.06692104041576385, data time: 0.007939621021873072\n",
      "step: 687344, loss: 0.0669776201248169, data time: 0.0077904432247846555\n",
      "step: 687345, loss: 0.055556770414114, data time: 0.007648313045501709\n",
      "step: 687346, loss: 0.057045839726924896, data time: 0.22063660621643066\n",
      "step: 687347, loss: 0.05926927179098129, data time: 0.11175668239593506\n",
      "step: 687348, loss: 0.06597821414470673, data time: 0.07555015881856282\n",
      "step: 687349, loss: 0.05789713189005852, data time: 0.057326674461364746\n",
      "step: 687350, loss: 0.059697702527046204, data time: 0.04613142013549805\n",
      "step: 687351, loss: 0.05975642055273056, data time: 0.038692077000935875\n",
      "step: 687352, loss: 0.05993380397558212, data time: 0.033372844968523295\n",
      "step: 687353, loss: 0.06068091839551926, data time: 0.02949392795562744\n",
      "step: 687354, loss: 0.060467563569545746, data time: 0.02640631463792589\n",
      "step: 687355, loss: 0.06581117957830429, data time: 0.024004244804382326\n",
      "step: 687356, loss: 0.06640353053808212, data time: 0.022053501822731712\n",
      "step: 687357, loss: 0.06209961697459221, data time: 0.02042311429977417\n",
      "step: 687358, loss: 0.06036486476659775, data time: 0.019042840370765098\n",
      "step: 687359, loss: 0.05985886976122856, data time: 0.01785121645246233\n",
      "step: 687360, loss: 0.06435059010982513, data time: 0.01685792605082194\n",
      "step: 687361, loss: 0.06292197108268738, data time: 0.01596444845199585\n",
      "step: 687362, loss: 0.061067670583724976, data time: 0.015163365532370174\n",
      "step: 687363, loss: 0.06432124227285385, data time: 0.014447304937574599\n",
      "step: 687364, loss: 0.058939963579177856, data time: 0.013793455926995529\n",
      "step: 687365, loss: 0.06846985220909119, data time: 0.01321502923965454\n",
      "step: 687366, loss: 0.06609600782394409, data time: 0.012689522334507533\n",
      "step: 687367, loss: 0.06232289597392082, data time: 0.01221554929559881\n",
      "step: 687368, loss: 0.06005650386214256, data time: 0.0117758149686067\n",
      "step: 687369, loss: 0.05976239964365959, data time: 0.011373112599054972\n",
      "step: 687370, loss: 0.06445662677288055, data time: 0.011000814437866211\n",
      "step: 687371, loss: 0.06762807816267014, data time: 0.010656980367807241\n",
      "step: 687372, loss: 0.05970172584056854, data time: 0.010334032553213614\n",
      "step: 687373, loss: 0.06268922239542007, data time: 0.010036613259996687\n",
      "step: 687374, loss: 0.06745964288711548, data time: 0.009766315591746363\n",
      "step: 687375, loss: 0.056908898055553436, data time: 0.009514705340067545\n",
      "step: 687376, loss: 0.0616784505546093, data time: 0.009281727575486707\n",
      "step: 687377, loss: 0.05871107429265976, data time: 0.009060867130756378\n",
      "step: 687378, loss: 0.0656675174832344, data time: 0.008845040292450876\n",
      "step: 687379, loss: 0.059409283101558685, data time: 0.008642414036919089\n",
      "step: 687380, loss: 0.06510438024997711, data time: 0.008466638837541852\n",
      "step: 687381, loss: 0.06007140874862671, data time: 0.008283495903015137\n",
      "step: 687382, loss: 0.058990031480789185, data time: 0.008111496229429502\n",
      "step: 687383, loss: 0.058933891355991364, data time: 0.007952828156320672\n",
      "step: 687384, loss: 0.05796108394861221, data time: 0.007800933642265124\n",
      "step: 687385, loss: 0.07451493293046951, data time: 0.007658851146697998\n",
      "step: 687386, loss: 0.06476488709449768, data time: 0.22882771492004395\n",
      "step: 687387, loss: 0.061537083238363266, data time: 0.11551737785339355\n",
      "step: 687388, loss: 0.059919487684965134, data time: 0.07804131507873535\n",
      "step: 687389, loss: 0.06490831077098846, data time: 0.05933016538619995\n",
      "step: 687390, loss: 0.0690971240401268, data time: 0.047758054733276364\n",
      "step: 687391, loss: 0.06827355921268463, data time: 0.04005114237467448\n",
      "step: 687392, loss: 0.058268021792173386, data time: 0.03452587127685547\n",
      "step: 687393, loss: 0.06477721780538559, data time: 0.030470073223114014\n",
      "step: 687394, loss: 0.060371220111846924, data time: 0.027246342764960393\n",
      "step: 687395, loss: 0.061353906989097595, data time: 0.024727439880371092\n",
      "step: 687396, loss: 0.05864641070365906, data time: 0.022709629752419212\n",
      "step: 687397, loss: 0.06018706411123276, data time: 0.021023988723754883\n",
      "step: 687398, loss: 0.06296729296445847, data time: 0.019598997556246243\n",
      "step: 687399, loss: 0.06439085304737091, data time: 0.018372518675667898\n",
      "step: 687400, loss: 0.06009857356548309, data time: 0.017311843236287434\n",
      "step: 687401, loss: 0.06358393281698227, data time: 0.016378894448280334\n",
      "step: 687402, loss: 0.062168292701244354, data time: 0.015556910458733053\n",
      "step: 687403, loss: 0.06817828118801117, data time: 0.014822469817267524\n",
      "step: 687404, loss: 0.06692300736904144, data time: 0.01416912831758198\n",
      "step: 687405, loss: 0.05813289433717728, data time: 0.013585925102233887\n",
      "step: 687406, loss: 0.062379881739616394, data time: 0.013062817709786552\n",
      "step: 687407, loss: 0.05690756067633629, data time: 0.01258662613955411\n",
      "step: 687408, loss: 0.06343068927526474, data time: 0.012143528979757557\n",
      "step: 687409, loss: 0.06819695234298706, data time: 0.011739571889241537\n",
      "step: 687410, loss: 0.06115415319800377, data time: 0.011364164352416993\n",
      "step: 687411, loss: 0.0676829069852829, data time: 0.011016937402578501\n",
      "step: 687412, loss: 0.06299849599599838, data time: 0.010697541413483795\n",
      "step: 687413, loss: 0.06228397786617279, data time: 0.010400831699371338\n",
      "step: 687414, loss: 0.06594046950340271, data time: 0.010132057913418474\n",
      "step: 687415, loss: 0.05757121369242668, data time: 0.009879620869954427\n",
      "step: 687416, loss: 0.06280426681041718, data time: 0.009642154939713017\n",
      "step: 687417, loss: 0.060598015785217285, data time: 0.009421169757843018\n",
      "step: 687418, loss: 0.05669645592570305, data time: 0.009198535572398792\n",
      "step: 687419, loss: 0.062021687626838684, data time: 0.008987588040968952\n",
      "step: 687420, loss: 0.057239972054958344, data time: 0.008789260046822685\n",
      "step: 687421, loss: 0.06411005556583405, data time: 0.008600592613220215\n",
      "step: 687422, loss: 0.057153113186359406, data time: 0.00842417253030313\n",
      "step: 687423, loss: 0.060580961406230927, data time: 0.008259610125893041\n",
      "step: 687424, loss: 0.06445673853158951, data time: 0.0081024292187813\n",
      "step: 687425, loss: 0.05970854312181473, data time: 0.007953745126724244\n",
      "step: 687426, loss: 0.0618092380464077, data time: 0.21474099159240723\n",
      "step: 687427, loss: 0.06478209793567657, data time: 0.10813605785369873\n",
      "step: 687428, loss: 0.05971524119377136, data time: 0.07312941551208496\n",
      "step: 687429, loss: 0.062310557812452316, data time: 0.05562096834182739\n",
      "step: 687430, loss: 0.057768307626247406, data time: 0.044766712188720706\n",
      "step: 687431, loss: 0.05885053426027298, data time: 0.03755760192871094\n",
      "step: 687432, loss: 0.06045633554458618, data time: 0.03241729736328125\n",
      "step: 687433, loss: 0.0630374550819397, data time: 0.028625428676605225\n",
      "step: 687434, loss: 0.0635896772146225, data time: 0.025594075520833332\n",
      "step: 687435, loss: 0.06254345923662186, data time: 0.023232507705688476\n",
      "step: 687436, loss: 0.05912313982844353, data time: 0.021321556784889915\n",
      "step: 687437, loss: 0.06290218979120255, data time: 0.01972631613413493\n",
      "step: 687438, loss: 0.06475654244422913, data time: 0.018374131276057318\n",
      "step: 687439, loss: 0.0596613883972168, data time: 0.017226219177246094\n",
      "step: 687440, loss: 0.06695107370615005, data time: 0.016242408752441408\n",
      "step: 687441, loss: 0.061389416456222534, data time: 0.015376687049865723\n",
      "step: 687442, loss: 0.0625169649720192, data time: 0.014617078444536994\n",
      "step: 687443, loss: 0.0701323002576828, data time: 0.013934612274169922\n",
      "step: 687444, loss: 0.06669051200151443, data time: 0.013328502052708677\n",
      "step: 687445, loss: 0.0630909726023674, data time: 0.012787044048309326\n",
      "step: 687446, loss: 0.061307474970817566, data time: 0.012299605778285436\n",
      "step: 687447, loss: 0.06166036054491997, data time: 0.01185353235764937\n",
      "step: 687448, loss: 0.05792033672332764, data time: 0.011438017306120499\n",
      "step: 687449, loss: 0.06098948046565056, data time: 0.011063347260157267\n",
      "step: 687450, loss: 0.0559365451335907, data time: 0.010717563629150391\n",
      "step: 687451, loss: 0.06181632727384567, data time: 0.010397819372323843\n",
      "step: 687452, loss: 0.060712866485118866, data time: 0.010101256547150788\n",
      "step: 687453, loss: 0.056770212948322296, data time: 0.009825084890638078\n",
      "step: 687454, loss: 0.06229081749916077, data time: 0.009573426739922885\n",
      "step: 687455, loss: 0.05476510152220726, data time: 0.009337290128072103\n",
      "step: 687456, loss: 0.06557685136795044, data time: 0.009117857102424867\n",
      "step: 687457, loss: 0.05772071331739426, data time: 0.00891450047492981\n",
      "step: 687458, loss: 0.06110496073961258, data time: 0.008709185051195549\n",
      "step: 687459, loss: 0.06859266757965088, data time: 0.00851286860073314\n",
      "step: 687460, loss: 0.06024862825870514, data time: 0.008327715737479074\n",
      "step: 687461, loss: 0.06710746884346008, data time: 0.008151272932688395\n",
      "step: 687462, loss: 0.06235412508249283, data time: 0.007985862525733741\n",
      "step: 687463, loss: 0.06053833290934563, data time: 0.007832414225528115\n",
      "step: 687464, loss: 0.05988535284996033, data time: 0.007687177413549178\n",
      "step: 687465, loss: 0.048244066536426544, data time: 0.00754961371421814\n",
      "step: 687466, loss: 0.0644124299287796, data time: 0.22418594360351562\n",
      "step: 687467, loss: 0.058619167655706406, data time: 0.1139059066772461\n",
      "step: 687468, loss: 0.05804595351219177, data time: 0.07644693056742351\n",
      "step: 687469, loss: 0.06348656117916107, data time: 0.058108508586883545\n",
      "step: 687470, loss: 0.06263791024684906, data time: 0.046779632568359375\n",
      "step: 687471, loss: 0.05854090675711632, data time: 0.039226015408833824\n",
      "step: 687472, loss: 0.05558084696531296, data time: 0.03382713454110282\n",
      "step: 687473, loss: 0.06893719732761383, data time: 0.029860377311706543\n",
      "step: 687474, loss: 0.055470146238803864, data time: 0.02668695979648166\n",
      "step: 687475, loss: 0.06133546680212021, data time: 0.02421746253967285\n",
      "step: 687476, loss: 0.0625469833612442, data time: 0.02222533659501509\n",
      "step: 687477, loss: 0.06707888841629028, data time: 0.02055490016937256\n",
      "step: 687478, loss: 0.06130624935030937, data time: 0.01914352637070876\n",
      "step: 687479, loss: 0.06440888345241547, data time: 0.01791722433907645\n",
      "step: 687480, loss: 0.06290183961391449, data time: 0.016862154006958008\n",
      "step: 687481, loss: 0.06242409348487854, data time: 0.01594342291355133\n",
      "step: 687482, loss: 0.06276555359363556, data time: 0.015128149705774644\n",
      "step: 687483, loss: 0.060161128640174866, data time: 0.01439805825551351\n",
      "step: 687484, loss: 0.05757956951856613, data time: 0.013748156396966232\n",
      "step: 687485, loss: 0.06430810689926147, data time: 0.013166856765747071\n",
      "step: 687486, loss: 0.05875317379832268, data time: 0.012643382662818545\n",
      "step: 687487, loss: 0.06871576607227325, data time: 0.012185877019708807\n",
      "step: 687488, loss: 0.07072581350803375, data time: 0.011746696803880774\n",
      "step: 687489, loss: 0.06053060293197632, data time: 0.01134525736172994\n",
      "step: 687490, loss: 0.0664963647723198, data time: 0.010971221923828125\n",
      "step: 687491, loss: 0.05972384661436081, data time: 0.010628370138315054\n",
      "step: 687492, loss: 0.06711331009864807, data time: 0.010308097909998011\n",
      "step: 687493, loss: 0.062420427799224854, data time: 0.010018834045955114\n",
      "step: 687494, loss: 0.05872504040598869, data time: 0.009752914823334792\n",
      "step: 687495, loss: 0.05537723749876022, data time: 0.009500416119893391\n",
      "step: 687496, loss: 0.06588298827409744, data time: 0.00926384618205409\n",
      "step: 687497, loss: 0.0662546306848526, data time: 0.009045161306858063\n",
      "step: 687498, loss: 0.06480681896209717, data time: 0.008829991022745768\n",
      "step: 687499, loss: 0.05925465747714043, data time: 0.00862585095798268\n",
      "step: 687500, loss: 0.062472909688949585, data time: 0.008435644422258649\n",
      "step: 687501, loss: 0.057793643325567245, data time: 0.008252501487731934\n",
      "step: 687502, loss: 0.06163438409566879, data time: 0.008081468375953468\n",
      "step: 687503, loss: 0.06259796023368835, data time: 0.00792165806418971\n",
      "step: 687504, loss: 0.06074035167694092, data time: 0.0077720544277093346\n",
      "step: 687505, loss: 0.08029700070619583, data time: 0.007631361484527588\n",
      "step: 687506, loss: 0.06232624500989914, data time: 0.22853636741638184\n",
      "step: 687507, loss: 0.06583000719547272, data time: 0.1158515214920044\n",
      "step: 687508, loss: 0.06072268635034561, data time: 0.07774114608764648\n",
      "step: 687509, loss: 0.05964262783527374, data time: 0.05917811393737793\n",
      "step: 687510, loss: 0.061931535601615906, data time: 0.04763360023498535\n",
      "step: 687511, loss: 0.06047181040048599, data time: 0.03995748360951742\n",
      "step: 687512, loss: 0.06264115869998932, data time: 0.03445284707205636\n",
      "step: 687513, loss: 0.062044769525527954, data time: 0.030404210090637207\n",
      "step: 687514, loss: 0.06392797827720642, data time: 0.027178049087524414\n",
      "step: 687515, loss: 0.06047574803233147, data time: 0.024665164947509765\n",
      "step: 687516, loss: 0.0585288405418396, data time: 0.02261851050636985\n",
      "step: 687517, loss: 0.05722632631659508, data time: 0.020917654037475586\n",
      "step: 687518, loss: 0.05560775101184845, data time: 0.01947626700768104\n",
      "step: 687519, loss: 0.0523042194545269, data time: 0.018237267221723284\n",
      "step: 687520, loss: 0.06439056992530823, data time: 0.017159732182820638\n",
      "step: 687521, loss: 0.06021222472190857, data time: 0.016212642192840576\n",
      "step: 687522, loss: 0.06121398136019707, data time: 0.015380620956420898\n",
      "step: 687523, loss: 0.06316760182380676, data time: 0.014634384049309624\n",
      "step: 687524, loss: 0.06425832957029343, data time: 0.013976498654014185\n",
      "step: 687525, loss: 0.06358231604099274, data time: 0.01338413953781128\n",
      "step: 687526, loss: 0.06566685438156128, data time: 0.012848490760439918\n",
      "step: 687527, loss: 0.06416196376085281, data time: 0.012363444675098766\n",
      "step: 687528, loss: 0.0594957172870636, data time: 0.011912760527237602\n",
      "step: 687529, loss: 0.06254926323890686, data time: 0.011503249406814575\n",
      "step: 687530, loss: 0.054415296763181686, data time: 0.011124420166015624\n",
      "step: 687531, loss: 0.06051649898290634, data time: 0.010777051632220928\n",
      "step: 687532, loss: 0.061660654842853546, data time: 0.010452765005606192\n",
      "step: 687533, loss: 0.06176122650504112, data time: 0.01015394926071167\n",
      "step: 687534, loss: 0.06533946841955185, data time: 0.00988038654985099\n",
      "step: 687535, loss: 0.0623864009976387, data time: 0.009623042742411296\n",
      "step: 687536, loss: 0.06538697332143784, data time: 0.009380886631627236\n",
      "step: 687537, loss: 0.057658642530441284, data time: 0.009164921939373016\n",
      "step: 687538, loss: 0.06333804130554199, data time: 0.008950580250133167\n",
      "step: 687539, loss: 0.061574727296829224, data time: 0.008744050474727856\n",
      "step: 687540, loss: 0.0622713528573513, data time: 0.008551951817103795\n",
      "step: 687541, loss: 0.06769509613513947, data time: 0.00836793581644694\n",
      "step: 687542, loss: 0.06145055219531059, data time: 0.008195831968977645\n",
      "step: 687543, loss: 0.05371636897325516, data time: 0.008034825325012207\n",
      "step: 687544, loss: 0.058466725051403046, data time: 0.007881451875735551\n",
      "step: 687545, loss: 0.05330159515142441, data time: 0.007735979557037353\n",
      "step: 687546, loss: 0.0640370175242424, data time: 0.2198941707611084\n",
      "step: 687547, loss: 0.057390451431274414, data time: 0.1113436222076416\n",
      "step: 687548, loss: 0.06323792785406113, data time: 0.07514055569966634\n",
      "step: 687549, loss: 0.06121288985013962, data time: 0.05712240934371948\n",
      "step: 687550, loss: 0.05741443485021591, data time: 0.04598379135131836\n",
      "step: 687551, loss: 0.06460200995206833, data time: 0.038573265075683594\n",
      "step: 687552, loss: 0.05928923934698105, data time: 0.033263138362339566\n",
      "step: 687553, loss: 0.06013612076640129, data time: 0.0293712317943573\n",
      "step: 687554, loss: 0.06537886708974838, data time: 0.02625605795118544\n",
      "step: 687555, loss: 0.0626678466796875, data time: 0.02383267879486084\n",
      "step: 687556, loss: 0.06346455216407776, data time: 0.02187568491155451\n",
      "step: 687557, loss: 0.05860293656587601, data time: 0.0202289621035258\n",
      "step: 687558, loss: 0.059955768287181854, data time: 0.018838038811316855\n",
      "step: 687559, loss: 0.06568387895822525, data time: 0.017638070242745534\n",
      "step: 687560, loss: 0.05903792381286621, data time: 0.016606918970743813\n",
      "step: 687561, loss: 0.056392520666122437, data time: 0.015696316957473755\n",
      "step: 687562, loss: 0.06063375622034073, data time: 0.01489392448874081\n",
      "step: 687563, loss: 0.058745041489601135, data time: 0.014176487922668457\n",
      "step: 687564, loss: 0.06573771685361862, data time: 0.013537996693661338\n",
      "step: 687565, loss: 0.06115012988448143, data time: 0.012975502014160156\n",
      "step: 687566, loss: 0.06659740954637527, data time: 0.012468985148838587\n",
      "step: 687567, loss: 0.06428928673267365, data time: 0.011998642574657093\n",
      "step: 687568, loss: 0.06413256376981735, data time: 0.011565944422846254\n",
      "step: 687569, loss: 0.060660116374492645, data time: 0.011172900597254435\n",
      "step: 687570, loss: 0.061192840337753296, data time: 0.010807228088378907\n",
      "step: 687571, loss: 0.06382860243320465, data time: 0.010472416877746582\n",
      "step: 687572, loss: 0.05721189081668854, data time: 0.010158874370433666\n",
      "step: 687573, loss: 0.06403863430023193, data time: 0.009867327553885323\n",
      "step: 687574, loss: 0.062167100608348846, data time: 0.009600606457940463\n",
      "step: 687575, loss: 0.058939263224601746, data time: 0.009351666768391926\n",
      "step: 687576, loss: 0.06456783413887024, data time: 0.00911927992297757\n",
      "step: 687577, loss: 0.062266699969768524, data time: 0.0089053213596344\n",
      "step: 687578, loss: 0.05886601656675339, data time: 0.008697141300548206\n",
      "step: 687579, loss: 0.0636218935251236, data time: 0.008498121710384594\n",
      "step: 687580, loss: 0.061732493340969086, data time: 0.00831242288861956\n",
      "step: 687581, loss: 0.06059340760111809, data time: 0.008132411373986138\n",
      "step: 687582, loss: 0.058729689568281174, data time: 0.007965216765532622\n",
      "step: 687583, loss: 0.061512336134910583, data time: 0.007809030382256759\n",
      "step: 687584, loss: 0.060849301517009735, data time: 0.007660816877316206\n",
      "step: 687585, loss: 0.06042534112930298, data time: 0.007520502805709839\n",
      "step: 687586, loss: 0.06748373806476593, data time: 0.2444605827331543\n",
      "step: 687587, loss: 0.05894021689891815, data time: 0.12301099300384521\n",
      "step: 687588, loss: 0.0666252076625824, data time: 0.08292269706726074\n",
      "step: 687589, loss: 0.06356558203697205, data time: 0.06291699409484863\n",
      "step: 687590, loss: 0.06272265315055847, data time: 0.050635242462158205\n",
      "step: 687591, loss: 0.05883891507983208, data time: 0.04243584473927816\n",
      "step: 687592, loss: 0.07027433067560196, data time: 0.03658638681684222\n",
      "step: 687593, loss: 0.06370724737644196, data time: 0.03226694464683533\n",
      "step: 687594, loss: 0.056966692209243774, data time: 0.028913709852430556\n",
      "step: 687595, loss: 0.06446302682161331, data time: 0.02623138427734375\n",
      "step: 687596, loss: 0.06207110732793808, data time: 0.02404264970259233\n",
      "step: 687597, loss: 0.06590069830417633, data time: 0.02222589651743571\n",
      "step: 687598, loss: 0.07056780904531479, data time: 0.02068627797640287\n",
      "step: 687599, loss: 0.058816276490688324, data time: 0.019350647926330566\n",
      "step: 687600, loss: 0.060537051409482956, data time: 0.018198013305664062\n",
      "step: 687601, loss: 0.06404485553503036, data time: 0.017196446657180786\n",
      "step: 687602, loss: 0.05989344045519829, data time: 0.016309499740600586\n",
      "step: 687603, loss: 0.06617383658885956, data time: 0.015518214967515733\n",
      "step: 687604, loss: 0.06559321284294128, data time: 0.014805091054815995\n",
      "step: 687605, loss: 0.061842113733291626, data time: 0.014174413681030274\n",
      "step: 687606, loss: 0.05587943643331528, data time: 0.013600769497099378\n",
      "step: 687607, loss: 0.06113084405660629, data time: 0.013080521063371138\n",
      "step: 687608, loss: 0.05787164717912674, data time: 0.012602743895157524\n",
      "step: 687609, loss: 0.060409873723983765, data time: 0.012167861064275106\n",
      "step: 687610, loss: 0.0588054321706295, data time: 0.01176919937133789\n",
      "step: 687611, loss: 0.0631694421172142, data time: 0.011396307211655837\n",
      "step: 687612, loss: 0.05997972935438156, data time: 0.01105275860539189\n",
      "step: 687613, loss: 0.06364326179027557, data time: 0.010732557092394148\n",
      "step: 687614, loss: 0.0698973760008812, data time: 0.010453043312862002\n",
      "step: 687615, loss: 0.05162372812628746, data time: 0.010180155436197916\n",
      "step: 687616, loss: 0.05627027153968811, data time: 0.009924034918508223\n",
      "step: 687617, loss: 0.061609528958797455, data time: 0.00968439131975174\n",
      "step: 687618, loss: 0.05680031701922417, data time: 0.009450370615178888\n",
      "step: 687619, loss: 0.06358638405799866, data time: 0.009232542094062357\n",
      "step: 687620, loss: 0.06485555320978165, data time: 0.009023250852312361\n",
      "step: 687621, loss: 0.060307495296001434, data time: 0.008826507462395562\n",
      "step: 687622, loss: 0.05863332003355026, data time: 0.008638323964299383\n",
      "step: 687623, loss: 0.06100737303495407, data time: 0.00846372780046965\n",
      "step: 687624, loss: 0.061541780829429626, data time: 0.00829928960555639\n",
      "step: 687625, loss: 0.07741802930831909, data time: 0.008145958185195923\n",
      "step: 687626, loss: 0.05846453830599785, data time: 0.23769187927246094\n",
      "step: 687627, loss: 0.05853244662284851, data time: 0.11959338188171387\n",
      "step: 687628, loss: 0.06241869181394577, data time: 0.08046563466389973\n",
      "step: 687629, loss: 0.06519822031259537, data time: 0.06134140491485596\n",
      "step: 687630, loss: 0.06168801337480545, data time: 0.049361467361450195\n",
      "step: 687631, loss: 0.06415773928165436, data time: 0.041385253270467125\n",
      "step: 687632, loss: 0.05821773409843445, data time: 0.0356877190726144\n",
      "step: 687633, loss: 0.062414348125457764, data time: 0.0314827561378479\n",
      "step: 687634, loss: 0.06383266299962997, data time: 0.0281325446234809\n",
      "step: 687635, loss: 0.06231198459863663, data time: 0.025522899627685548\n",
      "step: 687636, loss: 0.06257139146327972, data time: 0.023395213213833897\n",
      "step: 687637, loss: 0.06084289401769638, data time: 0.021628777186075848\n",
      "step: 687638, loss: 0.05437875911593437, data time: 0.020130212490375225\n",
      "step: 687639, loss: 0.06078457832336426, data time: 0.018837809562683105\n",
      "step: 687640, loss: 0.055950626730918884, data time: 0.01773082415262858\n",
      "step: 687641, loss: 0.06145764887332916, data time: 0.016751885414123535\n",
      "step: 687642, loss: 0.060912203043699265, data time: 0.01588891534244313\n",
      "step: 687643, loss: 0.06270164251327515, data time: 0.015117274390326606\n",
      "step: 687644, loss: 0.06218209117650986, data time: 0.014430497822008635\n",
      "step: 687645, loss: 0.06653577089309692, data time: 0.013815772533416749\n",
      "step: 687646, loss: 0.06455610692501068, data time: 0.013258854548136393\n",
      "step: 687647, loss: 0.05742819234728813, data time: 0.012753302400762384\n",
      "step: 687648, loss: 0.06634692847728729, data time: 0.01228637280671493\n",
      "step: 687649, loss: 0.06264524161815643, data time: 0.011865516503651937\n",
      "step: 687650, loss: 0.06129708141088486, data time: 0.011474838256835937\n",
      "step: 687651, loss: 0.06190422177314758, data time: 0.011110617564274715\n",
      "step: 687652, loss: 0.05803823471069336, data time: 0.010771504154911748\n",
      "step: 687653, loss: 0.05495909973978996, data time: 0.010459167616707938\n",
      "step: 687654, loss: 0.06396342813968658, data time: 0.010172383538607893\n",
      "step: 687655, loss: 0.059546809643507004, data time: 0.009903875986735027\n",
      "step: 687656, loss: 0.06872348487377167, data time: 0.009654783433483492\n",
      "step: 687657, loss: 0.058113761246204376, data time: 0.009425848722457886\n",
      "step: 687658, loss: 0.05572374910116196, data time: 0.009199518145936909\n",
      "step: 687659, loss: 0.0721624344587326, data time: 0.008987188339233398\n",
      "step: 687660, loss: 0.061605602502822876, data time: 0.008786487579345702\n",
      "step: 687661, loss: 0.06236078590154648, data time: 0.008594996399349637\n",
      "step: 687662, loss: 0.0579187385737896, data time: 0.00841481621200974\n",
      "step: 687663, loss: 0.06315590441226959, data time: 0.008246039089403655\n",
      "step: 687664, loss: 0.05876137316226959, data time: 0.008085868297479091\n",
      "step: 687665, loss: 0.09087543189525604, data time: 0.007935577630996704\n",
      "step: 687666, loss: 0.06339320540428162, data time: 0.22750616073608398\n",
      "step: 687667, loss: 0.0708034336566925, data time: 0.11454284191131592\n",
      "step: 687668, loss: 0.06117694824934006, data time: 0.07712920506795247\n",
      "step: 687669, loss: 0.06452028453350067, data time: 0.05874431133270264\n",
      "step: 687670, loss: 0.06219324469566345, data time: 0.04729299545288086\n",
      "step: 687671, loss: 0.06434306502342224, data time: 0.039658149083455406\n",
      "step: 687672, loss: 0.06334834545850754, data time: 0.034202371324811666\n",
      "step: 687673, loss: 0.06044638156890869, data time: 0.030176877975463867\n",
      "step: 687674, loss: 0.05841834470629692, data time: 0.026977830462985568\n",
      "step: 687675, loss: 0.06068456172943115, data time: 0.024497318267822265\n",
      "step: 687676, loss: 0.06155119836330414, data time: 0.022484324195168236\n",
      "step: 687677, loss: 0.05909726023674011, data time: 0.020805915196736652\n",
      "step: 687678, loss: 0.06850011646747589, data time: 0.019381119654728815\n",
      "step: 687679, loss: 0.05790043994784355, data time: 0.018143653869628906\n",
      "step: 687680, loss: 0.06034918874502182, data time: 0.01707148551940918\n",
      "step: 687681, loss: 0.06319653987884521, data time: 0.016137361526489258\n",
      "step: 687682, loss: 0.0671832263469696, data time: 0.015311872257905848\n",
      "step: 687683, loss: 0.0568067729473114, data time: 0.014571971363491483\n",
      "step: 687684, loss: 0.06219898909330368, data time: 0.01391173663892244\n",
      "step: 687685, loss: 0.06845682859420776, data time: 0.013322699069976806\n",
      "step: 687686, loss: 0.06002955883741379, data time: 0.012790441513061523\n",
      "step: 687687, loss: 0.06123661994934082, data time: 0.012308077378706499\n",
      "step: 687688, loss: 0.05555647239089012, data time: 0.011863013972406801\n",
      "step: 687689, loss: 0.0591173991560936, data time: 0.011463811000188192\n",
      "step: 687690, loss: 0.06499812006950378, data time: 0.011096153259277344\n",
      "step: 687691, loss: 0.06582598388195038, data time: 0.010753759971031776\n",
      "step: 687692, loss: 0.06960809230804443, data time: 0.010432199195579247\n",
      "step: 687693, loss: 0.05899226665496826, data time: 0.010133751801082067\n",
      "step: 687694, loss: 0.05949193239212036, data time: 0.009860449823839912\n",
      "step: 687695, loss: 0.0602593794465065, data time: 0.00960528055826823\n",
      "step: 687696, loss: 0.05775752291083336, data time: 0.009366689189787834\n",
      "step: 687697, loss: 0.061542972922325134, data time: 0.009154446423053741\n",
      "step: 687698, loss: 0.06620021164417267, data time: 0.008941411972045898\n",
      "step: 687699, loss: 0.062292229384183884, data time: 0.00874026382670683\n",
      "step: 687700, loss: 0.06513458490371704, data time: 0.008548545837402343\n",
      "step: 687701, loss: 0.06645264476537704, data time: 0.008364710542890761\n",
      "step: 687702, loss: 0.0625237450003624, data time: 0.008195284250620249\n",
      "step: 687703, loss: 0.06304199993610382, data time: 0.008037046382301733\n",
      "step: 687704, loss: 0.05913464352488518, data time: 0.007886049075004382\n",
      "step: 687705, loss: 0.07435990869998932, data time: 0.007742196321487427\n",
      "step: 687706, loss: 0.06145600602030754, data time: 0.23736953735351562\n",
      "step: 687707, loss: 0.06095303222537041, data time: 0.11942338943481445\n",
      "step: 687708, loss: 0.059264689683914185, data time: 0.08012795448303223\n",
      "step: 687709, loss: 0.064371258020401, data time: 0.061022043228149414\n",
      "step: 687710, loss: 0.060401551425457, data time: 0.04914865493774414\n",
      "step: 687711, loss: 0.06564643979072571, data time: 0.041246374448140465\n",
      "step: 687712, loss: 0.0637912005186081, data time: 0.03562498092651367\n",
      "step: 687713, loss: 0.06479402631521225, data time: 0.031475186347961426\n",
      "step: 687714, loss: 0.05540696904063225, data time: 0.02816616164313422\n",
      "step: 687715, loss: 0.06042136251926422, data time: 0.025586915016174317\n",
      "step: 687716, loss: 0.06630980223417282, data time: 0.023489431901411575\n",
      "step: 687717, loss: 0.059763334691524506, data time: 0.021744032700856526\n",
      "step: 687718, loss: 0.062178172171115875, data time: 0.020268898743849535\n",
      "step: 687719, loss: 0.059916701167821884, data time: 0.018995932170322964\n",
      "step: 687720, loss: 0.061173200607299805, data time: 0.017894204457600912\n",
      "step: 687721, loss: 0.0642031580209732, data time: 0.016934454441070557\n",
      "step: 687722, loss: 0.0609818696975708, data time: 0.016080351436839384\n",
      "step: 687723, loss: 0.06537754088640213, data time: 0.01531566513909234\n",
      "step: 687724, loss: 0.06400991976261139, data time: 0.014635487606650904\n",
      "step: 687725, loss: 0.06286731362342834, data time: 0.01402980089187622\n",
      "step: 687726, loss: 0.05906054377555847, data time: 0.013482218696957543\n",
      "step: 687727, loss: 0.0668734535574913, data time: 0.012982390143654564\n",
      "step: 687728, loss: 0.06382274627685547, data time: 0.012521121812903362\n",
      "step: 687729, loss: 0.06560927629470825, data time: 0.012099385261535645\n",
      "step: 687730, loss: 0.06393660604953766, data time: 0.011721372604370117\n",
      "step: 687731, loss: 0.06127425283193588, data time: 0.011364890978886532\n",
      "step: 687732, loss: 0.06017372012138367, data time: 0.011031486369945385\n",
      "step: 687733, loss: 0.06859512627124786, data time: 0.010723701545170375\n",
      "step: 687734, loss: 0.06232450529932976, data time: 0.010441566335743871\n",
      "step: 687735, loss: 0.06161700189113617, data time: 0.010177008310953776\n",
      "step: 687736, loss: 0.06177745386958122, data time: 0.009928864817465506\n",
      "step: 687737, loss: 0.06637545675039291, data time: 0.009700529277324677\n",
      "step: 687738, loss: 0.06370651721954346, data time: 0.009471141930782434\n",
      "step: 687739, loss: 0.06217385455965996, data time: 0.009256811702952665\n",
      "step: 687740, loss: 0.06029687449336052, data time: 0.009050280707223074\n",
      "step: 687741, loss: 0.06467290222644806, data time: 0.008863700760735406\n",
      "step: 687742, loss: 0.05951153486967087, data time: 0.008679712140882338\n",
      "step: 687743, loss: 0.06277631968259811, data time: 0.00850934103915566\n",
      "step: 687744, loss: 0.060260109603405, data time: 0.008345891267825395\n",
      "step: 687745, loss: 0.06508482247591019, data time: 0.008191365003585815\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40931034088134766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 687746, loss: 0.06043412536382675, data time: 0.22345495223999023\n",
      "step: 687747, loss: 0.0633416622877121, data time: 0.11288309097290039\n",
      "step: 687748, loss: 0.06191098690032959, data time: 0.07629815737406413\n",
      "step: 687749, loss: 0.06217091530561447, data time: 0.0579676628112793\n",
      "step: 687750, loss: 0.06966544687747955, data time: 0.0466306209564209\n",
      "step: 687751, loss: 0.06426213681697845, data time: 0.039104620615641274\n",
      "step: 687752, loss: 0.06141392141580582, data time: 0.03372297968183245\n",
      "step: 687753, loss: 0.05715488642454147, data time: 0.029761284589767456\n",
      "step: 687754, loss: 0.06927773356437683, data time: 0.02659471829732259\n",
      "step: 687755, loss: 0.060591068118810654, data time: 0.024166584014892578\n",
      "step: 687756, loss: 0.0697527751326561, data time: 0.02219945734197443\n",
      "step: 687757, loss: 0.06375062465667725, data time: 0.020559767882029217\n",
      "step: 687758, loss: 0.0670454129576683, data time: 0.01916364523080679\n",
      "step: 687759, loss: 0.0635581761598587, data time: 0.017958981650216237\n",
      "step: 687760, loss: 0.0601787269115448, data time: 0.016919390360514323\n",
      "step: 687761, loss: 0.06467382609844208, data time: 0.01601281762123108\n",
      "step: 687762, loss: 0.06313278526067734, data time: 0.015212507808909696\n",
      "step: 687763, loss: 0.059325896203517914, data time: 0.014494895935058594\n",
      "step: 687764, loss: 0.061732880771160126, data time: 0.013854114632857474\n",
      "step: 687765, loss: 0.06226218864321709, data time: 0.013290762901306152\n",
      "step: 687766, loss: 0.060797736048698425, data time: 0.012776806240990049\n",
      "step: 687767, loss: 0.06474309414625168, data time: 0.01230575821616433\n",
      "step: 687768, loss: 0.05987280607223511, data time: 0.011869005534959875\n",
      "step: 687769, loss: 0.05591608211398125, data time: 0.011475682258605957\n",
      "step: 687770, loss: 0.06574785709381104, data time: 0.011111116409301758\n",
      "step: 687771, loss: 0.06401900947093964, data time: 0.010776822383587178\n",
      "step: 687772, loss: 0.06680197268724442, data time: 0.010461621814303927\n",
      "step: 687773, loss: 0.061528533697128296, data time: 0.010172086102621896\n",
      "step: 687774, loss: 0.06529885530471802, data time: 0.009910813693342537\n",
      "step: 687775, loss: 0.06438027322292328, data time: 0.009664225578308105\n",
      "step: 687776, loss: 0.05763709172606468, data time: 0.009429731676655432\n",
      "step: 687777, loss: 0.06257957220077515, data time: 0.009212583303451538\n",
      "step: 687778, loss: 0.06177625060081482, data time: 0.008996385516542377\n",
      "step: 687779, loss: 0.06359130144119263, data time: 0.008792414384729722\n",
      "step: 687780, loss: 0.05764223262667656, data time: 0.008600200925554548\n",
      "step: 687781, loss: 0.060667119920253754, data time: 0.00841428836186727\n",
      "step: 687782, loss: 0.06619209051132202, data time: 0.008241215267696896\n",
      "step: 687783, loss: 0.06423497200012207, data time: 0.008081549092342979\n",
      "step: 687784, loss: 0.06640760600566864, data time: 0.007929575748932667\n",
      "step: 687785, loss: 0.07960674166679382, data time: 0.007781410217285156\n",
      "step: 687786, loss: 0.06308652460575104, data time: 0.23779940605163574\n",
      "step: 687787, loss: 0.0616573840379715, data time: 0.11967003345489502\n",
      "step: 687788, loss: 0.060168322175741196, data time: 0.08027990659077962\n",
      "step: 687789, loss: 0.05931245535612106, data time: 0.06099623441696167\n",
      "step: 687790, loss: 0.060547661036252975, data time: 0.049072265625\n",
      "step: 687791, loss: 0.060326628386974335, data time: 0.041150450706481934\n",
      "step: 687792, loss: 0.06706317514181137, data time: 0.035478012902396064\n",
      "step: 687793, loss: 0.060347966849803925, data time: 0.031305551528930664\n",
      "step: 687794, loss: 0.06224159523844719, data time: 0.027968512641059026\n",
      "step: 687795, loss: 0.05727101117372513, data time: 0.025370121002197266\n",
      "step: 687796, loss: 0.06366350501775742, data time: 0.02326323769309304\n",
      "step: 687797, loss: 0.05985872074961662, data time: 0.021508455276489258\n",
      "step: 687798, loss: 0.06433951109647751, data time: 0.020020063106830303\n",
      "step: 687799, loss: 0.06095477193593979, data time: 0.018755061285836355\n",
      "step: 687800, loss: 0.06404225528240204, data time: 0.017664337158203126\n",
      "step: 687801, loss: 0.06409916281700134, data time: 0.01670829951763153\n",
      "step: 687802, loss: 0.059379905462265015, data time: 0.015866966808543485\n",
      "step: 687803, loss: 0.06453035771846771, data time: 0.015117737982008193\n",
      "step: 687804, loss: 0.06023581326007843, data time: 0.014446321286653218\n",
      "step: 687805, loss: 0.06450525671243668, data time: 0.013850748538970947\n",
      "step: 687806, loss: 0.05776306241750717, data time: 0.013309456053234282\n",
      "step: 687807, loss: 0.0668897032737732, data time: 0.012818791649558327\n",
      "step: 687808, loss: 0.06570431590080261, data time: 0.012363641158394192\n",
      "step: 687809, loss: 0.05975181609392166, data time: 0.01194724440574646\n",
      "step: 687810, loss: 0.06399642676115036, data time: 0.011566686630249023\n",
      "step: 687811, loss: 0.056620802730321884, data time: 0.01121679636148306\n",
      "step: 687812, loss: 0.06170888990163803, data time: 0.010887066523234049\n",
      "step: 687813, loss: 0.06613723933696747, data time: 0.010583111218043737\n",
      "step: 687814, loss: 0.05713174492120743, data time: 0.010304886719276166\n",
      "step: 687815, loss: 0.06043816730380058, data time: 0.010044240951538086\n",
      "step: 687816, loss: 0.061153627932071686, data time: 0.009802918280324629\n",
      "step: 687817, loss: 0.06496298313140869, data time: 0.00957634299993515\n",
      "step: 687818, loss: 0.06500580906867981, data time: 0.009348890998146751\n",
      "step: 687819, loss: 0.06489886343479156, data time: 0.009134650230407715\n",
      "step: 687820, loss: 0.05850571021437645, data time: 0.008932719911847796\n",
      "step: 687821, loss: 0.06732039153575897, data time: 0.008740829096900092\n",
      "step: 687822, loss: 0.06666342914104462, data time: 0.008558595502698744\n",
      "step: 687823, loss: 0.06641783565282822, data time: 0.0083905019258198\n",
      "step: 687824, loss: 0.0632115826010704, data time: 0.008230191010695238\n",
      "step: 687825, loss: 0.042605675756931305, data time: 0.008079105615615844\n",
      "step: 687826, loss: 0.0653509870171547, data time: 0.22539639472961426\n",
      "step: 687827, loss: 0.060870349407196045, data time: 0.1138453483581543\n",
      "step: 687828, loss: 0.06103528290987015, data time: 0.07695317268371582\n",
      "step: 687829, loss: 0.06767204403877258, data time: 0.058519184589385986\n",
      "step: 687830, loss: 0.06110285967588425, data time: 0.047088003158569335\n",
      "step: 687831, loss: 0.06239887326955795, data time: 0.03947607676188151\n",
      "step: 687832, loss: 0.05843178555369377, data time: 0.03403748784746442\n",
      "step: 687833, loss: 0.05822731554508209, data time: 0.030048251152038574\n",
      "step: 687834, loss: 0.061019182205200195, data time: 0.026855309804280598\n",
      "step: 687835, loss: 0.05625961348414421, data time: 0.0243807315826416\n",
      "step: 687836, loss: 0.06519487500190735, data time: 0.022392121228304775\n",
      "step: 687837, loss: 0.058359257876873016, data time: 0.020737628142038982\n",
      "step: 687838, loss: 0.06185000389814377, data time: 0.019341028653658353\n",
      "step: 687839, loss: 0.05705607682466507, data time: 0.018124410084315708\n",
      "step: 687840, loss: 0.06191164255142212, data time: 0.01707468032836914\n",
      "step: 687841, loss: 0.06062794849276543, data time: 0.016156822443008423\n",
      "step: 687842, loss: 0.0632161796092987, data time: 0.015347985660328585\n",
      "step: 687843, loss: 0.056615374982357025, data time: 0.014624251259697808\n",
      "step: 687844, loss: 0.061573825776576996, data time: 0.013980627059936523\n",
      "step: 687845, loss: 0.06438803672790527, data time: 0.013411235809326173\n",
      "step: 687846, loss: 0.06289955973625183, data time: 0.012894323893955775\n",
      "step: 687847, loss: 0.0648370012640953, data time: 0.012420751831748268\n",
      "step: 687848, loss: 0.06670351326465607, data time: 0.01197919638260551\n",
      "step: 687849, loss: 0.059436630457639694, data time: 0.011580457290013632\n",
      "step: 687850, loss: 0.0644170343875885, data time: 0.011211624145507812\n",
      "step: 687851, loss: 0.06717459857463837, data time: 0.01087347360757681\n",
      "step: 687852, loss: 0.05236394330859184, data time: 0.010558304963288483\n",
      "step: 687853, loss: 0.060846395790576935, data time: 0.010265767574310303\n",
      "step: 687854, loss: 0.07119133323431015, data time: 0.010000130225872171\n",
      "step: 687855, loss: 0.05994616076350212, data time: 0.009752360979715984\n",
      "step: 687856, loss: 0.06214567646384239, data time: 0.009519738535727225\n",
      "step: 687857, loss: 0.05861398205161095, data time: 0.009302295744419098\n",
      "step: 687858, loss: 0.06487695872783661, data time: 0.009083733414158676\n",
      "step: 687859, loss: 0.059430208057165146, data time: 0.008876912734087776\n",
      "step: 687860, loss: 0.06215212494134903, data time: 0.008681685583932059\n",
      "step: 687861, loss: 0.06032644957304001, data time: 0.008494748009575738\n",
      "step: 687862, loss: 0.06456989794969559, data time: 0.008319158811826963\n",
      "step: 687863, loss: 0.055731337517499924, data time: 0.00815728463624653\n",
      "step: 687864, loss: 0.06203503534197807, data time: 0.008003595547798352\n",
      "step: 687865, loss: 0.05385003983974457, data time: 0.007856804132461547\n",
      "step: 687866, loss: 0.06017593666911125, data time: 0.2352759838104248\n",
      "step: 687867, loss: 0.06648122519254684, data time: 0.11841034889221191\n",
      "step: 687868, loss: 0.05882210657000542, data time: 0.0794520378112793\n",
      "step: 687869, loss: 0.05901448056101799, data time: 0.060342252254486084\n",
      "step: 687870, loss: 0.06123928725719452, data time: 0.04855670928955078\n",
      "step: 687871, loss: 0.0632006973028183, data time: 0.040717124938964844\n",
      "step: 687872, loss: 0.06393768638372421, data time: 0.03511309623718262\n",
      "step: 687873, loss: 0.062392234802246094, data time: 0.030973970890045166\n",
      "step: 687874, loss: 0.06257687509059906, data time: 0.02767613199022081\n",
      "step: 687875, loss: 0.05854766070842743, data time: 0.02510678768157959\n",
      "step: 687876, loss: 0.05911625176668167, data time: 0.02302208813753995\n",
      "step: 687877, loss: 0.06028662249445915, data time: 0.02128299077351888\n",
      "step: 687878, loss: 0.06169858202338219, data time: 0.019819351343008187\n",
      "step: 687879, loss: 0.059792377054691315, data time: 0.018549442291259766\n",
      "step: 687880, loss: 0.07046421617269516, data time: 0.017453718185424804\n",
      "step: 687881, loss: 0.059068791568279266, data time: 0.01648963987827301\n",
      "step: 687882, loss: 0.062428127974271774, data time: 0.015641114291022804\n",
      "step: 687883, loss: 0.05897006392478943, data time: 0.01488559775882297\n",
      "step: 687884, loss: 0.06468522548675537, data time: 0.0142084422864412\n",
      "step: 687885, loss: 0.06450074166059494, data time: 0.01360914707183838\n",
      "step: 687886, loss: 0.056994616985321045, data time: 0.013064214161464147\n",
      "step: 687887, loss: 0.06467097997665405, data time: 0.012567585164850409\n",
      "step: 687888, loss: 0.061578087508678436, data time: 0.012109310730643894\n",
      "step: 687889, loss: 0.0626608356833458, data time: 0.011687467495600382\n",
      "step: 687890, loss: 0.05996984243392944, data time: 0.011303625106811523\n",
      "step: 687891, loss: 0.05889228731393814, data time: 0.010946090404803935\n",
      "step: 687892, loss: 0.06345681846141815, data time: 0.01061203744676378\n",
      "step: 687893, loss: 0.0636725127696991, data time: 0.010304306234632219\n",
      "step: 687894, loss: 0.06615705788135529, data time: 0.010023832321166992\n",
      "step: 687895, loss: 0.0632353350520134, data time: 0.00976110299428304\n",
      "step: 687896, loss: 0.0610746368765831, data time: 0.009516262239025484\n",
      "step: 687897, loss: 0.05992480367422104, data time: 0.009292103350162506\n",
      "step: 687898, loss: 0.05954590439796448, data time: 0.009072217074307528\n",
      "step: 687899, loss: 0.056567903608083725, data time: 0.008862221942228429\n",
      "step: 687900, loss: 0.06946872174739838, data time: 0.008665261949811662\n",
      "step: 687901, loss: 0.05932479351758957, data time: 0.008476985825432671\n",
      "step: 687902, loss: 0.06397341936826706, data time: 0.008298281076792124\n",
      "step: 687903, loss: 0.05582047998905182, data time: 0.008135582271375154\n",
      "step: 687904, loss: 0.06298726797103882, data time: 0.00797893450810359\n",
      "step: 687905, loss: 0.0610814169049263, data time: 0.007832938432693481\n",
      "step: 687906, loss: 0.060531601309776306, data time: 0.21753573417663574\n",
      "step: 687907, loss: 0.057772643864154816, data time: 0.11013293266296387\n",
      "step: 687908, loss: 0.06175297498703003, data time: 0.07432285944620769\n",
      "step: 687909, loss: 0.0621657595038414, data time: 0.05651432275772095\n",
      "step: 687910, loss: 0.06123299151659012, data time: 0.04550938606262207\n",
      "step: 687911, loss: 0.0618678443133831, data time: 0.03820395469665527\n",
      "step: 687912, loss: 0.06391232460737228, data time: 0.03295377322605678\n",
      "step: 687913, loss: 0.05712711811065674, data time: 0.02910110354423523\n",
      "step: 687914, loss: 0.05966437980532646, data time: 0.026017321480645075\n",
      "step: 687915, loss: 0.060744758695364, data time: 0.023613548278808592\n",
      "step: 687916, loss: 0.06957804411649704, data time: 0.021662018515846947\n",
      "step: 687917, loss: 0.05895613878965378, data time: 0.02003290255864461\n",
      "step: 687918, loss: 0.05942776799201965, data time: 0.018660325270432692\n",
      "step: 687919, loss: 0.06778927147388458, data time: 0.017473067556108748\n",
      "step: 687920, loss: 0.058577798306941986, data time: 0.0164582888285319\n",
      "step: 687921, loss: 0.060645174235105515, data time: 0.015558809041976929\n",
      "step: 687922, loss: 0.06880788505077362, data time: 0.014767576666439282\n",
      "step: 687923, loss: 0.06522046774625778, data time: 0.014055556721157499\n",
      "step: 687924, loss: 0.06018061563372612, data time: 0.013420029690391138\n",
      "step: 687925, loss: 0.05883954092860222, data time: 0.012861907482147217\n",
      "step: 687926, loss: 0.05749722570180893, data time: 0.012352795827956427\n",
      "step: 687927, loss: 0.0583961121737957, data time: 0.011890519749034534\n",
      "step: 687928, loss: 0.05953734368085861, data time: 0.011461900628131369\n",
      "step: 687929, loss: 0.05924123525619507, data time: 0.01107300321261088\n",
      "step: 687930, loss: 0.06232286989688873, data time: 0.010712604522705078\n",
      "step: 687931, loss: 0.06551385670900345, data time: 0.010381075052114634\n",
      "step: 687932, loss: 0.06002086028456688, data time: 0.010068513728954174\n",
      "step: 687933, loss: 0.0637219250202179, data time: 0.009793068681444441\n",
      "step: 687934, loss: 0.05679591745138168, data time: 0.009543246236340753\n",
      "step: 687935, loss: 0.06036503240466118, data time: 0.009309021631876628\n",
      "step: 687936, loss: 0.06506558507680893, data time: 0.009089700637325164\n",
      "step: 687937, loss: 0.0639333724975586, data time: 0.00889238715171814\n",
      "step: 687938, loss: 0.05507984012365341, data time: 0.008686448588515774\n",
      "step: 687939, loss: 0.06268368661403656, data time: 0.00849004352793974\n",
      "step: 687940, loss: 0.05888044089078903, data time: 0.00830486842564174\n",
      "step: 687941, loss: 0.061702191829681396, data time: 0.008127927780151367\n",
      "step: 687942, loss: 0.06645497679710388, data time: 0.007962039999059728\n",
      "step: 687943, loss: 0.06546294689178467, data time: 0.007809482122722425\n",
      "step: 687944, loss: 0.07335561513900757, data time: 0.007665193997896635\n",
      "step: 687945, loss: 0.08045303821563721, data time: 0.007528972625732422\n",
      "step: 687946, loss: 0.06803600490093231, data time: 0.22139263153076172\n",
      "step: 687947, loss: 0.05633687227964401, data time: 0.11185634136199951\n",
      "step: 687948, loss: 0.06514149904251099, data time: 0.0757463773091634\n",
      "step: 687949, loss: 0.059999674558639526, data time: 0.05750638246536255\n",
      "step: 687950, loss: 0.06374416500329971, data time: 0.04628233909606934\n",
      "step: 687951, loss: 0.06397785991430283, data time: 0.03881553808848063\n",
      "step: 687952, loss: 0.06233904883265495, data time: 0.03347584179469517\n",
      "step: 687953, loss: 0.06138461083173752, data time: 0.029542237520217896\n",
      "step: 687954, loss: 0.06432861089706421, data time: 0.026415242089165583\n",
      "step: 687955, loss: 0.06543827801942825, data time: 0.023972916603088378\n",
      "step: 687956, loss: 0.06411434710025787, data time: 0.021992575038563122\n",
      "step: 687957, loss: 0.06334670633077621, data time: 0.020345409711201985\n",
      "step: 687958, loss: 0.06111608445644379, data time: 0.018977770438561074\n",
      "step: 687959, loss: 0.06132262945175171, data time: 0.017791134970528737\n",
      "step: 687960, loss: 0.06584027409553528, data time: 0.01676947275797526\n",
      "step: 687961, loss: 0.06237832456827164, data time: 0.01586964726448059\n",
      "step: 687962, loss: 0.0662485808134079, data time: 0.015084603253532858\n",
      "step: 687963, loss: 0.06301072239875793, data time: 0.014375262790256076\n",
      "step: 687964, loss: 0.0690714567899704, data time: 0.01374867087916324\n",
      "step: 687965, loss: 0.06282433867454529, data time: 0.013186895847320556\n",
      "step: 687966, loss: 0.06475888192653656, data time: 0.012678907031104678\n",
      "step: 687967, loss: 0.06076638400554657, data time: 0.0122154409235174\n",
      "step: 687968, loss: 0.06531327962875366, data time: 0.011789280435313349\n",
      "step: 687969, loss: 0.058247439563274384, data time: 0.01140048106511434\n",
      "step: 687970, loss: 0.06875845044851303, data time: 0.01103515625\n",
      "step: 687971, loss: 0.06444863229990005, data time: 0.010689001816969652\n",
      "step: 687972, loss: 0.06324313580989838, data time: 0.010363949669731988\n",
      "step: 687973, loss: 0.05366946756839752, data time: 0.01006732668195452\n",
      "step: 687974, loss: 0.06021758168935776, data time: 0.009795377994405812\n",
      "step: 687975, loss: 0.05856996402144432, data time: 0.009541805585225422\n",
      "step: 687976, loss: 0.06417397409677505, data time: 0.009309099566551947\n",
      "step: 687977, loss: 0.060432419180870056, data time: 0.00908750295639038\n",
      "step: 687978, loss: 0.06135474890470505, data time: 0.008871273560957476\n",
      "step: 687979, loss: 0.0644967183470726, data time: 0.008666837916654698\n",
      "step: 687980, loss: 0.0664043202996254, data time: 0.00847297395978655\n",
      "step: 687981, loss: 0.06104917451739311, data time: 0.008288138442569308\n",
      "step: 687982, loss: 0.06605461239814758, data time: 0.008115465576584274\n",
      "step: 687983, loss: 0.061949264258146286, data time: 0.007955902501156456\n",
      "step: 687984, loss: 0.05643291398882866, data time: 0.007803684625870142\n",
      "step: 687985, loss: 0.07944916933774948, data time: 0.007659083604812622\n",
      "step: 687986, loss: 0.059707071632146835, data time: 0.22238850593566895\n",
      "step: 687987, loss: 0.06216111034154892, data time: 0.11232936382293701\n",
      "step: 687988, loss: 0.058079034090042114, data time: 0.07605799039204915\n",
      "step: 687989, loss: 0.05854460224509239, data time: 0.057746827602386475\n",
      "step: 687990, loss: 0.05854609236121178, data time: 0.046476125717163086\n",
      "step: 687991, loss: 0.0664382129907608, data time: 0.03899200757344564\n",
      "step: 687992, loss: 0.06233431398868561, data time: 0.033627680369785855\n",
      "step: 687993, loss: 0.059954531490802765, data time: 0.029730141162872314\n",
      "step: 687994, loss: 0.065569669008255, data time: 0.02657175064086914\n",
      "step: 687995, loss: 0.05911071226000786, data time: 0.02413167953491211\n",
      "step: 687996, loss: 0.06733117997646332, data time: 0.0221394192088734\n",
      "step: 687997, loss: 0.06049486994743347, data time: 0.02047799030939738\n",
      "step: 687998, loss: 0.058419063687324524, data time: 0.019066443810096152\n",
      "step: 687999, loss: 0.06269267946481705, data time: 0.017847861562456404\n",
      "step: 688000, loss: 0.06300392746925354, data time: 0.016797240575154623\n",
      "step: 688001, loss: 0.06075477600097656, data time: 0.01588059961795807\n",
      "step: 688002, loss: 0.06115881726145744, data time: 0.015076945809757008\n",
      "step: 688003, loss: 0.06387428939342499, data time: 0.014344943894280327\n",
      "step: 688004, loss: 0.06030906364321709, data time: 0.013699995843987716\n",
      "step: 688005, loss: 0.06606432050466537, data time: 0.013125574588775635\n",
      "step: 688006, loss: 0.06120782345533371, data time: 0.012602726618448893\n",
      "step: 688007, loss: 0.06541913747787476, data time: 0.0121274319562045\n",
      "step: 688008, loss: 0.05755673348903656, data time: 0.011688066565472147\n",
      "step: 688009, loss: 0.06174379587173462, data time: 0.011294146378835043\n",
      "step: 688010, loss: 0.06785997748374939, data time: 0.010922374725341797\n",
      "step: 688011, loss: 0.06708311289548874, data time: 0.010580347134516789\n",
      "step: 688012, loss: 0.05814066529273987, data time: 0.010259999169243706\n",
      "step: 688013, loss: 0.060533683747053146, data time: 0.009965036596570696\n",
      "step: 688014, loss: 0.06764738261699677, data time: 0.009696023217562971\n",
      "step: 688015, loss: 0.07206352055072784, data time: 0.009447439511617025\n",
      "step: 688016, loss: 0.06169119477272034, data time: 0.009214701191071541\n",
      "step: 688017, loss: 0.06182994693517685, data time: 0.008996114134788513\n",
      "step: 688018, loss: 0.05860060453414917, data time: 0.008783224857214725\n",
      "step: 688019, loss: 0.06558802723884583, data time: 0.008581259671379538\n",
      "step: 688020, loss: 0.06359909474849701, data time: 0.008390235900878906\n",
      "step: 688021, loss: 0.0652342215180397, data time: 0.008209466934204102\n",
      "step: 688022, loss: 0.06582614034414291, data time: 0.00803965491217536\n",
      "step: 688023, loss: 0.06918732821941376, data time: 0.007882645255640933\n",
      "step: 688024, loss: 0.06353114545345306, data time: 0.007734347612429888\n",
      "step: 688025, loss: 0.034327082335948944, data time: 0.0075940251350402836\n",
      "step: 688026, loss: 0.06679227203130722, data time: 0.23405957221984863\n",
      "step: 688027, loss: 0.061295509338378906, data time: 0.11857104301452637\n",
      "step: 688028, loss: 0.05811598151922226, data time: 0.07960128784179688\n",
      "step: 688029, loss: 0.06758910417556763, data time: 0.060579538345336914\n",
      "step: 688030, loss: 0.06316008418798447, data time: 0.04873514175415039\n",
      "step: 688031, loss: 0.060765039175748825, data time: 0.040854970614115395\n",
      "step: 688032, loss: 0.06393308937549591, data time: 0.03521224430629185\n",
      "step: 688033, loss: 0.0639202669262886, data time: 0.03107351064682007\n",
      "step: 688034, loss: 0.0633460208773613, data time: 0.027767923143174913\n",
      "step: 688035, loss: 0.06584589183330536, data time: 0.025193285942077637\n",
      "step: 688036, loss: 0.06236547231674194, data time: 0.023104949430985885\n",
      "step: 688037, loss: 0.07252545654773712, data time: 0.021354854106903076\n",
      "step: 688038, loss: 0.05465894192457199, data time: 0.019878405791062575\n",
      "step: 688039, loss: 0.06093200668692589, data time: 0.01860443183353969\n",
      "step: 688040, loss: 0.06160939857363701, data time: 0.017505486806233723\n",
      "step: 688041, loss: 0.0651891753077507, data time: 0.016535356640815735\n",
      "step: 688042, loss: 0.06369636952877045, data time: 0.015683440601124483\n",
      "step: 688043, loss: 0.0690709799528122, data time: 0.01492101616329617\n",
      "step: 688044, loss: 0.06191354990005493, data time: 0.014240867213199013\n",
      "step: 688045, loss: 0.06118454784154892, data time: 0.013636434078216552\n",
      "step: 688046, loss: 0.059284672141075134, data time: 0.013087976546514602\n",
      "step: 688047, loss: 0.06485289335250854, data time: 0.01259137283671986\n",
      "step: 688048, loss: 0.062405236065387726, data time: 0.012132509894992994\n",
      "step: 688049, loss: 0.06444820761680603, data time: 0.011714210112889608\n",
      "step: 688050, loss: 0.05841709300875664, data time: 0.011325979232788086\n",
      "step: 688051, loss: 0.06448236107826233, data time: 0.010973545221182017\n",
      "step: 688052, loss: 0.06431128084659576, data time: 0.010640532882125289\n",
      "step: 688053, loss: 0.06557787954807281, data time: 0.010331928730010986\n",
      "step: 688054, loss: 0.06028894707560539, data time: 0.01004820856554755\n",
      "step: 688055, loss: 0.05767050385475159, data time: 0.009791032473246256\n",
      "step: 688056, loss: 0.05807926878333092, data time: 0.00954529546922253\n",
      "step: 688057, loss: 0.06427660584449768, data time: 0.009321123361587524\n",
      "step: 688058, loss: 0.06111995875835419, data time: 0.009099859179872456\n",
      "step: 688059, loss: 0.05515238642692566, data time: 0.008887816877926098\n",
      "step: 688060, loss: 0.06182348355650902, data time: 0.008689682824271065\n",
      "step: 688061, loss: 0.056138187646865845, data time: 0.008499609099494087\n",
      "step: 688062, loss: 0.06250251829624176, data time: 0.008321388347728833\n",
      "step: 688063, loss: 0.0651429146528244, data time: 0.008158740244413676\n",
      "step: 688064, loss: 0.05665232986211777, data time: 0.00800182880499424\n",
      "step: 688065, loss: 0.09928859770298004, data time: 0.007852816581726074\n",
      "step: 688066, loss: 0.06046166643500328, data time: 0.23452472686767578\n",
      "step: 688067, loss: 0.06212809681892395, data time: 0.11865127086639404\n",
      "step: 688068, loss: 0.059650905430316925, data time: 0.08004037539164226\n",
      "step: 688069, loss: 0.0675446018576622, data time: 0.06078559160232544\n",
      "step: 688070, loss: 0.06181470304727554, data time: 0.048895406723022464\n",
      "step: 688071, loss: 0.06611289829015732, data time: 0.040979345639546715\n",
      "step: 688072, loss: 0.061509888619184494, data time: 0.035339151109967916\n",
      "step: 688073, loss: 0.06044132634997368, data time: 0.03117308020591736\n",
      "step: 688074, loss: 0.0601002536714077, data time: 0.027855952580769856\n",
      "step: 688075, loss: 0.06386207044124603, data time: 0.025272655487060546\n",
      "step: 688076, loss: 0.05525786802172661, data time: 0.023168650540438564\n",
      "step: 688077, loss: 0.061648957431316376, data time: 0.02141950527826945\n",
      "step: 688078, loss: 0.06382299959659576, data time: 0.019944209318894606\n",
      "step: 688079, loss: 0.0648399144411087, data time: 0.01866441113608224\n",
      "step: 688080, loss: 0.05954274535179138, data time: 0.017567221323649088\n",
      "step: 688081, loss: 0.0620899498462677, data time: 0.016605377197265625\n",
      "step: 688082, loss: 0.05794682726264, data time: 0.0157471965341007\n",
      "step: 688083, loss: 0.057008787989616394, data time: 0.014979004859924316\n",
      "step: 688084, loss: 0.06647088378667831, data time: 0.014294774908768503\n",
      "step: 688085, loss: 0.05843304842710495, data time: 0.013685715198516846\n",
      "step: 688086, loss: 0.06671859323978424, data time: 0.01313457034883045\n",
      "step: 688087, loss: 0.05769261717796326, data time: 0.012635231018066406\n",
      "step: 688088, loss: 0.056000083684921265, data time: 0.01217229469962742\n",
      "step: 688089, loss: 0.06133085489273071, data time: 0.011749327182769775\n",
      "step: 688090, loss: 0.06256070733070374, data time: 0.01136465072631836\n",
      "step: 688091, loss: 0.06662464141845703, data time: 0.01100326501406156\n",
      "step: 688092, loss: 0.06690959632396698, data time: 0.010682547533953632\n",
      "step: 688093, loss: 0.06029617041349411, data time: 0.010372357709067208\n",
      "step: 688094, loss: 0.06076384708285332, data time: 0.01008901102789517\n",
      "step: 688095, loss: 0.06965984404087067, data time: 0.009826032320658366\n",
      "step: 688096, loss: 0.06280557811260223, data time: 0.009578266451435705\n",
      "step: 688097, loss: 0.060986459255218506, data time: 0.009347520768642426\n",
      "step: 688098, loss: 0.06700578331947327, data time: 0.009121468572905569\n",
      "step: 688099, loss: 0.05711955949664116, data time: 0.008914737140431124\n",
      "step: 688100, loss: 0.06236901879310608, data time: 0.008714158194405693\n",
      "step: 688101, loss: 0.06401003897190094, data time: 0.008522901270124648\n",
      "step: 688102, loss: 0.059132009744644165, data time: 0.00834432808128563\n",
      "step: 688103, loss: 0.06355170160531998, data time: 0.008177857649953742\n",
      "step: 688104, loss: 0.06412909924983978, data time: 0.008020132015912961\n",
      "step: 688105, loss: 0.08538903295993805, data time: 0.00787353515625\n",
      "step: 688106, loss: 0.05524685978889465, data time: 0.23495817184448242\n",
      "step: 688107, loss: 0.058473046869039536, data time: 0.11861813068389893\n",
      "step: 688108, loss: 0.06459644436836243, data time: 0.08010339736938477\n",
      "step: 688109, loss: 0.059895969927310944, data time: 0.06085485219955444\n",
      "step: 688110, loss: 0.06027568131685257, data time: 0.048967599868774414\n",
      "step: 688111, loss: 0.06450502574443817, data time: 0.041041811307271324\n",
      "step: 688112, loss: 0.0618223212659359, data time: 0.03539868763514927\n",
      "step: 688113, loss: 0.06264136731624603, data time: 0.031227409839630127\n",
      "step: 688114, loss: 0.06205633282661438, data time: 0.027910470962524414\n",
      "step: 688115, loss: 0.06821320950984955, data time: 0.025316357612609863\n",
      "step: 688116, loss: 0.06344978511333466, data time: 0.02321460030295632\n",
      "step: 688117, loss: 0.062219586223363876, data time: 0.02146601676940918\n",
      "step: 688118, loss: 0.06492718309164047, data time: 0.019981017479529746\n",
      "step: 688119, loss: 0.06331638991832733, data time: 0.0186974150793893\n",
      "step: 688120, loss: 0.05371266230940819, data time: 0.017612346013387046\n",
      "step: 688121, loss: 0.06326290965080261, data time: 0.01664610207080841\n",
      "step: 688122, loss: 0.059462741017341614, data time: 0.015788008184993967\n",
      "step: 688123, loss: 0.05874430388212204, data time: 0.015023430188496908\n",
      "step: 688124, loss: 0.06540046632289886, data time: 0.01433740164104261\n",
      "step: 688125, loss: 0.05457371100783348, data time: 0.013732039928436279\n",
      "step: 688126, loss: 0.06485767662525177, data time: 0.013179972058250791\n",
      "step: 688127, loss: 0.06480394303798676, data time: 0.0126787315715443\n",
      "step: 688128, loss: 0.05420295149087906, data time: 0.012214650278506071\n",
      "step: 688129, loss: 0.06215616315603256, data time: 0.011790663003921509\n",
      "step: 688130, loss: 0.06205153092741966, data time: 0.011407995223999023\n",
      "step: 688131, loss: 0.06036772578954697, data time: 0.011053094497093787\n",
      "step: 688132, loss: 0.06145406886935234, data time: 0.010716164553606952\n",
      "step: 688133, loss: 0.06434939056634903, data time: 0.010406741074153356\n",
      "step: 688134, loss: 0.05664413422346115, data time: 0.010122151210390288\n",
      "step: 688135, loss: 0.06389543414115906, data time: 0.00985702673594157\n",
      "step: 688136, loss: 0.057242464274168015, data time: 0.00961347549192367\n",
      "step: 688137, loss: 0.05700179934501648, data time: 0.009385816752910614\n",
      "step: 688138, loss: 0.06303904205560684, data time: 0.00916101715781472\n",
      "step: 688139, loss: 0.06311589479446411, data time: 0.008950696272008559\n",
      "step: 688140, loss: 0.061283454298973083, data time: 0.008749730246407645\n",
      "step: 688141, loss: 0.06420709192752838, data time: 0.008559637599521212\n",
      "step: 688142, loss: 0.058811675757169724, data time: 0.008379498043575802\n",
      "step: 688143, loss: 0.0679331123828888, data time: 0.008211763281571237\n",
      "step: 688144, loss: 0.05434269830584526, data time: 0.00805349839039338\n",
      "step: 688145, loss: 0.061608221381902695, data time: 0.007903373241424561\n",
      "step: 688146, loss: 0.0677957683801651, data time: 0.22675561904907227\n",
      "step: 688147, loss: 0.063269704580307, data time: 0.11523711681365967\n",
      "step: 688148, loss: 0.06146977096796036, data time: 0.07737358411153157\n",
      "step: 688149, loss: 0.05670724809169769, data time: 0.058802127838134766\n",
      "step: 688150, loss: 0.06860433518886566, data time: 0.047321128845214847\n",
      "step: 688151, loss: 0.06544080376625061, data time: 0.03968250751495361\n",
      "step: 688152, loss: 0.06635385751724243, data time: 0.034215450286865234\n",
      "step: 688153, loss: 0.0598890483379364, data time: 0.030202418565750122\n",
      "step: 688154, loss: 0.05268900841474533, data time: 0.02700005637274848\n",
      "step: 688155, loss: 0.05816148966550827, data time: 0.024501442909240723\n",
      "step: 688156, loss: 0.057289864867925644, data time: 0.02246824177828702\n",
      "step: 688157, loss: 0.05959435924887657, data time: 0.020774940649668377\n",
      "step: 688158, loss: 0.06010933965444565, data time: 0.019347190856933594\n",
      "step: 688159, loss: 0.06684498488903046, data time: 0.018110718045915877\n",
      "step: 688160, loss: 0.05999266356229782, data time: 0.017046642303466798\n",
      "step: 688161, loss: 0.05986784026026726, data time: 0.016115814447402954\n",
      "step: 688162, loss: 0.06712807714939117, data time: 0.015289460911470302\n",
      "step: 688163, loss: 0.06447722762823105, data time: 0.014552924368116591\n",
      "step: 688164, loss: 0.06308774650096893, data time: 0.013895536723889802\n",
      "step: 688165, loss: 0.0617525652050972, data time: 0.013307249546051026\n",
      "step: 688166, loss: 0.06682202219963074, data time: 0.012794505982171921\n",
      "step: 688167, loss: 0.06122024357318878, data time: 0.012330380353060636\n",
      "step: 688168, loss: 0.06902425736188889, data time: 0.011903358542400858\n",
      "step: 688169, loss: 0.06714290380477905, data time: 0.011510719855626425\n",
      "step: 688170, loss: 0.0609152615070343, data time: 0.01114511489868164\n",
      "step: 688171, loss: 0.06372970342636108, data time: 0.010808761303241435\n",
      "step: 688172, loss: 0.06780391931533813, data time: 0.010495609707302518\n",
      "step: 688173, loss: 0.05710636079311371, data time: 0.010206852640424455\n",
      "step: 688174, loss: 0.05703311413526535, data time: 0.009943690793267611\n",
      "step: 688175, loss: 0.06744271516799927, data time: 0.009695760409037272\n",
      "step: 688176, loss: 0.062234945595264435, data time: 0.009466271246633223\n",
      "step: 688177, loss: 0.06258493661880493, data time: 0.009251788258552551\n",
      "step: 688178, loss: 0.059360407292842865, data time: 0.009034799806999437\n",
      "step: 688179, loss: 0.059609781950712204, data time: 0.008828219245461857\n",
      "step: 688180, loss: 0.06379599869251251, data time: 0.008634935106549944\n",
      "step: 688181, loss: 0.05538259446620941, data time: 0.00845088561375936\n",
      "step: 688182, loss: 0.06095251068472862, data time: 0.008278286134874498\n",
      "step: 688183, loss: 0.06466323137283325, data time: 0.008117399717632093\n",
      "step: 688184, loss: 0.06575241684913635, data time: 0.007965656427236704\n",
      "step: 688185, loss: 0.059412114322185516, data time: 0.00782114863395691\n",
      "step: 688186, loss: 0.05836153030395508, data time: 0.23220443725585938\n",
      "step: 688187, loss: 0.06226184219121933, data time: 0.11755847930908203\n",
      "step: 688188, loss: 0.060240089893341064, data time: 0.0788880983988444\n",
      "step: 688189, loss: 0.06444336473941803, data time: 0.060151636600494385\n",
      "step: 688190, loss: 0.059524696320295334, data time: 0.048400068283081056\n",
      "step: 688191, loss: 0.06075212359428406, data time: 0.040578365325927734\n",
      "step: 688192, loss: 0.05724988877773285, data time: 0.03499153682163784\n",
      "step: 688193, loss: 0.06272862106561661, data time: 0.030876368284225464\n",
      "step: 688194, loss: 0.060055725276470184, data time: 0.02759718894958496\n",
      "step: 688195, loss: 0.0654044821858406, data time: 0.025037717819213868\n",
      "step: 688196, loss: 0.057760246098041534, data time: 0.02295730330727317\n",
      "step: 688197, loss: 0.06274451315402985, data time: 0.021221717198689777\n",
      "step: 688198, loss: 0.06418447941541672, data time: 0.019755968680748574\n",
      "step: 688199, loss: 0.061758678406476974, data time: 0.01849445274897984\n",
      "step: 688200, loss: 0.059079479426145554, data time: 0.0174012025197347\n",
      "step: 688201, loss: 0.06372478604316711, data time: 0.016440704464912415\n",
      "step: 688202, loss: 0.05878790467977524, data time: 0.015593683018403895\n",
      "step: 688203, loss: 0.05902404710650444, data time: 0.014835225211249458\n",
      "step: 688204, loss: 0.06021059677004814, data time: 0.014159641767802992\n",
      "step: 688205, loss: 0.06228562444448471, data time: 0.013562393188476563\n",
      "step: 688206, loss: 0.05800984799861908, data time: 0.013023944128127326\n",
      "step: 688207, loss: 0.06581246852874756, data time: 0.012530912052501331\n",
      "step: 688208, loss: 0.06409754604101181, data time: 0.012073185132897419\n",
      "step: 688209, loss: 0.06147819757461548, data time: 0.011659512917200724\n",
      "step: 688210, loss: 0.06444571912288666, data time: 0.011275110244750976\n",
      "step: 688211, loss: 0.05345345661044121, data time: 0.010922670364379883\n",
      "step: 688212, loss: 0.06415852904319763, data time: 0.01059146280641909\n",
      "step: 688213, loss: 0.06518082320690155, data time: 0.01029059716633388\n",
      "step: 688214, loss: 0.0642182007431984, data time: 0.01000805558829472\n",
      "step: 688215, loss: 0.0634598582983017, data time: 0.009744954109191895\n",
      "step: 688216, loss: 0.06477239727973938, data time: 0.009499357592674994\n",
      "step: 688217, loss: 0.05657656118273735, data time: 0.009275861084461212\n",
      "step: 688218, loss: 0.057895272970199585, data time: 0.009055614471435547\n",
      "step: 688219, loss: 0.06377334892749786, data time: 0.00884549056782442\n",
      "step: 688220, loss: 0.06421326100826263, data time: 0.00865131105695452\n",
      "step: 688221, loss: 0.06314516067504883, data time: 0.00847768121295505\n",
      "step: 688222, loss: 0.059703387320041656, data time: 0.008300459062730943\n",
      "step: 688223, loss: 0.060997821390628815, data time: 0.00813479172556024\n",
      "step: 688224, loss: 0.057701289653778076, data time: 0.007978555483695788\n",
      "step: 688225, loss: 0.05221521481871605, data time: 0.007831984758377075\n",
      "step: 688226, loss: 0.06110424920916557, data time: 0.2529127597808838\n",
      "step: 688227, loss: 0.0640784427523613, data time: 0.12724030017852783\n",
      "step: 688228, loss: 0.05834197252988815, data time: 0.08577211697896321\n",
      "step: 688229, loss: 0.0631549283862114, data time: 0.06509464979171753\n",
      "step: 688230, loss: 0.06111964210867882, data time: 0.0523566722869873\n",
      "step: 688231, loss: 0.0671650618314743, data time: 0.04388117790222168\n",
      "step: 688232, loss: 0.06134935840964317, data time: 0.03782708304268973\n",
      "step: 688233, loss: 0.061010025441646576, data time: 0.03334912657737732\n",
      "step: 688234, loss: 0.06219131872057915, data time: 0.029788944456312392\n",
      "step: 688235, loss: 0.06281071156263351, data time: 0.027022194862365723\n",
      "step: 688236, loss: 0.06568249315023422, data time: 0.024758620695634323\n",
      "step: 688237, loss: 0.05978803336620331, data time: 0.022875507672627766\n",
      "step: 688238, loss: 0.06524507701396942, data time: 0.021279169962956354\n",
      "step: 688239, loss: 0.06725597381591797, data time: 0.019902927534920827\n",
      "step: 688240, loss: 0.0648360401391983, data time: 0.018717177708943687\n",
      "step: 688241, loss: 0.06259846687316895, data time: 0.017687171697616577\n",
      "step: 688242, loss: 0.06132137030363083, data time: 0.016770601272583008\n",
      "step: 688243, loss: 0.05547402799129486, data time: 0.015949898295932345\n",
      "step: 688244, loss: 0.060371510684490204, data time: 0.015216752102500513\n",
      "step: 688245, loss: 0.06690581142902374, data time: 0.014564895629882812\n",
      "step: 688246, loss: 0.060410067439079285, data time: 0.013975279671805245\n",
      "step: 688247, loss: 0.0648389607667923, data time: 0.013456301255659624\n",
      "step: 688248, loss: 0.06333594769239426, data time: 0.012960506522137186\n",
      "step: 688249, loss: 0.0668925866484642, data time: 0.012524296840031942\n",
      "step: 688250, loss: 0.05987292528152466, data time: 0.01212493896484375\n",
      "step: 688251, loss: 0.06176954507827759, data time: 0.01175240369943472\n",
      "step: 688252, loss: 0.06089082360267639, data time: 0.01140380788732458\n",
      "step: 688253, loss: 0.0609903447329998, data time: 0.011083756174360002\n",
      "step: 688254, loss: 0.053907379508018494, data time: 0.010791721015140927\n",
      "step: 688255, loss: 0.06226954981684685, data time: 0.010516532262166341\n",
      "step: 688256, loss: 0.05791347473859787, data time: 0.01025997438738423\n",
      "step: 688257, loss: 0.06400269269943237, data time: 0.010021045804023743\n",
      "step: 688258, loss: 0.06035931408405304, data time: 0.009780211882157759\n",
      "step: 688259, loss: 0.06492827832698822, data time: 0.00955519956700942\n",
      "step: 688260, loss: 0.05663173645734787, data time: 0.009339720862252371\n",
      "step: 688261, loss: 0.05851616710424423, data time: 0.009134504530164931\n",
      "step: 688262, loss: 0.06139255687594414, data time: 0.008941740603060336\n",
      "step: 688263, loss: 0.06427529454231262, data time: 0.008762685876143607\n",
      "step: 688264, loss: 0.07011183351278305, data time: 0.00859523430848733\n",
      "step: 688265, loss: 0.06409724056720734, data time: 0.008434808254241944\n",
      "step: 688266, loss: 0.06324721872806549, data time: 0.2340250015258789\n",
      "step: 688267, loss: 0.06084608659148216, data time: 0.11776590347290039\n",
      "step: 688268, loss: 0.05790697783231735, data time: 0.07958642641703288\n",
      "step: 688269, loss: 0.06302487105131149, data time: 0.06045275926589966\n",
      "step: 688270, loss: 0.05880899354815483, data time: 0.0486790657043457\n",
      "step: 688271, loss: 0.05796955153346062, data time: 0.04080275694529215\n",
      "step: 688272, loss: 0.06327983736991882, data time: 0.03518251010349819\n",
      "step: 688273, loss: 0.06292632222175598, data time: 0.031052201986312866\n",
      "step: 688274, loss: 0.06203722208738327, data time: 0.02774654494391547\n",
      "step: 688275, loss: 0.058802030980587006, data time: 0.025176525115966797\n",
      "step: 688276, loss: 0.062489770352840424, data time: 0.023101936687122692\n",
      "step: 688277, loss: 0.06329655647277832, data time: 0.021351794401804607\n",
      "step: 688278, loss: 0.05553961917757988, data time: 0.019874004217294548\n",
      "step: 688279, loss: 0.059524055570364, data time: 0.018599629402160645\n",
      "step: 688280, loss: 0.062200337648391724, data time: 0.017493422826131186\n",
      "step: 688281, loss: 0.06050816923379898, data time: 0.016524866223335266\n",
      "step: 688282, loss: 0.0636838972568512, data time: 0.015678139293895048\n",
      "step: 688283, loss: 0.0644349455833435, data time: 0.014918790923224555\n",
      "step: 688284, loss: 0.0672779306769371, data time: 0.014243138463873612\n",
      "step: 688285, loss: 0.05601485073566437, data time: 0.013636636734008788\n",
      "step: 688286, loss: 0.06295569986104965, data time: 0.013090235846383231\n",
      "step: 688287, loss: 0.060719653964042664, data time: 0.012593106790022417\n",
      "step: 688288, loss: 0.06251177191734314, data time: 0.012135795924974524\n",
      "step: 688289, loss: 0.06300662457942963, data time: 0.0117147167523702\n",
      "step: 688290, loss: 0.06356849521398544, data time: 0.011327743530273438\n",
      "step: 688291, loss: 0.06247891113162041, data time: 0.010974563085115872\n",
      "step: 688292, loss: 0.059985529631376266, data time: 0.010640903755470558\n",
      "step: 688293, loss: 0.062455516308546066, data time: 0.010346923555646623\n",
      "step: 688294, loss: 0.05938513204455376, data time: 0.01007742717348296\n",
      "step: 688295, loss: 0.05979697033762932, data time: 0.009825555483500163\n",
      "step: 688296, loss: 0.062449436634778976, data time: 0.009591002618112871\n",
      "step: 688297, loss: 0.06564174592494965, data time: 0.009372010827064514\n",
      "step: 688298, loss: 0.060730062425136566, data time: 0.0091497464613481\n",
      "step: 688299, loss: 0.06458404660224915, data time: 0.008940857999465045\n",
      "step: 688300, loss: 0.06324909627437592, data time: 0.008746835163661412\n",
      "step: 688301, loss: 0.0643053948879242, data time: 0.008558975325690376\n",
      "step: 688302, loss: 0.06282501667737961, data time: 0.008383319184586808\n",
      "step: 688303, loss: 0.06254175305366516, data time: 0.008219813045702483\n",
      "step: 688304, loss: 0.06164361536502838, data time: 0.008065376526270157\n",
      "step: 688305, loss: 0.05606343597173691, data time: 0.007917577028274536\n",
      "step: 688306, loss: 0.05702286586165428, data time: 0.23065853118896484\n",
      "step: 688307, loss: 0.06370579451322556, data time: 0.11673760414123535\n",
      "step: 688308, loss: 0.06700004637241364, data time: 0.07886632283528645\n",
      "step: 688309, loss: 0.06534742563962936, data time: 0.05982184410095215\n",
      "step: 688310, loss: 0.06241638585925102, data time: 0.04814038276672363\n",
      "step: 688311, loss: 0.06499454379081726, data time: 0.04036815961201986\n",
      "step: 688312, loss: 0.06248238682746887, data time: 0.03481881959097726\n",
      "step: 688313, loss: 0.05754038691520691, data time: 0.030724018812179565\n",
      "step: 688314, loss: 0.06354722380638123, data time: 0.027456416024102107\n",
      "step: 688315, loss: 0.06650111824274063, data time: 0.02491490840911865\n",
      "step: 688316, loss: 0.06607149541378021, data time: 0.022846547040072353\n",
      "step: 688317, loss: 0.06566682457923889, data time: 0.02113819122314453\n",
      "step: 688318, loss: 0.06491044908761978, data time: 0.019681673783522386\n",
      "step: 688319, loss: 0.06376843899488449, data time: 0.01842059407915388\n",
      "step: 688320, loss: 0.059758394956588745, data time: 0.017334985733032226\n",
      "step: 688321, loss: 0.05951647460460663, data time: 0.01638314127922058\n",
      "step: 688322, loss: 0.0590408556163311, data time: 0.015540038838106044\n",
      "step: 688323, loss: 0.06163404509425163, data time: 0.014789329634772407\n",
      "step: 688324, loss: 0.05994602292776108, data time: 0.014116262134752776\n",
      "step: 688325, loss: 0.06163284182548523, data time: 0.01351778507232666\n",
      "step: 688326, loss: 0.061621084809303284, data time: 0.012975919814336868\n",
      "step: 688327, loss: 0.05831165239214897, data time: 0.012485959313132546\n",
      "step: 688328, loss: 0.06023910641670227, data time: 0.012034001557723335\n",
      "step: 688329, loss: 0.06466557085514069, data time: 0.011617889006932577\n",
      "step: 688330, loss: 0.06462176889181137, data time: 0.011235437393188476\n",
      "step: 688331, loss: 0.06056949496269226, data time: 0.010880011778611403\n",
      "step: 688332, loss: 0.05766860395669937, data time: 0.010549130263151947\n",
      "step: 688333, loss: 0.06145486980676651, data time: 0.010243867124829973\n",
      "step: 688334, loss: 0.05810413509607315, data time: 0.009965016924101731\n",
      "step: 688335, loss: 0.058078616857528687, data time: 0.009707363446553548\n",
      "step: 688336, loss: 0.060772091150283813, data time: 0.00946799401314028\n",
      "step: 688337, loss: 0.0650206059217453, data time: 0.00924423336982727\n",
      "step: 688338, loss: 0.05380364507436752, data time: 0.009025674877744732\n",
      "step: 688339, loss: 0.05968844145536423, data time: 0.008820113013772404\n",
      "step: 688340, loss: 0.0628877505660057, data time: 0.008624288014003209\n",
      "step: 688341, loss: 0.06272056698799133, data time: 0.008435772524939643\n",
      "step: 688342, loss: 0.05658618360757828, data time: 0.008259051554911846\n",
      "step: 688343, loss: 0.060463786125183105, data time: 0.0080965004469219\n",
      "step: 688344, loss: 0.06483904272317886, data time: 0.007941349958762145\n",
      "step: 688345, loss: 0.06814983487129211, data time: 0.007795888185501099\n",
      "step: 688346, loss: 0.061183005571365356, data time: 0.23914504051208496\n",
      "step: 688347, loss: 0.061505477875471115, data time: 0.1207195520401001\n",
      "step: 688348, loss: 0.05298922210931778, data time: 0.08165788650512695\n",
      "step: 688349, loss: 0.06627275049686432, data time: 0.06217014789581299\n",
      "step: 688350, loss: 0.06446181237697601, data time: 0.05006065368652344\n",
      "step: 688351, loss: 0.058440905064344406, data time: 0.041996280352274575\n",
      "step: 688352, loss: 0.05972683057188988, data time: 0.03624388149806431\n",
      "step: 688353, loss: 0.06670038402080536, data time: 0.03201824426651001\n",
      "step: 688354, loss: 0.06484605371952057, data time: 0.028643025292290583\n",
      "step: 688355, loss: 0.061472758650779724, data time: 0.02602407932281494\n",
      "step: 688356, loss: 0.06374022364616394, data time: 0.023883407766168766\n",
      "step: 688357, loss: 0.059937696903944016, data time: 0.022107601165771484\n",
      "step: 688358, loss: 0.06404061615467072, data time: 0.020606334392841045\n",
      "step: 688359, loss: 0.0711621418595314, data time: 0.019305688994271413\n",
      "step: 688360, loss: 0.060181401669979095, data time: 0.018178812662760415\n",
      "step: 688361, loss: 0.06479538977146149, data time: 0.017197713255882263\n",
      "step: 688362, loss: 0.057549428194761276, data time: 0.01632958299973432\n",
      "step: 688363, loss: 0.05989977717399597, data time: 0.01555172602335612\n",
      "step: 688364, loss: 0.0624244324862957, data time: 0.014858898363615336\n",
      "step: 688365, loss: 0.05753154680132866, data time: 0.01423959732055664\n",
      "step: 688366, loss: 0.05999185889959335, data time: 0.013682126998901367\n",
      "step: 688367, loss: 0.0668007954955101, data time: 0.013174718076532537\n",
      "step: 688368, loss: 0.06159673258662224, data time: 0.012706435245016346\n",
      "step: 688369, loss: 0.05891411751508713, data time: 0.012281397978464762\n",
      "step: 688370, loss: 0.06052688881754875, data time: 0.011889638900756836\n",
      "step: 688371, loss: 0.05987870320677757, data time: 0.011522595699016865\n",
      "step: 688372, loss: 0.06446637213230133, data time: 0.01118143399556478\n",
      "step: 688373, loss: 0.06481706351041794, data time: 0.010869128363473075\n",
      "step: 688374, loss: 0.07213642448186874, data time: 0.010596036911010742\n",
      "step: 688375, loss: 0.054330479353666306, data time: 0.010328722000122071\n",
      "step: 688376, loss: 0.0645129606127739, data time: 0.010076499754382719\n",
      "step: 688377, loss: 0.06463778018951416, data time: 0.009842753410339355\n",
      "step: 688378, loss: 0.06094039976596832, data time: 0.009607813575051048\n",
      "step: 688379, loss: 0.061901867389678955, data time: 0.009386399213005514\n",
      "step: 688380, loss: 0.056105997413396835, data time: 0.009175879614693778\n",
      "step: 688381, loss: 0.056855253875255585, data time: 0.008975042237175835\n",
      "step: 688382, loss: 0.05767098069190979, data time: 0.008788050831975164\n",
      "step: 688383, loss: 0.06556510925292969, data time: 0.00861563808039615\n",
      "step: 688384, loss: 0.05865136533975601, data time: 0.008450587590535482\n",
      "step: 688385, loss: 0.06735248863697052, data time: 0.008292967081069946\n",
      "step: 688386, loss: 0.06017044186592102, data time: 0.2268986701965332\n",
      "step: 688387, loss: 0.06592921912670135, data time: 0.11480748653411865\n",
      "step: 688388, loss: 0.0579872727394104, data time: 0.07814900080362956\n",
      "step: 688389, loss: 0.06213397905230522, data time: 0.0594218373298645\n",
      "step: 688390, loss: 0.0632534921169281, data time: 0.04786829948425293\n",
      "step: 688391, loss: 0.059897057712078094, data time: 0.040165980656941734\n",
      "step: 688392, loss: 0.060065578669309616, data time: 0.03467369079589844\n",
      "step: 688393, loss: 0.0601799301803112, data time: 0.030639290809631348\n",
      "step: 688394, loss: 0.06566812843084335, data time: 0.02742322285970052\n",
      "step: 688395, loss: 0.062449973076581955, data time: 0.024919295310974122\n",
      "step: 688396, loss: 0.06489107012748718, data time: 0.022885582663796165\n",
      "step: 688397, loss: 0.06699785590171814, data time: 0.02118961016337077\n",
      "step: 688398, loss: 0.06440626084804535, data time: 0.019754428129929762\n",
      "step: 688399, loss: 0.0678328350186348, data time: 0.018515910421098982\n",
      "step: 688400, loss: 0.06948763132095337, data time: 0.01744248072306315\n",
      "step: 688401, loss: 0.0711723268032074, data time: 0.016508668661117554\n",
      "step: 688402, loss: 0.0687352642416954, data time: 0.015677199644200942\n",
      "step: 688403, loss: 0.06017399951815605, data time: 0.014934553040398492\n",
      "step: 688404, loss: 0.060592830181121826, data time: 0.014277119385568719\n",
      "step: 688405, loss: 0.05719442293047905, data time: 0.013692748546600342\n",
      "step: 688406, loss: 0.05673687905073166, data time: 0.013161874952770415\n",
      "step: 688407, loss: 0.05824536830186844, data time: 0.012674851851029829\n",
      "step: 688408, loss: 0.06205442547798157, data time: 0.012229100517604662\n",
      "step: 688409, loss: 0.06600715965032578, data time: 0.011821985244750977\n",
      "step: 688410, loss: 0.0683620497584343, data time: 0.011451692581176757\n",
      "step: 688411, loss: 0.05981546640396118, data time: 0.011104592910179725\n",
      "step: 688412, loss: 0.062366485595703125, data time: 0.010779742841367368\n",
      "step: 688413, loss: 0.05782937631011009, data time: 0.010480914797101702\n",
      "step: 688414, loss: 0.05425581336021423, data time: 0.010206584272713497\n",
      "step: 688415, loss: 0.060575664043426514, data time: 0.009949549039204916\n",
      "step: 688416, loss: 0.06350436806678772, data time: 0.009711119436448621\n",
      "step: 688417, loss: 0.0598917119204998, data time: 0.00948796421289444\n",
      "step: 688418, loss: 0.0659816563129425, data time: 0.009261969364050663\n",
      "step: 688419, loss: 0.062408532947301865, data time: 0.009052564116085278\n",
      "step: 688420, loss: 0.06329100579023361, data time: 0.008851214817592075\n",
      "step: 688421, loss: 0.05832128971815109, data time: 0.00866095225016276\n",
      "step: 688422, loss: 0.06622324883937836, data time: 0.008483229456721125\n",
      "step: 688423, loss: 0.06051114946603775, data time: 0.008316956068340101\n",
      "step: 688424, loss: 0.058672260493040085, data time: 0.008159735263922276\n",
      "step: 688425, loss: 0.060811977833509445, data time: 0.008010196685791015\n",
      "step: 688426, loss: 0.06377922743558884, data time: 0.23708534240722656\n",
      "step: 688427, loss: 0.06405602395534515, data time: 0.11931586265563965\n",
      "step: 688428, loss: 0.062331050634384155, data time: 0.08045713106791179\n",
      "step: 688429, loss: 0.06166251748800278, data time: 0.061125755310058594\n",
      "step: 688430, loss: 0.06457837671041489, data time: 0.04918813705444336\n",
      "step: 688431, loss: 0.062187545001506805, data time: 0.04124422868092855\n",
      "step: 688432, loss: 0.061380114406347275, data time: 0.03559844834463937\n",
      "step: 688433, loss: 0.05710478127002716, data time: 0.03140333294868469\n",
      "step: 688434, loss: 0.0669483095407486, data time: 0.02806136343214247\n",
      "step: 688435, loss: 0.06193283200263977, data time: 0.02545626163482666\n",
      "step: 688436, loss: 0.07197760045528412, data time: 0.02333786270835183\n",
      "step: 688437, loss: 0.06460762023925781, data time: 0.021573781967163086\n",
      "step: 688438, loss: 0.0626164898276329, data time: 0.020079704431387093\n",
      "step: 688439, loss: 0.05449850484728813, data time: 0.018791913986206055\n",
      "step: 688440, loss: 0.06963501125574112, data time: 0.017676289876302084\n",
      "step: 688441, loss: 0.060074955224990845, data time: 0.016712427139282227\n",
      "step: 688442, loss: 0.056035272777080536, data time: 0.015851778142592487\n",
      "step: 688443, loss: 0.0629650354385376, data time: 0.015077657169765897\n",
      "step: 688444, loss: 0.062122222036123276, data time: 0.014389489826403166\n",
      "step: 688445, loss: 0.05962444469332695, data time: 0.013778865337371826\n",
      "step: 688446, loss: 0.060817599296569824, data time: 0.013234388260614304\n",
      "step: 688447, loss: 0.061232589185237885, data time: 0.01273721998388117\n",
      "step: 688448, loss: 0.06138267368078232, data time: 0.012271466462508492\n",
      "step: 688449, loss: 0.06068786606192589, data time: 0.011847396691640219\n",
      "step: 688450, loss: 0.06422696262598038, data time: 0.011461000442504882\n",
      "step: 688451, loss: 0.06451410055160522, data time: 0.011099631969745342\n",
      "step: 688452, loss: 0.06608061492443085, data time: 0.010764351597538701\n",
      "step: 688453, loss: 0.05661571025848389, data time: 0.010451861790248327\n",
      "step: 688454, loss: 0.06728745996952057, data time: 0.010167360305786133\n",
      "step: 688455, loss: 0.058780230581760406, data time: 0.009899457295735678\n",
      "step: 688456, loss: 0.054674625396728516, data time: 0.009654098941433814\n",
      "step: 688457, loss: 0.06323575973510742, data time: 0.009425044059753418\n",
      "step: 688458, loss: 0.053561609238386154, data time: 0.009203722982695608\n",
      "step: 688459, loss: 0.0578056275844574, data time: 0.008996058912838207\n",
      "step: 688460, loss: 0.06493806838989258, data time: 0.008797086988176618\n",
      "step: 688461, loss: 0.06708606332540512, data time: 0.008606804741753472\n",
      "step: 688462, loss: 0.056958626955747604, data time: 0.008428747589523727\n",
      "step: 688463, loss: 0.06472713500261307, data time: 0.008262935437654195\n",
      "step: 688464, loss: 0.06760342419147491, data time: 0.008106696300017528\n",
      "step: 688465, loss: 0.08266837894916534, data time: 0.007959717512130737\n",
      "step: 688466, loss: 0.07337431609630585, data time: 0.23312807083129883\n",
      "step: 688467, loss: 0.06054574251174927, data time: 0.11792290210723877\n",
      "step: 688468, loss: 0.05855731666088104, data time: 0.07952475547790527\n",
      "step: 688469, loss: 0.06454252451658249, data time: 0.06042104959487915\n",
      "step: 688470, loss: 0.06098045036196709, data time: 0.048609685897827146\n",
      "step: 688471, loss: 0.06444588303565979, data time: 0.040762106577555336\n",
      "step: 688472, loss: 0.06312866508960724, data time: 0.03514483996800014\n",
      "step: 688473, loss: 0.06644164025783539, data time: 0.0310075581073761\n",
      "step: 688474, loss: 0.06210945174098015, data time: 0.027707364824083116\n",
      "step: 688475, loss: 0.06613200902938843, data time: 0.02514359951019287\n",
      "step: 688476, loss: 0.05935686081647873, data time: 0.02306201241233132\n",
      "step: 688477, loss: 0.06507259607315063, data time: 0.021325925985972088\n",
      "step: 688478, loss: 0.062370382249355316, data time: 0.019854398874136116\n",
      "step: 688479, loss: 0.06287644803524017, data time: 0.018577677862984792\n",
      "step: 688480, loss: 0.06055816262960434, data time: 0.01748345692952474\n",
      "step: 688481, loss: 0.06723358482122421, data time: 0.0165194571018219\n",
      "step: 688482, loss: 0.05730506405234337, data time: 0.015665615306181067\n",
      "step: 688483, loss: 0.06189815327525139, data time: 0.014902869860331217\n",
      "step: 688484, loss: 0.06040181964635849, data time: 0.014227264805843956\n",
      "step: 688485, loss: 0.06161504238843918, data time: 0.013623595237731934\n",
      "step: 688486, loss: 0.06437592953443527, data time: 0.013076736813499815\n",
      "step: 688487, loss: 0.061332277953624725, data time: 0.012578346512534401\n",
      "step: 688488, loss: 0.06145758926868439, data time: 0.012122786563375721\n",
      "step: 688489, loss: 0.061175696551799774, data time: 0.011723210414250692\n",
      "step: 688490, loss: 0.05638431757688522, data time: 0.01135082244873047\n",
      "step: 688491, loss: 0.062131062150001526, data time: 0.011007905006408691\n",
      "step: 688492, loss: 0.061812933534383774, data time: 0.010687660287927699\n",
      "step: 688493, loss: 0.06451398134231567, data time: 0.010391277926308768\n",
      "step: 688494, loss: 0.06256325542926788, data time: 0.010118599595694706\n",
      "step: 688495, loss: 0.06272216141223907, data time: 0.009864878654479981\n",
      "step: 688496, loss: 0.05904997140169144, data time: 0.009627196096604871\n",
      "step: 688497, loss: 0.06860184669494629, data time: 0.009410299360752106\n",
      "step: 688498, loss: 0.06194201856851578, data time: 0.009191195170084635\n",
      "step: 688499, loss: 0.06286069750785828, data time: 0.008982125450583064\n",
      "step: 688500, loss: 0.05772736668586731, data time: 0.008784478051321848\n",
      "step: 688501, loss: 0.06010323017835617, data time: 0.0086061159769694\n",
      "step: 688502, loss: 0.055689431726932526, data time: 0.008428547833416913\n",
      "step: 688503, loss: 0.06310442090034485, data time: 0.008263939305355674\n",
      "step: 688504, loss: 0.0637211874127388, data time: 0.008107753900381235\n",
      "step: 688505, loss: 0.05975989252328873, data time: 0.00796060562133789\n",
      "step: 688506, loss: 0.05962339788675308, data time: 0.2360396385192871\n",
      "step: 688507, loss: 0.058998458087444305, data time: 0.11936020851135254\n",
      "step: 688508, loss: 0.06036556884646416, data time: 0.08063960075378418\n",
      "step: 688509, loss: 0.06011872738599777, data time: 0.061047911643981934\n",
      "step: 688510, loss: 0.06066541001200676, data time: 0.04912972450256348\n",
      "step: 688511, loss: 0.06409865617752075, data time: 0.04123854637145996\n",
      "step: 688512, loss: 0.06286013126373291, data time: 0.03577191489083426\n",
      "step: 688513, loss: 0.057846419513225555, data time: 0.03150373697280884\n",
      "step: 688514, loss: 0.06307381391525269, data time: 0.028176678551567927\n",
      "step: 688515, loss: 0.06090749427676201, data time: 0.025603652000427246\n",
      "step: 688516, loss: 0.06592898070812225, data time: 0.023513598875565964\n",
      "step: 688517, loss: 0.06836149096488953, data time: 0.021769126256306965\n",
      "step: 688518, loss: 0.06194647401571274, data time: 0.020302405724158652\n",
      "step: 688519, loss: 0.06427446752786636, data time: 0.019025070326668874\n",
      "step: 688520, loss: 0.05897746607661247, data time: 0.017924753824869792\n",
      "step: 688521, loss: 0.058562975376844406, data time: 0.01695658266544342\n",
      "step: 688522, loss: 0.062238678336143494, data time: 0.016098990159876207\n",
      "step: 688523, loss: 0.06657133251428604, data time: 0.015334950553046333\n",
      "step: 688524, loss: 0.0629798173904419, data time: 0.01465212671380294\n",
      "step: 688525, loss: 0.06391577422618866, data time: 0.014046478271484374\n",
      "step: 688526, loss: 0.061716675758361816, data time: 0.013500315802437919\n",
      "step: 688527, loss: 0.05701766535639763, data time: 0.0130024714903398\n",
      "step: 688528, loss: 0.05842602998018265, data time: 0.012542113013889479\n",
      "step: 688529, loss: 0.05532523989677429, data time: 0.012124170859654745\n",
      "step: 688530, loss: 0.05830208584666252, data time: 0.011737394332885741\n",
      "step: 688531, loss: 0.06689401715993881, data time: 0.011378792616037222\n",
      "step: 688532, loss: 0.066923588514328, data time: 0.0110439900998716\n",
      "step: 688533, loss: 0.060935523360967636, data time: 0.01073542663029262\n",
      "step: 688534, loss: 0.06385333091020584, data time: 0.010451621022717706\n",
      "step: 688535, loss: 0.06476995348930359, data time: 0.010186982154846192\n",
      "step: 688536, loss: 0.06022189185023308, data time: 0.00994002434515184\n",
      "step: 688537, loss: 0.0631852000951767, data time: 0.009714357554912567\n",
      "step: 688538, loss: 0.06306639313697815, data time: 0.009486631913618608\n",
      "step: 688539, loss: 0.0656459778547287, data time: 0.009270590894362506\n",
      "step: 688540, loss: 0.06389512121677399, data time: 0.009063155310494559\n",
      "step: 688541, loss: 0.06221611052751541, data time: 0.008865985605451796\n",
      "step: 688542, loss: 0.062323566526174545, data time: 0.008680762471379461\n",
      "step: 688543, loss: 0.06404711306095123, data time: 0.008509334764982524\n",
      "step: 688544, loss: 0.06917925179004669, data time: 0.008345989080575796\n",
      "step: 688545, loss: 0.06004440039396286, data time: 0.00819135308265686\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4108848571777344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 688546, loss: 0.06287798285484314, data time: 0.23111796379089355\n",
      "step: 688547, loss: 0.061370424926280975, data time: 0.11691629886627197\n",
      "step: 688548, loss: 0.06794945150613785, data time: 0.07885273297627766\n",
      "step: 688549, loss: 0.06022481247782707, data time: 0.05989706516265869\n",
      "step: 688550, loss: 0.06382276117801666, data time: 0.04817018508911133\n",
      "step: 688551, loss: 0.06627625226974487, data time: 0.04037956396738688\n",
      "step: 688552, loss: 0.05889640748500824, data time: 0.03482675552368164\n",
      "step: 688553, loss: 0.06512407213449478, data time: 0.03072291612625122\n",
      "step: 688554, loss: 0.060384027659893036, data time: 0.027445104387071397\n",
      "step: 688555, loss: 0.06349479407072067, data time: 0.024892902374267577\n",
      "step: 688556, loss: 0.06096076965332031, data time: 0.02283599159934304\n",
      "step: 688557, loss: 0.06095176190137863, data time: 0.02111359437306722\n",
      "step: 688558, loss: 0.0659283846616745, data time: 0.01966129816495455\n",
      "step: 688559, loss: 0.06180587038397789, data time: 0.01839024680001395\n",
      "step: 688560, loss: 0.06356913596391678, data time: 0.017297808329264322\n",
      "step: 688561, loss: 0.0628255158662796, data time: 0.016341224312782288\n",
      "step: 688562, loss: 0.06320055574178696, data time: 0.015496408238130458\n",
      "step: 688563, loss: 0.06192317605018616, data time: 0.014738268322414823\n",
      "step: 688564, loss: 0.05903734266757965, data time: 0.014064989591899672\n",
      "step: 688565, loss: 0.05850410461425781, data time: 0.013469099998474121\n",
      "step: 688566, loss: 0.06275375187397003, data time: 0.012931948616391136\n",
      "step: 688567, loss: 0.06399476528167725, data time: 0.012436530806801536\n",
      "step: 688568, loss: 0.06283107399940491, data time: 0.011977475622425909\n",
      "step: 688569, loss: 0.06242769956588745, data time: 0.011563539505004883\n",
      "step: 688570, loss: 0.05521528422832489, data time: 0.011180791854858398\n",
      "step: 688571, loss: 0.05598393827676773, data time: 0.010826826095581055\n",
      "step: 688572, loss: 0.06451722234487534, data time: 0.010496316132722077\n",
      "step: 688573, loss: 0.05717400833964348, data time: 0.010190750871385847\n",
      "step: 688574, loss: 0.059604596346616745, data time: 0.00991389669221023\n",
      "step: 688575, loss: 0.06405673921108246, data time: 0.009653878211975098\n",
      "step: 688576, loss: 0.05625433847308159, data time: 0.009409389188212732\n",
      "step: 688577, loss: 0.06435893476009369, data time: 0.009181439876556396\n",
      "step: 688578, loss: 0.06219756603240967, data time: 0.008961186264500473\n",
      "step: 688579, loss: 0.05868375301361084, data time: 0.008752907023710363\n",
      "step: 688580, loss: 0.0626571998000145, data time: 0.008558375494820731\n",
      "step: 688581, loss: 0.07224267721176147, data time: 0.008370088206397163\n",
      "step: 688582, loss: 0.06563368439674377, data time: 0.008196875855729386\n",
      "step: 688583, loss: 0.0693431943655014, data time: 0.00803527079130474\n",
      "step: 688584, loss: 0.05747421830892563, data time: 0.007882503362802358\n",
      "step: 688585, loss: 0.055479954928159714, data time: 0.007733774185180664\n",
      "step: 688586, loss: 0.06122290715575218, data time: 0.24449825286865234\n",
      "step: 688587, loss: 0.054660242050886154, data time: 0.12360084056854248\n",
      "step: 688588, loss: 0.06397902965545654, data time: 0.08327865600585938\n",
      "step: 688589, loss: 0.06548938155174255, data time: 0.06324398517608643\n",
      "step: 688590, loss: 0.06389743089675903, data time: 0.05088028907775879\n",
      "step: 688591, loss: 0.058742623776197433, data time: 0.04262967904408773\n",
      "step: 688592, loss: 0.06501398235559464, data time: 0.03673693111964634\n",
      "step: 688593, loss: 0.05175282061100006, data time: 0.03239583969116211\n",
      "step: 688594, loss: 0.0658772811293602, data time: 0.028932836320665147\n",
      "step: 688595, loss: 0.0625685304403305, data time: 0.02623782157897949\n",
      "step: 688596, loss: 0.07113133370876312, data time: 0.02404310486533425\n",
      "step: 688597, loss: 0.05772598832845688, data time: 0.02222001552581787\n",
      "step: 688598, loss: 0.05395812913775444, data time: 0.020670175552368164\n",
      "step: 688599, loss: 0.06437352299690247, data time: 0.019331608499799455\n",
      "step: 688600, loss: 0.06372803449630737, data time: 0.018178415298461915\n",
      "step: 688601, loss: 0.05674639344215393, data time: 0.01716923713684082\n",
      "step: 688602, loss: 0.06432463228702545, data time: 0.016278505325317383\n",
      "step: 688603, loss: 0.065045066177845, data time: 0.015477736790974935\n",
      "step: 688604, loss: 0.06044374033808708, data time: 0.014768776140714946\n",
      "step: 688605, loss: 0.06062212586402893, data time: 0.014142370223999024\n",
      "step: 688606, loss: 0.06414728611707687, data time: 0.013571841376168388\n",
      "step: 688607, loss: 0.06359931826591492, data time: 0.01305180246179754\n",
      "step: 688608, loss: 0.06268565356731415, data time: 0.012569738470989725\n",
      "step: 688609, loss: 0.05969684571027756, data time: 0.012132743994394938\n",
      "step: 688610, loss: 0.06832244992256165, data time: 0.011727666854858399\n",
      "step: 688611, loss: 0.05918952822685242, data time: 0.011360562764681302\n",
      "step: 688612, loss: 0.061572350561618805, data time: 0.011008880756519459\n",
      "step: 688613, loss: 0.07301713526248932, data time: 0.010686448642185755\n",
      "step: 688614, loss: 0.0630916953086853, data time: 0.010391218908901873\n",
      "step: 688615, loss: 0.06143111735582352, data time: 0.010115424791971842\n",
      "step: 688616, loss: 0.06373944133520126, data time: 0.009858315990817162\n",
      "step: 688617, loss: 0.056639980524778366, data time: 0.00962337851524353\n",
      "step: 688618, loss: 0.059887126088142395, data time: 0.00939067927273837\n",
      "step: 688619, loss: 0.06108003854751587, data time: 0.009169634650735295\n",
      "step: 688620, loss: 0.06699445098638535, data time: 0.008962038585117885\n",
      "step: 688621, loss: 0.06341937184333801, data time: 0.008762333128187392\n",
      "step: 688622, loss: 0.06505043804645538, data time: 0.008575774527884819\n",
      "step: 688623, loss: 0.06237896531820297, data time: 0.008404926249855444\n",
      "step: 688624, loss: 0.05917181819677353, data time: 0.008241518949851012\n",
      "step: 688625, loss: 0.06967860460281372, data time: 0.00808766484260559\n",
      "step: 688626, loss: 0.06380853056907654, data time: 0.24880504608154297\n",
      "step: 688627, loss: 0.060136448591947556, data time: 0.12515997886657715\n",
      "step: 688628, loss: 0.06750853359699249, data time: 0.08435869216918945\n",
      "step: 688629, loss: 0.06224146485328674, data time: 0.06393373012542725\n",
      "step: 688630, loss: 0.05559266731142998, data time: 0.05145530700683594\n",
      "step: 688631, loss: 0.061371009796857834, data time: 0.04310766855875651\n",
      "step: 688632, loss: 0.05989966541528702, data time: 0.037145955221993585\n",
      "step: 688633, loss: 0.0600251667201519, data time: 0.03275644779205322\n",
      "step: 688634, loss: 0.06066351383924484, data time: 0.029338677724202473\n",
      "step: 688635, loss: 0.060322295874357224, data time: 0.026595187187194825\n",
      "step: 688636, loss: 0.059315674006938934, data time: 0.024371255527843128\n",
      "step: 688637, loss: 0.05863848328590393, data time: 0.02252143621444702\n",
      "step: 688638, loss: 0.06149451434612274, data time: 0.02095459057734563\n",
      "step: 688639, loss: 0.05877329036593437, data time: 0.019601804869515554\n",
      "step: 688640, loss: 0.06712501496076584, data time: 0.018429899215698244\n",
      "step: 688641, loss: 0.05688100308179855, data time: 0.017409294843673706\n",
      "step: 688642, loss: 0.05823949724435806, data time: 0.016507906072279987\n",
      "step: 688643, loss: 0.0633833110332489, data time: 0.01569807529449463\n",
      "step: 688644, loss: 0.06090841442346573, data time: 0.014971243707757247\n",
      "step: 688645, loss: 0.05851802974939346, data time: 0.014331066608428955\n",
      "step: 688646, loss: 0.06799623370170593, data time: 0.013749349684942336\n",
      "step: 688647, loss: 0.05849999189376831, data time: 0.013221057978543367\n",
      "step: 688648, loss: 0.06031537055969238, data time: 0.012750864028930664\n",
      "step: 688649, loss: 0.05135613679885864, data time: 0.012322505315144857\n",
      "step: 688650, loss: 0.06469307839870453, data time: 0.011928920745849609\n",
      "step: 688651, loss: 0.061631232500076294, data time: 0.011561879744896522\n",
      "step: 688652, loss: 0.06261494755744934, data time: 0.011219157112969292\n",
      "step: 688653, loss: 0.06901302933692932, data time: 0.010899577822004045\n",
      "step: 688654, loss: 0.0586848258972168, data time: 0.010610185820480874\n",
      "step: 688655, loss: 0.0674407035112381, data time: 0.010342923800150554\n",
      "step: 688656, loss: 0.056765854358673096, data time: 0.01008076052511892\n",
      "step: 688657, loss: 0.06597619503736496, data time: 0.009840749204158783\n",
      "step: 688658, loss: 0.058922067284584045, data time: 0.009601535219134706\n",
      "step: 688659, loss: 0.06184249371290207, data time: 0.009375761536990894\n",
      "step: 688660, loss: 0.06143561005592346, data time: 0.00916557993207659\n",
      "step: 688661, loss: 0.06662382185459137, data time: 0.008962028556399874\n",
      "step: 688662, loss: 0.06589193642139435, data time: 0.008768152546238256\n",
      "step: 688663, loss: 0.06285491585731506, data time: 0.008591683287369577\n",
      "step: 688664, loss: 0.059897057712078094, data time: 0.008422790429530999\n",
      "step: 688665, loss: 0.06878778338432312, data time: 0.008262741565704345\n",
      "step: 688666, loss: 0.05961526557803154, data time: 0.24039244651794434\n",
      "step: 688667, loss: 0.06330681592226028, data time: 0.12103116512298584\n",
      "step: 688668, loss: 0.06314026564359665, data time: 0.08121665318806966\n",
      "step: 688669, loss: 0.06556021422147751, data time: 0.061661720275878906\n",
      "step: 688670, loss: 0.06509123742580414, data time: 0.04961352348327637\n",
      "step: 688671, loss: 0.06904806196689606, data time: 0.04158735275268555\n",
      "step: 688672, loss: 0.06525374948978424, data time: 0.035853419985089986\n",
      "step: 688673, loss: 0.06474195420742035, data time: 0.03162655234336853\n",
      "step: 688674, loss: 0.06026142090559006, data time: 0.02827151616414388\n",
      "step: 688675, loss: 0.06386019289493561, data time: 0.025652003288269044\n",
      "step: 688676, loss: 0.06420084834098816, data time: 0.023516741665926846\n",
      "step: 688677, loss: 0.061870984733104706, data time: 0.021745920181274414\n",
      "step: 688678, loss: 0.06480427831411362, data time: 0.020238197766817533\n",
      "step: 688679, loss: 0.06149807572364807, data time: 0.018944808415004184\n",
      "step: 688680, loss: 0.06418804824352264, data time: 0.01781635284423828\n",
      "step: 688681, loss: 0.0620587021112442, data time: 0.016832858324050903\n",
      "step: 688682, loss: 0.05747336149215698, data time: 0.01596340011147892\n",
      "step: 688683, loss: 0.05835176631808281, data time: 0.015185832977294922\n",
      "step: 688684, loss: 0.06027018278837204, data time: 0.01449207255714818\n",
      "step: 688685, loss: 0.06143505126237869, data time: 0.013874208927154541\n",
      "step: 688686, loss: 0.07164782285690308, data time: 0.013314269837878999\n",
      "step: 688687, loss: 0.05970744043588638, data time: 0.012806849046186968\n",
      "step: 688688, loss: 0.06619346141815186, data time: 0.012335787648740023\n",
      "step: 688689, loss: 0.05965663120150566, data time: 0.011906474828720093\n",
      "step: 688690, loss: 0.060899730771780014, data time: 0.01151413917541504\n",
      "step: 688691, loss: 0.0601649209856987, data time: 0.011148883746220516\n",
      "step: 688692, loss: 0.055793553590774536, data time: 0.010807982197514287\n",
      "step: 688693, loss: 0.0633447989821434, data time: 0.01049274206161499\n",
      "step: 688694, loss: 0.06150177866220474, data time: 0.010203969889673693\n",
      "step: 688695, loss: 0.055472224950790405, data time: 0.009936380386352538\n",
      "step: 688696, loss: 0.05512898415327072, data time: 0.009687862088603357\n",
      "step: 688697, loss: 0.06731920689344406, data time: 0.009456023573875427\n",
      "step: 688698, loss: 0.06393470615148544, data time: 0.009229573336514559\n",
      "step: 688699, loss: 0.059595875442028046, data time: 0.009015209534588982\n",
      "step: 688700, loss: 0.0698288083076477, data time: 0.00881190299987793\n",
      "step: 688701, loss: 0.06156129762530327, data time: 0.008617844846513536\n",
      "step: 688702, loss: 0.062243252992630005, data time: 0.008435204222395614\n",
      "step: 688703, loss: 0.05909695476293564, data time: 0.008266216830203408\n",
      "step: 688704, loss: 0.06113598495721817, data time: 0.008107252610035432\n",
      "step: 688705, loss: 0.0706004649400711, data time: 0.007955306768417358\n",
      "step: 688706, loss: 0.06489676237106323, data time: 0.23818731307983398\n",
      "step: 688707, loss: 0.06379791349172592, data time: 0.12020576000213623\n",
      "step: 688708, loss: 0.06090499088168144, data time: 0.08118653297424316\n",
      "step: 688709, loss: 0.061517488211393356, data time: 0.061725497245788574\n",
      "step: 688710, loss: 0.06182797998189926, data time: 0.04966073036193848\n",
      "step: 688711, loss: 0.06262549012899399, data time: 0.04162184397379557\n",
      "step: 688712, loss: 0.058458298444747925, data time: 0.03586973462785993\n",
      "step: 688713, loss: 0.057288479059934616, data time: 0.03164204955101013\n",
      "step: 688714, loss: 0.06062835454940796, data time: 0.028275781207614474\n",
      "step: 688715, loss: 0.06138133630156517, data time: 0.025656771659851075\n",
      "step: 688716, loss: 0.06460186094045639, data time: 0.023517478596080433\n",
      "step: 688717, loss: 0.06491455435752869, data time: 0.021748145421346027\n",
      "step: 688718, loss: 0.06415567547082901, data time: 0.02024413989140437\n",
      "step: 688719, loss: 0.06367134302854538, data time: 0.018941146986825124\n",
      "step: 688720, loss: 0.06406845152378082, data time: 0.01781322161356608\n",
      "step: 688721, loss: 0.06639982759952545, data time: 0.016826346516609192\n",
      "step: 688722, loss: 0.05761110410094261, data time: 0.015958337222828585\n",
      "step: 688723, loss: 0.05700574442744255, data time: 0.015177726745605469\n",
      "step: 688724, loss: 0.06596887111663818, data time: 0.01448520861173931\n",
      "step: 688725, loss: 0.060701094567775726, data time: 0.013869667053222656\n",
      "step: 688726, loss: 0.06332041323184967, data time: 0.013318254834129697\n",
      "step: 688727, loss: 0.0613645575940609, data time: 0.012811216441067782\n",
      "step: 688728, loss: 0.060861676931381226, data time: 0.01234063894852348\n",
      "step: 688729, loss: 0.06755155324935913, data time: 0.01191037893295288\n",
      "step: 688730, loss: 0.055519141256809235, data time: 0.011518096923828125\n",
      "step: 688731, loss: 0.05985961854457855, data time: 0.011158209580641527\n",
      "step: 688732, loss: 0.06169484183192253, data time: 0.010817730868304218\n",
      "step: 688733, loss: 0.0596451573073864, data time: 0.010502942970820836\n",
      "step: 688734, loss: 0.057849522680044174, data time: 0.01021383548605031\n",
      "step: 688735, loss: 0.06190410256385803, data time: 0.009943842887878418\n",
      "step: 688736, loss: 0.05968029424548149, data time: 0.009691561422040385\n",
      "step: 688737, loss: 0.06150013208389282, data time: 0.009459547698497772\n",
      "step: 688738, loss: 0.06642305105924606, data time: 0.009232391010631214\n",
      "step: 688739, loss: 0.05649867653846741, data time: 0.009017306215622845\n",
      "step: 688740, loss: 0.06509435176849365, data time: 0.008814484732491629\n",
      "step: 688741, loss: 0.0616619698703289, data time: 0.008623970879448785\n",
      "step: 688742, loss: 0.06088964268565178, data time: 0.008443658416335648\n",
      "step: 688743, loss: 0.06278221309185028, data time: 0.008275602993212249\n",
      "step: 688744, loss: 0.0628463551402092, data time: 0.008118085372142302\n",
      "step: 688745, loss: 0.05237869545817375, data time: 0.007966530323028565\n",
      "step: 688746, loss: 0.05349971354007721, data time: 0.23187541961669922\n",
      "step: 688747, loss: 0.06547987461090088, data time: 0.11756610870361328\n",
      "step: 688748, loss: 0.0663902536034584, data time: 0.07888293266296387\n",
      "step: 688749, loss: 0.060070931911468506, data time: 0.0601237416267395\n",
      "step: 688750, loss: 0.06287627667188644, data time: 0.04837455749511719\n",
      "step: 688751, loss: 0.05873677134513855, data time: 0.04057276248931885\n",
      "step: 688752, loss: 0.060846246778964996, data time: 0.0349949768611363\n",
      "step: 688753, loss: 0.05893944203853607, data time: 0.030920058488845825\n",
      "step: 688754, loss: 0.06367115676403046, data time: 0.027631070878770616\n",
      "step: 688755, loss: 0.06208212301135063, data time: 0.025081348419189454\n",
      "step: 688756, loss: 0.06000548228621483, data time: 0.023006872697310013\n",
      "step: 688757, loss: 0.05668594688177109, data time: 0.021280348300933838\n",
      "step: 688758, loss: 0.06814597547054291, data time: 0.019807173655583307\n",
      "step: 688759, loss: 0.06623153388500214, data time: 0.018539888518197194\n",
      "step: 688760, loss: 0.06353923678398132, data time: 0.01745435396830241\n",
      "step: 688761, loss: 0.06153213977813721, data time: 0.01649099588394165\n",
      "step: 688762, loss: 0.06752882897853851, data time: 0.015643947264727426\n",
      "step: 688763, loss: 0.058998581022024155, data time: 0.01488820711771647\n",
      "step: 688764, loss: 0.06686826795339584, data time: 0.01422296072307386\n",
      "step: 688765, loss: 0.06408774107694626, data time: 0.013618755340576171\n",
      "step: 688766, loss: 0.06075591593980789, data time: 0.013073069708687919\n",
      "step: 688767, loss: 0.06498700380325317, data time: 0.012577837163751776\n",
      "step: 688768, loss: 0.06227453798055649, data time: 0.01211851576100225\n",
      "step: 688769, loss: 0.06666634976863861, data time: 0.011709551016489664\n",
      "step: 688770, loss: 0.054848432540893555, data time: 0.011339244842529296\n",
      "step: 688771, loss: 0.06154976412653923, data time: 0.010979597385113057\n",
      "step: 688772, loss: 0.0633988082408905, data time: 0.010643950215092412\n",
      "step: 688773, loss: 0.05586734414100647, data time: 0.010334866387503487\n",
      "step: 688774, loss: 0.060986995697021484, data time: 0.01005956222271097\n",
      "step: 688775, loss: 0.06116097420454025, data time: 0.009797485669453938\n",
      "step: 688776, loss: 0.06239666789770126, data time: 0.00955338631906817\n",
      "step: 688777, loss: 0.0575314536690712, data time: 0.009325668215751648\n",
      "step: 688778, loss: 0.0635148137807846, data time: 0.009102929722179066\n",
      "step: 688779, loss: 0.0633149966597557, data time: 0.008894892299876493\n",
      "step: 688780, loss: 0.05866345763206482, data time: 0.008696031570434571\n",
      "step: 688781, loss: 0.06079831346869469, data time: 0.008507774935828315\n",
      "step: 688782, loss: 0.06074618548154831, data time: 0.008333773226351352\n",
      "step: 688783, loss: 0.05705619603395462, data time: 0.008168741276389673\n",
      "step: 688784, loss: 0.06248161941766739, data time: 0.008013254556900416\n",
      "step: 688785, loss: 0.06189229339361191, data time: 0.007864326238632202\n",
      "step: 688786, loss: 0.06705772876739502, data time: 0.2445521354675293\n",
      "step: 688787, loss: 0.05805469676852226, data time: 0.12304484844207764\n",
      "step: 688788, loss: 0.06558878719806671, data time: 0.0825495719909668\n",
      "step: 688789, loss: 0.06405000388622284, data time: 0.06269729137420654\n",
      "step: 688790, loss: 0.06072226166725159, data time: 0.05043158531188965\n",
      "step: 688791, loss: 0.06142468377947807, data time: 0.0422677199045817\n",
      "step: 688792, loss: 0.0605442151427269, data time: 0.03643270901271275\n",
      "step: 688793, loss: 0.06198588013648987, data time: 0.032133519649505615\n",
      "step: 688794, loss: 0.06294642388820648, data time: 0.028706736034817167\n",
      "step: 688795, loss: 0.0643375888466835, data time: 0.026033616065979003\n",
      "step: 688796, loss: 0.06508337706327438, data time: 0.02385934916409579\n",
      "step: 688797, loss: 0.06157071888446808, data time: 0.022047777970631916\n",
      "step: 688798, loss: 0.05637187510728836, data time: 0.020518926473764274\n",
      "step: 688799, loss: 0.06652145832777023, data time: 0.019198077065604075\n",
      "step: 688800, loss: 0.0588647834956646, data time: 0.018055295944213866\n",
      "step: 688801, loss: 0.06654965132474899, data time: 0.017058908939361572\n",
      "step: 688802, loss: 0.06256204843521118, data time: 0.01617247918072869\n",
      "step: 688803, loss: 0.06397613137960434, data time: 0.015382687250773111\n",
      "step: 688804, loss: 0.0644671618938446, data time: 0.014675868184942948\n",
      "step: 688805, loss: 0.06219211220741272, data time: 0.014049971103668213\n",
      "step: 688806, loss: 0.0713544636964798, data time: 0.013482888539632162\n",
      "step: 688807, loss: 0.0641525536775589, data time: 0.012969970703125\n",
      "step: 688808, loss: 0.06841546297073364, data time: 0.012495196383932362\n",
      "step: 688809, loss: 0.06947571039199829, data time: 0.012058516343434652\n",
      "step: 688810, loss: 0.057539813220500946, data time: 0.011657705307006836\n",
      "step: 688811, loss: 0.06636577099561691, data time: 0.011288762092590332\n",
      "step: 688812, loss: 0.06285548955202103, data time: 0.010943245004724574\n",
      "step: 688813, loss: 0.06792514026165009, data time: 0.010623855250222343\n",
      "step: 688814, loss: 0.05763663351535797, data time: 0.010331014107013571\n",
      "step: 688815, loss: 0.06610894203186035, data time: 0.010061891873677571\n",
      "step: 688816, loss: 0.059840939939022064, data time: 0.009805863903414818\n",
      "step: 688817, loss: 0.06542428582906723, data time: 0.009570211172103882\n",
      "step: 688818, loss: 0.06333771347999573, data time: 0.009338328332612009\n",
      "step: 688819, loss: 0.06627652049064636, data time: 0.009120990248287426\n",
      "step: 688820, loss: 0.05802639573812485, data time: 0.008915008817400252\n",
      "step: 688821, loss: 0.06029006093740463, data time: 0.008719232347276475\n",
      "step: 688822, loss: 0.06414905190467834, data time: 0.008536757649602118\n",
      "step: 688823, loss: 0.05693792924284935, data time: 0.00836981597699617\n",
      "step: 688824, loss: 0.06316995620727539, data time: 0.008208390993949695\n",
      "step: 688825, loss: 0.04699718952178955, data time: 0.008054107427597046\n",
      "step: 688826, loss: 0.06420370936393738, data time: 0.2339305877685547\n",
      "step: 688827, loss: 0.061517633497714996, data time: 0.11856400966644287\n",
      "step: 688828, loss: 0.061729904264211655, data time: 0.07957935333251953\n",
      "step: 688829, loss: 0.06279878318309784, data time: 0.0604478120803833\n",
      "step: 688830, loss: 0.05966572090983391, data time: 0.048630571365356444\n",
      "step: 688831, loss: 0.06068418547511101, data time: 0.04077009359995524\n",
      "step: 688832, loss: 0.06142544746398926, data time: 0.03515059607369559\n",
      "step: 688833, loss: 0.060560017824172974, data time: 0.031017154455184937\n",
      "step: 688834, loss: 0.060555778443813324, data time: 0.02771864997016059\n",
      "step: 688835, loss: 0.06218595802783966, data time: 0.02514681816101074\n",
      "step: 688836, loss: 0.06016312167048454, data time: 0.023051565343683415\n",
      "step: 688837, loss: 0.0641615018248558, data time: 0.021311481793721516\n",
      "step: 688838, loss: 0.057489585131406784, data time: 0.019836884278517503\n",
      "step: 688839, loss: 0.059343334287405014, data time: 0.01856802191053118\n",
      "step: 688840, loss: 0.06443014740943909, data time: 0.0174622376759847\n",
      "step: 688841, loss: 0.05814770609140396, data time: 0.016499057412147522\n",
      "step: 688842, loss: 0.0644753947854042, data time: 0.015651057748233572\n",
      "step: 688843, loss: 0.06276236474514008, data time: 0.014891452259487577\n",
      "step: 688844, loss: 0.060534678399562836, data time: 0.01422035066705001\n",
      "step: 688845, loss: 0.0595829039812088, data time: 0.013621997833251954\n",
      "step: 688846, loss: 0.05255689471960068, data time: 0.013075544720604307\n",
      "step: 688847, loss: 0.05879230424761772, data time: 0.012577251954512163\n",
      "step: 688848, loss: 0.0665622353553772, data time: 0.012115779130355171\n",
      "step: 688849, loss: 0.06050136312842369, data time: 0.011695832014083862\n",
      "step: 688850, loss: 0.06514076888561249, data time: 0.011310005187988281\n",
      "step: 688851, loss: 0.06490808725357056, data time: 0.010953380511357235\n",
      "step: 688852, loss: 0.05923949182033539, data time: 0.01061915468286585\n",
      "step: 688853, loss: 0.06402774155139923, data time: 0.010311884539467948\n",
      "step: 688854, loss: 0.06578853726387024, data time: 0.010030927329227841\n",
      "step: 688855, loss: 0.056832313537597656, data time: 0.00976865291595459\n",
      "step: 688856, loss: 0.06485290825366974, data time: 0.009522022739533455\n",
      "step: 688857, loss: 0.06135619431734085, data time: 0.009300723671913147\n",
      "step: 688858, loss: 0.061595942825078964, data time: 0.00907941298051314\n",
      "step: 688859, loss: 0.06348758935928345, data time: 0.00886846289915197\n",
      "step: 688860, loss: 0.06166185066103935, data time: 0.00867013931274414\n",
      "step: 688861, loss: 0.05848067253828049, data time: 0.008481290605333116\n",
      "step: 688862, loss: 0.06208672374486923, data time: 0.008304460628612622\n",
      "step: 688863, loss: 0.06518832594156265, data time: 0.008139076985810933\n",
      "step: 688864, loss: 0.06755023449659348, data time: 0.00798361729352902\n",
      "step: 688865, loss: 0.08124464005231857, data time: 0.007836169004440308\n",
      "step: 688866, loss: 0.05942001938819885, data time: 0.2283000946044922\n",
      "step: 688867, loss: 0.06029335781931877, data time: 0.11594033241271973\n",
      "step: 688868, loss: 0.06357293576002121, data time: 0.07780051231384277\n",
      "step: 688869, loss: 0.06647773087024689, data time: 0.059127986431121826\n",
      "step: 688870, loss: 0.060193829238414764, data time: 0.04758005142211914\n",
      "step: 688871, loss: 0.06042180582880974, data time: 0.03988893826802572\n",
      "step: 688872, loss: 0.06182907149195671, data time: 0.03439170973641532\n",
      "step: 688873, loss: 0.0633525401353836, data time: 0.030350416898727417\n",
      "step: 688874, loss: 0.05995622277259827, data time: 0.027129941516452365\n",
      "step: 688875, loss: 0.057065896689891815, data time: 0.024620866775512694\n",
      "step: 688876, loss: 0.057973604649305344, data time: 0.022580341859297318\n",
      "step: 688877, loss: 0.06432750076055527, data time: 0.020881632963816326\n",
      "step: 688878, loss: 0.05487219989299774, data time: 0.01943861521207369\n",
      "step: 688879, loss: 0.06619060039520264, data time: 0.01819089480808803\n",
      "step: 688880, loss: 0.06096790358424187, data time: 0.01714315414428711\n",
      "step: 688881, loss: 0.06737945973873138, data time: 0.016198739409446716\n",
      "step: 688882, loss: 0.06175609678030014, data time: 0.015369373209336224\n",
      "step: 688883, loss: 0.0642361044883728, data time: 0.01462521817949083\n",
      "step: 688884, loss: 0.06026860326528549, data time: 0.013962105700844213\n",
      "step: 688885, loss: 0.06101980805397034, data time: 0.013375592231750489\n",
      "step: 688886, loss: 0.0582345649600029, data time: 0.012841610681442987\n",
      "step: 688887, loss: 0.0665447860956192, data time: 0.012357213280417702\n",
      "step: 688888, loss: 0.06201793998479843, data time: 0.011912180029827616\n",
      "step: 688889, loss: 0.06431153416633606, data time: 0.01150412360827128\n",
      "step: 688890, loss: 0.060955993831157684, data time: 0.011125030517578125\n",
      "step: 688891, loss: 0.06494545936584473, data time: 0.010776794873751126\n",
      "step: 688892, loss: 0.07084464281797409, data time: 0.010450601577758789\n",
      "step: 688893, loss: 0.06378267705440521, data time: 0.010151752403804235\n",
      "step: 688894, loss: 0.05822518467903137, data time: 0.009880263229896283\n",
      "step: 688895, loss: 0.0550612136721611, data time: 0.009621596336364746\n",
      "step: 688896, loss: 0.06057476997375488, data time: 0.009380386721703314\n",
      "step: 688897, loss: 0.059674039483070374, data time: 0.009157724678516388\n",
      "step: 688898, loss: 0.06387549638748169, data time: 0.008941859909982391\n",
      "step: 688899, loss: 0.06584478914737701, data time: 0.008739801014170927\n",
      "step: 688900, loss: 0.06265893578529358, data time: 0.008548600333077567\n",
      "step: 688901, loss: 0.06376975774765015, data time: 0.008365770181020101\n",
      "step: 688902, loss: 0.06018224358558655, data time: 0.008196160599992081\n",
      "step: 688903, loss: 0.05746108293533325, data time: 0.008036299755698756\n",
      "step: 688904, loss: 0.06470020860433578, data time: 0.007884856982108874\n",
      "step: 688905, loss: 0.05681145191192627, data time: 0.007742857933044434\n",
      "step: 688906, loss: 0.06326087564229965, data time: 0.2462630271911621\n",
      "step: 688907, loss: 0.0628109872341156, data time: 0.12390017509460449\n",
      "step: 688908, loss: 0.06024302542209625, data time: 0.08350666364034016\n",
      "step: 688909, loss: 0.061576418578624725, data time: 0.06343519687652588\n",
      "step: 688910, loss: 0.06552241742610931, data time: 0.05102944374084473\n",
      "step: 688911, loss: 0.0638274997472763, data time: 0.04275846481323242\n",
      "step: 688912, loss: 0.06375886499881744, data time: 0.036865234375\n",
      "step: 688913, loss: 0.06467133015394211, data time: 0.03250586986541748\n",
      "step: 688914, loss: 0.06759016215801239, data time: 0.02904388639662001\n",
      "step: 688915, loss: 0.06379657238721848, data time: 0.026335382461547853\n",
      "step: 688916, loss: 0.059922706335783005, data time: 0.024144779552112926\n",
      "step: 688917, loss: 0.05975339934229851, data time: 0.022321343421936035\n",
      "step: 688918, loss: 0.06648791581392288, data time: 0.02076860574575571\n",
      "step: 688919, loss: 0.060814421623945236, data time: 0.01943208490099226\n",
      "step: 688920, loss: 0.0617218092083931, data time: 0.018268330891927084\n",
      "step: 688921, loss: 0.06483140587806702, data time: 0.017263904213905334\n",
      "step: 688922, loss: 0.060860343277454376, data time: 0.016368613523595473\n",
      "step: 688923, loss: 0.05938328057527542, data time: 0.015570759773254395\n",
      "step: 688924, loss: 0.05916276574134827, data time: 0.01486374202527498\n",
      "step: 688925, loss: 0.06196290999650955, data time: 0.014226484298706054\n",
      "step: 688926, loss: 0.06297515332698822, data time: 0.01365192731221517\n",
      "step: 688927, loss: 0.055963702499866486, data time: 0.013135194778442383\n",
      "step: 688928, loss: 0.05914018675684929, data time: 0.012651619703873344\n",
      "step: 688929, loss: 0.06326928734779358, data time: 0.012211372454961142\n",
      "step: 688930, loss: 0.05255311727523804, data time: 0.011807842254638672\n",
      "step: 688931, loss: 0.05790441855788231, data time: 0.011430841225844163\n",
      "step: 688932, loss: 0.06449895352125168, data time: 0.011081669065687392\n",
      "step: 688933, loss: 0.06714704632759094, data time: 0.010755777359008789\n",
      "step: 688934, loss: 0.06172800809144974, data time: 0.010462029226895037\n",
      "step: 688935, loss: 0.058518216013908386, data time: 0.010187888145446777\n",
      "step: 688936, loss: 0.06451782584190369, data time: 0.00993096443914598\n",
      "step: 688937, loss: 0.053526028990745544, data time: 0.009690508246421814\n",
      "step: 688938, loss: 0.06413080543279648, data time: 0.009455847017692797\n",
      "step: 688939, loss: 0.06387906521558762, data time: 0.009237036985509536\n",
      "step: 688940, loss: 0.061237066984176636, data time: 0.009028502873011999\n",
      "step: 688941, loss: 0.06039946898818016, data time: 0.008829885058932833\n",
      "step: 688942, loss: 0.06197518855333328, data time: 0.008643962241507866\n",
      "step: 688943, loss: 0.061894312500953674, data time: 0.008470823890284487\n",
      "step: 688944, loss: 0.06140017509460449, data time: 0.008305940872583633\n",
      "step: 688945, loss: 0.08073508739471436, data time: 0.008150935173034668\n",
      "step: 688946, loss: 0.057439081370830536, data time: 0.2315669059753418\n",
      "step: 688947, loss: 0.05851193517446518, data time: 0.11694133281707764\n",
      "step: 688948, loss: 0.0594719797372818, data time: 0.07899165153503418\n",
      "step: 688949, loss: 0.062084000557661057, data time: 0.06002974510192871\n",
      "step: 688950, loss: 0.06585594266653061, data time: 0.04829697608947754\n",
      "step: 688951, loss: 0.06455053389072418, data time: 0.04050254821777344\n",
      "step: 688952, loss: 0.06133435666561127, data time: 0.0349337032863072\n",
      "step: 688953, loss: 0.06904219835996628, data time: 0.030830562114715576\n",
      "step: 688954, loss: 0.06396739929914474, data time: 0.027553002039591473\n",
      "step: 688955, loss: 0.058639686554670334, data time: 0.02500143051147461\n",
      "step: 688956, loss: 0.0632365271449089, data time: 0.02295431223782626\n",
      "step: 688957, loss: 0.062186650931835175, data time: 0.021251877148946125\n",
      "step: 688958, loss: 0.06307395547628403, data time: 0.019777389673086312\n",
      "step: 688959, loss: 0.06626671552658081, data time: 0.0185175963810512\n",
      "step: 688960, loss: 0.06495115160942078, data time: 0.01742275555928548\n",
      "step: 688961, loss: 0.06351073086261749, data time: 0.01648770272731781\n",
      "step: 688962, loss: 0.06410665810108185, data time: 0.015662824406343347\n",
      "step: 688963, loss: 0.0601518452167511, data time: 0.014926513036092123\n",
      "step: 688964, loss: 0.05991065502166748, data time: 0.014266754451550935\n",
      "step: 688965, loss: 0.06987593322992325, data time: 0.013677656650543213\n",
      "step: 688966, loss: 0.058202676475048065, data time: 0.01314477693466913\n",
      "step: 688967, loss: 0.05824682116508484, data time: 0.012662334875626997\n",
      "step: 688968, loss: 0.0656822994351387, data time: 0.01221522040989088\n",
      "step: 688969, loss: 0.05640355870127678, data time: 0.01180940866470337\n",
      "step: 688970, loss: 0.05947035551071167, data time: 0.011433830261230469\n",
      "step: 688971, loss: 0.0632222443819046, data time: 0.011086225509643555\n",
      "step: 688972, loss: 0.06188882514834404, data time: 0.01076205571492513\n",
      "step: 688973, loss: 0.06782129406929016, data time: 0.01046104942049299\n",
      "step: 688974, loss: 0.05913188308477402, data time: 0.010187971180882948\n",
      "step: 688975, loss: 0.0554400235414505, data time: 0.009932319323221842\n",
      "step: 688976, loss: 0.06591706722974777, data time: 0.009692791969545426\n",
      "step: 688977, loss: 0.06491072475910187, data time: 0.00947117805480957\n",
      "step: 688978, loss: 0.06585684418678284, data time: 0.009247815970218542\n",
      "step: 688979, loss: 0.05627724528312683, data time: 0.009034381193273208\n",
      "step: 688980, loss: 0.06094295531511307, data time: 0.008833088193620954\n",
      "step: 688981, loss: 0.059021443128585815, data time: 0.008641183376312256\n",
      "step: 688982, loss: 0.056249286979436874, data time: 0.008462100415616422\n",
      "step: 688983, loss: 0.06184716150164604, data time: 0.008296100716841849\n",
      "step: 688984, loss: 0.0622798316180706, data time: 0.008138583256648136\n",
      "step: 688985, loss: 0.07183768600225449, data time: 0.007989650964736939\n",
      "step: 688986, loss: 0.06326252222061157, data time: 0.2460923194885254\n",
      "step: 688987, loss: 0.06756673753261566, data time: 0.12416255474090576\n",
      "step: 688988, loss: 0.06418336927890778, data time: 0.08327937126159668\n",
      "step: 688989, loss: 0.06072531268000603, data time: 0.06332272291183472\n",
      "step: 688990, loss: 0.06046696752309799, data time: 0.05094852447509766\n",
      "step: 688991, loss: 0.06139148399233818, data time: 0.04268765449523926\n",
      "step: 688992, loss: 0.05949203670024872, data time: 0.0368147577558245\n",
      "step: 688993, loss: 0.059994764626026154, data time: 0.03247278928756714\n",
      "step: 688994, loss: 0.0673542469739914, data time: 0.029010083940294053\n",
      "step: 688995, loss: 0.057844653725624084, data time: 0.026308107376098632\n",
      "step: 688996, loss: 0.06269274652004242, data time: 0.024114825508811256\n",
      "step: 688997, loss: 0.06398583948612213, data time: 0.0222858985265096\n",
      "step: 688998, loss: 0.06300961226224899, data time: 0.020737537970909707\n",
      "step: 688999, loss: 0.05566985905170441, data time: 0.019400324140276228\n",
      "step: 689000, loss: 0.05737684667110443, data time: 0.018239545822143554\n",
      "step: 689001, loss: 0.05653813108801842, data time: 0.017229169607162476\n",
      "step: 689002, loss: 0.06414267420768738, data time: 0.016335627611945656\n",
      "step: 689003, loss: 0.06076670438051224, data time: 0.01553764608171251\n",
      "step: 689004, loss: 0.05440784618258476, data time: 0.01483154296875\n",
      "step: 689005, loss: 0.05868355557322502, data time: 0.014196252822875977\n",
      "step: 689006, loss: 0.05648907274007797, data time: 0.013628630411057245\n",
      "step: 689007, loss: 0.05647087097167969, data time: 0.01312609152360396\n",
      "step: 689008, loss: 0.058390360325574875, data time: 0.012645835461823837\n",
      "step: 689009, loss: 0.06277556717395782, data time: 0.012207816044489542\n",
      "step: 689010, loss: 0.07218050211668015, data time: 0.011802206039428711\n",
      "step: 689011, loss: 0.06324343383312225, data time: 0.011423927087050218\n",
      "step: 689012, loss: 0.059145908802747726, data time: 0.01108072422168873\n",
      "step: 689013, loss: 0.06490040570497513, data time: 0.010758757591247559\n",
      "step: 689014, loss: 0.06379739940166473, data time: 0.010463048671853954\n",
      "step: 689015, loss: 0.06358929723501205, data time: 0.010186004638671874\n",
      "step: 689016, loss: 0.06561257690191269, data time: 0.009931725840414725\n",
      "step: 689017, loss: 0.06876122206449509, data time: 0.009693466126918793\n",
      "step: 689018, loss: 0.05814330279827118, data time: 0.009458086707375267\n",
      "step: 689019, loss: 0.06342177838087082, data time: 0.009236623259151684\n",
      "step: 689020, loss: 0.06159866973757744, data time: 0.00902714729309082\n",
      "step: 689021, loss: 0.06440181285142899, data time: 0.008826355139414469\n",
      "step: 689022, loss: 0.06337735056877136, data time: 0.008641288087174698\n",
      "step: 689023, loss: 0.05906546115875244, data time: 0.008468157366702431\n",
      "step: 689024, loss: 0.06315059959888458, data time: 0.008304015184060121\n",
      "step: 689025, loss: 0.04548652097582817, data time: 0.008146709203720093\n",
      "step: 689026, loss: 0.0629667341709137, data time: 0.24112296104431152\n",
      "step: 689027, loss: 0.06493303179740906, data time: 0.12133049964904785\n",
      "step: 689028, loss: 0.058673977851867676, data time: 0.08193548520406087\n",
      "step: 689029, loss: 0.07040464878082275, data time: 0.06222599744796753\n",
      "step: 689030, loss: 0.06298808753490448, data time: 0.05005292892456055\n",
      "step: 689031, loss: 0.06188741326332092, data time: 0.041949828465779625\n",
      "step: 689032, loss: 0.05733155459165573, data time: 0.03616680417742048\n",
      "step: 689033, loss: 0.06020306795835495, data time: 0.031896233558654785\n",
      "step: 689034, loss: 0.06421318650245667, data time: 0.028503470950656466\n",
      "step: 689035, loss: 0.05703675001859665, data time: 0.025854969024658205\n",
      "step: 689036, loss: 0.06499048322439194, data time: 0.023698936809193005\n",
      "step: 689037, loss: 0.0636487603187561, data time: 0.02190069357554118\n",
      "step: 689038, loss: 0.061580486595630646, data time: 0.020383449701162484\n",
      "step: 689039, loss: 0.05984774976968765, data time: 0.01908212048666818\n",
      "step: 689040, loss: 0.06050577014684677, data time: 0.017950916290283205\n",
      "step: 689041, loss: 0.06401894986629486, data time: 0.016959071159362793\n",
      "step: 689042, loss: 0.05816346034407616, data time: 0.01607967825496898\n",
      "step: 689043, loss: 0.06400351226329803, data time: 0.01529367764790853\n",
      "step: 689044, loss: 0.057824552059173584, data time: 0.014601080041182669\n",
      "step: 689045, loss: 0.06255023926496506, data time: 0.013979136943817139\n",
      "step: 689046, loss: 0.06374391913414001, data time: 0.013417346136910575\n",
      "step: 689047, loss: 0.07259893417358398, data time: 0.012909585779363459\n",
      "step: 689048, loss: 0.06300956010818481, data time: 0.012437011884606403\n",
      "step: 689049, loss: 0.05745987221598625, data time: 0.012001246213912964\n",
      "step: 689050, loss: 0.06320567429065704, data time: 0.011604375839233398\n",
      "step: 689051, loss: 0.06156709045171738, data time: 0.011237676327045146\n",
      "step: 689052, loss: 0.06435320526361465, data time: 0.010895596610175239\n",
      "step: 689053, loss: 0.058771900832653046, data time: 0.01057908364704677\n",
      "step: 689054, loss: 0.06455769389867783, data time: 0.010287983664150896\n",
      "step: 689055, loss: 0.06007954478263855, data time: 0.01001876990000407\n",
      "step: 689056, loss: 0.06718461215496063, data time: 0.00976390992441485\n",
      "step: 689057, loss: 0.06044282019138336, data time: 0.009528614580631256\n",
      "step: 689058, loss: 0.06159225106239319, data time: 0.009297515406753078\n",
      "step: 689059, loss: 0.05920831114053726, data time: 0.009084533242618336\n",
      "step: 689060, loss: 0.0539793036878109, data time: 0.008881126131330217\n",
      "step: 689061, loss: 0.06434822082519531, data time: 0.008684800730811225\n",
      "step: 689062, loss: 0.062227897346019745, data time: 0.008501755224691855\n",
      "step: 689063, loss: 0.059853896498680115, data time: 0.008331706649378726\n",
      "step: 689064, loss: 0.06126964092254639, data time: 0.008170812557905149\n",
      "step: 689065, loss: 0.06337253749370575, data time: 0.008017152547836304\n",
      "step: 689066, loss: 0.06274878233671188, data time: 0.2407212257385254\n",
      "step: 689067, loss: 0.06195349618792534, data time: 0.12118268013000488\n",
      "step: 689068, loss: 0.06333276629447937, data time: 0.08130319913228352\n",
      "step: 689069, loss: 0.05681421607732773, data time: 0.06173419952392578\n",
      "step: 689070, loss: 0.058826744556427, data time: 0.04967589378356933\n",
      "step: 689071, loss: 0.06364999711513519, data time: 0.0416332483291626\n",
      "step: 689072, loss: 0.062353502959012985, data time: 0.035902363913399835\n",
      "step: 689073, loss: 0.06547985970973969, data time: 0.03166523575782776\n",
      "step: 689074, loss: 0.057137053459882736, data time: 0.02830349074469672\n",
      "step: 689075, loss: 0.06488457322120667, data time: 0.025673747062683105\n",
      "step: 689076, loss: 0.056576959788799286, data time: 0.023542165756225586\n",
      "step: 689077, loss: 0.05515215918421745, data time: 0.021758556365966797\n",
      "step: 689078, loss: 0.06115787476301193, data time: 0.020254006752601035\n",
      "step: 689079, loss: 0.06294012069702148, data time: 0.018954975264413015\n",
      "step: 689080, loss: 0.056603770703077316, data time: 0.01782533327738444\n",
      "step: 689081, loss: 0.06519008427858353, data time: 0.016842439770698547\n",
      "step: 689082, loss: 0.06147211790084839, data time: 0.015968070310704848\n",
      "step: 689083, loss: 0.06440357863903046, data time: 0.015190892749362521\n",
      "step: 689084, loss: 0.05570586770772934, data time: 0.014501534010234633\n",
      "step: 689085, loss: 0.060168784111738205, data time: 0.013883233070373535\n",
      "step: 689086, loss: 0.06530937552452087, data time: 0.013324680782499768\n",
      "step: 689087, loss: 0.05979909747838974, data time: 0.012815009463917126\n",
      "step: 689088, loss: 0.05733533948659897, data time: 0.012344868286796238\n",
      "step: 689089, loss: 0.056724950671195984, data time: 0.011913220087687174\n",
      "step: 689090, loss: 0.06195725128054619, data time: 0.011523685455322265\n",
      "step: 689091, loss: 0.06371442973613739, data time: 0.011158227920532227\n",
      "step: 689092, loss: 0.06297174096107483, data time: 0.010815364343148691\n",
      "step: 689093, loss: 0.058584392070770264, data time: 0.010502951485770089\n",
      "step: 689094, loss: 0.06155503913760185, data time: 0.01021319422228583\n",
      "step: 689095, loss: 0.05718843266367912, data time: 0.009944383303324382\n",
      "step: 689096, loss: 0.06023155897855759, data time: 0.009694730081865865\n",
      "step: 689097, loss: 0.05471057817339897, data time: 0.009464368224143982\n",
      "step: 689098, loss: 0.058713749051094055, data time: 0.00923447897939971\n",
      "step: 689099, loss: 0.06322693079710007, data time: 0.009021737996269675\n",
      "step: 689100, loss: 0.059076838195323944, data time: 0.008819961547851562\n",
      "step: 689101, loss: 0.06645610928535461, data time: 0.008626653088463677\n",
      "step: 689102, loss: 0.053531765937805176, data time: 0.00844498582788416\n",
      "step: 689103, loss: 0.05846893787384033, data time: 0.00827761072861521\n",
      "step: 689104, loss: 0.06702981144189835, data time: 0.008118733381613707\n",
      "step: 689105, loss: 0.044040873646736145, data time: 0.007966697216033936\n",
      "step: 689106, loss: 0.0627957209944725, data time: 0.24138522148132324\n",
      "step: 689107, loss: 0.06273816525936127, data time: 0.1221088171005249\n",
      "step: 689108, loss: 0.059576451778411865, data time: 0.08228826522827148\n",
      "step: 689109, loss: 0.05777771770954132, data time: 0.06250333786010742\n",
      "step: 689110, loss: 0.06086369603872299, data time: 0.05027828216552734\n",
      "step: 689111, loss: 0.06161971390247345, data time: 0.042142788569132485\n",
      "step: 689112, loss: 0.06312452256679535, data time: 0.03632943970816476\n",
      "step: 689113, loss: 0.05591118335723877, data time: 0.03203701972961426\n",
      "step: 689114, loss: 0.052042156457901, data time: 0.028623104095458984\n",
      "step: 689115, loss: 0.06749603152275085, data time: 0.025968456268310548\n",
      "step: 689116, loss: 0.06947606056928635, data time: 0.023800568147139115\n",
      "step: 689117, loss: 0.06367781013250351, data time: 0.02200716733932495\n",
      "step: 689118, loss: 0.05409041792154312, data time: 0.020479220610398512\n",
      "step: 689119, loss: 0.0669889748096466, data time: 0.019168070384434292\n",
      "step: 689120, loss: 0.06170620769262314, data time: 0.018028863271077476\n",
      "step: 689121, loss: 0.0662265345454216, data time: 0.017036467790603638\n",
      "step: 689122, loss: 0.060339491814374924, data time: 0.016152143478393555\n",
      "step: 689123, loss: 0.061662059277296066, data time: 0.015362554126315646\n",
      "step: 689124, loss: 0.06562171876430511, data time: 0.014659015755904349\n",
      "step: 689125, loss: 0.06130344793200493, data time: 0.014036059379577637\n",
      "step: 689126, loss: 0.061757441610097885, data time: 0.013470456713721865\n",
      "step: 689127, loss: 0.060024749487638474, data time: 0.01295549219304865\n",
      "step: 689128, loss: 0.06417515873908997, data time: 0.012480310771776281\n",
      "step: 689129, loss: 0.06454622745513916, data time: 0.012045522530873617\n",
      "step: 689130, loss: 0.060587458312511444, data time: 0.011645965576171875\n",
      "step: 689131, loss: 0.06099545210599899, data time: 0.011277602269099308\n",
      "step: 689132, loss: 0.06552433967590332, data time: 0.010933266745673286\n",
      "step: 689133, loss: 0.05877469480037689, data time: 0.010618652616228377\n",
      "step: 689134, loss: 0.060368187725543976, data time: 0.01034221155890103\n",
      "step: 689135, loss: 0.07346399873495102, data time: 0.010068949063618977\n",
      "step: 689136, loss: 0.06724992394447327, data time: 0.009813524061633695\n",
      "step: 689137, loss: 0.06700124591588974, data time: 0.00957789272069931\n",
      "step: 689138, loss: 0.06143788993358612, data time: 0.009346217820138641\n",
      "step: 689139, loss: 0.05894052982330322, data time: 0.00912933489855598\n",
      "step: 689140, loss: 0.06324268877506256, data time: 0.008927229472569057\n",
      "step: 689141, loss: 0.0611039400100708, data time: 0.008731928136613634\n",
      "step: 689142, loss: 0.05966206640005112, data time: 0.008548169522672086\n",
      "step: 689143, loss: 0.05808929726481438, data time: 0.008376472874691612\n",
      "step: 689144, loss: 0.06370683014392853, data time: 0.008216955722906651\n",
      "step: 689145, loss: 0.07016648352146149, data time: 0.008066058158874512\n",
      "step: 689146, loss: 0.06345050036907196, data time: 0.2386760711669922\n",
      "step: 689147, loss: 0.06612110137939453, data time: 0.12013673782348633\n",
      "step: 689148, loss: 0.06470438092947006, data time: 0.08107217152913411\n",
      "step: 689149, loss: 0.06189633533358574, data time: 0.06160438060760498\n",
      "step: 689150, loss: 0.0616355761885643, data time: 0.04956793785095215\n",
      "step: 689151, loss: 0.06700150668621063, data time: 0.041553497314453125\n",
      "step: 689152, loss: 0.06086207553744316, data time: 0.035835095814296176\n",
      "step: 689153, loss: 0.06412480771541595, data time: 0.03162449598312378\n",
      "step: 689154, loss: 0.056886762380599976, data time: 0.028256124920315213\n",
      "step: 689155, loss: 0.06166250258684158, data time: 0.025629568099975585\n",
      "step: 689156, loss: 0.06252941489219666, data time: 0.02350395376032049\n",
      "step: 689157, loss: 0.06087635084986687, data time: 0.021724919478098553\n",
      "step: 689158, loss: 0.06361525505781174, data time: 0.02022409439086914\n",
      "step: 689159, loss: 0.060921043157577515, data time: 0.01892345292227609\n",
      "step: 689160, loss: 0.06565280258655548, data time: 0.017796723047892253\n",
      "step: 689161, loss: 0.06242088973522186, data time: 0.016825780272483826\n",
      "step: 689162, loss: 0.06770328432321548, data time: 0.015957145129933077\n",
      "step: 689163, loss: 0.06586860120296478, data time: 0.015177753236558702\n",
      "step: 689164, loss: 0.06784238666296005, data time: 0.01448822021484375\n",
      "step: 689165, loss: 0.05987280234694481, data time: 0.013873958587646484\n",
      "step: 689166, loss: 0.06222176551818848, data time: 0.013314088185628256\n",
      "step: 689167, loss: 0.05849215388298035, data time: 0.012808008627458052\n",
      "step: 689168, loss: 0.05987115949392319, data time: 0.012346817099529764\n",
      "step: 689169, loss: 0.061541154980659485, data time: 0.011918375889460245\n",
      "step: 689170, loss: 0.06325580924749374, data time: 0.011625442504882812\n",
      "step: 689171, loss: 0.06012040004134178, data time: 0.011269110899705153\n",
      "step: 689172, loss: 0.06548713892698288, data time: 0.010936057126080548\n",
      "step: 689173, loss: 0.06539389491081238, data time: 0.010631757123129708\n",
      "step: 689174, loss: 0.06212504953145981, data time: 0.01035300616560311\n",
      "step: 689175, loss: 0.06296103447675705, data time: 0.010092894236246744\n",
      "step: 689176, loss: 0.05901847407221794, data time: 0.009850202068205803\n",
      "step: 689177, loss: 0.05838274955749512, data time: 0.009623482823371887\n",
      "step: 689178, loss: 0.06349118053913116, data time: 0.009392543272538618\n",
      "step: 689179, loss: 0.06273402273654938, data time: 0.009179304627811207\n",
      "step: 689180, loss: 0.05856950208544731, data time: 0.008974770137241909\n",
      "step: 689181, loss: 0.0673736184835434, data time: 0.008779280715518527\n",
      "step: 689182, loss: 0.05689036101102829, data time: 0.008597322412439296\n",
      "step: 689183, loss: 0.05796197056770325, data time: 0.00842813441627904\n",
      "step: 689184, loss: 0.06514748930931091, data time: 0.008268472475883288\n",
      "step: 689185, loss: 0.08244583010673523, data time: 0.008116143941879272\n",
      "step: 689186, loss: 0.06546483933925629, data time: 0.24161934852600098\n",
      "step: 689187, loss: 0.05811162292957306, data time: 0.1221691370010376\n",
      "step: 689188, loss: 0.06525305658578873, data time: 0.08235406875610352\n",
      "step: 689189, loss: 0.05592267960309982, data time: 0.06256186962127686\n",
      "step: 689190, loss: 0.06826916337013245, data time: 0.05033245086669922\n",
      "step: 689191, loss: 0.05691131204366684, data time: 0.04218761126200358\n",
      "step: 689192, loss: 0.06541427224874496, data time: 0.03637334278651646\n",
      "step: 689193, loss: 0.06129450723528862, data time: 0.03207632899284363\n",
      "step: 689194, loss: 0.058331988751888275, data time: 0.028660138448079426\n",
      "step: 689195, loss: 0.0674624890089035, data time: 0.025994205474853517\n",
      "step: 689196, loss: 0.05909185856580734, data time: 0.023829200051047585\n",
      "step: 689197, loss: 0.0650227814912796, data time: 0.022023995717366535\n",
      "step: 689198, loss: 0.06024879962205887, data time: 0.020494442719679613\n",
      "step: 689199, loss: 0.0574529767036438, data time: 0.019173690250941684\n",
      "step: 689200, loss: 0.06446435302495956, data time: 0.01803763707478841\n",
      "step: 689201, loss: 0.0594058632850647, data time: 0.017039790749549866\n",
      "step: 689202, loss: 0.06029481813311577, data time: 0.01616058630101821\n",
      "step: 689203, loss: 0.06293418258428574, data time: 0.015371958414713541\n",
      "step: 689204, loss: 0.0661173164844513, data time: 0.01467709792287726\n",
      "step: 689205, loss: 0.06312797963619232, data time: 0.014049601554870606\n",
      "step: 689206, loss: 0.0647084042429924, data time: 0.013482298169817244\n",
      "step: 689207, loss: 0.06746555864810944, data time: 0.012964606285095215\n",
      "step: 689208, loss: 0.06276487559080124, data time: 0.012489349945731785\n",
      "step: 689209, loss: 0.05867505073547363, data time: 0.012055804332097372\n",
      "step: 689210, loss: 0.06958770751953125, data time: 0.011658124923706055\n",
      "step: 689211, loss: 0.06233064457774162, data time: 0.011293246195866512\n",
      "step: 689212, loss: 0.058862946927547455, data time: 0.010950097331294307\n",
      "step: 689213, loss: 0.06029751896858215, data time: 0.010630241462162562\n",
      "step: 689214, loss: 0.06562761217355728, data time: 0.01033746785130994\n",
      "step: 689215, loss: 0.06655889004468918, data time: 0.010065658887227377\n",
      "step: 689216, loss: 0.06301476806402206, data time: 0.009812270441362935\n",
      "step: 689217, loss: 0.06266510486602783, data time: 0.009582042694091797\n",
      "step: 689218, loss: 0.06216088682413101, data time: 0.00935158585057114\n",
      "step: 689219, loss: 0.06281664967536926, data time: 0.00913411729476031\n",
      "step: 689220, loss: 0.06561419367790222, data time: 0.008930213110787528\n",
      "step: 689221, loss: 0.06469528377056122, data time: 0.008735199769337973\n",
      "step: 689222, loss: 0.06623826920986176, data time: 0.008550018877596469\n",
      "step: 689223, loss: 0.059048544615507126, data time: 0.008377890837819953\n",
      "step: 689224, loss: 0.06570014357566833, data time: 0.00821405190687913\n",
      "step: 689225, loss: 0.06764273345470428, data time: 0.008059865236282349\n",
      "step: 689226, loss: 0.06575765460729599, data time: 0.2353661060333252\n",
      "step: 689227, loss: 0.06562069058418274, data time: 0.11849009990692139\n",
      "step: 689228, loss: 0.06946324557065964, data time: 0.07951521873474121\n",
      "step: 689229, loss: 0.06434379518032074, data time: 0.06040847301483154\n",
      "step: 689230, loss: 0.057181090116500854, data time: 0.04863157272338867\n",
      "step: 689231, loss: 0.06411077827215195, data time: 0.04077160358428955\n",
      "step: 689232, loss: 0.06174762174487114, data time: 0.0351672853742327\n",
      "step: 689233, loss: 0.05777699872851372, data time: 0.031031787395477295\n",
      "step: 689234, loss: 0.06310087442398071, data time: 0.027737405565049913\n",
      "step: 689235, loss: 0.06417407095432281, data time: 0.02516472339630127\n",
      "step: 689236, loss: 0.06450369954109192, data time: 0.02306920831853693\n",
      "step: 689237, loss: 0.05573210120201111, data time: 0.02132713794708252\n",
      "step: 689238, loss: 0.06567035615444183, data time: 0.01986105625446026\n",
      "step: 689239, loss: 0.05626906082034111, data time: 0.01859073979513986\n",
      "step: 689240, loss: 0.06331021338701248, data time: 0.017492214838663738\n",
      "step: 689241, loss: 0.059232644736766815, data time: 0.01652906835079193\n",
      "step: 689242, loss: 0.05790220946073532, data time: 0.01568151922786937\n",
      "step: 689243, loss: 0.06559479236602783, data time: 0.014920314153035482\n",
      "step: 689244, loss: 0.06199510395526886, data time: 0.014241456985473633\n",
      "step: 689245, loss: 0.05955442041158676, data time: 0.013637924194335937\n",
      "step: 689246, loss: 0.060106586664915085, data time: 0.01309506098429362\n",
      "step: 689247, loss: 0.06274354457855225, data time: 0.012597138231450861\n",
      "step: 689248, loss: 0.06243124604225159, data time: 0.0121380557184634\n",
      "step: 689249, loss: 0.06224825233221054, data time: 0.011717120806376139\n",
      "step: 689250, loss: 0.05998419225215912, data time: 0.011330766677856445\n",
      "step: 689251, loss: 0.05798323452472687, data time: 0.010973416841947116\n",
      "step: 689252, loss: 0.06012476608157158, data time: 0.01064032095449942\n",
      "step: 689253, loss: 0.06172111630439758, data time: 0.010336620467049735\n",
      "step: 689254, loss: 0.06200043112039566, data time: 0.010053774406170023\n",
      "step: 689255, loss: 0.06439734250307083, data time: 0.009793464342753093\n",
      "step: 689256, loss: 0.058971911668777466, data time: 0.009551217479090537\n",
      "step: 689257, loss: 0.06278949975967407, data time: 0.009325362741947174\n",
      "step: 689258, loss: 0.06529735028743744, data time: 0.00910011204806241\n",
      "step: 689259, loss: 0.06083786487579346, data time: 0.00888978032504811\n",
      "step: 689260, loss: 0.06518013030290604, data time: 0.008692114693777902\n",
      "step: 689261, loss: 0.06156229227781296, data time: 0.008514437410566542\n",
      "step: 689262, loss: 0.061658184975385666, data time: 0.008339965665662611\n",
      "step: 689263, loss: 0.06069794297218323, data time: 0.00817389864670603\n",
      "step: 689264, loss: 0.059249214828014374, data time: 0.008017539978027344\n",
      "step: 689265, loss: 0.07546481490135193, data time: 0.007867878675460816\n",
      "step: 689266, loss: 0.05820998549461365, data time: 0.24546599388122559\n",
      "step: 689267, loss: 0.06003794074058533, data time: 0.12386453151702881\n",
      "step: 689268, loss: 0.06491097807884216, data time: 0.08361474672953288\n",
      "step: 689269, loss: 0.06765066087245941, data time: 0.06348317861557007\n",
      "step: 689270, loss: 0.06189412623643875, data time: 0.051067686080932616\n",
      "step: 689271, loss: 0.06404206156730652, data time: 0.04280916849772135\n",
      "step: 689272, loss: 0.06802821159362793, data time: 0.03688955307006836\n",
      "step: 689273, loss: 0.06564396619796753, data time: 0.03254437446594238\n",
      "step: 689274, loss: 0.052402570843696594, data time: 0.029075807995266385\n",
      "step: 689275, loss: 0.05881350859999657, data time: 0.026370906829833986\n",
      "step: 689276, loss: 0.059307828545570374, data time: 0.024170073595913975\n",
      "step: 689277, loss: 0.05947691202163696, data time: 0.02233054240544637\n",
      "step: 689278, loss: 0.05307600647211075, data time: 0.020776015061598558\n",
      "step: 689279, loss: 0.06586123257875443, data time: 0.019439237458365306\n",
      "step: 689280, loss: 0.06128523871302605, data time: 0.018314901987711588\n",
      "step: 689281, loss: 0.05723614990711212, data time: 0.017320826649665833\n",
      "step: 689282, loss: 0.05685741826891899, data time: 0.0164487502154182\n",
      "step: 689283, loss: 0.06303641945123672, data time: 0.01566448476579454\n",
      "step: 689284, loss: 0.06396301090717316, data time: 0.01496535853335732\n",
      "step: 689285, loss: 0.061734460294246674, data time: 0.014342021942138673\n",
      "step: 689286, loss: 0.06869497895240784, data time: 0.013777959914434524\n",
      "step: 689287, loss: 0.05860146880149841, data time: 0.013266476717862215\n",
      "step: 689288, loss: 0.06202275678515434, data time: 0.012795313544895338\n",
      "step: 689289, loss: 0.06715620309114456, data time: 0.01236651341120402\n",
      "step: 689290, loss: 0.05852628871798515, data time: 0.011966018676757813\n",
      "step: 689291, loss: 0.05260276049375534, data time: 0.011599549880394569\n",
      "step: 689292, loss: 0.06314599514007568, data time: 0.011255935386375145\n",
      "step: 689293, loss: 0.05488200485706329, data time: 0.010939930166516985\n",
      "step: 689294, loss: 0.06360753625631332, data time: 0.010648809630295327\n",
      "step: 689295, loss: 0.05945723503828049, data time: 0.01038064956665039\n",
      "step: 689296, loss: 0.05940285325050354, data time: 0.010127313675418977\n",
      "step: 689297, loss: 0.062117304652929306, data time: 0.009892180562019348\n",
      "step: 689298, loss: 0.06112305074930191, data time: 0.009657556360418146\n",
      "step: 689299, loss: 0.05772332102060318, data time: 0.009433234439176671\n",
      "step: 689300, loss: 0.06274928152561188, data time: 0.009222800391060965\n",
      "step: 689301, loss: 0.05737749859690666, data time: 0.009021951092614068\n",
      "step: 689302, loss: 0.05415753647685051, data time: 0.008832751093684015\n",
      "step: 689303, loss: 0.06310053169727325, data time: 0.008656928413792661\n",
      "step: 689304, loss: 0.06398724764585495, data time: 0.00849007337521284\n",
      "step: 689305, loss: 0.06180604547262192, data time: 0.008331757783889771\n",
      "step: 689306, loss: 0.06731313467025757, data time: 0.24533987045288086\n",
      "step: 689307, loss: 0.05882777273654938, data time: 0.1234276294708252\n",
      "step: 689308, loss: 0.06329256296157837, data time: 0.0828396479288737\n",
      "step: 689309, loss: 0.059184443205595016, data time: 0.06295430660247803\n",
      "step: 689310, loss: 0.05706623196601868, data time: 0.050644969940185545\n",
      "step: 689311, loss: 0.06472502648830414, data time: 0.04244112968444824\n",
      "step: 689312, loss: 0.055885396897792816, data time: 0.03660729953220913\n",
      "step: 689313, loss: 0.06626978516578674, data time: 0.032282501459121704\n",
      "step: 689314, loss: 0.058036983013153076, data time: 0.028847588433159724\n",
      "step: 689315, loss: 0.0624626986682415, data time: 0.026169872283935545\n",
      "step: 689316, loss: 0.060670625418424606, data time: 0.02398803017356179\n",
      "step: 689317, loss: 0.05727919191122055, data time: 0.02216603358586629\n",
      "step: 689318, loss: 0.06287932395935059, data time: 0.020632175298837516\n",
      "step: 689319, loss: 0.05926969647407532, data time: 0.019305433545793806\n",
      "step: 689320, loss: 0.05925543233752251, data time: 0.01815331776936849\n",
      "step: 689321, loss: 0.056736480444669724, data time: 0.017163723707199097\n",
      "step: 689322, loss: 0.06017526611685753, data time: 0.016279136433320886\n",
      "step: 689323, loss: 0.06427539885044098, data time: 0.015484558211432563\n",
      "step: 689324, loss: 0.05759182572364807, data time: 0.014774422896535773\n",
      "step: 689325, loss: 0.06298213452100754, data time: 0.01415112018585205\n",
      "step: 689326, loss: 0.060642123222351074, data time: 0.013584148316156296\n",
      "step: 689327, loss: 0.06555461883544922, data time: 0.013067624785683372\n",
      "step: 689328, loss: 0.05791967734694481, data time: 0.012587267419566278\n",
      "step: 689329, loss: 0.07038625329732895, data time: 0.012150535980860392\n",
      "step: 689330, loss: 0.05894908308982849, data time: 0.011749725341796875\n",
      "step: 689331, loss: 0.058764711022377014, data time: 0.011376729378333459\n",
      "step: 689332, loss: 0.06434817612171173, data time: 0.011029031541612413\n",
      "step: 689333, loss: 0.06399726122617722, data time: 0.010709575244358607\n",
      "step: 689334, loss: 0.06299592554569244, data time: 0.010414994996169517\n",
      "step: 689335, loss: 0.06158790737390518, data time: 0.010139942169189453\n",
      "step: 689336, loss: 0.06243882700800896, data time: 0.009883711414952432\n",
      "step: 689337, loss: 0.06076669320464134, data time: 0.009645931422710419\n",
      "step: 689338, loss: 0.06090135499835014, data time: 0.009413105068784771\n",
      "step: 689339, loss: 0.06999416649341583, data time: 0.009196169236127068\n",
      "step: 689340, loss: 0.061805617064237595, data time: 0.008988237380981446\n",
      "step: 689341, loss: 0.06361069530248642, data time: 0.008790221479203966\n",
      "step: 689342, loss: 0.05756738781929016, data time: 0.008604114120071\n",
      "step: 689343, loss: 0.0686120092868805, data time: 0.008432501240780479\n",
      "step: 689344, loss: 0.05950409173965454, data time: 0.008267806126521183\n",
      "step: 689345, loss: 0.06366901099681854, data time: 0.008111584186553954\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.41191744804382324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 689346, loss: 0.05866977199912071, data time: 0.24580025672912598\n",
      "step: 689347, loss: 0.06734085828065872, data time: 0.12421619892120361\n",
      "step: 689348, loss: 0.07042258977890015, data time: 0.0838477611541748\n",
      "step: 689349, loss: 0.06105038523674011, data time: 0.06367206573486328\n",
      "step: 689350, loss: 0.061003923416137695, data time: 0.05124025344848633\n",
      "step: 689351, loss: 0.057460129261016846, data time: 0.04298082987467448\n",
      "step: 689352, loss: 0.06423060595989227, data time: 0.03709803308759417\n",
      "step: 689353, loss: 0.0604553148150444, data time: 0.03275859355926514\n",
      "step: 689354, loss: 0.059226568788290024, data time: 0.029299550586276583\n",
      "step: 689355, loss: 0.06390681862831116, data time: 0.026604437828063966\n",
      "step: 689356, loss: 0.06406185030937195, data time: 0.024416338313709606\n",
      "step: 689357, loss: 0.06430593132972717, data time: 0.022592743237813313\n",
      "step: 689358, loss: 0.06485245376825333, data time: 0.021040861423198994\n",
      "step: 689359, loss: 0.06397354602813721, data time: 0.019701344626290456\n",
      "step: 689360, loss: 0.05723576247692108, data time: 0.018548234303792318\n",
      "step: 689361, loss: 0.06331752240657806, data time: 0.017537862062454224\n",
      "step: 689362, loss: 0.06531701982021332, data time: 0.016646343119004193\n",
      "step: 689363, loss: 0.059220969676971436, data time: 0.01584939161936442\n",
      "step: 689364, loss: 0.06426359713077545, data time: 0.015138538260208933\n",
      "step: 689365, loss: 0.060565873980522156, data time: 0.014506006240844726\n",
      "step: 689366, loss: 0.07192867249250412, data time: 0.013937371117728097\n",
      "step: 689367, loss: 0.06525976955890656, data time: 0.013413754376498136\n",
      "step: 689368, loss: 0.05891533941030502, data time: 0.012930424317069675\n",
      "step: 689369, loss: 0.05654538795351982, data time: 0.012491583824157715\n",
      "step: 689370, loss: 0.061423614621162415, data time: 0.012087230682373046\n",
      "step: 689371, loss: 0.06491892784833908, data time: 0.011713587320767917\n",
      "step: 689372, loss: 0.0614587664604187, data time: 0.011366128921508789\n",
      "step: 689373, loss: 0.061700742691755295, data time: 0.011046051979064941\n",
      "step: 689374, loss: 0.06276541948318481, data time: 0.01075003064911941\n",
      "step: 689375, loss: 0.06298872083425522, data time: 0.010478194554646809\n",
      "step: 689376, loss: 0.062325891107320786, data time: 0.010218643373058688\n",
      "step: 689377, loss: 0.05760511755943298, data time: 0.009977571666240692\n",
      "step: 689378, loss: 0.06346072256565094, data time: 0.00973618391788367\n",
      "step: 689379, loss: 0.05685322731733322, data time: 0.009509703692267923\n",
      "step: 689380, loss: 0.06261633336544037, data time: 0.009296301433018276\n",
      "step: 689381, loss: 0.059530287981033325, data time: 0.009090622266133627\n",
      "step: 689382, loss: 0.061859142035245895, data time: 0.008897575172218116\n",
      "step: 689383, loss: 0.06531130522489548, data time: 0.008717762796502364\n",
      "step: 689384, loss: 0.058408599346876144, data time: 0.008549152276454827\n",
      "step: 689385, loss: 0.0763401985168457, data time: 0.008386927843093871\n",
      "step: 689386, loss: 0.06454436480998993, data time: 0.23997187614440918\n",
      "step: 689387, loss: 0.06586475670337677, data time: 0.12072896957397461\n",
      "step: 689388, loss: 0.06925829499959946, data time: 0.08100517590840657\n",
      "step: 689389, loss: 0.062150366604328156, data time: 0.06161254644393921\n",
      "step: 689390, loss: 0.05994070693850517, data time: 0.049565458297729494\n",
      "step: 689391, loss: 0.06314796954393387, data time: 0.04154344399770101\n",
      "step: 689392, loss: 0.06274166703224182, data time: 0.03581653322492327\n",
      "step: 689393, loss: 0.0625804141163826, data time: 0.03160366415977478\n",
      "step: 689394, loss: 0.057984717190265656, data time: 0.028238322999742296\n",
      "step: 689395, loss: 0.061146330088377, data time: 0.02560901641845703\n",
      "step: 689396, loss: 0.0686430111527443, data time: 0.023476232181895863\n",
      "step: 689397, loss: 0.06234496459364891, data time: 0.02171125014623006\n",
      "step: 689398, loss: 0.06826871633529663, data time: 0.02020549774169922\n",
      "step: 689399, loss: 0.06318069994449615, data time: 0.01890756402696882\n",
      "step: 689400, loss: 0.062409378588199615, data time: 0.017781893412272137\n",
      "step: 689401, loss: 0.06089119240641594, data time: 0.016801893711090088\n",
      "step: 689402, loss: 0.06185346469283104, data time: 0.01593804359436035\n",
      "step: 689403, loss: 0.06463345885276794, data time: 0.015160785781012641\n",
      "step: 689404, loss: 0.06394805014133453, data time: 0.014473588843094675\n",
      "step: 689405, loss: 0.060517460107803345, data time: 0.013855624198913574\n",
      "step: 689406, loss: 0.06364444643259048, data time: 0.013300759451729911\n",
      "step: 689407, loss: 0.06176362559199333, data time: 0.012793237512761896\n",
      "step: 689408, loss: 0.05973007529973984, data time: 0.012327349704244862\n",
      "step: 689409, loss: 0.061297036707401276, data time: 0.011902799208958944\n",
      "step: 689410, loss: 0.06471890956163406, data time: 0.011516542434692382\n",
      "step: 689411, loss: 0.06464680284261703, data time: 0.01115117623255803\n",
      "step: 689412, loss: 0.06035172939300537, data time: 0.010810525329024703\n",
      "step: 689413, loss: 0.06905864924192429, data time: 0.010496250220707484\n",
      "step: 689414, loss: 0.05715743824839592, data time: 0.010210916913788894\n",
      "step: 689415, loss: 0.0557326003909111, data time: 0.009944542249043783\n",
      "step: 689416, loss: 0.06492626667022705, data time: 0.009699236962103074\n",
      "step: 689417, loss: 0.06410939246416092, data time: 0.009468071162700653\n",
      "step: 689418, loss: 0.06018339842557907, data time: 0.009239702513723663\n",
      "step: 689419, loss: 0.062650665640831, data time: 0.009025089880999397\n",
      "step: 689420, loss: 0.06196689233183861, data time: 0.008825554166521345\n",
      "step: 689421, loss: 0.05723513662815094, data time: 0.00863109032313029\n",
      "step: 689422, loss: 0.07216735184192657, data time: 0.008448768306422877\n",
      "step: 689423, loss: 0.05765245109796524, data time: 0.00828078546022114\n",
      "step: 689424, loss: 0.06698236614465714, data time: 0.008121857276329627\n",
      "step: 689425, loss: 0.05363134294748306, data time: 0.007970237731933593\n",
      "step: 689426, loss: 0.06153484433889389, data time: 0.24543976783752441\n",
      "step: 689427, loss: 0.055938564240932465, data time: 0.12350976467132568\n",
      "step: 689428, loss: 0.06026662886142731, data time: 0.0828553835550944\n",
      "step: 689429, loss: 0.06042584776878357, data time: 0.06305909156799316\n",
      "step: 689430, loss: 0.06257575750350952, data time: 0.05077624320983887\n",
      "step: 689431, loss: 0.062142662703990936, data time: 0.042590975761413574\n",
      "step: 689432, loss: 0.05781165882945061, data time: 0.036747864314488\n",
      "step: 689433, loss: 0.06380978226661682, data time: 0.032452553510665894\n",
      "step: 689434, loss: 0.06830711662769318, data time: 0.029028468661838107\n",
      "step: 689435, loss: 0.057941436767578125, data time: 0.02635948657989502\n",
      "step: 689436, loss: 0.06270882487297058, data time: 0.024188670245083897\n",
      "step: 689437, loss: 0.06393398344516754, data time: 0.022383232911427815\n",
      "step: 689438, loss: 0.0562610886991024, data time: 0.020853537779587965\n",
      "step: 689439, loss: 0.06335338205099106, data time: 0.01954061644417899\n",
      "step: 689440, loss: 0.06134265661239624, data time: 0.018398173650105796\n",
      "step: 689441, loss: 0.05752017721533775, data time: 0.017395764589309692\n",
      "step: 689442, loss: 0.06206453591585159, data time: 0.0165099817163804\n",
      "step: 689443, loss: 0.059718094766139984, data time: 0.015720698568556044\n",
      "step: 689444, loss: 0.06382626295089722, data time: 0.015021612769679019\n",
      "step: 689445, loss: 0.06812922656536102, data time: 0.014398002624511718\n",
      "step: 689446, loss: 0.06659119576215744, data time: 0.01383277348109654\n",
      "step: 689447, loss: 0.05697712302207947, data time: 0.01331891796805642\n",
      "step: 689448, loss: 0.061600618064403534, data time: 0.01284318384916886\n",
      "step: 689449, loss: 0.06850631535053253, data time: 0.012390514214833578\n",
      "step: 689450, loss: 0.0668083056807518, data time: 0.01197800636291504\n",
      "step: 689451, loss: 0.060895174741744995, data time: 0.011595450914823092\n",
      "step: 689452, loss: 0.0581049919128418, data time: 0.011238186447708695\n",
      "step: 689453, loss: 0.06457256525754929, data time: 0.010912214006696428\n",
      "step: 689454, loss: 0.061817288398742676, data time: 0.010612158939756196\n",
      "step: 689455, loss: 0.05922026187181473, data time: 0.01033177375793457\n",
      "step: 689456, loss: 0.05820995196700096, data time: 0.010067309102704447\n",
      "step: 689457, loss: 0.058802079409360886, data time: 0.009822480380535126\n",
      "step: 689458, loss: 0.06848017871379852, data time: 0.009582996368408203\n",
      "step: 689459, loss: 0.0588323138654232, data time: 0.009358139599070829\n",
      "step: 689460, loss: 0.06524761021137238, data time: 0.00914459228515625\n",
      "step: 689461, loss: 0.06539104878902435, data time: 0.008942941824595133\n",
      "step: 689462, loss: 0.0581413209438324, data time: 0.008756972648002006\n",
      "step: 689463, loss: 0.06012066453695297, data time: 0.00858118032154284\n",
      "step: 689464, loss: 0.05999539792537689, data time: 0.008416475393833259\n",
      "step: 689465, loss: 0.04521597549319267, data time: 0.008257687091827393\n",
      "step: 689466, loss: 0.06185705587267876, data time: 0.23773980140686035\n",
      "step: 689467, loss: 0.05931807681918144, data time: 0.11999273300170898\n",
      "step: 689468, loss: 0.06556546688079834, data time: 0.08105850219726562\n",
      "step: 689469, loss: 0.05906475707888603, data time: 0.061556339263916016\n",
      "step: 689470, loss: 0.06253394484519958, data time: 0.04954848289489746\n",
      "step: 689471, loss: 0.060370590537786484, data time: 0.041541496912638344\n",
      "step: 689472, loss: 0.06354017555713654, data time: 0.0358100278036935\n",
      "step: 689473, loss: 0.06008296459913254, data time: 0.0315898060798645\n",
      "step: 689474, loss: 0.06265364587306976, data time: 0.028227382236056857\n",
      "step: 689475, loss: 0.06004208326339722, data time: 0.02560453414916992\n",
      "step: 689476, loss: 0.0613897442817688, data time: 0.023470076647671787\n",
      "step: 689477, loss: 0.06401098519563675, data time: 0.021691560745239258\n",
      "step: 689478, loss: 0.0646384060382843, data time: 0.02018906519963191\n",
      "step: 689479, loss: 0.0642382800579071, data time: 0.018889188766479492\n",
      "step: 689480, loss: 0.061932455748319626, data time: 0.01777370770772298\n",
      "step: 689481, loss: 0.06458663940429688, data time: 0.016787156462669373\n",
      "step: 689482, loss: 0.06504049897193909, data time: 0.01591985365923713\n",
      "step: 689483, loss: 0.06266012787818909, data time: 0.015141924222310385\n",
      "step: 689484, loss: 0.05964121222496033, data time: 0.014451553947047183\n",
      "step: 689485, loss: 0.059107448905706406, data time: 0.013838958740234376\n",
      "step: 689486, loss: 0.06093812733888626, data time: 0.013282344454810732\n",
      "step: 689487, loss: 0.057617999613285065, data time: 0.012776418165727095\n",
      "step: 689488, loss: 0.06061439961194992, data time: 0.012314298878545347\n",
      "step: 689489, loss: 0.0621892474591732, data time: 0.011887222528457642\n",
      "step: 689490, loss: 0.06158434599637985, data time: 0.01149552345275879\n",
      "step: 689491, loss: 0.06382810324430466, data time: 0.011130314606886644\n",
      "step: 689492, loss: 0.06254032254219055, data time: 0.010792273062246817\n",
      "step: 689493, loss: 0.058097146451473236, data time: 0.010480548654283797\n",
      "step: 689494, loss: 0.07193402945995331, data time: 0.010192303821958345\n",
      "step: 689495, loss: 0.06145469471812248, data time: 0.009925270080566406\n",
      "step: 689496, loss: 0.06246964633464813, data time: 0.00968185547859438\n",
      "step: 689497, loss: 0.07101621478796005, data time: 0.009453065693378448\n",
      "step: 689498, loss: 0.0665115937590599, data time: 0.009224891662597656\n",
      "step: 689499, loss: 0.061073701828718185, data time: 0.009011177455677706\n",
      "step: 689500, loss: 0.06277865171432495, data time: 0.008808531079973493\n",
      "step: 689501, loss: 0.06468990445137024, data time: 0.008614261945088705\n",
      "step: 689502, loss: 0.0626012310385704, data time: 0.008432581618025497\n",
      "step: 689503, loss: 0.05622563883662224, data time: 0.008264284384878058\n",
      "step: 689504, loss: 0.0649467185139656, data time: 0.008107491028614534\n",
      "step: 689505, loss: 0.05743883177638054, data time: 0.00795641541481018\n",
      "step: 689506, loss: 0.057716064155101776, data time: 0.24499249458312988\n",
      "step: 689507, loss: 0.06103386729955673, data time: 0.12329185009002686\n",
      "step: 689508, loss: 0.058348339051008224, data time: 0.08338514963785808\n",
      "step: 689509, loss: 0.061391815543174744, data time: 0.06320881843566895\n",
      "step: 689510, loss: 0.061347510665655136, data time: 0.05086708068847656\n",
      "step: 689511, loss: 0.060731805860996246, data time: 0.042636473973592125\n",
      "step: 689512, loss: 0.06349290907382965, data time: 0.03674445833478655\n",
      "step: 689513, loss: 0.05801042914390564, data time: 0.03246510028839111\n",
      "step: 689514, loss: 0.07180274277925491, data time: 0.029005183113945857\n",
      "step: 689515, loss: 0.06538262963294983, data time: 0.026314425468444824\n",
      "step: 689516, loss: 0.0574304424226284, data time: 0.024117122996937145\n",
      "step: 689517, loss: 0.06380324810743332, data time: 0.022287289301554363\n",
      "step: 689518, loss: 0.06062775105237961, data time: 0.020742453061617337\n",
      "step: 689519, loss: 0.060571786016225815, data time: 0.01940441131591797\n",
      "step: 689520, loss: 0.06425093114376068, data time: 0.01824533144632975\n",
      "step: 689521, loss: 0.06259755790233612, data time: 0.017231345176696777\n",
      "step: 689522, loss: 0.0633842945098877, data time: 0.01633799777311437\n",
      "step: 689523, loss: 0.06868922710418701, data time: 0.01555927594502767\n",
      "step: 689524, loss: 0.0673823356628418, data time: 0.014866577951531661\n",
      "step: 689525, loss: 0.05898406729102135, data time: 0.014249956607818604\n",
      "step: 689526, loss: 0.06286803632974625, data time: 0.013693639210292272\n",
      "step: 689527, loss: 0.06299915909767151, data time: 0.01318824291229248\n",
      "step: 689528, loss: 0.06694088131189346, data time: 0.012720999510391899\n",
      "step: 689529, loss: 0.057327933609485626, data time: 0.012292136748631796\n",
      "step: 689530, loss: 0.05934710055589676, data time: 0.01189986228942871\n",
      "step: 689531, loss: 0.06396245211362839, data time: 0.011533471254202036\n",
      "step: 689532, loss: 0.06594517081975937, data time: 0.011192092189082393\n",
      "step: 689533, loss: 0.058174483478069305, data time: 0.010877566678183419\n",
      "step: 689534, loss: 0.061110179871320724, data time: 0.010590298422451678\n",
      "step: 689535, loss: 0.06812391430139542, data time: 0.010322340329488118\n",
      "step: 689536, loss: 0.052942514419555664, data time: 0.010072115928896012\n",
      "step: 689537, loss: 0.06752379983663559, data time: 0.00984056293964386\n",
      "step: 689538, loss: 0.0630711019039154, data time: 0.009604808055993282\n",
      "step: 689539, loss: 0.056706301867961884, data time: 0.009382156764759737\n",
      "step: 689540, loss: 0.058499179780483246, data time: 0.009171758379255022\n",
      "step: 689541, loss: 0.06285350024700165, data time: 0.008971002366807725\n",
      "step: 689542, loss: 0.061699818819761276, data time: 0.008782760517017261\n",
      "step: 689543, loss: 0.06277380883693695, data time: 0.008608341217041016\n",
      "step: 689544, loss: 0.06698930263519287, data time: 0.008442970422598032\n",
      "step: 689545, loss: 0.03579971194267273, data time: 0.008287018537521363\n",
      "step: 689546, loss: 0.06037428230047226, data time: 0.2372722625732422\n",
      "step: 689547, loss: 0.058875542134046555, data time: 0.12045574188232422\n",
      "step: 689548, loss: 0.06442050635814667, data time: 0.08080935478210449\n",
      "step: 689549, loss: 0.061996933072805405, data time: 0.061504364013671875\n",
      "step: 689550, loss: 0.06930553913116455, data time: 0.049489498138427734\n",
      "step: 689551, loss: 0.06238684803247452, data time: 0.041486501693725586\n",
      "step: 689552, loss: 0.0649385154247284, data time: 0.03575927870614188\n",
      "step: 689553, loss: 0.05737541615962982, data time: 0.03154563903808594\n",
      "step: 689554, loss: 0.054737966507673264, data time: 0.02819477187262641\n",
      "step: 689555, loss: 0.06360159814357758, data time: 0.025582170486450194\n",
      "step: 689556, loss: 0.0652267262339592, data time: 0.023456920276988636\n",
      "step: 689557, loss: 0.05593406409025192, data time: 0.021681706110636394\n",
      "step: 689558, loss: 0.06289444863796234, data time: 0.02017980355482835\n",
      "step: 689559, loss: 0.0635605901479721, data time: 0.01889448506491525\n",
      "step: 689560, loss: 0.06068506836891174, data time: 0.017772944768269856\n",
      "step: 689561, loss: 0.06422607600688934, data time: 0.016790643334388733\n",
      "step: 689562, loss: 0.06409372389316559, data time: 0.015930456273696002\n",
      "step: 689563, loss: 0.0578744113445282, data time: 0.015152745776706271\n",
      "step: 689564, loss: 0.06146515533328056, data time: 0.014461429495560495\n",
      "step: 689565, loss: 0.06266569346189499, data time: 0.013846480846405029\n",
      "step: 689566, loss: 0.06677351891994476, data time: 0.013289008821759905\n",
      "step: 689567, loss: 0.06235833466053009, data time: 0.01278201016512784\n",
      "step: 689568, loss: 0.06224783882498741, data time: 0.012313739113185717\n",
      "step: 689569, loss: 0.07089325040578842, data time: 0.011890421311060587\n",
      "step: 689570, loss: 0.061651717871427536, data time: 0.011510725021362305\n",
      "step: 689571, loss: 0.0669129490852356, data time: 0.011160071079547588\n",
      "step: 689572, loss: 0.05979470536112785, data time: 0.010820636042842158\n",
      "step: 689573, loss: 0.06296180188655853, data time: 0.010507217475346156\n",
      "step: 689574, loss: 0.06549155712127686, data time: 0.010218233897768218\n",
      "step: 689575, loss: 0.06645892560482025, data time: 0.00994861125946045\n",
      "step: 689576, loss: 0.061161160469055176, data time: 0.009698583233741022\n",
      "step: 689577, loss: 0.061821430921554565, data time: 0.009466730058193207\n",
      "step: 689578, loss: 0.062044158577919006, data time: 0.009239290699814304\n",
      "step: 689579, loss: 0.05592406913638115, data time: 0.009023266680100384\n",
      "step: 689580, loss: 0.06121004745364189, data time: 0.008821725845336914\n",
      "step: 689581, loss: 0.058774467557668686, data time: 0.008627778953976102\n",
      "step: 689582, loss: 0.06024569272994995, data time: 0.00844699627644307\n",
      "step: 689583, loss: 0.06706450879573822, data time: 0.008278376177737587\n",
      "step: 689584, loss: 0.06101464480161667, data time: 0.008118073145548502\n",
      "step: 689585, loss: 0.06169379502534866, data time: 0.00796690583229065\n",
      "step: 689586, loss: 0.05776035785675049, data time: 0.23895478248596191\n",
      "step: 689587, loss: 0.05822047218680382, data time: 0.12029457092285156\n",
      "step: 689588, loss: 0.057397034019231796, data time: 0.08110419909159343\n",
      "step: 689589, loss: 0.06744763255119324, data time: 0.06158173084259033\n",
      "step: 689590, loss: 0.06227482110261917, data time: 0.049540376663208006\n",
      "step: 689591, loss: 0.0612630657851696, data time: 0.04153597354888916\n",
      "step: 689592, loss: 0.06279868632555008, data time: 0.035803522382463725\n",
      "step: 689593, loss: 0.060240648686885834, data time: 0.031589776277542114\n",
      "step: 689594, loss: 0.06082306429743767, data time: 0.028240097893608943\n",
      "step: 689595, loss: 0.0669168159365654, data time: 0.0256211519241333\n",
      "step: 689596, loss: 0.05921412631869316, data time: 0.023483233018354935\n",
      "step: 689597, loss: 0.06260993331670761, data time: 0.021703859170277912\n",
      "step: 689598, loss: 0.060154661536216736, data time: 0.020197391510009766\n",
      "step: 689599, loss: 0.062046539038419724, data time: 0.018899287496294295\n",
      "step: 689600, loss: 0.06897905468940735, data time: 0.01777361234029134\n",
      "step: 689601, loss: 0.0633610188961029, data time: 0.016799360513687134\n",
      "step: 689602, loss: 0.05954437702894211, data time: 0.01593173251432531\n",
      "step: 689603, loss: 0.06800027191638947, data time: 0.015158123440212674\n",
      "step: 689604, loss: 0.06081167981028557, data time: 0.014464880290784334\n",
      "step: 689605, loss: 0.06090787053108215, data time: 0.013846778869628906\n",
      "step: 689606, loss: 0.05815243721008301, data time: 0.01329290299188523\n",
      "step: 689607, loss: 0.06401215493679047, data time: 0.01278807900168679\n",
      "step: 689608, loss: 0.0678274929523468, data time: 0.01232085020645805\n",
      "step: 689609, loss: 0.06056533381342888, data time: 0.011892706155776978\n",
      "step: 689610, loss: 0.057923756539821625, data time: 0.011499929428100585\n",
      "step: 689611, loss: 0.06260279566049576, data time: 0.011135128828195425\n",
      "step: 689612, loss: 0.06420840322971344, data time: 0.010793880180076317\n",
      "step: 689613, loss: 0.06517985463142395, data time: 0.010479373591286796\n",
      "step: 689614, loss: 0.06185312569141388, data time: 0.0101918023208092\n",
      "step: 689615, loss: 0.06029203534126282, data time: 0.009923402468363445\n",
      "step: 689616, loss: 0.06598471850156784, data time: 0.009671265079129127\n",
      "step: 689617, loss: 0.0584920309484005, data time: 0.009438954293727875\n",
      "step: 689618, loss: 0.06065557897090912, data time: 0.009210969462539211\n",
      "step: 689619, loss: 0.06293056160211563, data time: 0.008997131796444164\n",
      "step: 689620, loss: 0.060975249856710434, data time: 0.00879577909197126\n",
      "step: 689621, loss: 0.06270268559455872, data time: 0.00860269864400228\n",
      "step: 689622, loss: 0.06745365262031555, data time: 0.008421859225711308\n",
      "step: 689623, loss: 0.060270022600889206, data time: 0.008254346094633403\n",
      "step: 689624, loss: 0.05571054667234421, data time: 0.008097642507308569\n",
      "step: 689625, loss: 0.055716969072818756, data time: 0.007946169376373291\n",
      "step: 689626, loss: 0.06604888290166855, data time: 0.2530522346496582\n",
      "step: 689627, loss: 0.060573168098926544, data time: 0.12747108936309814\n",
      "step: 689628, loss: 0.05931911990046501, data time: 0.08548823992411296\n",
      "step: 689629, loss: 0.06586312502622604, data time: 0.06498157978057861\n",
      "step: 689630, loss: 0.06418352574110031, data time: 0.052298355102539065\n",
      "step: 689631, loss: 0.060455530881881714, data time: 0.04381601015726725\n",
      "step: 689632, loss: 0.06443226337432861, data time: 0.03776359558105469\n",
      "step: 689633, loss: 0.0640643835067749, data time: 0.033291369676589966\n",
      "step: 689634, loss: 0.061282701790332794, data time: 0.02973453203837077\n",
      "step: 689635, loss: 0.0657850131392479, data time: 0.027002358436584474\n",
      "step: 689636, loss: 0.062371328473091125, data time: 0.024776632135564632\n",
      "step: 689637, loss: 0.05915548652410507, data time: 0.022921423117319744\n",
      "step: 689638, loss: 0.059711754322052, data time: 0.021349778542151816\n",
      "step: 689639, loss: 0.0642152726650238, data time: 0.019992760249546597\n",
      "step: 689640, loss: 0.058546241372823715, data time: 0.018848991394042967\n",
      "step: 689641, loss: 0.060622915625572205, data time: 0.017824828624725342\n",
      "step: 689642, loss: 0.06391920149326324, data time: 0.01691602258121266\n",
      "step: 689643, loss: 0.059441447257995605, data time: 0.016106486320495605\n",
      "step: 689644, loss: 0.059794824570417404, data time: 0.015384862297459653\n",
      "step: 689645, loss: 0.06701318919658661, data time: 0.014740145206451416\n",
      "step: 689646, loss: 0.06387471407651901, data time: 0.014159985951014928\n",
      "step: 689647, loss: 0.05732620880007744, data time: 0.013631473888050426\n",
      "step: 689648, loss: 0.06111278384923935, data time: 0.013143902239592178\n",
      "step: 689649, loss: 0.0628318339586258, data time: 0.01269585887591044\n",
      "step: 689650, loss: 0.06091871112585068, data time: 0.012285614013671875\n",
      "step: 689651, loss: 0.06411291658878326, data time: 0.01190342352940486\n",
      "step: 689652, loss: 0.06500732898712158, data time: 0.01153538845203541\n",
      "step: 689653, loss: 0.06481568515300751, data time: 0.011196281228746687\n",
      "step: 689654, loss: 0.06337140500545502, data time: 0.010885731927279768\n",
      "step: 689655, loss: 0.06505441665649414, data time: 0.010594654083251952\n",
      "step: 689656, loss: 0.06314370036125183, data time: 0.010326577771094537\n",
      "step: 689657, loss: 0.06127938628196716, data time: 0.010074235498905182\n",
      "step: 689658, loss: 0.06277097761631012, data time: 0.009832505023840702\n",
      "step: 689659, loss: 0.0523710660636425, data time: 0.009602918344385484\n",
      "step: 689660, loss: 0.06924247741699219, data time: 0.009386280604771206\n",
      "step: 689661, loss: 0.0663008913397789, data time: 0.009179439809587266\n",
      "step: 689662, loss: 0.06692042946815491, data time: 0.008986002690083272\n",
      "step: 689663, loss: 0.05748499184846878, data time: 0.008807521117360969\n",
      "step: 689664, loss: 0.055406540632247925, data time: 0.008637153185330905\n",
      "step: 689665, loss: 0.05643738806247711, data time: 0.00847533941268921\n",
      "step: 689666, loss: 0.06738890707492828, data time: 0.25463128089904785\n",
      "step: 689667, loss: 0.059894487261772156, data time: 0.1280531883239746\n",
      "step: 689668, loss: 0.061572492122650146, data time: 0.08639192581176758\n",
      "step: 689669, loss: 0.06006466969847679, data time: 0.06557118892669678\n",
      "step: 689670, loss: 0.05892321094870567, data time: 0.05274739265441895\n",
      "step: 689671, loss: 0.06344430148601532, data time: 0.0441965659459432\n",
      "step: 689672, loss: 0.058118343353271484, data time: 0.03807837622506278\n",
      "step: 689673, loss: 0.06264598667621613, data time: 0.033567607402801514\n",
      "step: 689674, loss: 0.06116008758544922, data time: 0.029982116487291124\n",
      "step: 689675, loss: 0.05976065248250961, data time: 0.0271970272064209\n",
      "step: 689676, loss: 0.0592586025595665, data time: 0.024929501793601296\n",
      "step: 689677, loss: 0.06431888788938522, data time: 0.02302853266398112\n",
      "step: 689678, loss: 0.0591515377163887, data time: 0.02144617300767165\n",
      "step: 689679, loss: 0.06085784360766411, data time: 0.020084738731384277\n",
      "step: 689680, loss: 0.06750026345252991, data time: 0.018908945719401042\n",
      "step: 689681, loss: 0.06279832124710083, data time: 0.017874926328659058\n",
      "step: 689682, loss: 0.06063386797904968, data time: 0.0169679557575899\n",
      "step: 689683, loss: 0.06295160204172134, data time: 0.016155123710632324\n",
      "step: 689684, loss: 0.06118684634566307, data time: 0.01542898228293971\n",
      "step: 689685, loss: 0.059692516922950745, data time: 0.01478588581085205\n",
      "step: 689686, loss: 0.06610798835754395, data time: 0.014186155228387742\n",
      "step: 689687, loss: 0.06365460902452469, data time: 0.01363935253836892\n",
      "step: 689688, loss: 0.060055360198020935, data time: 0.01313776555268661\n",
      "step: 689689, loss: 0.07306190580129623, data time: 0.012676894664764404\n",
      "step: 689690, loss: 0.06544101238250732, data time: 0.012256526947021484\n",
      "step: 689691, loss: 0.06121615692973137, data time: 0.01186231466440054\n",
      "step: 689692, loss: 0.06195757910609245, data time: 0.011497285630967882\n",
      "step: 689693, loss: 0.06074634939432144, data time: 0.011164103235517229\n",
      "step: 689694, loss: 0.06350091099739075, data time: 0.010852747950060615\n",
      "step: 689695, loss: 0.06730931997299194, data time: 0.01056220531463623\n",
      "step: 689696, loss: 0.06185653433203697, data time: 0.010291845567764775\n",
      "step: 689697, loss: 0.0563153401017189, data time: 0.01004084199666977\n",
      "step: 689698, loss: 0.059469953179359436, data time: 0.009799119197961056\n",
      "step: 689699, loss: 0.06268845498561859, data time: 0.009566615609561695\n",
      "step: 689700, loss: 0.06066892668604851, data time: 0.009348276683262416\n",
      "step: 689701, loss: 0.059925615787506104, data time: 0.009142769707573785\n",
      "step: 689702, loss: 0.06789340078830719, data time: 0.00894824878589527\n",
      "step: 689703, loss: 0.06714542955160141, data time: 0.00876663233104505\n",
      "step: 689704, loss: 0.05340331047773361, data time: 0.008594005535810422\n",
      "step: 689705, loss: 0.047772448509931564, data time: 0.008430135250091553\n",
      "step: 689706, loss: 0.06156185269355774, data time: 0.2497413158416748\n",
      "step: 689707, loss: 0.06365731358528137, data time: 0.12566053867340088\n",
      "step: 689708, loss: 0.06547023355960846, data time: 0.08428541819254558\n",
      "step: 689709, loss: 0.059218525886535645, data time: 0.06410551071166992\n",
      "step: 689710, loss: 0.06306011974811554, data time: 0.051564598083496095\n",
      "step: 689711, loss: 0.06186447665095329, data time: 0.043201724688212075\n",
      "step: 689712, loss: 0.05866297706961632, data time: 0.03723178591047015\n",
      "step: 689713, loss: 0.05759764835238457, data time: 0.03283444046974182\n",
      "step: 689714, loss: 0.07054124772548676, data time: 0.029354068968031142\n",
      "step: 689715, loss: 0.06669845432043076, data time: 0.026617002487182618\n",
      "step: 689716, loss: 0.06148942559957504, data time: 0.024392778223211117\n",
      "step: 689717, loss: 0.06763184070587158, data time: 0.022537291049957275\n",
      "step: 689718, loss: 0.0700569748878479, data time: 0.020974067541269157\n",
      "step: 689719, loss: 0.060579460114240646, data time: 0.019623500960213796\n",
      "step: 689720, loss: 0.06437504291534424, data time: 0.01845072110493978\n",
      "step: 689721, loss: 0.06079215928912163, data time: 0.017427250742912292\n",
      "step: 689722, loss: 0.058711014688014984, data time: 0.016520107493681067\n",
      "step: 689723, loss: 0.06506248563528061, data time: 0.01571215523613824\n",
      "step: 689724, loss: 0.06439497321844101, data time: 0.014988447490491365\n",
      "step: 689725, loss: 0.05707680806517601, data time: 0.014350450038909912\n",
      "step: 689726, loss: 0.06251483410596848, data time: 0.013786906287783668\n",
      "step: 689727, loss: 0.06718339025974274, data time: 0.013273228298534046\n",
      "step: 689728, loss: 0.06502115726470947, data time: 0.012799221536387568\n",
      "step: 689729, loss: 0.061218541115522385, data time: 0.012365599473317465\n",
      "step: 689730, loss: 0.06394574046134949, data time: 0.01196711540222168\n",
      "step: 689731, loss: 0.05519619584083557, data time: 0.011597367433401255\n",
      "step: 689732, loss: 0.0606624037027359, data time: 0.011257825074372467\n",
      "step: 689733, loss: 0.06315838545560837, data time: 0.01093958956854684\n",
      "step: 689734, loss: 0.06034539267420769, data time: 0.010650495003009665\n",
      "step: 689735, loss: 0.06487995386123657, data time: 0.010380411148071289\n",
      "step: 689736, loss: 0.06345643103122711, data time: 0.010126221564508254\n",
      "step: 689737, loss: 0.059494949877262115, data time: 0.009891942143440247\n",
      "step: 689738, loss: 0.06364857405424118, data time: 0.009655396143595377\n",
      "step: 689739, loss: 0.05605870112776756, data time: 0.009429020040175495\n",
      "step: 689740, loss: 0.059298064559698105, data time: 0.009214932577950613\n",
      "step: 689741, loss: 0.06461629271507263, data time: 0.009012295140160454\n",
      "step: 689742, loss: 0.06004352867603302, data time: 0.008822350888638883\n",
      "step: 689743, loss: 0.05964360758662224, data time: 0.00864382166611521\n",
      "step: 689744, loss: 0.06098254770040512, data time: 0.008474936852088341\n",
      "step: 689745, loss: 0.06812593340873718, data time: 0.0083135724067688\n",
      "step: 689746, loss: 0.0648951530456543, data time: 0.2433633804321289\n",
      "step: 689747, loss: 0.06266006082296371, data time: 0.12246513366699219\n",
      "step: 689748, loss: 0.05998481437563896, data time: 0.08279824256896973\n",
      "step: 689749, loss: 0.05759831890463829, data time: 0.0627787709236145\n",
      "step: 689750, loss: 0.06655891239643097, data time: 0.05049881935119629\n",
      "step: 689751, loss: 0.059103380888700485, data time: 0.04232553641001383\n",
      "step: 689752, loss: 0.06369172036647797, data time: 0.03648018836975098\n",
      "step: 689753, loss: 0.059752851724624634, data time: 0.03217148780822754\n",
      "step: 689754, loss: 0.06347203254699707, data time: 0.02874334653218587\n",
      "step: 689755, loss: 0.05981381982564926, data time: 0.02608380317687988\n",
      "step: 689756, loss: 0.06426943838596344, data time: 0.023912776600230824\n",
      "step: 689757, loss: 0.0656832680106163, data time: 0.02210293213526408\n",
      "step: 689758, loss: 0.06105060502886772, data time: 0.020570681645320013\n",
      "step: 689759, loss: 0.06673695147037506, data time: 0.019242269652230398\n",
      "step: 689760, loss: 0.06246769055724144, data time: 0.018095763524373372\n",
      "step: 689761, loss: 0.06482847779989243, data time: 0.017090976238250732\n",
      "step: 689762, loss: 0.06418484449386597, data time: 0.016205282772288603\n",
      "step: 689763, loss: 0.06644238531589508, data time: 0.015414953231811523\n",
      "step: 689764, loss: 0.05646057054400444, data time: 0.01471484334845292\n",
      "step: 689765, loss: 0.06892302632331848, data time: 0.014090025424957275\n",
      "step: 689766, loss: 0.061783503741025925, data time: 0.01352125122433617\n",
      "step: 689767, loss: 0.06318459659814835, data time: 0.01301918246529319\n",
      "step: 689768, loss: 0.06288310140371323, data time: 0.01254135629405146\n",
      "step: 689769, loss: 0.06518477201461792, data time: 0.012106875578562418\n",
      "step: 689770, loss: 0.06334319710731506, data time: 0.011705617904663086\n",
      "step: 689771, loss: 0.0565635971724987, data time: 0.011334657669067383\n",
      "step: 689772, loss: 0.07014539837837219, data time: 0.010988050036960177\n",
      "step: 689773, loss: 0.06760915368795395, data time: 0.010667562484741211\n",
      "step: 689774, loss: 0.0597509890794754, data time: 0.01037411854184907\n",
      "step: 689775, loss: 0.061243027448654175, data time: 0.010098759333292644\n",
      "step: 689776, loss: 0.06300808489322662, data time: 0.00984348020245952\n",
      "step: 689777, loss: 0.06766848266124725, data time: 0.009608037769794464\n",
      "step: 689778, loss: 0.05228649824857712, data time: 0.009375839522390655\n",
      "step: 689779, loss: 0.06273531168699265, data time: 0.009159200331744026\n",
      "step: 689780, loss: 0.058746885508298874, data time: 0.008957420076642717\n",
      "step: 689781, loss: 0.06353673338890076, data time: 0.008764207363128662\n",
      "step: 689782, loss: 0.06538756936788559, data time: 0.008582907754021723\n",
      "step: 689783, loss: 0.06318369507789612, data time: 0.008413063852410568\n",
      "step: 689784, loss: 0.06352619081735611, data time: 0.008252186652941581\n",
      "step: 689785, loss: 0.045087553560733795, data time: 0.00810052752494812\n",
      "step: 689786, loss: 0.06330176442861557, data time: 0.23675131797790527\n",
      "step: 689787, loss: 0.06263195723295212, data time: 0.11953747272491455\n",
      "step: 689788, loss: 0.06592684984207153, data time: 0.08084893226623535\n",
      "step: 689789, loss: 0.05857983976602554, data time: 0.0613102912902832\n",
      "step: 689790, loss: 0.06147010251879692, data time: 0.04932112693786621\n",
      "step: 689791, loss: 0.06584569811820984, data time: 0.04134639104207357\n",
      "step: 689792, loss: 0.06536081433296204, data time: 0.03563860484531948\n",
      "step: 689793, loss: 0.05914180725812912, data time: 0.03143790364265442\n",
      "step: 689794, loss: 0.061475031077861786, data time: 0.028088755077785917\n",
      "step: 689795, loss: 0.0628843605518341, data time: 0.025481247901916505\n",
      "step: 689796, loss: 0.061636194586753845, data time: 0.023364912379871716\n",
      "step: 689797, loss: 0.06133950874209404, data time: 0.021601001421610515\n",
      "step: 689798, loss: 0.06465085595846176, data time: 0.020116274173443135\n",
      "step: 689799, loss: 0.06469163298606873, data time: 0.01882028579711914\n",
      "step: 689800, loss: 0.06074871867895126, data time: 0.01769984563191732\n",
      "step: 689801, loss: 0.06534069776535034, data time: 0.016718432307243347\n",
      "step: 689802, loss: 0.05964088439941406, data time: 0.01585865020751953\n",
      "step: 689803, loss: 0.06583594530820847, data time: 0.015091088083055284\n",
      "step: 689804, loss: 0.06196580454707146, data time: 0.014403506329185084\n",
      "step: 689805, loss: 0.0648142620921135, data time: 0.013798654079437256\n",
      "step: 689806, loss: 0.057496339082717896, data time: 0.01325014659336635\n",
      "step: 689807, loss: 0.05933940410614014, data time: 0.012748674912886187\n",
      "step: 689808, loss: 0.06469368189573288, data time: 0.01228233005689538\n",
      "step: 689809, loss: 0.059148818254470825, data time: 0.011859923601150513\n",
      "step: 689810, loss: 0.06040984392166138, data time: 0.0114666748046875\n",
      "step: 689811, loss: 0.06612200289964676, data time: 0.011110535034766564\n",
      "step: 689812, loss: 0.05601545795798302, data time: 0.010770912523622866\n",
      "step: 689813, loss: 0.056503623723983765, data time: 0.010460223470415388\n",
      "step: 689814, loss: 0.06092575192451477, data time: 0.010175236340226799\n",
      "step: 689815, loss: 0.058878879994153976, data time: 0.00991045633951823\n",
      "step: 689816, loss: 0.06696970015764236, data time: 0.009665681469825006\n",
      "step: 689817, loss: 0.06492955982685089, data time: 0.0094384104013443\n",
      "step: 689818, loss: 0.06531205773353577, data time: 0.00921390995834813\n",
      "step: 689819, loss: 0.06018324941396713, data time: 0.008997650707469267\n",
      "step: 689820, loss: 0.06007614731788635, data time: 0.008795111519949777\n",
      "step: 689821, loss: 0.062249522656202316, data time: 0.00860126150978936\n",
      "step: 689822, loss: 0.06047603487968445, data time: 0.008424127424085463\n",
      "step: 689823, loss: 0.0642964243888855, data time: 0.008258047856782613\n",
      "step: 689824, loss: 0.06631991267204285, data time: 0.008098107117872972\n",
      "step: 689825, loss: 0.05959402024745941, data time: 0.007946079969406128\n",
      "step: 689826, loss: 0.06074485182762146, data time: 0.25116777420043945\n",
      "step: 689827, loss: 0.05987198278307915, data time: 0.12634563446044922\n",
      "step: 689828, loss: 0.06961105763912201, data time: 0.08531769116719563\n",
      "step: 689829, loss: 0.055088065564632416, data time: 0.06492465734481812\n",
      "step: 689830, loss: 0.06401252746582031, data time: 0.0522982120513916\n",
      "step: 689831, loss: 0.06328564882278442, data time: 0.043886383374532066\n",
      "step: 689832, loss: 0.05982273444533348, data time: 0.037867069244384766\n",
      "step: 689833, loss: 0.0609593391418457, data time: 0.03343003988265991\n",
      "step: 689834, loss: 0.061890095472335815, data time: 0.029890139897664387\n",
      "step: 689835, loss: 0.05829612538218498, data time: 0.027135348320007323\n",
      "step: 689836, loss: 0.05643047019839287, data time: 0.024905291470614346\n",
      "step: 689837, loss: 0.06088557466864586, data time: 0.023042539755503338\n",
      "step: 689838, loss: 0.059948574751615524, data time: 0.021468749413123496\n",
      "step: 689839, loss: 0.06076500937342644, data time: 0.02010667324066162\n",
      "step: 689840, loss: 0.060419850051403046, data time: 0.018931245803833006\n",
      "step: 689841, loss: 0.05646584555506706, data time: 0.017895877361297607\n",
      "step: 689842, loss: 0.05870534107089043, data time: 0.016985444461598116\n",
      "step: 689843, loss: 0.06335940212011337, data time: 0.01617288589477539\n",
      "step: 689844, loss: 0.05343728885054588, data time: 0.015449034540276779\n",
      "step: 689845, loss: 0.06932751834392548, data time: 0.014803409576416016\n",
      "step: 689846, loss: 0.059639375656843185, data time: 0.014222065607706705\n",
      "step: 689847, loss: 0.05609079450368881, data time: 0.013686255975203081\n",
      "step: 689848, loss: 0.056657448410987854, data time: 0.013194488442462423\n",
      "step: 689849, loss: 0.0618707612156868, data time: 0.012747019529342651\n",
      "step: 689850, loss: 0.0632803812623024, data time: 0.012334747314453125\n",
      "step: 689851, loss: 0.060504376888275146, data time: 0.01195369316981389\n",
      "step: 689852, loss: 0.06269451975822449, data time: 0.011597801137853551\n",
      "step: 689853, loss: 0.054802440106868744, data time: 0.011266631739480155\n",
      "step: 689854, loss: 0.0581863634288311, data time: 0.010964492271686423\n",
      "step: 689855, loss: 0.05920828506350517, data time: 0.010686357816060385\n",
      "step: 689856, loss: 0.061619408428668976, data time: 0.010422283603299049\n",
      "step: 689857, loss: 0.06510203331708908, data time: 0.01017875224351883\n",
      "step: 689858, loss: 0.0666278824210167, data time: 0.009933009292140152\n",
      "step: 689859, loss: 0.059778016060590744, data time: 0.009702037362491383\n",
      "step: 689860, loss: 0.06240027770400047, data time: 0.009484032222202846\n",
      "step: 689861, loss: 0.056276094168424606, data time: 0.009276343716515435\n",
      "step: 689862, loss: 0.0641683042049408, data time: 0.009080332678717535\n",
      "step: 689863, loss: 0.0629439577460289, data time: 0.008898666030482241\n",
      "step: 689864, loss: 0.059279054403305054, data time: 0.008727581073076297\n",
      "step: 689865, loss: 0.05507100373506546, data time: 0.008563470840454102\n",
      "step: 689866, loss: 0.06325750797986984, data time: 0.24692273139953613\n",
      "step: 689867, loss: 0.0638355016708374, data time: 0.12426471710205078\n",
      "step: 689868, loss: 0.06301627308130264, data time: 0.0838940938313802\n",
      "step: 689869, loss: 0.061727579683065414, data time: 0.06368863582611084\n",
      "step: 689870, loss: 0.05924227833747864, data time: 0.051232099533081055\n",
      "step: 689871, loss: 0.06346094608306885, data time: 0.04292492071787516\n",
      "step: 689872, loss: 0.06400762498378754, data time: 0.03701588085719517\n",
      "step: 689873, loss: 0.06662659347057343, data time: 0.03265151381492615\n",
      "step: 689874, loss: 0.061439149081707, data time: 0.029170062806871202\n",
      "step: 689875, loss: 0.06156653165817261, data time: 0.026451539993286134\n",
      "step: 689876, loss: 0.06372074037790298, data time: 0.024255882609974255\n",
      "step: 689877, loss: 0.06232316792011261, data time: 0.022424161434173584\n",
      "step: 689878, loss: 0.0659877136349678, data time: 0.02087257458613469\n",
      "step: 689879, loss: 0.0610489659011364, data time: 0.0195223263331822\n",
      "step: 689880, loss: 0.0558006688952446, data time: 0.018355035781860353\n",
      "step: 689881, loss: 0.0633741170167923, data time: 0.0173407644033432\n",
      "step: 689882, loss: 0.06540827453136444, data time: 0.016444739173440373\n",
      "step: 689883, loss: 0.06352280080318451, data time: 0.015638523631625705\n",
      "step: 689884, loss: 0.06374253332614899, data time: 0.014925730855841386\n",
      "step: 689885, loss: 0.06074829027056694, data time: 0.014285242557525635\n",
      "step: 689886, loss: 0.05842665955424309, data time: 0.01370699065072196\n",
      "step: 689887, loss: 0.06578701734542847, data time: 0.013182293285023083\n",
      "step: 689888, loss: 0.0615709125995636, data time: 0.012698816216510275\n",
      "step: 689889, loss: 0.060020968317985535, data time: 0.012254377206166586\n",
      "step: 689890, loss: 0.06111564859747887, data time: 0.011856803894042969\n",
      "step: 689891, loss: 0.06499163806438446, data time: 0.011478387392484225\n",
      "step: 689892, loss: 0.057708337903022766, data time: 0.011125882466634115\n",
      "step: 689893, loss: 0.0628439337015152, data time: 0.010800676686423165\n",
      "step: 689894, loss: 0.05399031937122345, data time: 0.010514259338378906\n",
      "step: 689895, loss: 0.058528415858745575, data time: 0.010235134760538738\n",
      "step: 689896, loss: 0.059718310832977295, data time: 0.00997456427543394\n",
      "step: 689897, loss: 0.05901645869016647, data time: 0.009735889732837677\n",
      "step: 689898, loss: 0.06054116412997246, data time: 0.009498502268935696\n",
      "step: 689899, loss: 0.06403481960296631, data time: 0.009277925771825454\n",
      "step: 689900, loss: 0.06343477964401245, data time: 0.009066806520734515\n",
      "step: 689901, loss: 0.06058788299560547, data time: 0.008865998850928413\n",
      "step: 689902, loss: 0.05646025389432907, data time: 0.008678790685292837\n",
      "step: 689903, loss: 0.06296414136886597, data time: 0.008505896518104955\n",
      "step: 689904, loss: 0.06251652538776398, data time: 0.008341526373838767\n",
      "step: 689905, loss: 0.05846484750509262, data time: 0.008185493946075439\n",
      "step: 689906, loss: 0.06628919392824173, data time: 0.23769140243530273\n",
      "step: 689907, loss: 0.0649695098400116, data time: 0.12043249607086182\n",
      "step: 689908, loss: 0.06233348697423935, data time: 0.08080911636352539\n",
      "step: 689909, loss: 0.05816107988357544, data time: 0.06148636341094971\n",
      "step: 689910, loss: 0.062495723366737366, data time: 0.049472475051879884\n",
      "step: 689911, loss: 0.07108598947525024, data time: 0.041469057401021324\n",
      "step: 689912, loss: 0.05677071213722229, data time: 0.035753113882882256\n",
      "step: 689913, loss: 0.061422985047101974, data time: 0.03155350685119629\n",
      "step: 689914, loss: 0.061419934034347534, data time: 0.028195407655504014\n",
      "step: 689915, loss: 0.0665193498134613, data time: 0.025586795806884766\n",
      "step: 689916, loss: 0.05480637401342392, data time: 0.023464181206443092\n",
      "step: 689917, loss: 0.060937099158763885, data time: 0.02169179916381836\n",
      "step: 689918, loss: 0.0605231337249279, data time: 0.020187469629141\n",
      "step: 689919, loss: 0.06179752200841904, data time: 0.01889012541089739\n",
      "step: 689920, loss: 0.058077141642570496, data time: 0.017770783106486002\n",
      "step: 689921, loss: 0.06125009059906006, data time: 0.016789942979812622\n",
      "step: 689922, loss: 0.061981480568647385, data time: 0.01591985365923713\n",
      "step: 689923, loss: 0.05862930417060852, data time: 0.015143010351392958\n",
      "step: 689924, loss: 0.06341584771871567, data time: 0.014455004742271021\n",
      "step: 689925, loss: 0.06464304774999619, data time: 0.013837122917175293\n",
      "step: 689926, loss: 0.06123984977602959, data time: 0.013280789057413736\n",
      "step: 689927, loss: 0.061881598085165024, data time: 0.012777079235423695\n",
      "step: 689928, loss: 0.06136462092399597, data time: 0.012308245119841202\n",
      "step: 689929, loss: 0.05845912545919418, data time: 0.011881540218989054\n",
      "step: 689930, loss: 0.061016883701086044, data time: 0.011486234664916993\n",
      "step: 689931, loss: 0.06678365916013718, data time: 0.011123116199786846\n",
      "step: 689932, loss: 0.06301122903823853, data time: 0.010783248477511935\n",
      "step: 689933, loss: 0.06448428332805634, data time: 0.010469717638833182\n",
      "step: 689934, loss: 0.06084451079368591, data time: 0.010188464460701778\n",
      "step: 689935, loss: 0.051899105310440063, data time: 0.009920287132263183\n",
      "step: 689936, loss: 0.06234240159392357, data time: 0.009672718663369455\n",
      "step: 689937, loss: 0.05955655500292778, data time: 0.009442150592803955\n",
      "step: 689938, loss: 0.059236831963062286, data time: 0.009216474764274828\n",
      "step: 689939, loss: 0.06393367052078247, data time: 0.00900126204771154\n",
      "step: 689940, loss: 0.06586389988660812, data time: 0.008803067888532365\n",
      "step: 689941, loss: 0.0639905110001564, data time: 0.008609188927544488\n",
      "step: 689942, loss: 0.06783964484930038, data time: 0.00842801944629566\n",
      "step: 689943, loss: 0.05901050567626953, data time: 0.008259302691409462\n",
      "step: 689944, loss: 0.06238831207156181, data time: 0.008104043129162911\n",
      "step: 689945, loss: 0.08284444361925125, data time: 0.007956165075302123\n",
      "step: 689946, loss: 0.058197759091854095, data time: 0.2507514953613281\n",
      "step: 689947, loss: 0.05930010601878166, data time: 0.12614893913269043\n",
      "step: 689948, loss: 0.06460767984390259, data time: 0.08460005124409993\n",
      "step: 689949, loss: 0.06081639230251312, data time: 0.06431126594543457\n",
      "step: 689950, loss: 0.06234390661120415, data time: 0.051729869842529294\n",
      "step: 689951, loss: 0.06319289654493332, data time: 0.04334219296773275\n",
      "step: 689952, loss: 0.06528333574533463, data time: 0.03737524577549526\n",
      "step: 689953, loss: 0.06085212528705597, data time: 0.03296574950218201\n",
      "step: 689954, loss: 0.061403073370456696, data time: 0.02944909201727973\n",
      "step: 689955, loss: 0.05556587874889374, data time: 0.026704955101013183\n",
      "step: 689956, loss: 0.06700961291790009, data time: 0.0244737755168568\n",
      "step: 689957, loss: 0.06132079288363457, data time: 0.022627731164296467\n",
      "step: 689958, loss: 0.06211766228079796, data time: 0.02105261729313777\n",
      "step: 689959, loss: 0.06316418945789337, data time: 0.019691739763532366\n",
      "step: 689960, loss: 0.06337069720029831, data time: 0.018513266245524088\n",
      "step: 689961, loss: 0.05998154729604721, data time: 0.01749032735824585\n",
      "step: 689962, loss: 0.050751954317092896, data time: 0.016579557867611155\n",
      "step: 689963, loss: 0.06009923666715622, data time: 0.015769786304897733\n",
      "step: 689964, loss: 0.06755714863538742, data time: 0.015051389995374177\n",
      "step: 689965, loss: 0.05821017175912857, data time: 0.014406049251556396\n",
      "step: 689966, loss: 0.060480665415525436, data time: 0.013821011497860863\n",
      "step: 689967, loss: 0.05187125504016876, data time: 0.013288443738763983\n",
      "step: 689968, loss: 0.06582456827163696, data time: 0.01279790505118992\n",
      "step: 689969, loss: 0.06903456151485443, data time: 0.012348920106887817\n",
      "step: 689970, loss: 0.057004742324352264, data time: 0.011938724517822265\n",
      "step: 689971, loss: 0.061051927506923676, data time: 0.011557368131784292\n",
      "step: 689972, loss: 0.05745261162519455, data time: 0.011201646592881944\n",
      "step: 689973, loss: 0.06095258519053459, data time: 0.010876008442470006\n",
      "step: 689974, loss: 0.05635165795683861, data time: 0.010574184615036538\n",
      "step: 689975, loss: 0.05603281781077385, data time: 0.01029670238494873\n",
      "step: 689976, loss: 0.05928847938776016, data time: 0.010036045505154518\n",
      "step: 689977, loss: 0.0660829022526741, data time: 0.009795688092708588\n",
      "step: 689978, loss: 0.06424438953399658, data time: 0.009557796247077711\n",
      "step: 689979, loss: 0.06619858741760254, data time: 0.009334578233606675\n",
      "step: 689980, loss: 0.06805458664894104, data time: 0.009122950690133231\n",
      "step: 689981, loss: 0.06121304631233215, data time: 0.008922153049045138\n",
      "step: 689982, loss: 0.06648972630500793, data time: 0.00873192580970558\n",
      "step: 689983, loss: 0.06051911041140556, data time: 0.00855711259339985\n",
      "step: 689984, loss: 0.06181306764483452, data time: 0.008393183732644105\n",
      "step: 689985, loss: 0.05296264588832855, data time: 0.00823482871055603\n",
      "step: 689986, loss: 0.05825779587030411, data time: 0.25568437576293945\n",
      "step: 689987, loss: 0.05606905743479729, data time: 0.12937664985656738\n",
      "step: 689988, loss: 0.061041735112667084, data time: 0.08731786410013835\n",
      "step: 689989, loss: 0.06104900687932968, data time: 0.06641244888305664\n",
      "step: 689990, loss: 0.061566006392240524, data time: 0.05344724655151367\n",
      "step: 689991, loss: 0.059883445501327515, data time: 0.04482491811116537\n",
      "step: 689992, loss: 0.058070600032806396, data time: 0.038680859974452426\n",
      "step: 689993, loss: 0.06178358942270279, data time: 0.034150779247283936\n",
      "step: 689994, loss: 0.0627102181315422, data time: 0.030533552169799805\n",
      "step: 689995, loss: 0.06108413636684418, data time: 0.02772374153137207\n",
      "step: 689996, loss: 0.0645967423915863, data time: 0.02543540434403853\n",
      "step: 689997, loss: 0.05379275605082512, data time: 0.023523449897766113\n",
      "step: 689998, loss: 0.06086435914039612, data time: 0.02190367992107685\n",
      "step: 689999, loss: 0.06320499628782272, data time: 0.020510554313659668\n",
      "step: 690000, loss: 0.056059613823890686, data time: 0.019307931264241535\n",
      "step: 690001, loss: 0.061103276908397675, data time: 0.018253937363624573\n",
      "step: 690002, loss: 0.06013926491141319, data time: 0.017325667773976046\n",
      "step: 690003, loss: 0.06403686106204987, data time: 0.016491876708136663\n",
      "step: 690004, loss: 0.0582880936563015, data time: 0.015753720936022307\n",
      "step: 690005, loss: 0.06517183780670166, data time: 0.015093541145324707\n",
      "step: 690006, loss: 0.0671248584985733, data time: 0.014496655691237677\n",
      "step: 690007, loss: 0.060760967433452606, data time: 0.013950716365467419\n",
      "step: 690008, loss: 0.0652252733707428, data time: 0.013449710348378057\n",
      "step: 690009, loss: 0.06329795718193054, data time: 0.012992540995279947\n",
      "step: 690010, loss: 0.059368401765823364, data time: 0.012571306228637695\n",
      "step: 690011, loss: 0.059268951416015625, data time: 0.01218112615438608\n",
      "step: 690012, loss: 0.05628777667880058, data time: 0.011814629590069806\n",
      "step: 690013, loss: 0.06449812650680542, data time: 0.011479173387799944\n",
      "step: 690014, loss: 0.05718819424510002, data time: 0.011171702680916622\n",
      "step: 690015, loss: 0.06489971280097961, data time: 0.010883831977844238\n",
      "step: 690016, loss: 0.06488309800624847, data time: 0.010616340944843908\n",
      "step: 690017, loss: 0.06185292452573776, data time: 0.010366737842559814\n",
      "step: 690018, loss: 0.06071074306964874, data time: 0.010116483225966946\n",
      "step: 690019, loss: 0.059476710855960846, data time: 0.0098800729302799\n",
      "step: 690020, loss: 0.05908697098493576, data time: 0.009656592777797153\n",
      "step: 690021, loss: 0.06787019968032837, data time: 0.009450528356764052\n",
      "step: 690022, loss: 0.06609148532152176, data time: 0.009246929271801098\n",
      "step: 690023, loss: 0.06173133850097656, data time: 0.009056856757716128\n",
      "step: 690024, loss: 0.06583426892757416, data time: 0.008877980403411083\n",
      "step: 690025, loss: 0.09411218017339706, data time: 0.008707809448242187\n",
      "step: 690026, loss: 0.06693489849567413, data time: 0.24350237846374512\n",
      "step: 690027, loss: 0.059048861265182495, data time: 0.12351357936859131\n",
      "step: 690028, loss: 0.06503906846046448, data time: 0.08298103014628093\n",
      "step: 690029, loss: 0.059761837124824524, data time: 0.06327557563781738\n",
      "step: 690030, loss: 0.061588089913129807, data time: 0.050939321517944336\n",
      "step: 690031, loss: 0.0601666197180748, data time: 0.04272139072418213\n",
      "step: 690032, loss: 0.062292784452438354, data time: 0.036849805286952426\n",
      "step: 690033, loss: 0.060190290212631226, data time: 0.032554060220718384\n",
      "step: 690034, loss: 0.06065525859594345, data time: 0.029111862182617188\n",
      "step: 690035, loss: 0.06267772614955902, data time: 0.026446032524108886\n",
      "step: 690036, loss: 0.06198723241686821, data time: 0.024275779724121094\n",
      "step: 690037, loss: 0.06113327667117119, data time: 0.02246359984079997\n",
      "step: 690038, loss: 0.06171713024377823, data time: 0.020926603904137246\n",
      "step: 690039, loss: 0.0584263801574707, data time: 0.019606471061706543\n",
      "step: 690040, loss: 0.07274159044027328, data time: 0.018464438120524087\n",
      "step: 690041, loss: 0.058627378195524216, data time: 0.017459437251091003\n",
      "step: 690042, loss: 0.06290224194526672, data time: 0.01657742612502154\n",
      "step: 690043, loss: 0.061963796615600586, data time: 0.015788740581936307\n",
      "step: 690044, loss: 0.056932203471660614, data time: 0.015081895025152909\n",
      "step: 690045, loss: 0.0630774050951004, data time: 0.014456522464752198\n",
      "step: 690046, loss: 0.06212816759943962, data time: 0.013888472602480934\n",
      "step: 690047, loss: 0.05851740390062332, data time: 0.013369592753323641\n",
      "step: 690048, loss: 0.06308441609144211, data time: 0.012892028559809145\n",
      "step: 690049, loss: 0.06970156729221344, data time: 0.012455562750498453\n",
      "step: 690050, loss: 0.06774400174617767, data time: 0.012054681777954102\n",
      "step: 690051, loss: 0.06217465549707413, data time: 0.011684289345374474\n",
      "step: 690052, loss: 0.06444478034973145, data time: 0.011338401723791051\n",
      "step: 690053, loss: 0.06710385531187057, data time: 0.011017944131578718\n",
      "step: 690054, loss: 0.056878283619880676, data time: 0.010725440650150693\n",
      "step: 690055, loss: 0.059463515877723694, data time: 0.010451761881510417\n",
      "step: 690056, loss: 0.060505762696266174, data time: 0.010196285863076486\n",
      "step: 690057, loss: 0.062319956719875336, data time: 0.009957566857337952\n",
      "step: 690058, loss: 0.0610959492623806, data time: 0.009716951485836145\n",
      "step: 690059, loss: 0.05968733876943588, data time: 0.009490812526029698\n",
      "step: 690060, loss: 0.06406515836715698, data time: 0.009280252456665038\n",
      "step: 690061, loss: 0.0672699585556984, data time: 0.00907808542251587\n",
      "step: 690062, loss: 0.06630967557430267, data time: 0.008888315510105443\n",
      "step: 690063, loss: 0.07046236097812653, data time: 0.008710930221959165\n",
      "step: 690064, loss: 0.06680962443351746, data time: 0.008543246831649389\n",
      "step: 690065, loss: 0.04818087816238403, data time: 0.008383160829544068\n",
      "step: 690066, loss: 0.0634239912033081, data time: 0.23579740524291992\n",
      "step: 690067, loss: 0.059174224734306335, data time: 0.11868429183959961\n",
      "step: 690068, loss: 0.05711156874895096, data time: 0.0800340970357259\n",
      "step: 690069, loss: 0.06250640749931335, data time: 0.06080889701843262\n",
      "step: 690070, loss: 0.059142954647541046, data time: 0.048920488357543944\n",
      "step: 690071, loss: 0.06417109817266464, data time: 0.040996273358662925\n",
      "step: 690072, loss: 0.066616952419281, data time: 0.03535144669669015\n",
      "step: 690073, loss: 0.06228967756032944, data time: 0.031186997890472412\n",
      "step: 690074, loss: 0.0619838647544384, data time: 0.027877012888590496\n",
      "step: 690075, loss: 0.06493981927633286, data time: 0.02529292106628418\n",
      "step: 690076, loss: 0.0677669420838356, data time: 0.023185426538640804\n",
      "step: 690077, loss: 0.059068143367767334, data time: 0.021439075469970703\n",
      "step: 690078, loss: 0.05380439758300781, data time: 0.01995679048391489\n",
      "step: 690079, loss: 0.06258528679609299, data time: 0.018674475806100026\n",
      "step: 690080, loss: 0.058538343757390976, data time: 0.01757074991861979\n",
      "step: 690081, loss: 0.06763647496700287, data time: 0.01660919189453125\n",
      "step: 690082, loss: 0.059579797089099884, data time: 0.015752722235286936\n",
      "step: 690083, loss: 0.06350325793027878, data time: 0.014985058042738173\n",
      "step: 690084, loss: 0.06254146993160248, data time: 0.014306106065448961\n",
      "step: 690085, loss: 0.06584754586219788, data time: 0.013699209690093994\n",
      "step: 690086, loss: 0.059911519289016724, data time: 0.013152780986967542\n",
      "step: 690087, loss: 0.058120619505643845, data time: 0.012653253295204857\n",
      "step: 690088, loss: 0.06481064856052399, data time: 0.012189502301423447\n",
      "step: 690089, loss: 0.06446686387062073, data time: 0.011770516633987427\n",
      "step: 690090, loss: 0.062036529183387756, data time: 0.01138514518737793\n",
      "step: 690091, loss: 0.058316510170698166, data time: 0.01102464015667255\n",
      "step: 690092, loss: 0.053845327347517014, data time: 0.010688967174953885\n",
      "step: 690093, loss: 0.06727580726146698, data time: 0.010382558618273054\n",
      "step: 690094, loss: 0.05861709639430046, data time: 0.010099558994687837\n",
      "step: 690095, loss: 0.06626290082931519, data time: 0.009833399454752605\n",
      "step: 690096, loss: 0.05696965381503105, data time: 0.009584611462008568\n",
      "step: 690097, loss: 0.05950925499200821, data time: 0.00935623049736023\n",
      "step: 690098, loss: 0.06543136388063431, data time: 0.009129849347201261\n",
      "step: 690099, loss: 0.06444478034973145, data time: 0.008922296411850873\n",
      "step: 690100, loss: 0.06883224844932556, data time: 0.008724369321550641\n",
      "step: 690101, loss: 0.06681826710700989, data time: 0.00853553745481703\n",
      "step: 690102, loss: 0.06271299719810486, data time: 0.008371933086498364\n",
      "step: 690103, loss: 0.06302380561828613, data time: 0.008205018545451918\n",
      "step: 690104, loss: 0.06390710920095444, data time: 0.008046743197318835\n",
      "step: 690105, loss: 0.06161317229270935, data time: 0.00789591670036316\n",
      "step: 690106, loss: 0.06520608067512512, data time: 0.2662520408630371\n",
      "step: 690107, loss: 0.06598696112632751, data time: 0.13390791416168213\n",
      "step: 690108, loss: 0.060163360089063644, data time: 0.09017372131347656\n",
      "step: 690109, loss: 0.05867365002632141, data time: 0.06840008497238159\n",
      "step: 690110, loss: 0.06306485831737518, data time: 0.05499987602233887\n",
      "step: 690111, loss: 0.06271621584892273, data time: 0.046080986658732094\n",
      "step: 690112, loss: 0.06293506920337677, data time: 0.03971035139901297\n",
      "step: 690113, loss: 0.06726522743701935, data time: 0.03500202298164368\n",
      "step: 690114, loss: 0.0624757818877697, data time: 0.03126875559488932\n",
      "step: 690115, loss: 0.057931989431381226, data time: 0.028352999687194826\n",
      "step: 690116, loss: 0.06507948786020279, data time: 0.0259693752635609\n",
      "step: 690117, loss: 0.0616462379693985, data time: 0.02397771676381429\n",
      "step: 690118, loss: 0.06123083457350731, data time: 0.022298317689162035\n",
      "step: 690119, loss: 0.05478934571146965, data time: 0.020858185631888255\n",
      "step: 690120, loss: 0.06236537545919418, data time: 0.019622166951497395\n",
      "step: 690121, loss: 0.06249615550041199, data time: 0.01852443814277649\n",
      "step: 690122, loss: 0.0619019940495491, data time: 0.01755547523498535\n",
      "step: 690123, loss: 0.06055902689695358, data time: 0.01669046613905165\n",
      "step: 690124, loss: 0.062469612807035446, data time: 0.015919032849763568\n",
      "step: 690125, loss: 0.06010451167821884, data time: 0.015228664875030518\n",
      "step: 690126, loss: 0.06031868979334831, data time: 0.014608667010352724\n",
      "step: 690127, loss: 0.06045730412006378, data time: 0.014046549797058105\n",
      "step: 690128, loss: 0.06355129182338715, data time: 0.013522801191910454\n",
      "step: 690129, loss: 0.059649087488651276, data time: 0.01304952303568522\n",
      "step: 690130, loss: 0.06807735562324524, data time: 0.01261164665222168\n",
      "step: 690131, loss: 0.06134888529777527, data time: 0.012207884054917555\n",
      "step: 690132, loss: 0.059378113597631454, data time: 0.01183719105190701\n",
      "step: 690133, loss: 0.05929219722747803, data time: 0.01148527009146554\n",
      "step: 690134, loss: 0.057195328176021576, data time: 0.011163785539824387\n",
      "step: 690135, loss: 0.060328587889671326, data time: 0.01086288293202718\n",
      "step: 690136, loss: 0.0615987591445446, data time: 0.010581270340950258\n",
      "step: 690137, loss: 0.0679580420255661, data time: 0.010320045053958893\n",
      "step: 690138, loss: 0.06625696271657944, data time: 0.010067600192445698\n",
      "step: 690139, loss: 0.0592721663415432, data time: 0.009828441283282112\n",
      "step: 690140, loss: 0.059620924293994904, data time: 0.009605087552751813\n",
      "step: 690141, loss: 0.060707129538059235, data time: 0.009391367435455322\n",
      "step: 690142, loss: 0.058522917330265045, data time: 0.009189335075584618\n",
      "step: 690143, loss: 0.06453311443328857, data time: 0.008999824523925781\n",
      "step: 690144, loss: 0.06298813968896866, data time: 0.008823462021656526\n",
      "step: 690145, loss: 0.0708109587430954, data time: 0.008655071258544922\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.414137601852417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 690146, loss: 0.06060267612338066, data time: 0.2451019287109375\n",
      "step: 690147, loss: 0.058735914528369904, data time: 0.1233670711517334\n",
      "step: 690148, loss: 0.059358805418014526, data time: 0.08313806851704915\n",
      "step: 690149, loss: 0.06937975436449051, data time: 0.06310230493545532\n",
      "step: 690150, loss: 0.06544859707355499, data time: 0.05076766014099121\n",
      "step: 690151, loss: 0.0696549043059349, data time: 0.04254003365834554\n",
      "step: 690152, loss: 0.06047719717025757, data time: 0.03666996955871582\n",
      "step: 690153, loss: 0.05980347841978073, data time: 0.03234341740608215\n",
      "step: 690154, loss: 0.05833509564399719, data time: 0.028898954391479492\n",
      "step: 690155, loss: 0.06626273691654205, data time: 0.026207160949707032\n",
      "step: 690156, loss: 0.059209249913692474, data time: 0.024023446169766514\n",
      "step: 690157, loss: 0.06001889333128929, data time: 0.0222017765045166\n",
      "step: 690158, loss: 0.061721883714199066, data time: 0.020653798029972956\n",
      "step: 690159, loss: 0.05675097554922104, data time: 0.019317695072719028\n",
      "step: 690160, loss: 0.0644049346446991, data time: 0.01816728909810384\n",
      "step: 690161, loss: 0.06262924522161484, data time: 0.017161652445793152\n",
      "step: 690162, loss: 0.059501659125089645, data time: 0.01626882833593032\n",
      "step: 690163, loss: 0.06013869866728783, data time: 0.015475855933295356\n",
      "step: 690164, loss: 0.06489557027816772, data time: 0.014763681512129935\n",
      "step: 690165, loss: 0.06118578836321831, data time: 0.014136278629302978\n",
      "step: 690166, loss: 0.059155818074941635, data time: 0.01356574467250279\n",
      "step: 690167, loss: 0.06379541009664536, data time: 0.013058402321555397\n",
      "step: 690168, loss: 0.06084982678294182, data time: 0.01258322466974673\n",
      "step: 690169, loss: 0.06098827347159386, data time: 0.012147307395935059\n",
      "step: 690170, loss: 0.06111465021967888, data time: 0.011741304397583007\n",
      "step: 690171, loss: 0.05728856846690178, data time: 0.011372016026423527\n",
      "step: 690172, loss: 0.06273379176855087, data time: 0.011024722346553096\n",
      "step: 690173, loss: 0.06343003362417221, data time: 0.010700966630663191\n",
      "step: 690174, loss: 0.06532079726457596, data time: 0.010405137621123215\n",
      "step: 690175, loss: 0.06896398961544037, data time: 0.01013011932373047\n",
      "step: 690176, loss: 0.061263617128133774, data time: 0.009872705705704228\n",
      "step: 690177, loss: 0.06262966245412827, data time: 0.009635187685489655\n",
      "step: 690178, loss: 0.060723960399627686, data time: 0.00940062782981179\n",
      "step: 690179, loss: 0.06916704773902893, data time: 0.009182284860049976\n",
      "step: 690180, loss: 0.0608413964509964, data time: 0.008974415915352958\n",
      "step: 690181, loss: 0.06089559942483902, data time: 0.008776757452223036\n",
      "step: 690182, loss: 0.06479115784168243, data time: 0.008590040980158625\n",
      "step: 690183, loss: 0.060026608407497406, data time: 0.008416872275503058\n",
      "step: 690184, loss: 0.05667070671916008, data time: 0.008254240720699996\n",
      "step: 690185, loss: 0.0630825012922287, data time: 0.008095401525497436\n",
      "step: 690186, loss: 0.06727498024702072, data time: 0.24895858764648438\n",
      "step: 690187, loss: 0.06160818785429001, data time: 0.12529802322387695\n",
      "step: 690188, loss: 0.06209447979927063, data time: 0.08403897285461426\n",
      "step: 690189, loss: 0.06268852949142456, data time: 0.06374365091323853\n",
      "step: 690190, loss: 0.06056949496269226, data time: 0.05127391815185547\n",
      "step: 690191, loss: 0.06310126185417175, data time: 0.04296127955118815\n",
      "step: 690192, loss: 0.06322740763425827, data time: 0.03702964101518903\n",
      "step: 690193, loss: 0.06118680536746979, data time: 0.03264954686164856\n",
      "step: 690194, loss: 0.06006314605474472, data time: 0.02916558583577474\n",
      "step: 690195, loss: 0.06373181939125061, data time: 0.0264559268951416\n",
      "step: 690196, loss: 0.06185300275683403, data time: 0.024258526888760654\n",
      "step: 690197, loss: 0.0642036646604538, data time: 0.02241472403208415\n",
      "step: 690198, loss: 0.06577123701572418, data time: 0.02084770569434533\n",
      "step: 690199, loss: 0.06188803166151047, data time: 0.019500579152788435\n",
      "step: 690200, loss: 0.062102776020765305, data time: 0.018337186177571616\n",
      "step: 690201, loss: 0.06213746219873428, data time: 0.017315953969955444\n",
      "step: 690202, loss: 0.0606706365942955, data time: 0.01642280466416303\n",
      "step: 690203, loss: 0.06759557127952576, data time: 0.015617635515001085\n",
      "step: 690204, loss: 0.06414972990751266, data time: 0.014903256767674497\n",
      "step: 690205, loss: 0.060727547854185104, data time: 0.0142655611038208\n",
      "step: 690206, loss: 0.05639694258570671, data time: 0.01369166374206543\n",
      "step: 690207, loss: 0.06177327781915665, data time: 0.01316314393823797\n",
      "step: 690208, loss: 0.06124920770525932, data time: 0.012680250665415888\n",
      "step: 690209, loss: 0.064138263463974, data time: 0.012242247660954794\n",
      "step: 690210, loss: 0.05860292166471481, data time: 0.01184107780456543\n",
      "step: 690211, loss: 0.05725865811109543, data time: 0.011462945204514723\n",
      "step: 690212, loss: 0.06583245098590851, data time: 0.011110968059963651\n",
      "step: 690213, loss: 0.0628638043999672, data time: 0.0107846770967756\n",
      "step: 690214, loss: 0.0687950998544693, data time: 0.010485484682280442\n",
      "step: 690215, loss: 0.06010351702570915, data time: 0.010208757718404134\n",
      "step: 690216, loss: 0.06295377761125565, data time: 0.009947138447915354\n",
      "step: 690217, loss: 0.054210029542446136, data time: 0.009706780314445496\n",
      "step: 690218, loss: 0.06554456055164337, data time: 0.009470433899850556\n",
      "step: 690219, loss: 0.06430467963218689, data time: 0.009249189320732565\n",
      "step: 690220, loss: 0.06894442439079285, data time: 0.009039415631975446\n",
      "step: 690221, loss: 0.061216145753860474, data time: 0.008840190039740669\n",
      "step: 690222, loss: 0.06166772171854973, data time: 0.008651939598289696\n",
      "step: 690223, loss: 0.06275501102209091, data time: 0.008477832141675447\n",
      "step: 690224, loss: 0.05780791491270065, data time: 0.008312622706095377\n",
      "step: 690225, loss: 0.04967142269015312, data time: 0.008157724142074585\n",
      "step: 690226, loss: 0.06578444689512253, data time: 0.249924898147583\n",
      "step: 690227, loss: 0.06346343457698822, data time: 0.12571656703948975\n",
      "step: 690228, loss: 0.06557166576385498, data time: 0.0850068728129069\n",
      "step: 690229, loss: 0.06361262500286102, data time: 0.06441211700439453\n",
      "step: 690230, loss: 0.06185165047645569, data time: 0.05180268287658692\n",
      "step: 690231, loss: 0.06195291876792908, data time: 0.04340481758117676\n",
      "step: 690232, loss: 0.061159998178482056, data time: 0.037402595792497904\n",
      "step: 690233, loss: 0.05997643992304802, data time: 0.032979756593704224\n",
      "step: 690234, loss: 0.06060077250003815, data time: 0.029461834165785048\n",
      "step: 690235, loss: 0.06388723105192184, data time: 0.026719307899475096\n",
      "step: 690236, loss: 0.061870425939559937, data time: 0.024488752538507633\n",
      "step: 690237, loss: 0.06118949502706528, data time: 0.02262457211812337\n",
      "step: 690238, loss: 0.05982575938105583, data time: 0.02105887119586651\n",
      "step: 690239, loss: 0.06565701961517334, data time: 0.019698483603341237\n",
      "step: 690240, loss: 0.05926719307899475, data time: 0.018519083658854168\n",
      "step: 690241, loss: 0.058482713997364044, data time: 0.017491266131401062\n",
      "step: 690242, loss: 0.06772030889987946, data time: 0.016583975623635686\n",
      "step: 690243, loss: 0.05796283483505249, data time: 0.015770567788018122\n",
      "step: 690244, loss: 0.057881902903318405, data time: 0.015049219131469727\n",
      "step: 690245, loss: 0.058783575892448425, data time: 0.014402472972869873\n",
      "step: 690246, loss: 0.06234830245375633, data time: 0.013819433393932525\n",
      "step: 690247, loss: 0.06232060119509697, data time: 0.0132904052734375\n",
      "step: 690248, loss: 0.06608837842941284, data time: 0.012799615445344345\n",
      "step: 690249, loss: 0.06348693370819092, data time: 0.01235636075337728\n",
      "step: 690250, loss: 0.06543424725532532, data time: 0.011951799392700196\n",
      "step: 690251, loss: 0.06401222944259644, data time: 0.011570141865656925\n",
      "step: 690252, loss: 0.06177869439125061, data time: 0.011217965020073785\n",
      "step: 690253, loss: 0.05761570855975151, data time: 0.010889930384499686\n",
      "step: 690254, loss: 0.06055275723338127, data time: 0.010591145219474003\n",
      "step: 690255, loss: 0.05992361158132553, data time: 0.01030886967976888\n",
      "step: 690256, loss: 0.06380664557218552, data time: 0.010045374593427104\n",
      "step: 690257, loss: 0.061176739633083344, data time: 0.009805068373680115\n",
      "step: 690258, loss: 0.060546450316905975, data time: 0.009565057176532167\n",
      "step: 690259, loss: 0.06164107844233513, data time: 0.009340138996348661\n",
      "step: 690260, loss: 0.06081471964716911, data time: 0.00912738527570452\n",
      "step: 690261, loss: 0.06751902401447296, data time: 0.008924855126274956\n",
      "step: 690262, loss: 0.05716944858431816, data time: 0.008734438870404218\n",
      "step: 690263, loss: 0.061661653220653534, data time: 0.008557118867572985\n",
      "step: 690264, loss: 0.055123504251241684, data time: 0.008388849405141978\n",
      "step: 690265, loss: 0.05282267928123474, data time: 0.0082292377948761\n",
      "step: 690266, loss: 0.06023665517568588, data time: 0.24972271919250488\n",
      "step: 690267, loss: 0.05505063757300377, data time: 0.125984787940979\n",
      "step: 690268, loss: 0.0711510181427002, data time: 0.08450539906819661\n",
      "step: 690269, loss: 0.06435612589120865, data time: 0.06434512138366699\n",
      "step: 690270, loss: 0.0655515044927597, data time: 0.051762962341308595\n",
      "step: 690271, loss: 0.06214071437716484, data time: 0.043365478515625\n",
      "step: 690272, loss: 0.06638628244400024, data time: 0.03736635616847447\n",
      "step: 690273, loss: 0.06394468247890472, data time: 0.03300097584724426\n",
      "step: 690274, loss: 0.06261356920003891, data time: 0.029480722215440538\n",
      "step: 690275, loss: 0.06389855593442917, data time: 0.026731228828430174\n",
      "step: 690276, loss: 0.05846120044589043, data time: 0.024495059793645687\n",
      "step: 690277, loss: 0.058811187744140625, data time: 0.02263253927230835\n",
      "step: 690278, loss: 0.06010942906141281, data time: 0.0210572389455942\n",
      "step: 690279, loss: 0.06544285267591476, data time: 0.0196990966796875\n",
      "step: 690280, loss: 0.05621598660945892, data time: 0.018546263376871746\n",
      "step: 690281, loss: 0.06618845462799072, data time: 0.01753312349319458\n",
      "step: 690282, loss: 0.06807108223438263, data time: 0.016640873516307157\n",
      "step: 690283, loss: 0.06399048864841461, data time: 0.015841258896721735\n",
      "step: 690284, loss: 0.059558968991041183, data time: 0.015132678182501542\n",
      "step: 690285, loss: 0.06001436337828636, data time: 0.014493417739868165\n",
      "step: 690286, loss: 0.06293746083974838, data time: 0.013908749534970238\n",
      "step: 690287, loss: 0.06451897323131561, data time: 0.013374621217901056\n",
      "step: 690288, loss: 0.062434226274490356, data time: 0.012881030207094938\n",
      "step: 690289, loss: 0.05872110649943352, data time: 0.01242708166440328\n",
      "step: 690290, loss: 0.06246523559093475, data time: 0.012012596130371095\n",
      "step: 690291, loss: 0.06387841701507568, data time: 0.011629627301142765\n",
      "step: 690292, loss: 0.06022993475198746, data time: 0.011270531901606807\n",
      "step: 690293, loss: 0.06521397083997726, data time: 0.010940407003675188\n",
      "step: 690294, loss: 0.06789343059062958, data time: 0.010641862606180125\n",
      "step: 690295, loss: 0.0622369647026062, data time: 0.010359684626261393\n",
      "step: 690296, loss: 0.06086248159408569, data time: 0.01009538096766318\n",
      "step: 690297, loss: 0.06460806727409363, data time: 0.009851805865764618\n",
      "step: 690298, loss: 0.06603454053401947, data time: 0.009611071962298769\n",
      "step: 690299, loss: 0.05971495807170868, data time: 0.009383986977969898\n",
      "step: 690300, loss: 0.06564443558454514, data time: 0.009170736585344588\n",
      "step: 690301, loss: 0.06361338496208191, data time: 0.008966432677374946\n",
      "step: 690302, loss: 0.060193657875061035, data time: 0.008775214891175966\n",
      "step: 690303, loss: 0.05736052617430687, data time: 0.008597499445865029\n",
      "step: 690304, loss: 0.06209108978509903, data time: 0.008431336818597255\n",
      "step: 690305, loss: 0.06235998123884201, data time: 0.008271092176437378\n",
      "step: 690306, loss: 0.06302335858345032, data time: 0.23947596549987793\n",
      "step: 690307, loss: 0.06252949684858322, data time: 0.12150490283966064\n",
      "step: 690308, loss: 0.05863223597407341, data time: 0.08150561650594075\n",
      "step: 690309, loss: 0.06255360692739487, data time: 0.06189465522766113\n",
      "step: 690310, loss: 0.06253233551979065, data time: 0.04979896545410156\n",
      "step: 690311, loss: 0.05751315876841545, data time: 0.041725754737854004\n",
      "step: 690312, loss: 0.0632113665342331, data time: 0.03596608979361398\n",
      "step: 690313, loss: 0.055871158838272095, data time: 0.03172224760055542\n",
      "step: 690314, loss: 0.06273116171360016, data time: 0.028357081943088107\n",
      "step: 690315, loss: 0.061744362115859985, data time: 0.025720858573913576\n",
      "step: 690316, loss: 0.06376659125089645, data time: 0.023581071333451706\n",
      "step: 690317, loss: 0.05699137970805168, data time: 0.021791934967041016\n",
      "step: 690318, loss: 0.06451144814491272, data time: 0.02028509286733774\n",
      "step: 690319, loss: 0.06170881912112236, data time: 0.01898918833051409\n",
      "step: 690320, loss: 0.06449362635612488, data time: 0.017867120107014973\n",
      "step: 690321, loss: 0.056017495691776276, data time: 0.016880393028259277\n",
      "step: 690322, loss: 0.061099790036678314, data time: 0.016005431904512292\n",
      "step: 690323, loss: 0.06579306721687317, data time: 0.015222218301561143\n",
      "step: 690324, loss: 0.06999507546424866, data time: 0.01452862588982833\n",
      "step: 690325, loss: 0.05802277475595474, data time: 0.013914167881011963\n",
      "step: 690326, loss: 0.05827782675623894, data time: 0.013356106621878487\n",
      "step: 690327, loss: 0.05710003897547722, data time: 0.012847423553466797\n",
      "step: 690328, loss: 0.06446905434131622, data time: 0.01237598709438158\n",
      "step: 690329, loss: 0.05947635695338249, data time: 0.01194488008817037\n",
      "step: 690330, loss: 0.06437093019485474, data time: 0.01154994010925293\n",
      "step: 690331, loss: 0.06425314396619797, data time: 0.011182216497567983\n",
      "step: 690332, loss: 0.06070197746157646, data time: 0.010840460106178566\n",
      "step: 690333, loss: 0.06437959522008896, data time: 0.010524996689387731\n",
      "step: 690334, loss: 0.06483271718025208, data time: 0.010237323826756971\n",
      "step: 690335, loss: 0.06973035633563995, data time: 0.009971849123636882\n",
      "step: 690336, loss: 0.06114504486322403, data time: 0.00972011012415732\n",
      "step: 690337, loss: 0.06112704798579216, data time: 0.009486563503742218\n",
      "step: 690338, loss: 0.061310410499572754, data time: 0.009257128744414358\n",
      "step: 690339, loss: 0.06338458508253098, data time: 0.009046435356140137\n",
      "step: 690340, loss: 0.05961410328745842, data time: 0.008844082696097239\n",
      "step: 690341, loss: 0.05889853462576866, data time: 0.008649024698469374\n",
      "step: 690342, loss: 0.059885524213314056, data time: 0.008466598149892446\n",
      "step: 690343, loss: 0.06869916617870331, data time: 0.008297355551468698\n",
      "step: 690344, loss: 0.06196398660540581, data time: 0.008138357064662835\n",
      "step: 690345, loss: 0.07784402370452881, data time: 0.0079903244972229\n",
      "step: 690346, loss: 0.05787551403045654, data time: 0.24277734756469727\n",
      "step: 690347, loss: 0.0671285018324852, data time: 0.1230008602142334\n",
      "step: 690348, loss: 0.05838348716497421, data time: 0.08251444498697917\n",
      "step: 690349, loss: 0.060490213334560394, data time: 0.06274771690368652\n",
      "step: 690350, loss: 0.062395572662353516, data time: 0.05048370361328125\n",
      "step: 690351, loss: 0.06361517310142517, data time: 0.04230960210164388\n",
      "step: 690352, loss: 0.06124608963727951, data time: 0.03646312441144671\n",
      "step: 690353, loss: 0.05534844845533371, data time: 0.03216022253036499\n",
      "step: 690354, loss: 0.06487775593996048, data time: 0.028730604383680556\n",
      "step: 690355, loss: 0.0673845112323761, data time: 0.026067161560058595\n",
      "step: 690356, loss: 0.05666036158800125, data time: 0.02389095046303489\n",
      "step: 690357, loss: 0.05930793285369873, data time: 0.022076785564422607\n",
      "step: 690358, loss: 0.0622749924659729, data time: 0.020541924696702223\n",
      "step: 690359, loss: 0.06357480585575104, data time: 0.01921975612640381\n",
      "step: 690360, loss: 0.05523332953453064, data time: 0.018081521987915038\n",
      "step: 690361, loss: 0.06175309047102928, data time: 0.01707969605922699\n",
      "step: 690362, loss: 0.06767990440130234, data time: 0.016194413690006033\n",
      "step: 690363, loss: 0.06063176319003105, data time: 0.015401906437344022\n",
      "step: 690364, loss: 0.057807665318250656, data time: 0.014695506346853156\n",
      "step: 690365, loss: 0.06479599326848984, data time: 0.014069592952728272\n",
      "step: 690366, loss: 0.06071343272924423, data time: 0.013500974291846865\n",
      "step: 690367, loss: 0.06453806161880493, data time: 0.0129845684224909\n",
      "step: 690368, loss: 0.059327226132154465, data time: 0.012506661207779594\n",
      "step: 690369, loss: 0.05945151299238205, data time: 0.012069404125213623\n",
      "step: 690370, loss: 0.054538171738386154, data time: 0.01166590690612793\n",
      "step: 690371, loss: 0.06494776159524918, data time: 0.011293466274554912\n",
      "step: 690372, loss: 0.062245361506938934, data time: 0.010946830113728842\n",
      "step: 690373, loss: 0.0583706796169281, data time: 0.010631203651428223\n",
      "step: 690374, loss: 0.058175813406705856, data time: 0.010336555283645103\n",
      "step: 690375, loss: 0.060406964272260666, data time: 0.010063473383585613\n",
      "step: 690376, loss: 0.06700389087200165, data time: 0.009808778762817383\n",
      "step: 690377, loss: 0.06290653347969055, data time: 0.009573303163051605\n",
      "step: 690378, loss: 0.062486737966537476, data time: 0.009342807711976948\n",
      "step: 690379, loss: 0.06571993231773376, data time: 0.009124159812927246\n",
      "step: 690380, loss: 0.05743240565061569, data time: 0.008917461122785296\n",
      "step: 690381, loss: 0.0659874975681305, data time: 0.00872039794921875\n",
      "step: 690382, loss: 0.060980163514614105, data time: 0.008537943298752243\n",
      "step: 690383, loss: 0.0635264590382576, data time: 0.008365863247921592\n",
      "step: 690384, loss: 0.06145518273115158, data time: 0.00820331695752266\n",
      "step: 690385, loss: 0.061836741864681244, data time: 0.008051562309265136\n",
      "step: 690386, loss: 0.06375598162412643, data time: 0.24052643775939941\n",
      "step: 690387, loss: 0.058236293494701385, data time: 0.12102866172790527\n",
      "step: 690388, loss: 0.06657826155424118, data time: 0.08169396718343098\n",
      "step: 690389, loss: 0.059882521629333496, data time: 0.061945319175720215\n",
      "step: 690390, loss: 0.06257450580596924, data time: 0.049837779998779294\n",
      "step: 690391, loss: 0.064917653799057, data time: 0.04177494843800863\n",
      "step: 690392, loss: 0.06044071540236473, data time: 0.036002976553780694\n",
      "step: 690393, loss: 0.058444052934646606, data time: 0.03175237774848938\n",
      "step: 690394, loss: 0.06429355591535568, data time: 0.028376075956556533\n",
      "step: 690395, loss: 0.06343536823987961, data time: 0.025822877883911133\n",
      "step: 690396, loss: 0.06213114783167839, data time: 0.0236674655567516\n",
      "step: 690397, loss: 0.063839390873909, data time: 0.021876295407613117\n",
      "step: 690398, loss: 0.06355346739292145, data time: 0.020368410990788385\n",
      "step: 690399, loss: 0.06319670379161835, data time: 0.01906350680759975\n",
      "step: 690400, loss: 0.05930308625102043, data time: 0.017957655588785808\n",
      "step: 690401, loss: 0.05308189243078232, data time: 0.016966909170150757\n",
      "step: 690402, loss: 0.06120773404836655, data time: 0.01609063148498535\n",
      "step: 690403, loss: 0.057518329471349716, data time: 0.015305717786153158\n",
      "step: 690404, loss: 0.05743802711367607, data time: 0.01460778085809005\n",
      "step: 690405, loss: 0.05880042910575867, data time: 0.013986551761627197\n",
      "step: 690406, loss: 0.06412601470947266, data time: 0.013425009591238839\n",
      "step: 690407, loss: 0.06676925718784332, data time: 0.012913400476629084\n",
      "step: 690408, loss: 0.05572625994682312, data time: 0.012445522391277811\n",
      "step: 690409, loss: 0.06253569573163986, data time: 0.012012859185536703\n",
      "step: 690410, loss: 0.057395100593566895, data time: 0.01161376953125\n",
      "step: 690411, loss: 0.06191039830446243, data time: 0.011249102078951322\n",
      "step: 690412, loss: 0.0595550499856472, data time: 0.010906184161150898\n",
      "step: 690413, loss: 0.06498049944639206, data time: 0.010591038635798864\n",
      "step: 690414, loss: 0.059587862342596054, data time: 0.010305832172262257\n",
      "step: 690415, loss: 0.06008217856287956, data time: 0.010033233960469564\n",
      "step: 690416, loss: 0.05670347064733505, data time: 0.009779937805668\n",
      "step: 690417, loss: 0.060311563313007355, data time: 0.00954483449459076\n",
      "step: 690418, loss: 0.056943029165267944, data time: 0.009313756769353693\n",
      "step: 690419, loss: 0.06139388680458069, data time: 0.009094932500053854\n",
      "step: 690420, loss: 0.0656924694776535, data time: 0.008888081141880581\n",
      "step: 690421, loss: 0.06129690632224083, data time: 0.00869257582558526\n",
      "step: 690422, loss: 0.06285606324672699, data time: 0.008510318962303368\n",
      "step: 690423, loss: 0.0628630742430687, data time: 0.008340396379169664\n",
      "step: 690424, loss: 0.06083337590098381, data time: 0.008179499552800106\n",
      "step: 690425, loss: 0.03572738170623779, data time: 0.008026158809661866\n",
      "step: 690426, loss: 0.06534576416015625, data time: 0.25075483322143555\n",
      "step: 690427, loss: 0.06286177039146423, data time: 0.12700259685516357\n",
      "step: 690428, loss: 0.05878400802612305, data time: 0.08520976702372234\n",
      "step: 690429, loss: 0.06395001709461212, data time: 0.0647650957107544\n",
      "step: 690430, loss: 0.061815522611141205, data time: 0.052091455459594725\n",
      "step: 690431, loss: 0.05932539328932762, data time: 0.04363934199015299\n",
      "step: 690432, loss: 0.061241861432790756, data time: 0.0376194885798863\n",
      "step: 690433, loss: 0.061955761164426804, data time: 0.03319123387336731\n",
      "step: 690434, loss: 0.05771856755018234, data time: 0.02964933713277181\n",
      "step: 690435, loss: 0.05814129114151001, data time: 0.026877403259277344\n",
      "step: 690436, loss: 0.06403626501560211, data time: 0.024626450105146927\n",
      "step: 690437, loss: 0.06500376760959625, data time: 0.022760450839996338\n",
      "step: 690438, loss: 0.058775268495082855, data time: 0.021174137408916768\n",
      "step: 690439, loss: 0.0588172972202301, data time: 0.01981018270765032\n",
      "step: 690440, loss: 0.06456348299980164, data time: 0.018625704447428386\n",
      "step: 690441, loss: 0.05863133817911148, data time: 0.01758638024330139\n",
      "step: 690442, loss: 0.06447476893663406, data time: 0.016672218547147864\n",
      "step: 690443, loss: 0.061114996671676636, data time: 0.015862186749776203\n",
      "step: 690444, loss: 0.06339491903781891, data time: 0.015132289183767219\n",
      "step: 690445, loss: 0.056422483175992966, data time: 0.014488470554351807\n",
      "step: 690446, loss: 0.05938444659113884, data time: 0.013901369912283761\n",
      "step: 690447, loss: 0.06533119082450867, data time: 0.01336526870727539\n",
      "step: 690448, loss: 0.06131933629512787, data time: 0.01287600268488345\n",
      "step: 690449, loss: 0.05975500866770744, data time: 0.012422064940134684\n",
      "step: 690450, loss: 0.05922381207346916, data time: 0.01200918197631836\n",
      "step: 690451, loss: 0.05486219376325607, data time: 0.01162582177382249\n",
      "step: 690452, loss: 0.06186063587665558, data time: 0.011270434768111617\n",
      "step: 690453, loss: 0.057987015694379807, data time: 0.01093912124633789\n",
      "step: 690454, loss: 0.06251552700996399, data time: 0.010635236213947165\n",
      "step: 690455, loss: 0.05967692658305168, data time: 0.010352738698323568\n",
      "step: 690456, loss: 0.058301519602537155, data time: 0.010089866576656219\n",
      "step: 690457, loss: 0.06316863000392914, data time: 0.009849615395069122\n",
      "step: 690458, loss: 0.06962467730045319, data time: 0.009613412799257221\n",
      "step: 690459, loss: 0.06348863989114761, data time: 0.009386805927052218\n",
      "step: 690460, loss: 0.06268878281116486, data time: 0.0091726439339774\n",
      "step: 690461, loss: 0.06070547550916672, data time: 0.008969220850202773\n",
      "step: 690462, loss: 0.062057726085186005, data time: 0.00877913913211307\n",
      "step: 690463, loss: 0.061479490250349045, data time: 0.008602248994927657\n",
      "step: 690464, loss: 0.062172092497348785, data time: 0.008433806590544872\n",
      "step: 690465, loss: 0.08452024310827255, data time: 0.0082741379737854\n",
      "step: 690466, loss: 0.06244027614593506, data time: 0.2614145278930664\n",
      "step: 690467, loss: 0.06511732190847397, data time: 0.13148021697998047\n",
      "step: 690468, loss: 0.06345611065626144, data time: 0.08861430486043294\n",
      "step: 690469, loss: 0.05895908921957016, data time: 0.06713849306106567\n",
      "step: 690470, loss: 0.06736975908279419, data time: 0.05398678779602051\n",
      "step: 690471, loss: 0.06120457872748375, data time: 0.045232295989990234\n",
      "step: 690472, loss: 0.05725261569023132, data time: 0.03896566799708775\n",
      "step: 690473, loss: 0.06668373942375183, data time: 0.03441712260246277\n",
      "step: 690474, loss: 0.05990203842520714, data time: 0.030817190806070965\n",
      "step: 690475, loss: 0.06543859839439392, data time: 0.0279263973236084\n",
      "step: 690476, loss: 0.058550190180540085, data time: 0.02558456767689098\n",
      "step: 690477, loss: 0.058024317026138306, data time: 0.02363695700963338\n",
      "step: 690478, loss: 0.06223278492689133, data time: 0.021979607068575345\n",
      "step: 690479, loss: 0.059798724949359894, data time: 0.02055340153830392\n",
      "step: 690480, loss: 0.05939459800720215, data time: 0.0193173885345459\n",
      "step: 690481, loss: 0.06242913007736206, data time: 0.01824222505092621\n",
      "step: 690482, loss: 0.06738560646772385, data time: 0.017289961085600013\n",
      "step: 690483, loss: 0.06938295066356659, data time: 0.0164417823155721\n",
      "step: 690484, loss: 0.06117038056254387, data time: 0.015681894201981395\n",
      "step: 690485, loss: 0.05979260057210922, data time: 0.015005016326904297\n",
      "step: 690486, loss: 0.06643911451101303, data time: 0.01439629282270159\n",
      "step: 690487, loss: 0.061857860535383224, data time: 0.013842788609591398\n",
      "step: 690488, loss: 0.05739747732877731, data time: 0.01333065654920495\n",
      "step: 690489, loss: 0.05602804571390152, data time: 0.012867361307144165\n",
      "step: 690490, loss: 0.058049969375133514, data time: 0.012435874938964843\n",
      "step: 690491, loss: 0.05822036787867546, data time: 0.01203449872823862\n",
      "step: 690492, loss: 0.06216038763523102, data time: 0.011665829905757197\n",
      "step: 690493, loss: 0.059872567653656006, data time: 0.011320897511073522\n",
      "step: 690494, loss: 0.06494968384504318, data time: 0.011005681136558795\n",
      "step: 690495, loss: 0.06831439584493637, data time: 0.010709460576375325\n",
      "step: 690496, loss: 0.06385394930839539, data time: 0.01043734242839198\n",
      "step: 690497, loss: 0.06534770876169205, data time: 0.010181337594985962\n",
      "step: 690498, loss: 0.06148076057434082, data time: 0.009931658253525242\n",
      "step: 690499, loss: 0.060557492077350616, data time: 0.009694821694317986\n",
      "step: 690500, loss: 0.05746626853942871, data time: 0.009472520010811942\n",
      "step: 690501, loss: 0.06650562584400177, data time: 0.009261892901526557\n",
      "step: 690502, loss: 0.05666426569223404, data time: 0.009061478279732369\n",
      "step: 690503, loss: 0.06299175322055817, data time: 0.008877277374267578\n",
      "step: 690504, loss: 0.06272462755441666, data time: 0.008702559348864432\n",
      "step: 690505, loss: 0.05767744779586792, data time: 0.008536940813064576\n",
      "step: 690506, loss: 0.06129363179206848, data time: 0.24772191047668457\n",
      "step: 690507, loss: 0.056591302156448364, data time: 0.12530720233917236\n",
      "step: 690508, loss: 0.06337273120880127, data time: 0.08442854881286621\n",
      "step: 690509, loss: 0.07256821542978287, data time: 0.06409645080566406\n",
      "step: 690510, loss: 0.05786126106977463, data time: 0.05155215263366699\n",
      "step: 690511, loss: 0.06330621242523193, data time: 0.043195883433024086\n",
      "step: 690512, loss: 0.055279575288295746, data time: 0.037242889404296875\n",
      "step: 690513, loss: 0.06658414006233215, data time: 0.032839298248291016\n",
      "step: 690514, loss: 0.06658119708299637, data time: 0.029336478975084092\n",
      "step: 690515, loss: 0.06210492178797722, data time: 0.026604270935058592\n",
      "step: 690516, loss: 0.06597396731376648, data time: 0.02438434687527743\n",
      "step: 690517, loss: 0.06354731321334839, data time: 0.022533694903055828\n",
      "step: 690518, loss: 0.061958249658346176, data time: 0.020965392772967998\n",
      "step: 690519, loss: 0.06918439269065857, data time: 0.01961554799761091\n",
      "step: 690520, loss: 0.05920913815498352, data time: 0.01844159762064616\n",
      "step: 690521, loss: 0.06570284813642502, data time: 0.017416059970855713\n",
      "step: 690522, loss: 0.06395550072193146, data time: 0.01651131405549891\n",
      "step: 690523, loss: 0.059466104954481125, data time: 0.01570341322157118\n",
      "step: 690524, loss: 0.062355753034353256, data time: 0.014987518912867495\n",
      "step: 690525, loss: 0.06371616572141647, data time: 0.014352011680603027\n",
      "step: 690526, loss: 0.0646464079618454, data time: 0.013772237868536086\n",
      "step: 690527, loss: 0.059431158006191254, data time: 0.013266032392328436\n",
      "step: 690528, loss: 0.05399470776319504, data time: 0.012777712034142536\n",
      "step: 690529, loss: 0.057001374661922455, data time: 0.012330442667007446\n",
      "step: 690530, loss: 0.05958488583564758, data time: 0.011921968460083008\n",
      "step: 690531, loss: 0.06368318945169449, data time: 0.011546565936161922\n",
      "step: 690532, loss: 0.060634106397628784, data time: 0.011192657329418041\n",
      "step: 690533, loss: 0.06392337381839752, data time: 0.010864198207855225\n",
      "step: 690534, loss: 0.05884423106908798, data time: 0.010576938760691676\n",
      "step: 690535, loss: 0.05745039880275726, data time: 0.010311619440714518\n",
      "step: 690536, loss: 0.06587119400501251, data time: 0.01006037958206669\n",
      "step: 690537, loss: 0.055889613926410675, data time: 0.009818844497203827\n",
      "step: 690538, loss: 0.059623152017593384, data time: 0.009579477888165098\n",
      "step: 690539, loss: 0.06355553865432739, data time: 0.0093535395229564\n",
      "step: 690540, loss: 0.062049757689237595, data time: 0.00914081164768764\n",
      "step: 690541, loss: 0.06207576394081116, data time: 0.008946849240197076\n",
      "step: 690542, loss: 0.0635979026556015, data time: 0.008760452270507812\n",
      "step: 690543, loss: 0.06244586035609245, data time: 0.008585591065256219\n",
      "step: 690544, loss: 0.0671185776591301, data time: 0.008418700633904872\n",
      "step: 690545, loss: 0.04780758172273636, data time: 0.008261311054229736\n",
      "step: 690546, loss: 0.05523959919810295, data time: 0.25131845474243164\n",
      "step: 690547, loss: 0.06086238473653793, data time: 0.12647247314453125\n",
      "step: 690548, loss: 0.06275748461484909, data time: 0.08521652221679688\n",
      "step: 690549, loss: 0.06634104251861572, data time: 0.0647156834602356\n",
      "step: 690550, loss: 0.054891061037778854, data time: 0.052059078216552736\n",
      "step: 690551, loss: 0.06351953744888306, data time: 0.04362265268961588\n",
      "step: 690552, loss: 0.06580643355846405, data time: 0.03758556502205985\n",
      "step: 690553, loss: 0.06705939024686813, data time: 0.03314375877380371\n",
      "step: 690554, loss: 0.05991464853286743, data time: 0.029606527752346463\n",
      "step: 690555, loss: 0.06571824848651886, data time: 0.02684674263000488\n",
      "step: 690556, loss: 0.059991560876369476, data time: 0.02463143522089178\n",
      "step: 690557, loss: 0.05825423449277878, data time: 0.022788365681966145\n",
      "step: 690558, loss: 0.0649603009223938, data time: 0.021229909016535833\n",
      "step: 690559, loss: 0.057828933000564575, data time: 0.0198857273374285\n",
      "step: 690560, loss: 0.06255710870027542, data time: 0.01873006820678711\n",
      "step: 690561, loss: 0.06096331402659416, data time: 0.017709970474243164\n",
      "step: 690562, loss: 0.05827227979898453, data time: 0.016815788605633902\n",
      "step: 690563, loss: 0.05587567016482353, data time: 0.016012006335788302\n",
      "step: 690564, loss: 0.06847032904624939, data time: 0.01529575649060701\n",
      "step: 690565, loss: 0.06105228513479233, data time: 0.014655661582946778\n",
      "step: 690566, loss: 0.06012580543756485, data time: 0.014082613445463635\n",
      "step: 690567, loss: 0.06461794674396515, data time: 0.013556588779796253\n",
      "step: 690568, loss: 0.06707236170768738, data time: 0.013072946797246519\n",
      "step: 690569, loss: 0.06426402926445007, data time: 0.012630691130956015\n",
      "step: 690570, loss: 0.061592258512973785, data time: 0.012220458984375\n",
      "step: 690571, loss: 0.05180509015917778, data time: 0.011840068376981296\n",
      "step: 690572, loss: 0.058815717697143555, data time: 0.011485161604704681\n",
      "step: 690573, loss: 0.059601347893476486, data time: 0.01115929228918893\n",
      "step: 690574, loss: 0.06575815379619598, data time: 0.010862679317079741\n",
      "step: 690575, loss: 0.05588036775588989, data time: 0.010584497451782226\n",
      "step: 690576, loss: 0.05852329730987549, data time: 0.010323447565878592\n",
      "step: 690577, loss: 0.065488301217556, data time: 0.010081246495246887\n",
      "step: 690578, loss: 0.06727363914251328, data time: 0.00983879060456247\n",
      "step: 690579, loss: 0.061833456158638, data time: 0.00960832483628217\n",
      "step: 690580, loss: 0.056724898517131805, data time: 0.009391328266688755\n",
      "step: 690581, loss: 0.0631847232580185, data time: 0.00918514198727078\n",
      "step: 690582, loss: 0.06821784377098083, data time: 0.008993361447308515\n",
      "step: 690583, loss: 0.06530551612377167, data time: 0.008812810245313142\n",
      "step: 690584, loss: 0.05523809790611267, data time: 0.008642789645072741\n",
      "step: 690585, loss: 0.06994716823101044, data time: 0.008481186628341675\n",
      "step: 690586, loss: 0.06322746723890305, data time: 0.25224804878234863\n",
      "step: 690587, loss: 0.06907409429550171, data time: 0.1272672414779663\n",
      "step: 690588, loss: 0.060553841292858124, data time: 0.08534510930379231\n",
      "step: 690589, loss: 0.06262939423322678, data time: 0.06488251686096191\n",
      "step: 690590, loss: 0.05939727649092674, data time: 0.05219798088073731\n",
      "step: 690591, loss: 0.061254095286130905, data time: 0.0437317689259847\n",
      "step: 690592, loss: 0.05773521959781647, data time: 0.037691559110369\n",
      "step: 690593, loss: 0.0675026997923851, data time: 0.03323864936828613\n",
      "step: 690594, loss: 0.06352254748344421, data time: 0.029690795474582247\n",
      "step: 690595, loss: 0.06609439849853516, data time: 0.026925206184387207\n",
      "step: 690596, loss: 0.056547924876213074, data time: 0.024689002470536667\n",
      "step: 690597, loss: 0.05755581706762314, data time: 0.022823949654897053\n",
      "step: 690598, loss: 0.05635719746351242, data time: 0.021239794217623197\n",
      "step: 690599, loss: 0.05891992151737213, data time: 0.01987048557826451\n",
      "step: 690600, loss: 0.06062369793653488, data time: 0.01868899663289388\n",
      "step: 690601, loss: 0.062127113342285156, data time: 0.01765240728855133\n",
      "step: 690602, loss: 0.056604281067848206, data time: 0.01673702632679659\n",
      "step: 690603, loss: 0.06559395045042038, data time: 0.01591679784986708\n",
      "step: 690604, loss: 0.06262083351612091, data time: 0.015189258675826224\n",
      "step: 690605, loss: 0.06198788434267044, data time: 0.014541351795196533\n",
      "step: 690606, loss: 0.05633167177438736, data time: 0.013951505933489119\n",
      "step: 690607, loss: 0.060083769261837006, data time: 0.013416637073863636\n",
      "step: 690608, loss: 0.061898089945316315, data time: 0.012924608976944633\n",
      "step: 690609, loss: 0.06711026281118393, data time: 0.01247483491897583\n",
      "step: 690610, loss: 0.06203889101743698, data time: 0.012057676315307617\n",
      "step: 690611, loss: 0.05701746791601181, data time: 0.011673835607675405\n",
      "step: 690612, loss: 0.05921987444162369, data time: 0.011313932913321036\n",
      "step: 690613, loss: 0.05987950414419174, data time: 0.010984522955758231\n",
      "step: 690614, loss: 0.057077400386333466, data time: 0.010680848154528388\n",
      "step: 690615, loss: 0.06006930023431778, data time: 0.010398077964782714\n",
      "step: 690616, loss: 0.06326249241828918, data time: 0.010133120321458387\n",
      "step: 690617, loss: 0.05678310617804527, data time: 0.009888052940368652\n",
      "step: 690618, loss: 0.061171721667051315, data time: 0.009648966066765062\n",
      "step: 690619, loss: 0.0628206878900528, data time: 0.009421839433557847\n",
      "step: 690620, loss: 0.06956977397203445, data time: 0.009208978925432479\n",
      "step: 690621, loss: 0.05205882713198662, data time: 0.009004672368367514\n",
      "step: 690622, loss: 0.06254278868436813, data time: 0.008815320762428077\n",
      "step: 690623, loss: 0.07139980792999268, data time: 0.008639643066807798\n",
      "step: 690624, loss: 0.060868047177791595, data time: 0.008473494114019932\n",
      "step: 690625, loss: 0.0451047420501709, data time: 0.008315765857696533\n",
      "step: 690626, loss: 0.06542448699474335, data time: 0.25713229179382324\n",
      "step: 690627, loss: 0.060303863137960434, data time: 0.12990617752075195\n",
      "step: 690628, loss: 0.06428825110197067, data time: 0.08770259221394856\n",
      "step: 690629, loss: 0.06048334389925003, data time: 0.06670576333999634\n",
      "step: 690630, loss: 0.06893487274646759, data time: 0.053708982467651364\n",
      "step: 690631, loss: 0.059417083859443665, data time: 0.04504032929738363\n",
      "step: 690632, loss: 0.06037832051515579, data time: 0.03884349550519671\n",
      "step: 690633, loss: 0.06174149364233017, data time: 0.03428623080253601\n",
      "step: 690634, loss: 0.060397058725357056, data time: 0.030650271309746638\n",
      "step: 690635, loss: 0.058975428342819214, data time: 0.02782449722290039\n",
      "step: 690636, loss: 0.05843086168169975, data time: 0.025525331497192383\n",
      "step: 690637, loss: 0.06406231969594955, data time: 0.023612757523854572\n",
      "step: 690638, loss: 0.06419514119625092, data time: 0.02199235329261193\n",
      "step: 690639, loss: 0.0606844499707222, data time: 0.020597611154828752\n",
      "step: 690640, loss: 0.05960331857204437, data time: 0.01938621203104655\n",
      "step: 690641, loss: 0.05886995047330856, data time: 0.018323957920074463\n",
      "step: 690642, loss: 0.06345324218273163, data time: 0.017385609009686637\n",
      "step: 690643, loss: 0.05900466442108154, data time: 0.016551799244350858\n",
      "step: 690644, loss: 0.059767600148916245, data time: 0.015803964514481395\n",
      "step: 690645, loss: 0.06156200170516968, data time: 0.015139877796173096\n",
      "step: 690646, loss: 0.06143411993980408, data time: 0.014539378029959542\n",
      "step: 690647, loss: 0.057053204625844955, data time: 0.01399219036102295\n",
      "step: 690648, loss: 0.057421766221523285, data time: 0.013489733571591585\n",
      "step: 690649, loss: 0.05764011666178703, data time: 0.013028661410013834\n",
      "step: 690650, loss: 0.05405842885375023, data time: 0.012603263854980468\n",
      "step: 690651, loss: 0.06783878058195114, data time: 0.01220848010136531\n",
      "step: 690652, loss: 0.05698647350072861, data time: 0.01183976067437066\n",
      "step: 690653, loss: 0.06530057638883591, data time: 0.011499438967023577\n",
      "step: 690654, loss: 0.06636354327201843, data time: 0.011203667213176859\n",
      "step: 690655, loss: 0.055602237582206726, data time: 0.010914444923400879\n",
      "step: 690656, loss: 0.06879948079586029, data time: 0.010642720806983209\n",
      "step: 690657, loss: 0.06596693396568298, data time: 0.010390318930149078\n",
      "step: 690658, loss: 0.06273201107978821, data time: 0.0101346391620058\n",
      "step: 690659, loss: 0.06691674888134003, data time: 0.00989240758559283\n",
      "step: 690660, loss: 0.05914955586194992, data time: 0.009663690839494978\n",
      "step: 690661, loss: 0.06009477376937866, data time: 0.009450270069970025\n",
      "step: 690662, loss: 0.060697875916957855, data time: 0.00924998360711175\n",
      "step: 690663, loss: 0.055900782346725464, data time: 0.009063852460760819\n",
      "step: 690664, loss: 0.06484778970479965, data time: 0.008886997516338643\n",
      "step: 690665, loss: 0.0679178535938263, data time: 0.008720165491104126\n",
      "step: 690666, loss: 0.05730093643069267, data time: 0.25124168395996094\n",
      "step: 690667, loss: 0.059991441667079926, data time: 0.12719810009002686\n",
      "step: 690668, loss: 0.07152196019887924, data time: 0.0862281322479248\n",
      "step: 690669, loss: 0.06115676090121269, data time: 0.0651252269744873\n",
      "step: 690670, loss: 0.06437914818525314, data time: 0.05245680809020996\n",
      "step: 690671, loss: 0.05713362991809845, data time: 0.04399291674296061\n",
      "step: 690672, loss: 0.06259553879499435, data time: 0.03804806300571987\n",
      "step: 690673, loss: 0.06429663300514221, data time: 0.033491671085357666\n",
      "step: 690674, loss: 0.059250153601169586, data time: 0.029954380459255643\n",
      "step: 690675, loss: 0.060969915241003036, data time: 0.02720186710357666\n",
      "step: 690676, loss: 0.0635184720158577, data time: 0.024955316023393112\n",
      "step: 690677, loss: 0.06660544872283936, data time: 0.023082117239634197\n",
      "step: 690678, loss: 0.06092458963394165, data time: 0.02150128437922551\n",
      "step: 690679, loss: 0.06304764002561569, data time: 0.020135590008326938\n",
      "step: 690680, loss: 0.06277760118246078, data time: 0.01895251274108887\n",
      "step: 690681, loss: 0.05950433760881424, data time: 0.017925724387168884\n",
      "step: 690682, loss: 0.06267233192920685, data time: 0.017008739359238568\n",
      "step: 690683, loss: 0.06347211450338364, data time: 0.016192939546373155\n",
      "step: 690684, loss: 0.06873039901256561, data time: 0.015467304932443719\n",
      "step: 690685, loss: 0.06563524901866913, data time: 0.014818644523620606\n",
      "step: 690686, loss: 0.06486798822879791, data time: 0.014232533318655831\n",
      "step: 690687, loss: 0.06333793699741364, data time: 0.013698436997153542\n",
      "step: 690688, loss: 0.058532826602458954, data time: 0.013210047846255095\n",
      "step: 690689, loss: 0.05970427393913269, data time: 0.012759685516357422\n",
      "step: 690690, loss: 0.05573124811053276, data time: 0.012347984313964843\n",
      "step: 690691, loss: 0.06515246629714966, data time: 0.011961560982924242\n",
      "step: 690692, loss: 0.061812665313482285, data time: 0.011606119297168873\n",
      "step: 690693, loss: 0.06671464443206787, data time: 0.011276449475969588\n",
      "step: 690694, loss: 0.06758638471364975, data time: 0.010972491626081795\n",
      "step: 690695, loss: 0.06689490377902985, data time: 0.010690927505493164\n",
      "step: 690696, loss: 0.05998658388853073, data time: 0.01042749035742975\n",
      "step: 690697, loss: 0.06332175433635712, data time: 0.010189689695835114\n",
      "step: 690698, loss: 0.061569731682538986, data time: 0.009943990996389679\n",
      "step: 690699, loss: 0.06536217778921127, data time: 0.009713207974153407\n",
      "step: 690700, loss: 0.06239995360374451, data time: 0.009491893223353795\n",
      "step: 690701, loss: 0.05842730402946472, data time: 0.009283363819122314\n",
      "step: 690702, loss: 0.06471572071313858, data time: 0.009088554897823849\n",
      "step: 690703, loss: 0.06533265858888626, data time: 0.008906251505801552\n",
      "step: 690704, loss: 0.06323835253715515, data time: 0.008732948547754532\n",
      "step: 690705, loss: 0.08766213059425354, data time: 0.00856899619102478\n",
      "step: 690706, loss: 0.06615519523620605, data time: 0.25591278076171875\n",
      "step: 690707, loss: 0.06572099775075912, data time: 0.12874364852905273\n",
      "step: 690708, loss: 0.0654657781124115, data time: 0.08673763275146484\n",
      "step: 690709, loss: 0.06178998202085495, data time: 0.06589305400848389\n",
      "step: 690710, loss: 0.06316913664340973, data time: 0.05299687385559082\n",
      "step: 690711, loss: 0.06341049075126648, data time: 0.044403791427612305\n",
      "step: 690712, loss: 0.05745410546660423, data time: 0.03827064377920968\n",
      "step: 690713, loss: 0.06282922625541687, data time: 0.033740073442459106\n",
      "step: 690714, loss: 0.06086685508489609, data time: 0.03013867802090115\n",
      "step: 690715, loss: 0.057066403329372406, data time: 0.027327370643615723\n",
      "step: 690716, loss: 0.06396777927875519, data time: 0.025037938898259945\n",
      "step: 690717, loss: 0.06078929826617241, data time: 0.023142000039418537\n",
      "step: 690718, loss: 0.061452578753232956, data time: 0.021526171610905573\n",
      "step: 690719, loss: 0.06174113601446152, data time: 0.020129169736589705\n",
      "step: 690720, loss: 0.06744265556335449, data time: 0.018921724955240884\n",
      "step: 690721, loss: 0.06381364911794662, data time: 0.017871439456939697\n",
      "step: 690722, loss: 0.06173016130924225, data time: 0.01694362303789924\n",
      "step: 690723, loss: 0.06350919604301453, data time: 0.016109426816304524\n",
      "step: 690724, loss: 0.060715001076459885, data time: 0.015374811072098581\n",
      "step: 690725, loss: 0.05994649976491928, data time: 0.014716434478759765\n",
      "step: 690726, loss: 0.066065214574337, data time: 0.014117195492699033\n",
      "step: 690727, loss: 0.05469667166471481, data time: 0.01357157663865523\n",
      "step: 690728, loss: 0.05978256091475487, data time: 0.013071754704351011\n",
      "step: 690729, loss: 0.06064435839653015, data time: 0.012613068024317423\n",
      "step: 690730, loss: 0.061655767261981964, data time: 0.012198381423950196\n",
      "step: 690731, loss: 0.0618402436375618, data time: 0.011807395861699032\n",
      "step: 690732, loss: 0.06147715449333191, data time: 0.01144495716801396\n",
      "step: 690733, loss: 0.06100502610206604, data time: 0.011106874261583601\n",
      "step: 690734, loss: 0.0665634274482727, data time: 0.010797221085120892\n",
      "step: 690735, loss: 0.05846855044364929, data time: 0.010507146517435709\n",
      "step: 690736, loss: 0.06348293274641037, data time: 0.010237693786621094\n",
      "step: 690737, loss: 0.061885811388492584, data time: 0.009991742670536041\n",
      "step: 690738, loss: 0.06032682955265045, data time: 0.009747497963182854\n",
      "step: 690739, loss: 0.0645761638879776, data time: 0.009521961212158203\n",
      "step: 690740, loss: 0.060494840145111084, data time: 0.009305007117135184\n",
      "step: 690741, loss: 0.058012574911117554, data time: 0.009097562895880805\n",
      "step: 690742, loss: 0.055995166301727295, data time: 0.008903890042691617\n",
      "step: 690743, loss: 0.058031223714351654, data time: 0.008724018147117213\n",
      "step: 690744, loss: 0.059305351227521896, data time: 0.008552227264795547\n",
      "step: 690745, loss: 0.06142218038439751, data time: 0.008388227224349976\n",
      "step: 690746, loss: 0.06554322689771652, data time: 0.2557706832885742\n",
      "step: 690747, loss: 0.06507429480552673, data time: 0.12903428077697754\n",
      "step: 690748, loss: 0.062374040484428406, data time: 0.08706871668497722\n",
      "step: 690749, loss: 0.05418815836310387, data time: 0.06610280275344849\n",
      "step: 690750, loss: 0.05744432657957077, data time: 0.05316414833068848\n",
      "step: 690751, loss: 0.06474220007658005, data time: 0.04454247156778971\n",
      "step: 690752, loss: 0.05813140794634819, data time: 0.03838559559413365\n",
      "step: 690753, loss: 0.05887138471007347, data time: 0.033841609954833984\n",
      "step: 690754, loss: 0.06765129417181015, data time: 0.03025205930074056\n",
      "step: 690755, loss: 0.0648646131157875, data time: 0.02742788791656494\n",
      "step: 690756, loss: 0.05784177780151367, data time: 0.025133891539140182\n",
      "step: 690757, loss: 0.0593501441180706, data time: 0.023220161596934002\n",
      "step: 690758, loss: 0.05691811442375183, data time: 0.021608004203209512\n",
      "step: 690759, loss: 0.06400828808546066, data time: 0.02020895481109619\n",
      "step: 690760, loss: 0.058922719210386276, data time: 0.019007349014282228\n",
      "step: 690761, loss: 0.06713955104351044, data time: 0.017947405576705933\n",
      "step: 690762, loss: 0.061879467219114304, data time: 0.017009636935065773\n",
      "step: 690763, loss: 0.0619765929877758, data time: 0.016172183884514704\n",
      "step: 690764, loss: 0.06116897612810135, data time: 0.015435745841578433\n",
      "step: 690765, loss: 0.05785127729177475, data time: 0.014770817756652833\n",
      "step: 690766, loss: 0.05581141635775566, data time: 0.014172270184471494\n",
      "step: 690767, loss: 0.06233842298388481, data time: 0.013624689795754173\n",
      "step: 690768, loss: 0.055638473480939865, data time: 0.013118785360585089\n",
      "step: 690769, loss: 0.05959024652838707, data time: 0.012660751740137735\n",
      "step: 690770, loss: 0.0645596906542778, data time: 0.012240753173828126\n",
      "step: 690771, loss: 0.057202622294425964, data time: 0.011847807810856746\n",
      "step: 690772, loss: 0.05916783958673477, data time: 0.011484252081976997\n",
      "step: 690773, loss: 0.056265681982040405, data time: 0.011145855699266707\n",
      "step: 690774, loss: 0.06182945892214775, data time: 0.010834833671306741\n",
      "step: 690775, loss: 0.05968955159187317, data time: 0.010546159744262696\n",
      "step: 690776, loss: 0.05926361680030823, data time: 0.0102804014759679\n",
      "step: 690777, loss: 0.05983700975775719, data time: 0.010030969977378845\n",
      "step: 690778, loss: 0.05727346986532211, data time: 0.009789062268806227\n",
      "step: 690779, loss: 0.0656346008181572, data time: 0.009557436494266285\n",
      "step: 690780, loss: 0.060322076082229614, data time: 0.009338862555367606\n",
      "step: 690781, loss: 0.0695669949054718, data time: 0.009139796098073324\n",
      "step: 690782, loss: 0.06115017458796501, data time: 0.008945458644145244\n",
      "step: 690783, loss: 0.06240694969892502, data time: 0.00876285527881823\n",
      "step: 690784, loss: 0.06381847709417343, data time: 0.008590606542733999\n",
      "step: 690785, loss: 0.06765575706958771, data time: 0.00842868685722351\n",
      "step: 690786, loss: 0.0613415502011776, data time: 0.2670135498046875\n",
      "step: 690787, loss: 0.06102680414915085, data time: 0.1346604824066162\n",
      "step: 690788, loss: 0.06640419363975525, data time: 0.09082698822021484\n",
      "step: 690789, loss: 0.056773923337459564, data time: 0.06889450550079346\n",
      "step: 690790, loss: 0.06203102692961693, data time: 0.05539417266845703\n",
      "step: 690791, loss: 0.05525656044483185, data time: 0.04640471935272217\n",
      "step: 690792, loss: 0.060988664627075195, data time: 0.03998044558933803\n",
      "step: 690793, loss: 0.06383904069662094, data time: 0.035239189863204956\n",
      "step: 690794, loss: 0.059256356209516525, data time: 0.03147914674546984\n",
      "step: 690795, loss: 0.05610305070877075, data time: 0.028529596328735352\n",
      "step: 690796, loss: 0.06233930587768555, data time: 0.026127620176835495\n",
      "step: 690797, loss: 0.06245587393641472, data time: 0.02412716547648112\n",
      "step: 690798, loss: 0.06746041774749756, data time: 0.02244118543771597\n",
      "step: 690799, loss: 0.06615303456783295, data time: 0.02099038873400007\n",
      "step: 690800, loss: 0.06040499731898308, data time: 0.01972482999165853\n",
      "step: 690801, loss: 0.06094970554113388, data time: 0.01862671971321106\n",
      "step: 690802, loss: 0.05998583883047104, data time: 0.017650421927956975\n",
      "step: 690803, loss: 0.06439358741044998, data time: 0.016780826780531142\n",
      "step: 690804, loss: 0.057894449681043625, data time: 0.016001964870252107\n",
      "step: 690805, loss: 0.06538930535316467, data time: 0.015306603908538819\n",
      "step: 690806, loss: 0.0540754459798336, data time: 0.014681407383510045\n",
      "step: 690807, loss: 0.05599918216466904, data time: 0.014115366068753328\n",
      "step: 690808, loss: 0.06670203059911728, data time: 0.01358751628709876\n",
      "step: 690809, loss: 0.06416524946689606, data time: 0.013108591238657633\n",
      "step: 690810, loss: 0.06216360256075859, data time: 0.012666301727294922\n",
      "step: 690811, loss: 0.06735242903232574, data time: 0.012255026743962215\n",
      "step: 690812, loss: 0.0577869638800621, data time: 0.011877510282728408\n",
      "step: 690813, loss: 0.061023205518722534, data time: 0.011529692581721715\n",
      "step: 690814, loss: 0.0626908391714096, data time: 0.011208616454025795\n",
      "step: 690815, loss: 0.06012105196714401, data time: 0.010906688372294108\n",
      "step: 690816, loss: 0.06275033205747604, data time: 0.010625585432975523\n",
      "step: 690817, loss: 0.05891662836074829, data time: 0.01036294549703598\n",
      "step: 690818, loss: 0.059009235352277756, data time: 0.01010735829671224\n",
      "step: 690819, loss: 0.06348425894975662, data time: 0.009868734023150276\n",
      "step: 690820, loss: 0.060935474932193756, data time: 0.009641804013933454\n",
      "step: 690821, loss: 0.06510651856660843, data time: 0.009425911638471816\n",
      "step: 690822, loss: 0.06212467700242996, data time: 0.009224588806564743\n",
      "step: 690823, loss: 0.06483648717403412, data time: 0.009034966167650725\n",
      "step: 690824, loss: 0.061560552567243576, data time: 0.008856614430745443\n",
      "step: 690825, loss: 0.05538561940193176, data time: 0.008685833215713501\n",
      "step: 690826, loss: 0.06411715596914291, data time: 0.2563152313232422\n",
      "step: 690827, loss: 0.0641469657421112, data time: 0.12891769409179688\n",
      "step: 690828, loss: 0.058897506445646286, data time: 0.08649166425069173\n",
      "step: 690829, loss: 0.0642581507563591, data time: 0.06562477350234985\n",
      "step: 690830, loss: 0.06249990314245224, data time: 0.052811193466186526\n",
      "step: 690831, loss: 0.0624241903424263, data time: 0.044240991274515785\n",
      "step: 690832, loss: 0.06068474054336548, data time: 0.038128818784441264\n",
      "step: 690833, loss: 0.0608229786157608, data time: 0.0336172878742218\n",
      "step: 690834, loss: 0.06235858052968979, data time: 0.030026091469658747\n",
      "step: 690835, loss: 0.06356066465377808, data time: 0.02722592353820801\n",
      "step: 690836, loss: 0.06054816395044327, data time: 0.02495696327903054\n",
      "step: 690837, loss: 0.059840261936187744, data time: 0.02305521567662557\n",
      "step: 690838, loss: 0.063321053981781, data time: 0.021447273401113656\n",
      "step: 690839, loss: 0.05974588543176651, data time: 0.02006122044154576\n",
      "step: 690840, loss: 0.06621860712766647, data time: 0.018860292434692384\n",
      "step: 690841, loss: 0.06259073317050934, data time: 0.017810583114624023\n",
      "step: 690842, loss: 0.05997928977012634, data time: 0.01689009105457979\n",
      "step: 690843, loss: 0.05764622241258621, data time: 0.016086988978915744\n",
      "step: 690844, loss: 0.06194595992565155, data time: 0.015368988639429995\n",
      "step: 690845, loss: 0.0605219267308712, data time: 0.014726126194000244\n",
      "step: 690846, loss: 0.06433456391096115, data time: 0.014143807547433036\n",
      "step: 690847, loss: 0.059542179107666016, data time: 0.013615250587463379\n",
      "step: 690848, loss: 0.06268644332885742, data time: 0.013131877650385317\n",
      "step: 690849, loss: 0.06772196292877197, data time: 0.012682120005289713\n",
      "step: 690850, loss: 0.06322422623634338, data time: 0.01227457046508789\n",
      "step: 690851, loss: 0.058789901435375214, data time: 0.011893464968754696\n",
      "step: 690852, loss: 0.06186429783701897, data time: 0.011539971386944806\n",
      "step: 690853, loss: 0.05640464276075363, data time: 0.01121387311390468\n",
      "step: 690854, loss: 0.05887448787689209, data time: 0.01091563290563123\n",
      "step: 690855, loss: 0.05718737840652466, data time: 0.010635733604431152\n",
      "step: 690856, loss: 0.06246674060821533, data time: 0.010372269538141066\n",
      "step: 690857, loss: 0.0560123473405838, data time: 0.010129928588867188\n",
      "step: 690858, loss: 0.06456688046455383, data time: 0.009884574196555397\n",
      "step: 690859, loss: 0.06376422941684723, data time: 0.00965584726894603\n",
      "step: 690860, loss: 0.06205614656209946, data time: 0.009441314424787248\n",
      "step: 690861, loss: 0.06462454795837402, data time: 0.009234170118967691\n",
      "step: 690862, loss: 0.0625200867652893, data time: 0.00904071653211439\n",
      "step: 690863, loss: 0.06375865638256073, data time: 0.008860832766482704\n",
      "step: 690864, loss: 0.06320827454328537, data time: 0.008689446327013847\n",
      "step: 690865, loss: 0.06604238599538803, data time: 0.008526802062988281\n",
      "step: 690866, loss: 0.06109189987182617, data time: 0.2636744976043701\n",
      "step: 690867, loss: 0.06920777261257172, data time: 0.13295745849609375\n",
      "step: 690868, loss: 0.06441667675971985, data time: 0.08972533543904622\n",
      "step: 690869, loss: 0.06097365543246269, data time: 0.06784659624099731\n",
      "step: 690870, loss: 0.06387010216712952, data time: 0.05456089973449707\n",
      "step: 690871, loss: 0.05956770107150078, data time: 0.04571239153544108\n",
      "step: 690872, loss: 0.06862305104732513, data time: 0.03959175518580845\n",
      "step: 690873, loss: 0.05654832720756531, data time: 0.03481924533843994\n",
      "step: 690874, loss: 0.06641079485416412, data time: 0.031118233998616535\n",
      "step: 690875, loss: 0.0664803758263588, data time: 0.028210902214050294\n",
      "step: 690876, loss: 0.058998577296733856, data time: 0.025839697230945934\n",
      "step: 690877, loss: 0.056810807436704636, data time: 0.023867468039194744\n",
      "step: 690878, loss: 0.0626954436302185, data time: 0.02220016259413499\n",
      "step: 690879, loss: 0.060710884630680084, data time: 0.020760553223746165\n",
      "step: 690880, loss: 0.06418687105178833, data time: 0.019512764612833657\n",
      "step: 690881, loss: 0.05647845193743706, data time: 0.01842254400253296\n",
      "step: 690882, loss: 0.06520257890224457, data time: 0.017453712575575885\n",
      "step: 690883, loss: 0.058737825602293015, data time: 0.016595549053615995\n",
      "step: 690884, loss: 0.05489708110690117, data time: 0.01582874749836169\n",
      "step: 690885, loss: 0.05703075975179672, data time: 0.01514580249786377\n",
      "step: 690886, loss: 0.057117562741041183, data time: 0.014528774079822358\n",
      "step: 690887, loss: 0.06545648723840714, data time: 0.013965357433665882\n",
      "step: 690888, loss: 0.06006237119436264, data time: 0.013453649437945822\n",
      "step: 690889, loss: 0.06030207499861717, data time: 0.012977570295333862\n",
      "step: 690890, loss: 0.05741066858172417, data time: 0.012555389404296876\n",
      "step: 690891, loss: 0.06547708064317703, data time: 0.01216108065385085\n",
      "step: 690892, loss: 0.05890919268131256, data time: 0.011799935941342954\n",
      "step: 690893, loss: 0.05932584032416344, data time: 0.011466894830976213\n",
      "step: 690894, loss: 0.06381548941135406, data time: 0.011158326576495993\n",
      "step: 690895, loss: 0.06196948140859604, data time: 0.010869741439819336\n",
      "step: 690896, loss: 0.056338995695114136, data time: 0.01058766149705456\n",
      "step: 690897, loss: 0.05733434483408928, data time: 0.010329112410545349\n",
      "step: 690898, loss: 0.06666120886802673, data time: 0.010076660098451557\n",
      "step: 690899, loss: 0.0658116489648819, data time: 0.00983717161066392\n",
      "step: 690900, loss: 0.06411482393741608, data time: 0.009608071190970284\n",
      "step: 690901, loss: 0.06372353434562683, data time: 0.009393751621246338\n",
      "step: 690902, loss: 0.06113002821803093, data time: 0.009192080111116977\n",
      "step: 690903, loss: 0.0638432651758194, data time: 0.00900436702527498\n",
      "step: 690904, loss: 0.06155340000987053, data time: 0.008826469763731346\n",
      "step: 690905, loss: 0.05465748533606529, data time: 0.008657753467559814\n",
      "step: 690906, loss: 0.06109830364584923, data time: 0.26532769203186035\n",
      "step: 690907, loss: 0.06165417283773422, data time: 0.13489151000976562\n",
      "step: 690908, loss: 0.06280908733606339, data time: 0.0904396375020345\n",
      "step: 690909, loss: 0.06444002687931061, data time: 0.06860911846160889\n",
      "step: 690910, loss: 0.06176160275936127, data time: 0.05522451400756836\n",
      "step: 690911, loss: 0.056966256350278854, data time: 0.046294569969177246\n",
      "step: 690912, loss: 0.06107386574149132, data time: 0.03993817738124302\n",
      "step: 690913, loss: 0.06497822701931, data time: 0.0352635383605957\n",
      "step: 690914, loss: 0.058405738323926926, data time: 0.031514247258504234\n",
      "step: 690915, loss: 0.06281237304210663, data time: 0.028598546981811523\n",
      "step: 690916, loss: 0.061900049448013306, data time: 0.026227821003306995\n",
      "step: 690917, loss: 0.06026598438620567, data time: 0.02425408363342285\n",
      "step: 690918, loss: 0.058915793895721436, data time: 0.022587996262770433\n",
      "step: 690919, loss: 0.059118565171957016, data time: 0.02115011215209961\n",
      "step: 690920, loss: 0.05795860290527344, data time: 0.019903532663981118\n",
      "step: 690921, loss: 0.06493332982063293, data time: 0.018812909722328186\n",
      "step: 690922, loss: 0.06228358671069145, data time: 0.01784968376159668\n",
      "step: 690923, loss: 0.0650511085987091, data time: 0.016988502608405218\n",
      "step: 690924, loss: 0.061345528811216354, data time: 0.016221661316721064\n",
      "step: 690925, loss: 0.06167083978652954, data time: 0.015535807609558106\n",
      "step: 690926, loss: 0.062092117965221405, data time: 0.01491743042355492\n",
      "step: 690927, loss: 0.06120115518569946, data time: 0.014352917671203613\n",
      "step: 690928, loss: 0.060945041477680206, data time: 0.013832372167836065\n",
      "step: 690929, loss: 0.06165125593543053, data time: 0.013356546560923258\n",
      "step: 690930, loss: 0.05886147543787956, data time: 0.012921018600463867\n",
      "step: 690931, loss: 0.05766543745994568, data time: 0.012517167971684383\n",
      "step: 690932, loss: 0.06356372684240341, data time: 0.012139488149572301\n",
      "step: 690933, loss: 0.06441639363765717, data time: 0.011791552816118513\n",
      "step: 690934, loss: 0.06303848326206207, data time: 0.011470597365806842\n",
      "step: 690935, loss: 0.060866184532642365, data time: 0.011173876126607259\n",
      "step: 690936, loss: 0.06127488613128662, data time: 0.01089469079048403\n",
      "step: 690937, loss: 0.06228002533316612, data time: 0.010636113584041595\n",
      "step: 690938, loss: 0.0633770227432251, data time: 0.01037417036114317\n",
      "step: 690939, loss: 0.058789730072021484, data time: 0.01013020206900204\n",
      "step: 690940, loss: 0.06094837188720703, data time: 0.009901114872523717\n",
      "step: 690941, loss: 0.05936024710536003, data time: 0.009681204954783121\n",
      "step: 690942, loss: 0.056881871074438095, data time: 0.009474051965249551\n",
      "step: 690943, loss: 0.06838130950927734, data time: 0.009282488571970086\n",
      "step: 690944, loss: 0.05841757357120514, data time: 0.00910044327760354\n",
      "step: 690945, loss: 0.05330262333154678, data time: 0.008927488327026367\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.410785436630249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 690946, loss: 0.06459200382232666, data time: 0.2522165775299072\n",
      "step: 690947, loss: 0.06837134808301926, data time: 0.12689447402954102\n",
      "step: 690948, loss: 0.06818219274282455, data time: 0.08513108889261882\n",
      "step: 690949, loss: 0.06375246495008469, data time: 0.0646851658821106\n",
      "step: 690950, loss: 0.06882892549037933, data time: 0.052042579650878905\n",
      "step: 690951, loss: 0.0575292631983757, data time: 0.04360008239746094\n",
      "step: 690952, loss: 0.061525218188762665, data time: 0.03757824216570173\n",
      "step: 690953, loss: 0.062198758125305176, data time: 0.03314611315727234\n",
      "step: 690954, loss: 0.06172098219394684, data time: 0.02961036894056532\n",
      "step: 690955, loss: 0.06507759541273117, data time: 0.02685248851776123\n",
      "step: 690956, loss: 0.0720805749297142, data time: 0.02459794824773615\n",
      "step: 690957, loss: 0.06438881158828735, data time: 0.022723337014516194\n",
      "step: 690958, loss: 0.06279872357845306, data time: 0.021137090829702523\n",
      "step: 690959, loss: 0.057373665273189545, data time: 0.01976893629346575\n",
      "step: 690960, loss: 0.06406151503324509, data time: 0.018590259552001952\n",
      "step: 690961, loss: 0.06602702289819717, data time: 0.017560958862304688\n",
      "step: 690962, loss: 0.06334865093231201, data time: 0.016650760875028724\n",
      "step: 690963, loss: 0.06514610350131989, data time: 0.015833006964789495\n",
      "step: 690964, loss: 0.062138378620147705, data time: 0.015100441480937758\n",
      "step: 690965, loss: 0.060393668711185455, data time: 0.014447855949401855\n",
      "step: 690966, loss: 0.06085577607154846, data time: 0.01386616343543643\n",
      "step: 690967, loss: 0.0578320138156414, data time: 0.013336365873163397\n",
      "step: 690968, loss: 0.055211007595062256, data time: 0.012842582619708517\n",
      "step: 690969, loss: 0.06809453666210175, data time: 0.012394289175669352\n",
      "step: 690970, loss: 0.06520482152700424, data time: 0.011978216171264648\n",
      "step: 690971, loss: 0.05769567936658859, data time: 0.01159672553722675\n",
      "step: 690972, loss: 0.05739539861679077, data time: 0.011236694124009874\n",
      "step: 690973, loss: 0.0651579201221466, data time: 0.010905844824654716\n",
      "step: 690974, loss: 0.06222837418317795, data time: 0.010604957054401266\n",
      "step: 690975, loss: 0.0590234100818634, data time: 0.010326480865478516\n",
      "step: 690976, loss: 0.059683628380298615, data time: 0.010061887002760363\n",
      "step: 690977, loss: 0.060239069163799286, data time: 0.00981789082288742\n",
      "step: 690978, loss: 0.06159747764468193, data time: 0.009578264120853308\n",
      "step: 690979, loss: 0.060947250574827194, data time: 0.009352543774773093\n",
      "step: 690980, loss: 0.06356200575828552, data time: 0.009140110015869141\n",
      "step: 690981, loss: 0.06592314690351486, data time: 0.008936054176754422\n",
      "step: 690982, loss: 0.05959845706820488, data time: 0.008747912741996147\n",
      "step: 690983, loss: 0.059671781957149506, data time: 0.008573086638199655\n",
      "step: 690984, loss: 0.062479034066200256, data time: 0.008409597934820713\n",
      "step: 690985, loss: 0.04238162189722061, data time: 0.00825108289718628\n",
      "step: 690986, loss: 0.06920638680458069, data time: 0.2561216354370117\n",
      "step: 690987, loss: 0.06793733686208725, data time: 0.12959802150726318\n",
      "step: 690988, loss: 0.061215490102767944, data time: 0.08690762519836426\n",
      "step: 690989, loss: 0.06007073447108269, data time: 0.06594347953796387\n",
      "step: 690990, loss: 0.06017330288887024, data time: 0.0530181884765625\n",
      "step: 690991, loss: 0.05750414356589317, data time: 0.04441678524017334\n",
      "step: 690992, loss: 0.05874362587928772, data time: 0.03827520779200962\n",
      "step: 690993, loss: 0.067709781229496, data time: 0.03374943137168884\n",
      "step: 690994, loss: 0.05494797229766846, data time: 0.03014161851671007\n",
      "step: 690995, loss: 0.06153348088264465, data time: 0.027335810661315917\n",
      "step: 690996, loss: 0.05896230787038803, data time: 0.025045373223044655\n",
      "step: 690997, loss: 0.05594179034233093, data time: 0.023139536380767822\n",
      "step: 690998, loss: 0.0586455836892128, data time: 0.021527418723473184\n",
      "step: 690999, loss: 0.0614713579416275, data time: 0.020130072321210588\n",
      "step: 691000, loss: 0.06491111218929291, data time: 0.018929052352905273\n",
      "step: 691001, loss: 0.06065330654382706, data time: 0.017873644828796387\n",
      "step: 691002, loss: 0.060684382915496826, data time: 0.016940257128547218\n",
      "step: 691003, loss: 0.05289013311266899, data time: 0.016106208165486652\n",
      "step: 691004, loss: 0.059124600142240524, data time: 0.015364747298391242\n",
      "step: 691005, loss: 0.06029677391052246, data time: 0.014700531959533691\n",
      "step: 691006, loss: 0.06602635979652405, data time: 0.014105944406418573\n",
      "step: 691007, loss: 0.06380944699048996, data time: 0.013562722639604048\n",
      "step: 691008, loss: 0.06099788099527359, data time: 0.013059263644011124\n",
      "step: 691009, loss: 0.06281641870737076, data time: 0.01260325312614441\n",
      "step: 691010, loss: 0.06533052772283554, data time: 0.012177457809448242\n",
      "step: 691011, loss: 0.05943223088979721, data time: 0.01178879921252911\n",
      "step: 691012, loss: 0.06277249753475189, data time: 0.011425345032303422\n",
      "step: 691013, loss: 0.06132137030363083, data time: 0.011088464941297258\n",
      "step: 691014, loss: 0.05954357236623764, data time: 0.010792419828217605\n",
      "step: 691015, loss: 0.06133696436882019, data time: 0.0105178435643514\n",
      "step: 691016, loss: 0.06882520765066147, data time: 0.010257982438610445\n",
      "step: 691017, loss: 0.06116388365626335, data time: 0.010015808045864105\n",
      "step: 691018, loss: 0.06746651232242584, data time: 0.009774020223906546\n",
      "step: 691019, loss: 0.062255557626485825, data time: 0.009551490054411046\n",
      "step: 691020, loss: 0.06181453913450241, data time: 0.00933664185660226\n",
      "step: 691021, loss: 0.05960346758365631, data time: 0.00913177596198188\n",
      "step: 691022, loss: 0.06304332613945007, data time: 0.008940496960201778\n",
      "step: 691023, loss: 0.061331965029239655, data time: 0.008763118794089869\n",
      "step: 691024, loss: 0.062069088220596313, data time: 0.008594476259671725\n",
      "step: 691025, loss: 0.06234250217676163, data time: 0.0084322988986969\n",
      "step: 691026, loss: 0.05688977986574173, data time: 0.2584500312805176\n",
      "step: 691027, loss: 0.0608765184879303, data time: 0.12997782230377197\n",
      "step: 691028, loss: 0.062540203332901, data time: 0.08754698435465495\n",
      "step: 691029, loss: 0.061055392026901245, data time: 0.06642335653305054\n",
      "step: 691030, loss: 0.0639372169971466, data time: 0.05341591835021973\n",
      "step: 691031, loss: 0.056810930371284485, data time: 0.04476396242777506\n",
      "step: 691032, loss: 0.06248413771390915, data time: 0.038569246019635885\n",
      "step: 691033, loss: 0.06041872501373291, data time: 0.03406062722206116\n",
      "step: 691034, loss: 0.06425421684980392, data time: 0.030423031912909612\n",
      "step: 691035, loss: 0.06204348802566528, data time: 0.02758042812347412\n",
      "step: 691036, loss: 0.06483094394207001, data time: 0.02527405998923562\n",
      "step: 691037, loss: 0.06590353697538376, data time: 0.02334680159886678\n",
      "step: 691038, loss: 0.06252112239599228, data time: 0.021716209558340218\n",
      "step: 691039, loss: 0.056450895965099335, data time: 0.020309942109244212\n",
      "step: 691040, loss: 0.06398987025022507, data time: 0.019094276428222656\n",
      "step: 691041, loss: 0.06600241363048553, data time: 0.018033206462860107\n",
      "step: 691042, loss: 0.06365586817264557, data time: 0.01709869328667136\n",
      "step: 691043, loss: 0.056579798460006714, data time: 0.016265286339653864\n",
      "step: 691044, loss: 0.06555192172527313, data time: 0.015513771458675987\n",
      "step: 691045, loss: 0.05931005999445915, data time: 0.014853405952453613\n",
      "step: 691046, loss: 0.06400712579488754, data time: 0.014255966459001814\n",
      "step: 691047, loss: 0.059621017426252365, data time: 0.013706554066051136\n",
      "step: 691048, loss: 0.061477180570364, data time: 0.013210275898809019\n",
      "step: 691049, loss: 0.06299535930156708, data time: 0.012748738129933676\n",
      "step: 691050, loss: 0.06635810434818268, data time: 0.012320232391357423\n",
      "step: 691051, loss: 0.06854091584682465, data time: 0.011925055430485653\n",
      "step: 691052, loss: 0.06764163076877594, data time: 0.011556272153501157\n",
      "step: 691053, loss: 0.06230437010526657, data time: 0.011214375495910645\n",
      "step: 691054, loss: 0.06254607439041138, data time: 0.010901130478957603\n",
      "step: 691055, loss: 0.0638691708445549, data time: 0.010613115628560384\n",
      "step: 691056, loss: 0.05776665732264519, data time: 0.010341882705688477\n",
      "step: 691057, loss: 0.06390032172203064, data time: 0.010087668895721436\n",
      "step: 691058, loss: 0.05810429900884628, data time: 0.009841254263213186\n",
      "step: 691059, loss: 0.0655316561460495, data time: 0.009607679703656365\n",
      "step: 691060, loss: 0.06262414157390594, data time: 0.009388821465628487\n",
      "step: 691061, loss: 0.06026551127433777, data time: 0.009178426530626085\n",
      "step: 691062, loss: 0.057535141706466675, data time: 0.008980867025014517\n",
      "step: 691063, loss: 0.06747418642044067, data time: 0.00879722519924766\n",
      "step: 691064, loss: 0.06697376817464828, data time: 0.008625244482969627\n",
      "step: 691065, loss: 0.06407684087753296, data time: 0.008462375402450562\n",
      "step: 691066, loss: 0.06818782538175583, data time: 0.25432729721069336\n",
      "step: 691067, loss: 0.058179404586553574, data time: 0.1279151439666748\n",
      "step: 691068, loss: 0.06271034479141235, data time: 0.085784912109375\n",
      "step: 691069, loss: 0.05702919885516167, data time: 0.06521362066268921\n",
      "step: 691070, loss: 0.06853792816400528, data time: 0.05245752334594726\n",
      "step: 691071, loss: 0.061712633818387985, data time: 0.043967644373575844\n",
      "step: 691072, loss: 0.05993722006678581, data time: 0.037887675421578546\n",
      "step: 691073, loss: 0.06304461508989334, data time: 0.0334286093711853\n",
      "step: 691074, loss: 0.059357017278671265, data time: 0.02985941039191352\n",
      "step: 691075, loss: 0.055744342505931854, data time: 0.027073240280151366\n",
      "step: 691076, loss: 0.06582428514957428, data time: 0.024807973341508346\n",
      "step: 691077, loss: 0.06399153172969818, data time: 0.022918721040089924\n",
      "step: 691078, loss: 0.06181688606739044, data time: 0.021326505220853366\n",
      "step: 691079, loss: 0.05648290365934372, data time: 0.019945178713117327\n",
      "step: 691080, loss: 0.06278927624225616, data time: 0.01875416437784831\n",
      "step: 691081, loss: 0.0663093775510788, data time: 0.017706796526908875\n",
      "step: 691082, loss: 0.06550067663192749, data time: 0.016791722353766945\n",
      "step: 691083, loss: 0.060340046882629395, data time: 0.015967382325066462\n",
      "step: 691084, loss: 0.062279701232910156, data time: 0.015235122881437602\n",
      "step: 691085, loss: 0.06410052627325058, data time: 0.014581215381622315\n",
      "step: 691086, loss: 0.058959536254405975, data time: 0.01398844945998419\n",
      "step: 691087, loss: 0.06086156517267227, data time: 0.013447999954223633\n",
      "step: 691088, loss: 0.05892819166183472, data time: 0.012948295344477114\n",
      "step: 691089, loss: 0.06427370011806488, data time: 0.01249682903289795\n",
      "step: 691090, loss: 0.061772048473358154, data time: 0.012081098556518555\n",
      "step: 691091, loss: 0.06313184648752213, data time: 0.011695430828974797\n",
      "step: 691092, loss: 0.06712734699249268, data time: 0.011333465576171875\n",
      "step: 691093, loss: 0.06950226426124573, data time: 0.011002702372414725\n",
      "step: 691094, loss: 0.06645163148641586, data time: 0.010708858226907664\n",
      "step: 691095, loss: 0.06816624850034714, data time: 0.010422515869140624\n",
      "step: 691096, loss: 0.05956067889928818, data time: 0.01015564703172253\n",
      "step: 691097, loss: 0.06428040564060211, data time: 0.009909659624099731\n",
      "step: 691098, loss: 0.06024066358804703, data time: 0.009667165351636482\n",
      "step: 691099, loss: 0.062036484479904175, data time: 0.009438262266271254\n",
      "step: 691100, loss: 0.06713978946208954, data time: 0.00922389030456543\n",
      "step: 691101, loss: 0.060150086879730225, data time: 0.009017381403181288\n",
      "step: 691102, loss: 0.06457780301570892, data time: 0.008824548205813847\n",
      "step: 691103, loss: 0.06296238303184509, data time: 0.008647222267953973\n",
      "step: 691104, loss: 0.06171143054962158, data time: 0.008481068488879081\n",
      "step: 691105, loss: 0.05073675513267517, data time: 0.008319878578186035\n",
      "step: 691106, loss: 0.05944116413593292, data time: 0.2590010166168213\n",
      "step: 691107, loss: 0.06465844064950943, data time: 0.13146352767944336\n",
      "step: 691108, loss: 0.058431606739759445, data time: 0.08816027641296387\n",
      "step: 691109, loss: 0.05650132894515991, data time: 0.0669255256652832\n",
      "step: 691110, loss: 0.0653904378414154, data time: 0.05387821197509766\n",
      "step: 691111, loss: 0.05990045517683029, data time: 0.045177181561787925\n",
      "step: 691112, loss: 0.05967111885547638, data time: 0.03895708492824009\n",
      "step: 691113, loss: 0.06293705850839615, data time: 0.034384191036224365\n",
      "step: 691114, loss: 0.062076907604932785, data time: 0.030741665098402236\n",
      "step: 691115, loss: 0.05842529237270355, data time: 0.027904510498046875\n",
      "step: 691116, loss: 0.06839775294065475, data time: 0.02559351921081543\n",
      "step: 691117, loss: 0.06450840830802917, data time: 0.02367045481999715\n",
      "step: 691118, loss: 0.06913311034440994, data time: 0.022043851705697868\n",
      "step: 691119, loss: 0.060206711292266846, data time: 0.020647270338875905\n",
      "step: 691120, loss: 0.058410726487636566, data time: 0.019427919387817384\n",
      "step: 691121, loss: 0.0625586062669754, data time: 0.01836244761943817\n",
      "step: 691122, loss: 0.059786997735500336, data time: 0.017421960830688477\n",
      "step: 691123, loss: 0.05698432773351669, data time: 0.016583098305596247\n",
      "step: 691124, loss: 0.06429901719093323, data time: 0.01583559889542429\n",
      "step: 691125, loss: 0.058877866715192795, data time: 0.015173709392547608\n",
      "step: 691126, loss: 0.05997995659708977, data time: 0.01457358541942778\n",
      "step: 691127, loss: 0.05756562948226929, data time: 0.014026457613164728\n",
      "step: 691128, loss: 0.06387071311473846, data time: 0.013520106025364088\n",
      "step: 691129, loss: 0.059932492673397064, data time: 0.01305548350016276\n",
      "step: 691130, loss: 0.060657478868961334, data time: 0.01262777328491211\n",
      "step: 691131, loss: 0.06506561487913132, data time: 0.012234238477853628\n",
      "step: 691132, loss: 0.05798774212598801, data time: 0.011854975311844438\n",
      "step: 691133, loss: 0.06203071400523186, data time: 0.011504973684038435\n",
      "step: 691134, loss: 0.061634354293346405, data time: 0.01118319610069538\n",
      "step: 691135, loss: 0.06727059185504913, data time: 0.010882918039957683\n",
      "step: 691136, loss: 0.057937901467084885, data time: 0.010600628391388924\n",
      "step: 691137, loss: 0.059787627309560776, data time: 0.010339975357055664\n",
      "step: 691138, loss: 0.06451674550771713, data time: 0.010084318392204515\n",
      "step: 691139, loss: 0.06349577754735947, data time: 0.009843237259808709\n",
      "step: 691140, loss: 0.06042566895484924, data time: 0.009616020747593471\n",
      "step: 691141, loss: 0.05462194234132767, data time: 0.00940030813217163\n",
      "step: 691142, loss: 0.06398339569568634, data time: 0.009196912920152818\n",
      "step: 691143, loss: 0.06686379015445709, data time: 0.00900876522064209\n",
      "step: 691144, loss: 0.06355460733175278, data time: 0.008831024169921875\n",
      "step: 691145, loss: 0.04478083550930023, data time: 0.008660632371902465\n",
      "step: 691146, loss: 0.05583907291293144, data time: 0.2695276737213135\n",
      "step: 691147, loss: 0.06587386131286621, data time: 0.1365816593170166\n",
      "step: 691148, loss: 0.058819301426410675, data time: 0.09156513214111328\n",
      "step: 691149, loss: 0.06736722588539124, data time: 0.06946045160293579\n",
      "step: 691150, loss: 0.05708199739456177, data time: 0.05584640502929687\n",
      "step: 691151, loss: 0.05914279818534851, data time: 0.04677418867746989\n",
      "step: 691152, loss: 0.06549197435379028, data time: 0.040312767028808594\n",
      "step: 691153, loss: 0.06911780685186386, data time: 0.035524606704711914\n",
      "step: 691154, loss: 0.06277354061603546, data time: 0.03172749943203396\n",
      "step: 691155, loss: 0.06210225448012352, data time: 0.02875363826751709\n",
      "step: 691156, loss: 0.06091129407286644, data time: 0.026370677081021397\n",
      "step: 691157, loss: 0.06105848401784897, data time: 0.024384717146555584\n",
      "step: 691158, loss: 0.06892530620098114, data time: 0.022701025009155273\n",
      "step: 691159, loss: 0.06040902063250542, data time: 0.02125241075243269\n",
      "step: 691160, loss: 0.07056868076324463, data time: 0.02002228101094564\n",
      "step: 691161, loss: 0.06448806822299957, data time: 0.01891748607158661\n",
      "step: 691162, loss: 0.05961529538035393, data time: 0.01794702866498162\n",
      "step: 691163, loss: 0.05422714352607727, data time: 0.017083485921223957\n",
      "step: 691164, loss: 0.06558605283498764, data time: 0.016309499740600586\n",
      "step: 691165, loss: 0.06042148917913437, data time: 0.015623068809509278\n",
      "step: 691166, loss: 0.06100409850478172, data time: 0.01500007084437779\n",
      "step: 691167, loss: 0.06705402582883835, data time: 0.014433091337030584\n",
      "step: 691168, loss: 0.05675702914595604, data time: 0.013911247253417969\n",
      "step: 691169, loss: 0.05693013221025467, data time: 0.013429492712020874\n",
      "step: 691170, loss: 0.05350642651319504, data time: 0.012986106872558594\n",
      "step: 691171, loss: 0.06415991485118866, data time: 0.012576598387498122\n",
      "step: 691172, loss: 0.06316380202770233, data time: 0.01219753865842466\n",
      "step: 691173, loss: 0.0681474506855011, data time: 0.011845248086111886\n",
      "step: 691174, loss: 0.061255715787410736, data time: 0.011523583839679587\n",
      "step: 691175, loss: 0.06549198925495148, data time: 0.011223347981770833\n",
      "step: 691176, loss: 0.06072906777262688, data time: 0.010942282215241463\n",
      "step: 691177, loss: 0.06711657345294952, data time: 0.010680466890335083\n",
      "step: 691178, loss: 0.06338337063789368, data time: 0.010418010480476149\n",
      "step: 691179, loss: 0.06320692598819733, data time: 0.010170389624202953\n",
      "step: 691180, loss: 0.06385661661624908, data time: 0.009937579291207449\n",
      "step: 691181, loss: 0.06520360708236694, data time: 0.009717232651180692\n",
      "step: 691182, loss: 0.05942082405090332, data time: 0.009510220708073797\n",
      "step: 691183, loss: 0.06019572168588638, data time: 0.009317492183886077\n",
      "step: 691184, loss: 0.06093178316950798, data time: 0.009135044538057767\n",
      "step: 691185, loss: 0.06485164910554886, data time: 0.00896075963973999\n",
      "step: 691186, loss: 0.062048010528087616, data time: 0.2514810562133789\n",
      "step: 691187, loss: 0.06984510272741318, data time: 0.12651240825653076\n",
      "step: 691188, loss: 0.05765330418944359, data time: 0.08511996269226074\n",
      "step: 691189, loss: 0.061073314398527145, data time: 0.06470108032226562\n",
      "step: 691190, loss: 0.06456582993268967, data time: 0.05203709602355957\n",
      "step: 691191, loss: 0.0618261992931366, data time: 0.043605685234069824\n",
      "step: 691192, loss: 0.05494861304759979, data time: 0.03757415499005999\n",
      "step: 691193, loss: 0.06563924252986908, data time: 0.03312993049621582\n",
      "step: 691194, loss: 0.061146512627601624, data time: 0.02959598435295953\n",
      "step: 691195, loss: 0.05945935845375061, data time: 0.026839184761047363\n",
      "step: 691196, loss: 0.06620808690786362, data time: 0.024592334573919124\n",
      "step: 691197, loss: 0.06856527924537659, data time: 0.02273331085840861\n",
      "step: 691198, loss: 0.06503184884786606, data time: 0.021147489547729492\n",
      "step: 691199, loss: 0.061795901507139206, data time: 0.01978315625871931\n",
      "step: 691200, loss: 0.06537463515996933, data time: 0.018597952524820962\n",
      "step: 691201, loss: 0.060176875442266464, data time: 0.017567679286003113\n",
      "step: 691202, loss: 0.06317564100027084, data time: 0.016662681803983802\n",
      "step: 691203, loss: 0.06372155994176865, data time: 0.015845974286397297\n",
      "step: 691204, loss: 0.05980057641863823, data time: 0.015116879814549497\n",
      "step: 691205, loss: 0.060940079391002655, data time: 0.014471626281738282\n",
      "step: 691206, loss: 0.06551729142665863, data time: 0.013885986237298874\n",
      "step: 691207, loss: 0.05833011120557785, data time: 0.013354247266596014\n",
      "step: 691208, loss: 0.061185549944639206, data time: 0.012860816457997198\n",
      "step: 691209, loss: 0.059180282056331635, data time: 0.012416690587997437\n",
      "step: 691210, loss: 0.06775233894586563, data time: 0.012004146575927735\n",
      "step: 691211, loss: 0.06349353492259979, data time: 0.011625418296227088\n",
      "step: 691212, loss: 0.06430958211421967, data time: 0.011267096908004195\n",
      "step: 691213, loss: 0.060125939548015594, data time: 0.01093907015664237\n",
      "step: 691214, loss: 0.058034174144268036, data time: 0.01063569660844474\n",
      "step: 691215, loss: 0.058384597301483154, data time: 0.010352826118469239\n",
      "step: 691216, loss: 0.06545765697956085, data time: 0.010088612956385459\n",
      "step: 691217, loss: 0.057625871151685715, data time: 0.009849198162555695\n",
      "step: 691218, loss: 0.06269293278455734, data time: 0.009610833543719667\n",
      "step: 691219, loss: 0.054609693586826324, data time: 0.009384505888995002\n",
      "step: 691220, loss: 0.05991914123296738, data time: 0.009173474993024554\n",
      "step: 691221, loss: 0.06196995824575424, data time: 0.008971114953358969\n",
      "step: 691222, loss: 0.061888404190540314, data time: 0.008780595418569204\n",
      "step: 691223, loss: 0.0621831938624382, data time: 0.008602380752563477\n",
      "step: 691224, loss: 0.05762325972318649, data time: 0.008434503506391477\n",
      "step: 691225, loss: 0.0510237030684948, data time: 0.008276307582855224\n",
      "step: 691226, loss: 0.05499524995684624, data time: 0.2607078552246094\n",
      "step: 691227, loss: 0.06441905349493027, data time: 0.13168859481811523\n",
      "step: 691228, loss: 0.06558046489953995, data time: 0.08853435516357422\n",
      "step: 691229, loss: 0.06303451955318451, data time: 0.06734633445739746\n",
      "step: 691230, loss: 0.06813524663448334, data time: 0.05416030883789062\n",
      "step: 691231, loss: 0.06394807994365692, data time: 0.045363942782084145\n",
      "step: 691232, loss: 0.05591399595141411, data time: 0.03908402579171317\n",
      "step: 691233, loss: 0.06010947749018669, data time: 0.03444632887840271\n",
      "step: 691234, loss: 0.06304141879081726, data time: 0.030765321519639757\n",
      "step: 691235, loss: 0.06058315560221672, data time: 0.027889776229858398\n",
      "step: 691236, loss: 0.06722685694694519, data time: 0.025547872890125622\n",
      "step: 691237, loss: 0.06714286655187607, data time: 0.023594995339711506\n",
      "step: 691238, loss: 0.06126443296670914, data time: 0.021944376138540413\n",
      "step: 691239, loss: 0.06520646810531616, data time: 0.020524297441755022\n",
      "step: 691240, loss: 0.06104298681020737, data time: 0.01929334004720052\n",
      "step: 691241, loss: 0.06436926126480103, data time: 0.018212944269180298\n",
      "step: 691242, loss: 0.06591317057609558, data time: 0.017265642390531653\n",
      "step: 691243, loss: 0.06081105396151543, data time: 0.016423702239990234\n",
      "step: 691244, loss: 0.06462517380714417, data time: 0.015665480965062192\n",
      "step: 691245, loss: 0.06171584129333496, data time: 0.014988124370574951\n",
      "step: 691246, loss: 0.06021305173635483, data time: 0.014376606260027205\n",
      "step: 691247, loss: 0.06388173997402191, data time: 0.013819336891174316\n",
      "step: 691248, loss: 0.05749417841434479, data time: 0.013307498848956564\n",
      "step: 691249, loss: 0.06138726323843002, data time: 0.01284492015838623\n",
      "step: 691250, loss: 0.06481675803661346, data time: 0.012415599822998048\n",
      "step: 691251, loss: 0.05697574466466904, data time: 0.012017763577974759\n",
      "step: 691252, loss: 0.0621940903365612, data time: 0.01164433691236708\n",
      "step: 691253, loss: 0.05836763605475426, data time: 0.011298741613115584\n",
      "step: 691254, loss: 0.0645948052406311, data time: 0.010981641966721108\n",
      "step: 691255, loss: 0.057646743953228, data time: 0.010687001546223958\n",
      "step: 691256, loss: 0.067323699593544, data time: 0.010412123895460559\n",
      "step: 691257, loss: 0.06432406604290009, data time: 0.01015770435333252\n",
      "step: 691258, loss: 0.05844181776046753, data time: 0.009906985542990944\n",
      "step: 691259, loss: 0.059974998235702515, data time: 0.009673371034509996\n",
      "step: 691260, loss: 0.059720687568187714, data time: 0.009451341629028321\n",
      "step: 691261, loss: 0.07030931115150452, data time: 0.009239223268296983\n",
      "step: 691262, loss: 0.060542862862348557, data time: 0.00903911203951449\n",
      "step: 691263, loss: 0.06364097446203232, data time: 0.008854100578709653\n",
      "step: 691264, loss: 0.06181185692548752, data time: 0.008680245815179287\n",
      "step: 691265, loss: 0.07698927819728851, data time: 0.00851583480834961\n",
      "step: 691266, loss: 0.062073539942502975, data time: 0.25761890411376953\n",
      "step: 691267, loss: 0.06156700849533081, data time: 0.12993192672729492\n",
      "step: 691268, loss: 0.058558396995067596, data time: 0.08712116877237956\n",
      "step: 691269, loss: 0.06817544996738434, data time: 0.06625568866729736\n",
      "step: 691270, loss: 0.06445557624101639, data time: 0.053284215927124026\n",
      "step: 691271, loss: 0.059264253824949265, data time: 0.044648329416910805\n",
      "step: 691272, loss: 0.06116480752825737, data time: 0.038477693285260885\n",
      "step: 691273, loss: 0.0643332377076149, data time: 0.03393760323524475\n",
      "step: 691274, loss: 0.06862016022205353, data time: 0.030314948823716905\n",
      "step: 691275, loss: 0.05922003835439682, data time: 0.027499938011169435\n",
      "step: 691276, loss: 0.06108305603265762, data time: 0.02519369125366211\n",
      "step: 691277, loss: 0.06408368051052094, data time: 0.02327696482340495\n",
      "step: 691278, loss: 0.05902083218097687, data time: 0.021651268005371094\n",
      "step: 691279, loss: 0.06507669389247894, data time: 0.020248344966343472\n",
      "step: 691280, loss: 0.06300436705350876, data time: 0.019042046864827473\n",
      "step: 691281, loss: 0.06634621322154999, data time: 0.017983868718147278\n",
      "step: 691282, loss: 0.05983702838420868, data time: 0.01705457182491527\n",
      "step: 691283, loss: 0.06440947949886322, data time: 0.016218119197421603\n",
      "step: 691284, loss: 0.06517575681209564, data time: 0.015467932349757144\n",
      "step: 691285, loss: 0.06603448837995529, data time: 0.014805352687835694\n",
      "step: 691286, loss: 0.06133506819605827, data time: 0.014202515284220377\n",
      "step: 691287, loss: 0.05887221544981003, data time: 0.013688423416831276\n",
      "step: 691288, loss: 0.06120603531599045, data time: 0.013198033623073412\n",
      "step: 691289, loss: 0.06366544961929321, data time: 0.012746671835581461\n",
      "step: 691290, loss: 0.060451358556747437, data time: 0.01233220100402832\n",
      "step: 691291, loss: 0.05806048959493637, data time: 0.011948842268723708\n",
      "step: 691292, loss: 0.06270048022270203, data time: 0.011592193886085792\n",
      "step: 691293, loss: 0.061099521815776825, data time: 0.011264468942369734\n",
      "step: 691294, loss: 0.06128397583961487, data time: 0.010963859229252255\n",
      "step: 691295, loss: 0.06050936505198479, data time: 0.010675001144409179\n",
      "step: 691296, loss: 0.06398104876279831, data time: 0.010399687674737746\n",
      "step: 691297, loss: 0.05691900849342346, data time: 0.010147757828235626\n",
      "step: 691298, loss: 0.0649736076593399, data time: 0.009898062908288204\n",
      "step: 691299, loss: 0.06478103995323181, data time: 0.009662943727829876\n",
      "step: 691300, loss: 0.060988686978816986, data time: 0.009442070552280971\n",
      "step: 691301, loss: 0.058098651468753815, data time: 0.009231819046868218\n",
      "step: 691302, loss: 0.05948202311992645, data time: 0.009034253455497123\n",
      "step: 691303, loss: 0.059927813708782196, data time: 0.00885310925935444\n",
      "step: 691304, loss: 0.06960608810186386, data time: 0.008678540205344176\n",
      "step: 691305, loss: 0.08092125505208969, data time: 0.00851532220840454\n",
      "step: 691306, loss: 0.06234041601419449, data time: 0.25553250312805176\n",
      "step: 691307, loss: 0.0646568238735199, data time: 0.1285412311553955\n",
      "step: 691308, loss: 0.06643983721733093, data time: 0.08620007832845052\n",
      "step: 691309, loss: 0.05969524383544922, data time: 0.06550133228302002\n",
      "step: 691310, loss: 0.057227905839681625, data time: 0.0526796817779541\n",
      "step: 691311, loss: 0.06858634948730469, data time: 0.044133385022481285\n",
      "step: 691312, loss: 0.06972461938858032, data time: 0.03803443908691406\n",
      "step: 691313, loss: 0.06411105394363403, data time: 0.03354710340499878\n",
      "step: 691314, loss: 0.058981239795684814, data time: 0.029981692632039387\n",
      "step: 691315, loss: 0.058838244527578354, data time: 0.027190041542053223\n",
      "step: 691316, loss: 0.061441317200660706, data time: 0.02490709044716575\n",
      "step: 691317, loss: 0.06370560824871063, data time: 0.023009538650512695\n",
      "step: 691318, loss: 0.06334230303764343, data time: 0.021407127380371094\n",
      "step: 691319, loss: 0.06250981241464615, data time: 0.020026803016662598\n",
      "step: 691320, loss: 0.06236953288316727, data time: 0.018827438354492188\n",
      "step: 691321, loss: 0.05948358401656151, data time: 0.017782092094421387\n",
      "step: 691322, loss: 0.05312345549464226, data time: 0.016856768551994774\n",
      "step: 691323, loss: 0.06529596447944641, data time: 0.016036510467529297\n",
      "step: 691324, loss: 0.06428137421607971, data time: 0.015295806683992086\n",
      "step: 691325, loss: 0.06496766209602356, data time: 0.014643311500549316\n",
      "step: 691326, loss: 0.06208237633109093, data time: 0.014050120399111793\n",
      "step: 691327, loss: 0.06164069473743439, data time: 0.013512568040327593\n",
      "step: 691328, loss: 0.05763011425733566, data time: 0.013013746427453083\n",
      "step: 691329, loss: 0.061509422957897186, data time: 0.012555072704950968\n",
      "step: 691330, loss: 0.05680328607559204, data time: 0.012134838104248046\n",
      "step: 691331, loss: 0.059633832424879074, data time: 0.011751055717468262\n",
      "step: 691332, loss: 0.05764050409197807, data time: 0.011392425607751918\n",
      "step: 691333, loss: 0.06069766730070114, data time: 0.011056976658957345\n",
      "step: 691334, loss: 0.062495820224285126, data time: 0.010749636025264346\n",
      "step: 691335, loss: 0.06064341962337494, data time: 0.010466504096984863\n",
      "step: 691336, loss: 0.05980248749256134, data time: 0.010197524101503434\n",
      "step: 691337, loss: 0.06184899061918259, data time: 0.00995086133480072\n",
      "step: 691338, loss: 0.07201708853244781, data time: 0.009707002928762726\n",
      "step: 691339, loss: 0.06546811759471893, data time: 0.009478835498585421\n",
      "step: 691340, loss: 0.06467889249324799, data time: 0.009262657165527344\n",
      "step: 691341, loss: 0.06652228534221649, data time: 0.009060932530297173\n",
      "step: 691342, loss: 0.055806320160627365, data time: 0.008866554981953389\n",
      "step: 691343, loss: 0.06155001372098923, data time: 0.008685576288323654\n",
      "step: 691344, loss: 0.06293940544128418, data time: 0.0085144165234688\n",
      "step: 691345, loss: 0.040932439267635345, data time: 0.00835191011428833\n",
      "step: 691346, loss: 0.06267853826284409, data time: 0.25394344329833984\n",
      "step: 691347, loss: 0.0635170042514801, data time: 0.12807965278625488\n",
      "step: 691348, loss: 0.05570349842309952, data time: 0.0859069029490153\n",
      "step: 691349, loss: 0.060749657452106476, data time: 0.06533342599868774\n",
      "step: 691350, loss: 0.05527544766664505, data time: 0.05254440307617188\n",
      "step: 691351, loss: 0.06094369292259216, data time: 0.04402343432108561\n",
      "step: 691352, loss: 0.06803193688392639, data time: 0.03793225969587054\n",
      "step: 691353, loss: 0.06089097261428833, data time: 0.03344321250915527\n",
      "step: 691354, loss: 0.06300803273916245, data time: 0.029870165718926325\n",
      "step: 691355, loss: 0.06202017515897751, data time: 0.027081727981567383\n",
      "step: 691356, loss: 0.059649884700775146, data time: 0.024816469712690872\n",
      "step: 691357, loss: 0.06831301748752594, data time: 0.022928357124328613\n",
      "step: 691358, loss: 0.06281282007694244, data time: 0.021334758171668418\n",
      "step: 691359, loss: 0.06498607993125916, data time: 0.019960624831063405\n",
      "step: 691360, loss: 0.06260056793689728, data time: 0.01876220703125\n",
      "step: 691361, loss: 0.05812842398881912, data time: 0.01771247386932373\n",
      "step: 691362, loss: 0.05957373231649399, data time: 0.016792044920079848\n",
      "step: 691363, loss: 0.06419272720813751, data time: 0.015969038009643555\n",
      "step: 691364, loss: 0.07001407444477081, data time: 0.015237507067228618\n",
      "step: 691365, loss: 0.06837664544582367, data time: 0.014584434032440186\n",
      "step: 691366, loss: 0.07126334309577942, data time: 0.01399110612415132\n",
      "step: 691367, loss: 0.0594731867313385, data time: 0.013452681628140535\n",
      "step: 691368, loss: 0.06090502440929413, data time: 0.012956100961436396\n",
      "step: 691369, loss: 0.06265200674533844, data time: 0.012503931919733683\n",
      "step: 691370, loss: 0.0645895004272461, data time: 0.012091960906982422\n",
      "step: 691371, loss: 0.06145641207695007, data time: 0.01170784693497878\n",
      "step: 691372, loss: 0.06617891043424606, data time: 0.011352592044406466\n",
      "step: 691373, loss: 0.06112983077764511, data time: 0.011022014277321952\n",
      "step: 691374, loss: 0.056250594556331635, data time: 0.010716216317538557\n",
      "step: 691375, loss: 0.05923253297805786, data time: 0.0104293425877889\n",
      "step: 691376, loss: 0.07060850411653519, data time: 0.010163030316752772\n",
      "step: 691377, loss: 0.06605850160121918, data time: 0.009917579591274261\n",
      "step: 691378, loss: 0.06364186853170395, data time: 0.009677641319506096\n",
      "step: 691379, loss: 0.06395319104194641, data time: 0.00944971337037928\n",
      "step: 691380, loss: 0.05979905650019646, data time: 0.00923593384878976\n",
      "step: 691381, loss: 0.06399773806333542, data time: 0.009030249383714464\n",
      "step: 691382, loss: 0.06596072018146515, data time: 0.008837500134029903\n",
      "step: 691383, loss: 0.06565693020820618, data time: 0.008658333828574732\n",
      "step: 691384, loss: 0.05255576968193054, data time: 0.008489248080131335\n",
      "step: 691385, loss: 0.06389065831899643, data time: 0.008327668905258179\n",
      "step: 691386, loss: 0.06264065951108932, data time: 0.26499414443969727\n",
      "step: 691387, loss: 0.06240513175725937, data time: 0.1332634687423706\n",
      "step: 691388, loss: 0.07119974493980408, data time: 0.08936079343159993\n",
      "step: 691389, loss: 0.06420555710792542, data time: 0.0677838921546936\n",
      "step: 691390, loss: 0.05959145352244377, data time: 0.05452427864074707\n",
      "step: 691391, loss: 0.06119311973452568, data time: 0.04567710558573405\n",
      "step: 691392, loss: 0.05786086618900299, data time: 0.03935401780264718\n",
      "step: 691393, loss: 0.0605539008975029, data time: 0.03468805551528931\n",
      "step: 691394, loss: 0.06233964487910271, data time: 0.03097973929511176\n",
      "step: 691395, loss: 0.06631484627723694, data time: 0.02808082103729248\n",
      "step: 691396, loss: 0.059819139540195465, data time: 0.025719751011241566\n",
      "step: 691397, loss: 0.06397846341133118, data time: 0.023754556973775227\n",
      "step: 691398, loss: 0.06262130290269852, data time: 0.02208867439856896\n",
      "step: 691399, loss: 0.06377281993627548, data time: 0.020659736224583218\n",
      "step: 691400, loss: 0.06390413641929626, data time: 0.019416666030883788\n",
      "step: 691401, loss: 0.06199120730161667, data time: 0.01832956075668335\n",
      "step: 691402, loss: 0.05960991233587265, data time: 0.01737830218146829\n",
      "step: 691403, loss: 0.05885123834013939, data time: 0.016526023546854656\n",
      "step: 691404, loss: 0.061863012611866, data time: 0.0157600578508879\n",
      "step: 691405, loss: 0.0603775791823864, data time: 0.015079212188720704\n",
      "step: 691406, loss: 0.062232695519924164, data time: 0.014465820221673874\n",
      "step: 691407, loss: 0.05691685900092125, data time: 0.013910737904635343\n",
      "step: 691408, loss: 0.06952791661024094, data time: 0.013394138087397036\n",
      "step: 691409, loss: 0.06279419362545013, data time: 0.012919813394546509\n",
      "step: 691410, loss: 0.0605781227350235, data time: 0.012484035491943359\n",
      "step: 691411, loss: 0.06493198126554489, data time: 0.012080935331491323\n",
      "step: 691412, loss: 0.057618387043476105, data time: 0.011706758428502965\n",
      "step: 691413, loss: 0.06594919413328171, data time: 0.01136277403150286\n",
      "step: 691414, loss: 0.06355828046798706, data time: 0.011059432194150728\n",
      "step: 691415, loss: 0.061610788106918335, data time: 0.010761682192484539\n",
      "step: 691416, loss: 0.06295204162597656, data time: 0.010482803467781313\n",
      "step: 691417, loss: 0.06160861253738403, data time: 0.01022724062204361\n",
      "step: 691418, loss: 0.060924749821424484, data time: 0.009975274403889975\n",
      "step: 691419, loss: 0.06022077798843384, data time: 0.009738851996029125\n",
      "step: 691420, loss: 0.0624447725713253, data time: 0.009516320909772601\n",
      "step: 691421, loss: 0.060509294271469116, data time: 0.009304516845279269\n",
      "step: 691422, loss: 0.05949199199676514, data time: 0.00910439362397065\n",
      "step: 691423, loss: 0.05893636867403984, data time: 0.008917739516810366\n",
      "step: 691424, loss: 0.06944780051708221, data time: 0.008741189271975786\n",
      "step: 691425, loss: 0.06314373761415482, data time: 0.00857393741607666\n",
      "step: 691426, loss: 0.06440065056085587, data time: 0.26264262199401855\n",
      "step: 691427, loss: 0.06334821879863739, data time: 0.1321021318435669\n",
      "step: 691428, loss: 0.061558496206998825, data time: 0.08897256851196289\n",
      "step: 691429, loss: 0.056392475962638855, data time: 0.06750768423080444\n",
      "step: 691430, loss: 0.06521186977624893, data time: 0.05428013801574707\n",
      "step: 691431, loss: 0.06194314360618591, data time: 0.04546761512756348\n",
      "step: 691432, loss: 0.06206081435084343, data time: 0.03917414801461356\n",
      "step: 691433, loss: 0.06232420727610588, data time: 0.034522801637649536\n",
      "step: 691434, loss: 0.06165526807308197, data time: 0.030837217966715496\n",
      "step: 691435, loss: 0.06659609079360962, data time: 0.02799222469329834\n",
      "step: 691436, loss: 0.0564805269241333, data time: 0.025682124224576084\n",
      "step: 691437, loss: 0.059065937995910645, data time: 0.02375217278798421\n",
      "step: 691438, loss: 0.07046230137348175, data time: 0.022126124455378607\n",
      "step: 691439, loss: 0.05783062428236008, data time: 0.02072129930768694\n",
      "step: 691440, loss: 0.0610947422683239, data time: 0.01949923833211263\n",
      "step: 691441, loss: 0.05901501700282097, data time: 0.018435463309288025\n",
      "step: 691442, loss: 0.06301712244749069, data time: 0.01749441202949075\n",
      "step: 691443, loss: 0.06628444790840149, data time: 0.016651895311143663\n",
      "step: 691444, loss: 0.060037653893232346, data time: 0.015901628293489154\n",
      "step: 691445, loss: 0.06808294355869293, data time: 0.015232455730438233\n",
      "step: 691446, loss: 0.06768487393856049, data time: 0.01462931860060919\n",
      "step: 691447, loss: 0.06620818376541138, data time: 0.014077262444929644\n",
      "step: 691448, loss: 0.06912526488304138, data time: 0.013565758000249449\n",
      "step: 691449, loss: 0.06375318020582199, data time: 0.013102322816848755\n",
      "step: 691450, loss: 0.06031527370214462, data time: 0.012672700881958009\n",
      "step: 691451, loss: 0.061184316873550415, data time: 0.012282059742854191\n",
      "step: 691452, loss: 0.06181539222598076, data time: 0.011914924339011864\n",
      "step: 691453, loss: 0.059603482484817505, data time: 0.011574711118425642\n",
      "step: 691454, loss: 0.0639440044760704, data time: 0.011262959447400323\n",
      "step: 691455, loss: 0.05750637501478195, data time: 0.010970354080200195\n",
      "step: 691456, loss: 0.06710941344499588, data time: 0.01069847999080535\n",
      "step: 691457, loss: 0.06145504117012024, data time: 0.010444298386573792\n",
      "step: 691458, loss: 0.06610517203807831, data time: 0.010188276117498224\n",
      "step: 691459, loss: 0.05987442657351494, data time: 0.009949410662931554\n",
      "step: 691460, loss: 0.06531959772109985, data time: 0.00972374507359096\n",
      "step: 691461, loss: 0.05724923312664032, data time: 0.009509027004241943\n",
      "step: 691462, loss: 0.0641331747174263, data time: 0.009306778778900971\n",
      "step: 691463, loss: 0.06435635685920715, data time: 0.009118331106085526\n",
      "step: 691464, loss: 0.06180071085691452, data time: 0.008938483702830778\n",
      "step: 691465, loss: 0.0818684995174408, data time: 0.008768445253372193\n",
      "step: 691466, loss: 0.06078505516052246, data time: 0.26612043380737305\n",
      "step: 691467, loss: 0.062136076390743256, data time: 0.1338801383972168\n",
      "step: 691468, loss: 0.06906925141811371, data time: 0.09039243062337239\n",
      "step: 691469, loss: 0.059419602155685425, data time: 0.06845879554748535\n",
      "step: 691470, loss: 0.060896359384059906, data time: 0.055051755905151364\n",
      "step: 691471, loss: 0.06129363179206848, data time: 0.04610510667165121\n",
      "step: 691472, loss: 0.057627372443675995, data time: 0.03971583502633231\n",
      "step: 691473, loss: 0.06418901681900024, data time: 0.03502070903778076\n",
      "step: 691474, loss: 0.058838874101638794, data time: 0.031275033950805664\n",
      "step: 691475, loss: 0.06361111998558044, data time: 0.02834658622741699\n",
      "step: 691476, loss: 0.058410994708538055, data time: 0.025961204008622604\n",
      "step: 691477, loss: 0.06299542635679245, data time: 0.02398363749186198\n",
      "step: 691478, loss: 0.05876406282186508, data time: 0.02230497506948618\n",
      "step: 691479, loss: 0.06057724356651306, data time: 0.020856278283255442\n",
      "step: 691480, loss: 0.06702586263418198, data time: 0.01960156758626302\n",
      "step: 691481, loss: 0.05943908914923668, data time: 0.01850239932537079\n",
      "step: 691482, loss: 0.06275925785303116, data time: 0.01753686456119313\n",
      "step: 691483, loss: 0.06257684528827667, data time: 0.016669273376464844\n",
      "step: 691484, loss: 0.06966017186641693, data time: 0.015898127304880244\n",
      "step: 691485, loss: 0.06294731795787811, data time: 0.015211927890777587\n",
      "step: 691486, loss: 0.05925879627466202, data time: 0.014598301478794642\n",
      "step: 691487, loss: 0.05965732783079147, data time: 0.01403156193819913\n",
      "step: 691488, loss: 0.060232531279325485, data time: 0.013509957686714504\n",
      "step: 691489, loss: 0.0650906041264534, data time: 0.01303096612294515\n",
      "step: 691490, loss: 0.05737469717860222, data time: 0.012592277526855468\n",
      "step: 691491, loss: 0.05903242155909538, data time: 0.012187517606295072\n",
      "step: 691492, loss: 0.05944281816482544, data time: 0.011815177069769965\n",
      "step: 691493, loss: 0.06493404507637024, data time: 0.011469602584838867\n",
      "step: 691494, loss: 0.05751371383666992, data time: 0.011149020030580718\n",
      "step: 691495, loss: 0.06905612349510193, data time: 0.010847671826680502\n",
      "step: 691496, loss: 0.06099409610033035, data time: 0.01056765740917575\n",
      "step: 691497, loss: 0.06281521171331406, data time: 0.010308094322681427\n",
      "step: 691498, loss: 0.05601668357849121, data time: 0.010053909186160925\n",
      "step: 691499, loss: 0.05791756138205528, data time: 0.00981459196876077\n",
      "step: 691500, loss: 0.06360410153865814, data time: 0.009588990892682756\n",
      "step: 691501, loss: 0.0678209513425827, data time: 0.009373181396060519\n",
      "step: 691502, loss: 0.06368522346019745, data time: 0.009175094398292335\n",
      "step: 691503, loss: 0.0684780478477478, data time: 0.008986698953728927\n",
      "step: 691504, loss: 0.06264778971672058, data time: 0.008807188425308619\n",
      "step: 691505, loss: 0.07377031445503235, data time: 0.008636510372161866\n",
      "step: 691506, loss: 0.07418119162321091, data time: 0.2473914623260498\n",
      "step: 691507, loss: 0.0657099112868309, data time: 0.1250922679901123\n",
      "step: 691508, loss: 0.0619194358587265, data time: 0.08446749051411946\n",
      "step: 691509, loss: 0.063329316675663, data time: 0.06400221586227417\n",
      "step: 691510, loss: 0.05995131656527519, data time: 0.05147991180419922\n",
      "step: 691511, loss: 0.06463022530078888, data time: 0.04313973585764567\n",
      "step: 691512, loss: 0.05713484436273575, data time: 0.037179878779820034\n",
      "step: 691513, loss: 0.05696600675582886, data time: 0.03279072046279907\n",
      "step: 691514, loss: 0.061925794929265976, data time: 0.029295841852823894\n",
      "step: 691515, loss: 0.05994906648993492, data time: 0.026572179794311524\n",
      "step: 691516, loss: 0.06161707639694214, data time: 0.02435057813471014\n",
      "step: 691517, loss: 0.05937927961349487, data time: 0.022505919138590496\n",
      "step: 691518, loss: 0.05790219455957413, data time: 0.020938396453857422\n",
      "step: 691519, loss: 0.06255130469799042, data time: 0.019592932292393277\n",
      "step: 691520, loss: 0.06557948887348175, data time: 0.018428500493367514\n",
      "step: 691521, loss: 0.05708665773272514, data time: 0.01740509271621704\n",
      "step: 691522, loss: 0.06713192164897919, data time: 0.016497962615069223\n",
      "step: 691523, loss: 0.06052203103899956, data time: 0.01569325394100613\n",
      "step: 691524, loss: 0.06934496760368347, data time: 0.014975623080604955\n",
      "step: 691525, loss: 0.058679282665252686, data time: 0.014335286617279053\n",
      "step: 691526, loss: 0.05768663436174393, data time: 0.013760986782255628\n",
      "step: 691527, loss: 0.05873614177107811, data time: 0.01323168927972967\n",
      "step: 691528, loss: 0.06354737281799316, data time: 0.012744727342025093\n",
      "step: 691529, loss: 0.057914718985557556, data time: 0.012301027774810791\n",
      "step: 691530, loss: 0.06021038442850113, data time: 0.011890974044799805\n",
      "step: 691531, loss: 0.0607176348567009, data time: 0.01151445278754601\n",
      "step: 691532, loss: 0.060530781745910645, data time: 0.011164470955177589\n",
      "step: 691533, loss: 0.06834924221038818, data time: 0.010838133948189872\n",
      "step: 691534, loss: 0.058698516339063644, data time: 0.010541134867174872\n",
      "step: 691535, loss: 0.056697264313697815, data time: 0.010262584686279297\n",
      "step: 691536, loss: 0.06265164166688919, data time: 0.010000874919276084\n",
      "step: 691537, loss: 0.0524272695183754, data time: 0.009759165346622467\n",
      "step: 691538, loss: 0.06393773853778839, data time: 0.009532061490145597\n",
      "step: 691539, loss: 0.06004175543785095, data time: 0.009309249765732708\n",
      "step: 691540, loss: 0.06094520911574364, data time: 0.009097399030412947\n",
      "step: 691541, loss: 0.062343619763851166, data time: 0.008907920784420438\n",
      "step: 691542, loss: 0.0606459304690361, data time: 0.008719238075050147\n",
      "step: 691543, loss: 0.06503336876630783, data time: 0.00854307726809853\n",
      "step: 691544, loss: 0.055711619555950165, data time: 0.008375149506788988\n",
      "step: 691545, loss: 0.0529489628970623, data time: 0.008215123414993286\n",
      "step: 691546, loss: 0.06318840384483337, data time: 0.25017428398132324\n",
      "step: 691547, loss: 0.06209686025977135, data time: 0.12621688842773438\n",
      "step: 691548, loss: 0.05875125527381897, data time: 0.08536505699157715\n",
      "step: 691549, loss: 0.060459643602371216, data time: 0.06471598148345947\n",
      "step: 691550, loss: 0.06513696163892746, data time: 0.05203962326049805\n",
      "step: 691551, loss: 0.06557059288024902, data time: 0.04360572497049967\n",
      "step: 691552, loss: 0.06316036731004715, data time: 0.03757694789341518\n",
      "step: 691553, loss: 0.05646156519651413, data time: 0.03313708305358887\n",
      "step: 691554, loss: 0.059860385954380035, data time: 0.029601653416951496\n",
      "step: 691555, loss: 0.05942734703421593, data time: 0.02684917449951172\n",
      "step: 691556, loss: 0.06464345753192902, data time: 0.024604168805209072\n",
      "step: 691557, loss: 0.06120959669351578, data time: 0.022736966609954834\n",
      "step: 691558, loss: 0.05748039484024048, data time: 0.021152422978327826\n",
      "step: 691559, loss: 0.06593938171863556, data time: 0.01978257724217006\n",
      "step: 691560, loss: 0.06417536735534668, data time: 0.01860194206237793\n",
      "step: 691561, loss: 0.0631100982427597, data time: 0.01756998896598816\n",
      "step: 691562, loss: 0.06340710818767548, data time: 0.016663284862742704\n",
      "step: 691563, loss: 0.05864476412534714, data time: 0.01584658357832167\n",
      "step: 691564, loss: 0.058505624532699585, data time: 0.015117708005403218\n",
      "step: 691565, loss: 0.06306424736976624, data time: 0.014469587802886963\n",
      "step: 691566, loss: 0.05842050164937973, data time: 0.0138853391011556\n",
      "step: 691567, loss: 0.06413482129573822, data time: 0.013353564522483131\n",
      "step: 691568, loss: 0.061074089258909225, data time: 0.012864320174507473\n",
      "step: 691569, loss: 0.066771499812603, data time: 0.012415011723836264\n",
      "step: 691570, loss: 0.0676911249756813, data time: 0.011998920440673829\n",
      "step: 691571, loss: 0.06569111347198486, data time: 0.011615230486943172\n",
      "step: 691572, loss: 0.05672562122344971, data time: 0.011259176112987377\n",
      "step: 691573, loss: 0.06230977177619934, data time: 0.010931296007973807\n",
      "step: 691574, loss: 0.05973796173930168, data time: 0.010629226421487743\n",
      "step: 691575, loss: 0.06203547120094299, data time: 0.010347294807434081\n",
      "step: 691576, loss: 0.06202731654047966, data time: 0.010082483291625977\n",
      "step: 691577, loss: 0.05984960123896599, data time: 0.009839795529842377\n",
      "step: 691578, loss: 0.06390896439552307, data time: 0.009600682692094282\n",
      "step: 691579, loss: 0.06498656421899796, data time: 0.009374555419473088\n",
      "step: 691580, loss: 0.06136767193675041, data time: 0.009161676679338728\n",
      "step: 691581, loss: 0.05519908666610718, data time: 0.008960935804578993\n",
      "step: 691582, loss: 0.06935045123100281, data time: 0.00877191569354083\n",
      "step: 691583, loss: 0.06515543162822723, data time: 0.008594901938187448\n",
      "step: 691584, loss: 0.06606864929199219, data time: 0.00842596934391902\n",
      "step: 691585, loss: 0.08108299225568771, data time: 0.008266103267669678\n",
      "step: 691586, loss: 0.06607690453529358, data time: 0.2644670009613037\n",
      "step: 691587, loss: 0.060389891266822815, data time: 0.13304340839385986\n",
      "step: 691588, loss: 0.05923355743288994, data time: 0.08919676144917806\n",
      "step: 691589, loss: 0.06311696767807007, data time: 0.06765884160995483\n",
      "step: 691590, loss: 0.06069788336753845, data time: 0.054418516159057614\n",
      "step: 691591, loss: 0.06009865552186966, data time: 0.04558273156483968\n",
      "step: 691592, loss: 0.05987664312124252, data time: 0.039277110780988424\n",
      "step: 691593, loss: 0.06558042019605637, data time: 0.03462210297584534\n",
      "step: 691594, loss: 0.061701513826847076, data time: 0.03092058499654134\n",
      "step: 691595, loss: 0.06080282852053642, data time: 0.028049945831298828\n",
      "step: 691596, loss: 0.06467273831367493, data time: 0.025698466734452682\n",
      "step: 691597, loss: 0.06679295003414154, data time: 0.02373627821604411\n",
      "step: 691598, loss: 0.06033461540937424, data time: 0.02207326889038086\n",
      "step: 691599, loss: 0.06111852824687958, data time: 0.020645907946995327\n",
      "step: 691600, loss: 0.0629090890288353, data time: 0.01940455436706543\n",
      "step: 691601, loss: 0.05665188282728195, data time: 0.01831880211830139\n",
      "step: 691602, loss: 0.06702351570129395, data time: 0.017359200645895564\n",
      "step: 691603, loss: 0.06336718052625656, data time: 0.01650306913587782\n",
      "step: 691604, loss: 0.061892323195934296, data time: 0.01574313013177169\n",
      "step: 691605, loss: 0.06754564493894577, data time: 0.01506263017654419\n",
      "step: 691606, loss: 0.061284974217414856, data time: 0.014449664524623327\n",
      "step: 691607, loss: 0.05828133225440979, data time: 0.013886538418856535\n",
      "step: 691608, loss: 0.0657603070139885, data time: 0.01337012000705885\n",
      "step: 691609, loss: 0.0662686675786972, data time: 0.012902915477752686\n",
      "step: 691610, loss: 0.059983909130096436, data time: 0.012469654083251952\n",
      "step: 691611, loss: 0.06732134521007538, data time: 0.012071316058819111\n",
      "step: 691612, loss: 0.06406664848327637, data time: 0.011699888441297743\n",
      "step: 691613, loss: 0.05921938270330429, data time: 0.011352871145520891\n",
      "step: 691614, loss: 0.06469278037548065, data time: 0.01103633847729913\n",
      "step: 691615, loss: 0.06640394777059555, data time: 0.010738325119018555\n",
      "step: 691616, loss: 0.06124015897512436, data time: 0.01046020753922001\n",
      "step: 691617, loss: 0.06484204530715942, data time: 0.010205037891864777\n",
      "step: 691618, loss: 0.0686020702123642, data time: 0.009953058127200964\n",
      "step: 691619, loss: 0.06253290176391602, data time: 0.009717864148757038\n",
      "step: 691620, loss: 0.05947200208902359, data time: 0.009494788306100028\n",
      "step: 691621, loss: 0.06075103208422661, data time: 0.00928166839811537\n",
      "step: 691622, loss: 0.05918396636843681, data time: 0.00908494640041042\n",
      "step: 691623, loss: 0.063609279692173, data time: 0.008898308402613589\n",
      "step: 691624, loss: 0.06674294173717499, data time: 0.008721339396941356\n",
      "step: 691625, loss: 0.04647240787744522, data time: 0.008553230762481689\n",
      "step: 691626, loss: 0.05635714530944824, data time: 0.2597355842590332\n",
      "step: 691627, loss: 0.059130214154720306, data time: 0.13099884986877441\n",
      "step: 691628, loss: 0.06843919306993484, data time: 0.0878280798594157\n",
      "step: 691629, loss: 0.0690125823020935, data time: 0.066744863986969\n",
      "step: 691630, loss: 0.06815606355667114, data time: 0.053700494766235354\n",
      "step: 691631, loss: 0.06292437762022018, data time: 0.04499546686808268\n",
      "step: 691632, loss: 0.06323878467082977, data time: 0.0387636593409947\n",
      "step: 691633, loss: 0.062352102249860764, data time: 0.03418588638305664\n",
      "step: 691634, loss: 0.055133186280727386, data time: 0.030533128314548068\n",
      "step: 691635, loss: 0.06224757432937622, data time: 0.027685713768005372\n",
      "step: 691636, loss: 0.0591755211353302, data time: 0.025361624631014736\n",
      "step: 691637, loss: 0.060223519802093506, data time: 0.023429930210113525\n",
      "step: 691638, loss: 0.06023483723402023, data time: 0.021799894479604866\n",
      "step: 691639, loss: 0.057190269231796265, data time: 0.020386440413338796\n",
      "step: 691640, loss: 0.06145907938480377, data time: 0.019159491856892905\n",
      "step: 691641, loss: 0.0651925802230835, data time: 0.018086150288581848\n",
      "step: 691642, loss: 0.06504819542169571, data time: 0.017147190430585074\n",
      "step: 691643, loss: 0.0676838755607605, data time: 0.01630898316701253\n",
      "step: 691644, loss: 0.062377601861953735, data time: 0.015558870215164987\n",
      "step: 691645, loss: 0.06392403692007065, data time: 0.014889335632324219\n",
      "step: 691646, loss: 0.06384145468473434, data time: 0.014280137561616443\n",
      "step: 691647, loss: 0.06251734495162964, data time: 0.013726928017356178\n",
      "step: 691648, loss: 0.06642629206180573, data time: 0.013217231501703676\n",
      "step: 691649, loss: 0.06257511675357819, data time: 0.012750466664632162\n",
      "step: 691650, loss: 0.065947525203228, data time: 0.012321624755859375\n",
      "step: 691651, loss: 0.0669029951095581, data time: 0.01192956704359788\n",
      "step: 691652, loss: 0.06243429332971573, data time: 0.011562974364669234\n",
      "step: 691653, loss: 0.05607215687632561, data time: 0.011222336973462785\n",
      "step: 691654, loss: 0.0656614601612091, data time: 0.010909861531750909\n",
      "step: 691655, loss: 0.05769355222582817, data time: 0.010618797938028971\n",
      "step: 691656, loss: 0.06169804185628891, data time: 0.010345420529765467\n",
      "step: 691657, loss: 0.05949719250202179, data time: 0.010091662406921387\n",
      "step: 691658, loss: 0.05885709449648857, data time: 0.00984469327059659\n",
      "step: 691659, loss: 0.06191341206431389, data time: 0.009610835243673885\n",
      "step: 691660, loss: 0.06752810627222061, data time: 0.009394243785313198\n",
      "step: 691661, loss: 0.05531050264835358, data time: 0.009186486403147379\n",
      "step: 691662, loss: 0.062139853835105896, data time: 0.008993367891053896\n",
      "step: 691663, loss: 0.054778121411800385, data time: 0.008813061212238512\n",
      "step: 691664, loss: 0.05871802568435669, data time: 0.008643180896074344\n",
      "step: 691665, loss: 0.053976576775312424, data time: 0.008482247591018677\n",
      "step: 691666, loss: 0.07339029759168625, data time: 0.2621424198150635\n",
      "step: 691667, loss: 0.05486327409744263, data time: 0.13290345668792725\n",
      "step: 691668, loss: 0.06146010011434555, data time: 0.08910250663757324\n",
      "step: 691669, loss: 0.06158451363444328, data time: 0.06772559881210327\n",
      "step: 691670, loss: 0.0587262362241745, data time: 0.05446834564208984\n",
      "step: 691671, loss: 0.06388827413320541, data time: 0.04566641648610433\n",
      "step: 691672, loss: 0.0587192177772522, data time: 0.03934758050101144\n",
      "step: 691673, loss: 0.06668706238269806, data time: 0.03468480706214905\n",
      "step: 691674, loss: 0.060290008783340454, data time: 0.03098138173421224\n",
      "step: 691675, loss: 0.06385252624750137, data time: 0.028094983100891112\n",
      "step: 691676, loss: 0.058724772185087204, data time: 0.025735746730457653\n",
      "step: 691677, loss: 0.064727284014225, data time: 0.02377176284790039\n",
      "step: 691678, loss: 0.06447665393352509, data time: 0.02210820638216459\n",
      "step: 691679, loss: 0.05822586268186569, data time: 0.02067312172480992\n",
      "step: 691680, loss: 0.06723585724830627, data time: 0.0194337526957194\n",
      "step: 691681, loss: 0.06374378502368927, data time: 0.01834455132484436\n",
      "step: 691682, loss: 0.06018230319023132, data time: 0.017412494210635916\n",
      "step: 691683, loss: 0.05681885406374931, data time: 0.01657778686947293\n",
      "step: 691684, loss: 0.0642765462398529, data time: 0.01583206026177657\n",
      "step: 691685, loss: 0.06646560877561569, data time: 0.01516571044921875\n",
      "step: 691686, loss: 0.07146961241960526, data time: 0.01456307229541597\n",
      "step: 691687, loss: 0.06385503709316254, data time: 0.014017354358326305\n",
      "step: 691688, loss: 0.059379369020462036, data time: 0.013514684594195822\n",
      "step: 691689, loss: 0.05995746701955795, data time: 0.01305664579073588\n",
      "step: 691690, loss: 0.06111554428935051, data time: 0.01263204574584961\n",
      "step: 691691, loss: 0.06459200382232666, data time: 0.0122376772073599\n",
      "step: 691692, loss: 0.061554502695798874, data time: 0.011872688929239908\n",
      "step: 691693, loss: 0.060784321278333664, data time: 0.011534980365208216\n",
      "step: 691694, loss: 0.06017128750681877, data time: 0.011222403624962116\n",
      "step: 691695, loss: 0.05888776481151581, data time: 0.010931722323099772\n",
      "step: 691696, loss: 0.057248279452323914, data time: 0.010660732946088236\n",
      "step: 691697, loss: 0.06378143280744553, data time: 0.010409615933895111\n",
      "step: 691698, loss: 0.06357593834400177, data time: 0.010159030105128433\n",
      "step: 691699, loss: 0.058887630701065063, data time: 0.00992130531984217\n",
      "step: 691700, loss: 0.059511296451091766, data time: 0.00969642230442592\n",
      "step: 691701, loss: 0.0607968270778656, data time: 0.009481529394785563\n",
      "step: 691702, loss: 0.060771644115448, data time: 0.009279934135643212\n",
      "step: 691703, loss: 0.05259913206100464, data time: 0.009091339613261976\n",
      "step: 691704, loss: 0.06441851705312729, data time: 0.008913217446742913\n",
      "step: 691705, loss: 0.07281513512134552, data time: 0.00874362587928772\n",
      "step: 691706, loss: 0.057192981243133545, data time: 0.26838016510009766\n",
      "step: 691707, loss: 0.0567011758685112, data time: 0.13497638702392578\n",
      "step: 691708, loss: 0.060774050652980804, data time: 0.0904994010925293\n",
      "step: 691709, loss: 0.06584011018276215, data time: 0.06863480806350708\n",
      "step: 691710, loss: 0.05512798950076103, data time: 0.05518879890441895\n",
      "step: 691711, loss: 0.0648500993847847, data time: 0.04623866081237793\n",
      "step: 691712, loss: 0.062106262892484665, data time: 0.03983330726623535\n",
      "step: 691713, loss: 0.06596900522708893, data time: 0.035122573375701904\n",
      "step: 691714, loss: 0.05739728733897209, data time: 0.03137389818827311\n",
      "step: 691715, loss: 0.06969526410102844, data time: 0.028443455696105957\n",
      "step: 691716, loss: 0.05731634423136711, data time: 0.02605106613852761\n",
      "step: 691717, loss: 0.06046314164996147, data time: 0.02406849463780721\n",
      "step: 691718, loss: 0.0654267817735672, data time: 0.022385817307692308\n",
      "step: 691719, loss: 0.06598272919654846, data time: 0.020935331072126116\n",
      "step: 691720, loss: 0.06108212471008301, data time: 0.01967962582906087\n",
      "step: 691721, loss: 0.06285146623849869, data time: 0.018576934933662415\n",
      "step: 691722, loss: 0.0633731335401535, data time: 0.017609540153952205\n",
      "step: 691723, loss: 0.06353934854269028, data time: 0.01674030886756049\n",
      "step: 691724, loss: 0.05691182613372803, data time: 0.01596386809098093\n",
      "step: 691725, loss: 0.06307508796453476, data time: 0.015276181697845458\n",
      "step: 691726, loss: 0.06608958542346954, data time: 0.01464933440798805\n",
      "step: 691727, loss: 0.05930768698453903, data time: 0.01407964663072066\n",
      "step: 691728, loss: 0.06413143873214722, data time: 0.013557143833326258\n",
      "step: 691729, loss: 0.0637022852897644, data time: 0.013078908125559488\n",
      "step: 691730, loss: 0.06253080070018768, data time: 0.01263655662536621\n",
      "step: 691731, loss: 0.06586135923862457, data time: 0.012235540610093337\n",
      "step: 691732, loss: 0.05944155901670456, data time: 0.011854021637528031\n",
      "step: 691733, loss: 0.05951797217130661, data time: 0.01150258949824742\n",
      "step: 691734, loss: 0.06561094522476196, data time: 0.01118044195504024\n",
      "step: 691735, loss: 0.05849621817469597, data time: 0.010883593559265136\n",
      "step: 691736, loss: 0.058513790369033813, data time: 0.010603504796181955\n",
      "step: 691737, loss: 0.06712951511144638, data time: 0.010345861315727234\n",
      "step: 691738, loss: 0.06258378177881241, data time: 0.010092634143251362\n",
      "step: 691739, loss: 0.06544473767280579, data time: 0.009854709400850184\n",
      "step: 691740, loss: 0.058343999087810516, data time: 0.009629998888288225\n",
      "step: 691741, loss: 0.061198532581329346, data time: 0.009413222471872965\n",
      "step: 691742, loss: 0.060455113649368286, data time: 0.009211926846890836\n",
      "step: 691743, loss: 0.06171027943491936, data time: 0.009028315544128418\n",
      "step: 691744, loss: 0.060412000864744186, data time: 0.008849021715995593\n",
      "step: 691745, loss: 0.07018016278743744, data time: 0.008684396743774414\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4079122543334961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 691746, loss: 0.0667438954114914, data time: 0.28583312034606934\n",
      "step: 691747, loss: 0.06688573211431503, data time: 0.14374172687530518\n",
      "step: 691748, loss: 0.06256256997585297, data time: 0.09630902608235677\n",
      "step: 691749, loss: 0.06868233531713486, data time: 0.07307064533233643\n",
      "step: 691750, loss: 0.06235438585281372, data time: 0.058727073669433597\n",
      "step: 691751, loss: 0.05506724864244461, data time: 0.04917713006337484\n",
      "step: 691752, loss: 0.06205272674560547, data time: 0.04236824171883719\n",
      "step: 691753, loss: 0.06279711425304413, data time: 0.037328749895095825\n",
      "step: 691754, loss: 0.05589812248945236, data time: 0.03331987063090006\n",
      "step: 691755, loss: 0.06080074608325958, data time: 0.030197000503540038\n",
      "step: 691756, loss: 0.06568380445241928, data time: 0.027651743455366654\n",
      "step: 691757, loss: 0.06345340609550476, data time: 0.025518178939819336\n",
      "step: 691758, loss: 0.05911032855510712, data time: 0.023707078053401068\n",
      "step: 691759, loss: 0.06518353521823883, data time: 0.022155114582606723\n",
      "step: 691760, loss: 0.06155340000987053, data time: 0.02082516352335612\n",
      "step: 691761, loss: 0.06357087194919586, data time: 0.01964893937110901\n",
      "step: 691762, loss: 0.058987557888031006, data time: 0.018611753688139075\n",
      "step: 691763, loss: 0.060563988983631134, data time: 0.01769231425391303\n",
      "step: 691764, loss: 0.06090259552001953, data time: 0.01686112504256399\n",
      "step: 691765, loss: 0.06156448274850845, data time: 0.01612318754196167\n",
      "step: 691766, loss: 0.05976312607526779, data time: 0.015452748253231957\n",
      "step: 691767, loss: 0.06533291190862656, data time: 0.014844428409229626\n",
      "step: 691768, loss: 0.058760013431310654, data time: 0.014287140058434528\n",
      "step: 691769, loss: 0.06390037387609482, data time: 0.013776610294977823\n",
      "step: 691770, loss: 0.06139218062162399, data time: 0.013310651779174804\n",
      "step: 691771, loss: 0.06133206933736801, data time: 0.01287691409771259\n",
      "step: 691772, loss: 0.06488390266895294, data time: 0.012469494784319843\n",
      "step: 691773, loss: 0.07099699974060059, data time: 0.012092615876879011\n",
      "step: 691774, loss: 0.06173534318804741, data time: 0.011748667421012089\n",
      "step: 691775, loss: 0.05481647700071335, data time: 0.011429723103841145\n",
      "step: 691776, loss: 0.0633249506354332, data time: 0.011126972013904203\n",
      "step: 691777, loss: 0.059323545545339584, data time: 0.01085064560174942\n",
      "step: 691778, loss: 0.05883514881134033, data time: 0.010581016540527344\n",
      "step: 691779, loss: 0.06813357770442963, data time: 0.010326539768892177\n",
      "step: 691780, loss: 0.060566987842321396, data time: 0.010084778921944754\n",
      "step: 691781, loss: 0.06204447150230408, data time: 0.009854959117041694\n",
      "step: 691782, loss: 0.07003419101238251, data time: 0.00963856722857501\n",
      "step: 691783, loss: 0.0587417408823967, data time: 0.009438169629950272\n",
      "step: 691784, loss: 0.05895375460386276, data time: 0.009246147595919095\n",
      "step: 691785, loss: 0.06957633793354034, data time: 0.009068310260772705\n",
      "step: 691786, loss: 0.06507927924394608, data time: 0.26129984855651855\n",
      "step: 691787, loss: 0.06264771521091461, data time: 0.13147306442260742\n",
      "step: 691788, loss: 0.07051794230937958, data time: 0.08840107917785645\n",
      "step: 691789, loss: 0.05580657348036766, data time: 0.06721794605255127\n",
      "step: 691790, loss: 0.058771487325429916, data time: 0.05408997535705566\n",
      "step: 691791, loss: 0.059910595417022705, data time: 0.04534629980723063\n",
      "step: 691792, loss: 0.06598573178052902, data time: 0.03912690707615444\n",
      "step: 691793, loss: 0.06981424242258072, data time: 0.03459176421165466\n",
      "step: 691794, loss: 0.06597869098186493, data time: 0.030921194288465712\n",
      "step: 691795, loss: 0.06328685581684113, data time: 0.028071117401123048\n",
      "step: 691796, loss: 0.06098206341266632, data time: 0.025750030170787464\n",
      "step: 691797, loss: 0.06107020378112793, data time: 0.02381265163421631\n",
      "step: 691798, loss: 0.06622124463319778, data time: 0.022169168178851787\n",
      "step: 691799, loss: 0.05707865580916405, data time: 0.020754132952008928\n",
      "step: 691800, loss: 0.060134731233119965, data time: 0.019530200958251955\n",
      "step: 691801, loss: 0.0639493316411972, data time: 0.01846100389957428\n",
      "step: 691802, loss: 0.05699344724416733, data time: 0.017514284919289982\n",
      "step: 691803, loss: 0.0671638697385788, data time: 0.016671048270331487\n",
      "step: 691804, loss: 0.06020820140838623, data time: 0.01592018729762027\n",
      "step: 691805, loss: 0.06504533439874649, data time: 0.015250825881958007\n",
      "step: 691806, loss: 0.06423182785511017, data time: 0.014644475210280646\n",
      "step: 691807, loss: 0.055961571633815765, data time: 0.014075084166093306\n",
      "step: 691808, loss: 0.06256231665611267, data time: 0.013552126677139946\n",
      "step: 691809, loss: 0.061636194586753845, data time: 0.013069709142049154\n",
      "step: 691810, loss: 0.06193750724196434, data time: 0.012627944946289063\n",
      "step: 691811, loss: 0.05481798201799393, data time: 0.012220272651085487\n",
      "step: 691812, loss: 0.06187906488776207, data time: 0.01185598196806731\n",
      "step: 691813, loss: 0.06413495540618896, data time: 0.011517941951751709\n",
      "step: 691814, loss: 0.06057192385196686, data time: 0.011205747209746262\n",
      "step: 691815, loss: 0.06608054041862488, data time: 0.010915915171305338\n",
      "step: 691816, loss: 0.05776947736740112, data time: 0.01064192864202684\n",
      "step: 691817, loss: 0.0567815899848938, data time: 0.01038767397403717\n",
      "step: 691818, loss: 0.06321539729833603, data time: 0.010133967255101059\n",
      "step: 691819, loss: 0.05782010406255722, data time: 0.009897449437309714\n",
      "step: 691820, loss: 0.06448179483413696, data time: 0.009673561368669783\n",
      "step: 691821, loss: 0.059816136956214905, data time: 0.009460647900899252\n",
      "step: 691822, loss: 0.06320946663618088, data time: 0.009260151837323163\n",
      "step: 691823, loss: 0.06378713250160217, data time: 0.009073439397309957\n",
      "step: 691824, loss: 0.06029125675559044, data time: 0.008894950915605594\n",
      "step: 691825, loss: 0.055234551429748535, data time: 0.008724391460418701\n",
      "step: 691826, loss: 0.06186513230204582, data time: 0.25958728790283203\n",
      "step: 691827, loss: 0.06090174615383148, data time: 0.1305539608001709\n",
      "step: 691828, loss: 0.06148291379213333, data time: 0.08756430943806966\n",
      "step: 691829, loss: 0.0638241171836853, data time: 0.06650573015213013\n",
      "step: 691830, loss: 0.059643298387527466, data time: 0.05348491668701172\n",
      "step: 691831, loss: 0.06099351495504379, data time: 0.044833103815714516\n",
      "step: 691832, loss: 0.06886225938796997, data time: 0.0386481625693185\n",
      "step: 691833, loss: 0.06124114245176315, data time: 0.03407260775566101\n",
      "step: 691834, loss: 0.06466294080018997, data time: 0.03043389320373535\n",
      "step: 691835, loss: 0.0611085370182991, data time: 0.027587628364562987\n",
      "step: 691836, loss: 0.06031153351068497, data time: 0.02527542547746138\n",
      "step: 691837, loss: 0.06464139372110367, data time: 0.023346503575642902\n",
      "step: 691838, loss: 0.06107422336935997, data time: 0.02171045083266038\n",
      "step: 691839, loss: 0.06465068459510803, data time: 0.020304151943751743\n",
      "step: 691840, loss: 0.0619104765355587, data time: 0.019084072113037108\n",
      "step: 691841, loss: 0.06418321281671524, data time: 0.018014952540397644\n",
      "step: 691842, loss: 0.06199637055397034, data time: 0.017070952583761775\n",
      "step: 691843, loss: 0.05482702702283859, data time: 0.016227775149875216\n",
      "step: 691844, loss: 0.059362880885601044, data time: 0.015477318512765985\n",
      "step: 691845, loss: 0.06451424956321716, data time: 0.014811718463897705\n",
      "step: 691846, loss: 0.060792043805122375, data time: 0.014211166472662063\n",
      "step: 691847, loss: 0.06338101625442505, data time: 0.013664668256586248\n",
      "step: 691848, loss: 0.06205616518855095, data time: 0.013157751249230427\n",
      "step: 691849, loss: 0.06401951611042023, data time: 0.01269282897313436\n",
      "step: 691850, loss: 0.06120198220014572, data time: 0.012264537811279296\n",
      "step: 691851, loss: 0.057340919971466064, data time: 0.01186991654909574\n",
      "step: 691852, loss: 0.06452994048595428, data time: 0.011503943690547237\n",
      "step: 691853, loss: 0.063827283680439, data time: 0.011173299380711146\n",
      "step: 691854, loss: 0.06479550898075104, data time: 0.01086412627121498\n",
      "step: 691855, loss: 0.06764774024486542, data time: 0.010571908950805665\n",
      "step: 691856, loss: 0.05856914445757866, data time: 0.010298067523587135\n",
      "step: 691857, loss: 0.06353934109210968, data time: 0.010054580867290497\n",
      "step: 691858, loss: 0.06423817574977875, data time: 0.00980697978626598\n",
      "step: 691859, loss: 0.0648791640996933, data time: 0.009575549293966854\n",
      "step: 691860, loss: 0.06447267532348633, data time: 0.009356791632516043\n",
      "step: 691861, loss: 0.06191202625632286, data time: 0.009147571192847358\n",
      "step: 691862, loss: 0.054865628480911255, data time: 0.008951760627127983\n",
      "step: 691863, loss: 0.06318658590316772, data time: 0.008770905042949476\n",
      "step: 691864, loss: 0.06005703657865524, data time: 0.008598034198467549\n",
      "step: 691865, loss: 0.05642782151699066, data time: 0.008431714773178101\n",
      "step: 691866, loss: 0.06935402750968933, data time: 0.25247836112976074\n",
      "step: 691867, loss: 0.05966727063059807, data time: 0.1273719072341919\n",
      "step: 691868, loss: 0.06128591299057007, data time: 0.08606123924255371\n",
      "step: 691869, loss: 0.05922957509756088, data time: 0.06528842449188232\n",
      "step: 691870, loss: 0.0615876242518425, data time: 0.05249719619750977\n",
      "step: 691871, loss: 0.06769319623708725, data time: 0.04399001598358154\n",
      "step: 691872, loss: 0.06265859305858612, data time: 0.03790668078831264\n",
      "step: 691873, loss: 0.07375749945640564, data time: 0.03342607617378235\n",
      "step: 691874, loss: 0.059743862599134445, data time: 0.02985768847995334\n",
      "step: 691875, loss: 0.06343885511159897, data time: 0.02708413600921631\n",
      "step: 691876, loss: 0.06167950853705406, data time: 0.024813630364157936\n",
      "step: 691877, loss: 0.05361291021108627, data time: 0.022924562295277912\n",
      "step: 691878, loss: 0.06242683529853821, data time: 0.021323497478778545\n",
      "step: 691879, loss: 0.06054608151316643, data time: 0.019936885152544295\n",
      "step: 691880, loss: 0.05838637053966522, data time: 0.018741019566853843\n",
      "step: 691881, loss: 0.06608771532773972, data time: 0.01769450306892395\n",
      "step: 691882, loss: 0.061743270605802536, data time: 0.016770951888140512\n",
      "step: 691883, loss: 0.059932153671979904, data time: 0.015946507453918457\n",
      "step: 691884, loss: 0.056181132793426514, data time: 0.01521502043071546\n",
      "step: 691885, loss: 0.06259655952453613, data time: 0.01456061601638794\n",
      "step: 691886, loss: 0.06183585524559021, data time: 0.013966969081333705\n",
      "step: 691887, loss: 0.05649763345718384, data time: 0.013426141305403276\n",
      "step: 691888, loss: 0.06477507948875427, data time: 0.012927055358886719\n",
      "step: 691889, loss: 0.06209047883749008, data time: 0.012473344802856445\n",
      "step: 691890, loss: 0.06004394218325615, data time: 0.012057390213012695\n",
      "step: 691891, loss: 0.06378738582134247, data time: 0.011671139643742489\n",
      "step: 691892, loss: 0.06053844839334488, data time: 0.011309279335869683\n",
      "step: 691893, loss: 0.061698418110609055, data time: 0.010975837707519531\n",
      "step: 691894, loss: 0.06031282991170883, data time: 0.010671122320767107\n",
      "step: 691895, loss: 0.06042658910155296, data time: 0.010388549168904622\n",
      "step: 691896, loss: 0.06391589343547821, data time: 0.010124098870062059\n",
      "step: 691897, loss: 0.06124075502157211, data time: 0.009878762066364288\n",
      "step: 691898, loss: 0.06273870915174484, data time: 0.009636777820009174\n",
      "step: 691899, loss: 0.058785416185855865, data time: 0.00940840384539436\n",
      "step: 691900, loss: 0.06444296985864639, data time: 0.009193229675292968\n",
      "step: 691901, loss: 0.06045261025428772, data time: 0.008988142013549805\n",
      "step: 691902, loss: 0.05799923464655876, data time: 0.00879806441229743\n",
      "step: 691903, loss: 0.06001416966319084, data time: 0.008621811866760254\n",
      "step: 691904, loss: 0.05726485699415207, data time: 0.008454640706380209\n",
      "step: 691905, loss: 0.04425119608640671, data time: 0.008295613527297973\n",
      "step: 691906, loss: 0.05780540034174919, data time: 0.25482773780822754\n",
      "step: 691907, loss: 0.060865193605422974, data time: 0.12931513786315918\n",
      "step: 691908, loss: 0.06833137571811676, data time: 0.08672364552815755\n",
      "step: 691909, loss: 0.06423676013946533, data time: 0.06581300497055054\n",
      "step: 691910, loss: 0.06411044299602509, data time: 0.052917098999023436\n",
      "step: 691911, loss: 0.05978585407137871, data time: 0.04432948430379232\n",
      "step: 691912, loss: 0.05558756738901138, data time: 0.03821611404418945\n",
      "step: 691913, loss: 0.06002524122595787, data time: 0.03369849920272827\n",
      "step: 691914, loss: 0.06447768211364746, data time: 0.030100398593478732\n",
      "step: 691915, loss: 0.06936106085777283, data time: 0.027293014526367187\n",
      "step: 691916, loss: 0.06445662677288055, data time: 0.02500661936673251\n",
      "step: 691917, loss: 0.057475797832012177, data time: 0.023102939128875732\n",
      "step: 691918, loss: 0.05996248871088028, data time: 0.021495929131141074\n",
      "step: 691919, loss: 0.05973563715815544, data time: 0.020108393260410855\n",
      "step: 691920, loss: 0.06444483995437622, data time: 0.018934313456217447\n",
      "step: 691921, loss: 0.06463995575904846, data time: 0.01787593960762024\n",
      "step: 691922, loss: 0.05975016951560974, data time: 0.016946862725650564\n",
      "step: 691923, loss: 0.05933284014463425, data time: 0.016110844082302518\n",
      "step: 691924, loss: 0.06000130623579025, data time: 0.015369427831549393\n",
      "step: 691925, loss: 0.058674201369285583, data time: 0.014727592468261719\n",
      "step: 691926, loss: 0.06402802467346191, data time: 0.014137756256830124\n",
      "step: 691927, loss: 0.058911554515361786, data time: 0.013595310124483976\n",
      "step: 691928, loss: 0.07278110831975937, data time: 0.013089376947154169\n",
      "step: 691929, loss: 0.05730050057172775, data time: 0.012628555297851562\n",
      "step: 691930, loss: 0.06224862486124039, data time: 0.012205781936645508\n",
      "step: 691931, loss: 0.061812661588191986, data time: 0.011813548895028921\n",
      "step: 691932, loss: 0.06391653418540955, data time: 0.011447968306364837\n",
      "step: 691933, loss: 0.061722200363874435, data time: 0.011115218911852156\n",
      "step: 691934, loss: 0.06452488154172897, data time: 0.010810802722799367\n",
      "step: 691935, loss: 0.05498329922556877, data time: 0.010522786776224773\n",
      "step: 691936, loss: 0.06215662881731987, data time: 0.010254529214674426\n",
      "step: 691937, loss: 0.06285896897315979, data time: 0.010004080832004547\n",
      "step: 691938, loss: 0.05719117820262909, data time: 0.009759967977350409\n",
      "step: 691939, loss: 0.055863749235868454, data time: 0.009532781208262724\n",
      "step: 691940, loss: 0.059548478573560715, data time: 0.009318712779453822\n",
      "step: 691941, loss: 0.06382158398628235, data time: 0.009113437599605985\n",
      "step: 691942, loss: 0.06527063250541687, data time: 0.008923337266251847\n",
      "step: 691943, loss: 0.06471353024244308, data time: 0.008746824766460218\n",
      "step: 691944, loss: 0.05859827995300293, data time: 0.008580165031628732\n",
      "step: 691945, loss: 0.07809331268072128, data time: 0.008420723676681518\n",
      "step: 691946, loss: 0.06234735995531082, data time: 0.27271103858947754\n",
      "step: 691947, loss: 0.06284146755933762, data time: 0.13770747184753418\n",
      "step: 691948, loss: 0.05736442655324936, data time: 0.09284710884094238\n",
      "step: 691949, loss: 0.06198585033416748, data time: 0.07036280632019043\n",
      "step: 691950, loss: 0.0597987174987793, data time: 0.056581974029541016\n",
      "step: 691951, loss: 0.06389403343200684, data time: 0.0473858912785848\n",
      "step: 691952, loss: 0.0618872344493866, data time: 0.04081899779183524\n",
      "step: 691953, loss: 0.05435296520590782, data time: 0.0359727144241333\n",
      "step: 691954, loss: 0.0603865347802639, data time: 0.032119406594170466\n",
      "step: 691955, loss: 0.0647476390004158, data time: 0.029105377197265626\n",
      "step: 691956, loss: 0.06729365885257721, data time: 0.02665166421370073\n",
      "step: 691957, loss: 0.0686538815498352, data time: 0.024606247742970783\n",
      "step: 691958, loss: 0.06669800728559494, data time: 0.022887670076810397\n",
      "step: 691959, loss: 0.06121717020869255, data time: 0.021408353533063616\n",
      "step: 691960, loss: 0.06282563507556915, data time: 0.020116297403971355\n",
      "step: 691961, loss: 0.05719243735074997, data time: 0.018991366028785706\n",
      "step: 691962, loss: 0.058827951550483704, data time: 0.017994403839111328\n",
      "step: 691963, loss: 0.06700500845909119, data time: 0.01710375150044759\n",
      "step: 691964, loss: 0.06703781336545944, data time: 0.016308081777472245\n",
      "step: 691965, loss: 0.05531921982765198, data time: 0.015598702430725097\n",
      "step: 691966, loss: 0.06703951954841614, data time: 0.014956655956449964\n",
      "step: 691967, loss: 0.06352832913398743, data time: 0.01437609845941717\n",
      "step: 691968, loss: 0.06026680767536163, data time: 0.013836715532385784\n",
      "step: 691969, loss: 0.05467686057090759, data time: 0.013342867294947306\n",
      "step: 691970, loss: 0.060422640293836594, data time: 0.01288914680480957\n",
      "step: 691971, loss: 0.06324291974306107, data time: 0.012473381482637845\n",
      "step: 691972, loss: 0.06826190650463104, data time: 0.012082682715521919\n",
      "step: 691973, loss: 0.05411389470100403, data time: 0.011722402913229806\n",
      "step: 691974, loss: 0.06152290105819702, data time: 0.011391730144106108\n",
      "step: 691975, loss: 0.06015849858522415, data time: 0.0110961119333903\n",
      "step: 691976, loss: 0.06302550435066223, data time: 0.010819642774520381\n",
      "step: 691977, loss: 0.06233621761202812, data time: 0.010562188923358917\n",
      "step: 691978, loss: 0.05930013954639435, data time: 0.01030294100443522\n",
      "step: 691979, loss: 0.05958018824458122, data time: 0.01005842405207017\n",
      "step: 691980, loss: 0.06263582408428192, data time: 0.009828622000558035\n",
      "step: 691981, loss: 0.05773068591952324, data time: 0.009608911143408881\n",
      "step: 691982, loss: 0.06707412004470825, data time: 0.009402642378935943\n",
      "step: 691983, loss: 0.06108012795448303, data time: 0.00921136454532021\n",
      "step: 691984, loss: 0.055943191051483154, data time: 0.00903023817600348\n",
      "step: 691985, loss: 0.04659473896026611, data time: 0.008859509229660034\n",
      "step: 691986, loss: 0.06261478364467621, data time: 0.2570993900299072\n",
      "step: 691987, loss: 0.06511929631233215, data time: 0.13033723831176758\n",
      "step: 691988, loss: 0.06266053766012192, data time: 0.08744366963704427\n",
      "step: 691989, loss: 0.06825588643550873, data time: 0.066353440284729\n",
      "step: 691990, loss: 0.06683450937271118, data time: 0.05337209701538086\n",
      "step: 691991, loss: 0.0666409432888031, data time: 0.044709483782450356\n",
      "step: 691992, loss: 0.057690441608428955, data time: 0.0385195187159947\n",
      "step: 691993, loss: 0.06521749496459961, data time: 0.0339585542678833\n",
      "step: 691994, loss: 0.06420909613370895, data time: 0.030333677927652996\n",
      "step: 691995, loss: 0.07137268781661987, data time: 0.027511858940124513\n",
      "step: 691996, loss: 0.067511186003685, data time: 0.025204398415305397\n",
      "step: 691997, loss: 0.06909441947937012, data time: 0.02328006426493327\n",
      "step: 691998, loss: 0.057726822793483734, data time: 0.021654294087336615\n",
      "step: 691999, loss: 0.061886049807071686, data time: 0.02025095054081508\n",
      "step: 692000, loss: 0.06631306558847427, data time: 0.01903835932413737\n",
      "step: 692001, loss: 0.060983698815107346, data time: 0.01797601580619812\n",
      "step: 692002, loss: 0.060287751257419586, data time: 0.01704223015729119\n",
      "step: 692003, loss: 0.05746224522590637, data time: 0.01620634396870931\n",
      "step: 692004, loss: 0.06437075883150101, data time: 0.015463276913291529\n",
      "step: 692005, loss: 0.06902091205120087, data time: 0.014797234535217285\n",
      "step: 692006, loss: 0.06644114851951599, data time: 0.014193478084745862\n",
      "step: 692007, loss: 0.05225010961294174, data time: 0.013644576072692871\n",
      "step: 692008, loss: 0.06551089137792587, data time: 0.013143840043441109\n",
      "step: 692009, loss: 0.060839056968688965, data time: 0.01268260677655538\n",
      "step: 692010, loss: 0.057611189782619476, data time: 0.012256546020507813\n",
      "step: 692011, loss: 0.059381090104579926, data time: 0.011863415057842549\n",
      "step: 692012, loss: 0.05993714928627014, data time: 0.011500729454888238\n",
      "step: 692013, loss: 0.06254704296588898, data time: 0.011161455086299352\n",
      "step: 692014, loss: 0.05349231883883476, data time: 0.010851021470694706\n",
      "step: 692015, loss: 0.06639668345451355, data time: 0.010566147168477376\n",
      "step: 692016, loss: 0.058550916612148285, data time: 0.010294214371711977\n",
      "step: 692017, loss: 0.05358613654971123, data time: 0.010042205452919006\n",
      "step: 692018, loss: 0.06421103328466415, data time: 0.00979595473318389\n",
      "step: 692019, loss: 0.06577523797750473, data time: 0.009564042091369629\n",
      "step: 692020, loss: 0.05918588861823082, data time: 0.009344979694911411\n",
      "step: 692021, loss: 0.06433112174272537, data time: 0.009139127201504178\n",
      "step: 692022, loss: 0.061056867241859436, data time: 0.008942926252210463\n",
      "step: 692023, loss: 0.060229524970054626, data time: 0.008761851411116751\n",
      "step: 692024, loss: 0.055033475160598755, data time: 0.008590398690639397\n",
      "step: 692025, loss: 0.058719418942928314, data time: 0.008426207304000854\n",
      "step: 692026, loss: 0.06658295542001724, data time: 0.263491153717041\n",
      "step: 692027, loss: 0.06126970052719116, data time: 0.13249468803405762\n",
      "step: 692028, loss: 0.0571967214345932, data time: 0.08924055099487305\n",
      "step: 692029, loss: 0.0630098432302475, data time: 0.06761938333511353\n",
      "step: 692030, loss: 0.06573627889156342, data time: 0.05437474250793457\n",
      "step: 692031, loss: 0.060626477003097534, data time: 0.045561909675598145\n",
      "step: 692032, loss: 0.05945894867181778, data time: 0.03925865037100656\n",
      "step: 692033, loss: 0.06220446154475212, data time: 0.03461688756942749\n",
      "step: 692034, loss: 0.060202475637197495, data time: 0.030996322631835938\n",
      "step: 692035, loss: 0.06195260211825371, data time: 0.02809412479400635\n",
      "step: 692036, loss: 0.05623043701052666, data time: 0.025736743753606624\n",
      "step: 692037, loss: 0.05905679985880852, data time: 0.02377420663833618\n",
      "step: 692038, loss: 0.0633895993232727, data time: 0.022109050017136794\n",
      "step: 692039, loss: 0.06610240042209625, data time: 0.02067300251552037\n",
      "step: 692040, loss: 0.05941428989171982, data time: 0.019437662760416665\n",
      "step: 692041, loss: 0.06423945724964142, data time: 0.01835326850414276\n",
      "step: 692042, loss: 0.06791777908802032, data time: 0.017395524417652804\n",
      "step: 692043, loss: 0.06374483555555344, data time: 0.01654562685224745\n",
      "step: 692044, loss: 0.0620843879878521, data time: 0.015780223043341386\n",
      "step: 692045, loss: 0.05761885270476341, data time: 0.01510215997695923\n",
      "step: 692046, loss: 0.061291661113500595, data time: 0.014488038562593005\n",
      "step: 692047, loss: 0.0653543472290039, data time: 0.01394819129597057\n",
      "step: 692048, loss: 0.06295157968997955, data time: 0.013435343037480894\n",
      "step: 692049, loss: 0.06644756346940994, data time: 0.012963096300760904\n",
      "step: 692050, loss: 0.06380070745944977, data time: 0.012527484893798829\n",
      "step: 692051, loss: 0.06344198435544968, data time: 0.012123603087205153\n",
      "step: 692052, loss: 0.0582771860063076, data time: 0.011749956342909072\n",
      "step: 692053, loss: 0.05809581279754639, data time: 0.011401559625353132\n",
      "step: 692054, loss: 0.057959847152233124, data time: 0.011089982657596982\n",
      "step: 692055, loss: 0.06949494779109955, data time: 0.010795887311299641\n",
      "step: 692056, loss: 0.05874805897474289, data time: 0.010516751197076613\n",
      "step: 692057, loss: 0.05797038599848747, data time: 0.010258056223392487\n",
      "step: 692058, loss: 0.062182407826185226, data time: 0.01000480218367143\n",
      "step: 692059, loss: 0.06238316744565964, data time: 0.009766711908228257\n",
      "step: 692060, loss: 0.06436647474765778, data time: 0.009546293531145368\n",
      "step: 692061, loss: 0.05682177469134331, data time: 0.00933507415983412\n",
      "step: 692062, loss: 0.06586669385433197, data time: 0.009136895875673037\n",
      "step: 692063, loss: 0.06170200556516647, data time: 0.008950484426398026\n",
      "step: 692064, loss: 0.06673096120357513, data time: 0.008773449139717298\n",
      "step: 692065, loss: 0.08622288703918457, data time: 0.008608251810073853\n",
      "step: 692066, loss: 0.06398824602365494, data time: 0.2638130187988281\n",
      "step: 692067, loss: 0.058857887983322144, data time: 0.13273799419403076\n",
      "step: 692068, loss: 0.06034206598997116, data time: 0.08940013249715169\n",
      "step: 692069, loss: 0.062155626714229584, data time: 0.06782132387161255\n",
      "step: 692070, loss: 0.05733611434698105, data time: 0.05455074310302734\n",
      "step: 692071, loss: 0.05857197940349579, data time: 0.04568771521250407\n",
      "step: 692072, loss: 0.06548434495925903, data time: 0.03936546189444406\n",
      "step: 692073, loss: 0.0596483014523983, data time: 0.03469863533973694\n",
      "step: 692074, loss: 0.06959941983222961, data time: 0.03099015023973253\n",
      "step: 692075, loss: 0.060949280858039856, data time: 0.028090238571166992\n",
      "step: 692076, loss: 0.06702425330877304, data time: 0.025727207010442562\n",
      "step: 692077, loss: 0.06423159688711166, data time: 0.0237656037012736\n",
      "step: 692078, loss: 0.06900760531425476, data time: 0.022104905201838568\n",
      "step: 692079, loss: 0.06666950136423111, data time: 0.02067112922668457\n",
      "step: 692080, loss: 0.06477178633213043, data time: 0.019434595108032228\n",
      "step: 692081, loss: 0.06004580110311508, data time: 0.01835593581199646\n",
      "step: 692082, loss: 0.06654587388038635, data time: 0.017400390961590934\n",
      "step: 692083, loss: 0.05098138377070427, data time: 0.016540792253282335\n",
      "step: 692084, loss: 0.05852046236395836, data time: 0.01577649618449964\n",
      "step: 692085, loss: 0.06210457906126976, data time: 0.015101635456085205\n",
      "step: 692086, loss: 0.06575243920087814, data time: 0.014491592134748186\n",
      "step: 692087, loss: 0.06706944108009338, data time: 0.01393050497228449\n",
      "step: 692088, loss: 0.05791611969470978, data time: 0.013412921325020168\n",
      "step: 692089, loss: 0.05744142457842827, data time: 0.012938827276229858\n",
      "step: 692090, loss: 0.05948127806186676, data time: 0.012507591247558594\n",
      "step: 692091, loss: 0.062243085354566574, data time: 0.012104997268089881\n",
      "step: 692092, loss: 0.059905748814344406, data time: 0.011728887204770689\n",
      "step: 692093, loss: 0.06049291417002678, data time: 0.011382435049329485\n",
      "step: 692094, loss: 0.06444165110588074, data time: 0.01106490760013975\n",
      "step: 692095, loss: 0.060780249536037445, data time: 0.010767404238382976\n",
      "step: 692096, loss: 0.061385802924633026, data time: 0.010488779314102666\n",
      "step: 692097, loss: 0.06377702951431274, data time: 0.010231338441371918\n",
      "step: 692098, loss: 0.0626213476061821, data time: 0.009978857907381926\n",
      "step: 692099, loss: 0.06615126132965088, data time: 0.009742344126981847\n",
      "step: 692100, loss: 0.06240415573120117, data time: 0.009518793651035853\n",
      "step: 692101, loss: 0.061190858483314514, data time: 0.009305735429128012\n",
      "step: 692102, loss: 0.05772878974676132, data time: 0.009105211979634053\n",
      "step: 692103, loss: 0.06161540001630783, data time: 0.008920211541025262\n",
      "step: 692104, loss: 0.06196766719222069, data time: 0.008744637171427408\n",
      "step: 692105, loss: 0.05953272804617882, data time: 0.008580976724624633\n",
      "step: 692106, loss: 0.061084046959877014, data time: 0.2710607051849365\n",
      "step: 692107, loss: 0.058467429131269455, data time: 0.13713526725769043\n",
      "step: 692108, loss: 0.05940493196249008, data time: 0.09192466735839844\n",
      "step: 692109, loss: 0.0625738650560379, data time: 0.06972301006317139\n",
      "step: 692110, loss: 0.05918813496828079, data time: 0.056068944931030276\n",
      "step: 692111, loss: 0.06284303963184357, data time: 0.04696885744730631\n",
      "step: 692112, loss: 0.06217307969927788, data time: 0.04045275279453823\n",
      "step: 692113, loss: 0.06098892539739609, data time: 0.03566044569015503\n",
      "step: 692114, loss: 0.06395977735519409, data time: 0.03184448348151313\n",
      "step: 692115, loss: 0.06568828225135803, data time: 0.028859472274780272\n",
      "step: 692116, loss: 0.0550682470202446, data time: 0.026437044143676758\n",
      "step: 692117, loss: 0.05417598411440849, data time: 0.024410525957743328\n",
      "step: 692118, loss: 0.06086638569831848, data time: 0.022700126354510967\n",
      "step: 692119, loss: 0.06868763267993927, data time: 0.02122422627040318\n",
      "step: 692120, loss: 0.06509137153625488, data time: 0.019944190979003906\n",
      "step: 692121, loss: 0.06132187694311142, data time: 0.018823891878128052\n",
      "step: 692122, loss: 0.06043040007352829, data time: 0.017835574991562787\n",
      "step: 692123, loss: 0.061453163623809814, data time: 0.016952978240119085\n",
      "step: 692124, loss: 0.06367968022823334, data time: 0.016169748808208265\n",
      "step: 692125, loss: 0.05350576341152191, data time: 0.015475177764892578\n",
      "step: 692126, loss: 0.0599873885512352, data time: 0.01485300064086914\n",
      "step: 692127, loss: 0.06535515934228897, data time: 0.014278119260614569\n",
      "step: 692128, loss: 0.07109080255031586, data time: 0.013745929883873981\n",
      "step: 692129, loss: 0.061757590621709824, data time: 0.013259013493855795\n",
      "step: 692130, loss: 0.058684855699539185, data time: 0.012808666229248047\n",
      "step: 692131, loss: 0.05660080537199974, data time: 0.012394364063556377\n",
      "step: 692132, loss: 0.06604323536157608, data time: 0.012006353448938441\n",
      "step: 692133, loss: 0.06231391429901123, data time: 0.011652273791176932\n",
      "step: 692134, loss: 0.06426545232534409, data time: 0.011324940056636416\n",
      "step: 692135, loss: 0.0686645582318306, data time: 0.011023847262064616\n",
      "step: 692136, loss: 0.06810718029737473, data time: 0.010739411077191752\n",
      "step: 692137, loss: 0.059597305953502655, data time: 0.010476745665073395\n",
      "step: 692138, loss: 0.05833005905151367, data time: 0.01021721868804007\n",
      "step: 692139, loss: 0.06339670717716217, data time: 0.009971850058611701\n",
      "step: 692140, loss: 0.06310170143842697, data time: 0.009742457526070732\n",
      "step: 692141, loss: 0.05863533169031143, data time: 0.00952397452460395\n",
      "step: 692142, loss: 0.06140568107366562, data time: 0.00932076815012339\n",
      "step: 692143, loss: 0.061424996703863144, data time: 0.009128470169870477\n",
      "step: 692144, loss: 0.060045015066862106, data time: 0.008949065819764748\n",
      "step: 692145, loss: 0.059859417378902435, data time: 0.008775347471237182\n",
      "step: 692146, loss: 0.06885870546102524, data time: 0.26296281814575195\n",
      "step: 692147, loss: 0.06826677918434143, data time: 0.13222801685333252\n",
      "step: 692148, loss: 0.05890512466430664, data time: 0.08865960439046223\n",
      "step: 692149, loss: 0.06031867861747742, data time: 0.06735104322433472\n",
      "step: 692150, loss: 0.06388670206069946, data time: 0.054163885116577146\n",
      "step: 692151, loss: 0.06257181614637375, data time: 0.0453795591990153\n",
      "step: 692152, loss: 0.05647137388586998, data time: 0.03909543582371303\n",
      "step: 692153, loss: 0.06280107796192169, data time: 0.034465014934539795\n",
      "step: 692154, loss: 0.06088831275701523, data time: 0.030784315533108182\n",
      "step: 692155, loss: 0.05430585891008377, data time: 0.027913236618041994\n",
      "step: 692156, loss: 0.06491527706384659, data time: 0.02560357613997026\n",
      "step: 692157, loss: 0.05721050500869751, data time: 0.02367899815241496\n",
      "step: 692158, loss: 0.06434519588947296, data time: 0.022050013908973105\n",
      "step: 692159, loss: 0.06429028511047363, data time: 0.020647338458469937\n",
      "step: 692160, loss: 0.058356307446956635, data time: 0.01943068504333496\n",
      "step: 692161, loss: 0.061763979494571686, data time: 0.01836596429347992\n",
      "step: 692162, loss: 0.06556792557239532, data time: 0.017428931067971623\n",
      "step: 692163, loss: 0.07007575780153275, data time: 0.016591191291809082\n",
      "step: 692164, loss: 0.06182128190994263, data time: 0.015840015913310804\n",
      "step: 692165, loss: 0.060109030455350876, data time: 0.015172469615936279\n",
      "step: 692166, loss: 0.06469359248876572, data time: 0.01457063357035319\n",
      "step: 692167, loss: 0.0606965534389019, data time: 0.014005552638660778\n",
      "step: 692168, loss: 0.05915076658129692, data time: 0.013485504233318827\n",
      "step: 692169, loss: 0.053747206926345825, data time: 0.013009617726008097\n",
      "step: 692170, loss: 0.059842776507139206, data time: 0.012574529647827149\n",
      "step: 692171, loss: 0.06613831222057343, data time: 0.012170864985539364\n",
      "step: 692172, loss: 0.056665368378162384, data time: 0.011793719397650825\n",
      "step: 692173, loss: 0.06719311326742172, data time: 0.011443291391645159\n",
      "step: 692174, loss: 0.06536692380905151, data time: 0.011137156650937837\n",
      "step: 692175, loss: 0.06242416426539421, data time: 0.01083701451619466\n",
      "step: 692176, loss: 0.06326533854007721, data time: 0.010556205626456969\n",
      "step: 692177, loss: 0.06526315957307816, data time: 0.010297030210494995\n",
      "step: 692178, loss: 0.06393365561962128, data time: 0.010043700536092123\n",
      "step: 692179, loss: 0.06093202531337738, data time: 0.00980688543880687\n",
      "step: 692180, loss: 0.06260895729064941, data time: 0.009582390104021345\n",
      "step: 692181, loss: 0.06057599186897278, data time: 0.009368148114946153\n",
      "step: 692182, loss: 0.06497471034526825, data time: 0.00916685929169526\n",
      "step: 692183, loss: 0.06359207630157471, data time: 0.008979069559197677\n",
      "step: 692184, loss: 0.06842374801635742, data time: 0.008802041029318785\n",
      "step: 692185, loss: 0.07329744100570679, data time: 0.008632951974868774\n",
      "step: 692186, loss: 0.06318192183971405, data time: 0.2531869411468506\n",
      "step: 692187, loss: 0.06887409090995789, data time: 0.12773215770721436\n",
      "step: 692188, loss: 0.05664355307817459, data time: 0.08637356758117676\n",
      "step: 692189, loss: 0.06290215253829956, data time: 0.06543093919754028\n",
      "step: 692190, loss: 0.06578753888607025, data time: 0.052636146545410156\n",
      "step: 692191, loss: 0.06352639943361282, data time: 0.0440978209177653\n",
      "step: 692192, loss: 0.05992802605032921, data time: 0.038015433720179966\n",
      "step: 692193, loss: 0.061373770236968994, data time: 0.033514320850372314\n",
      "step: 692194, loss: 0.06385979801416397, data time: 0.029933796988593206\n",
      "step: 692195, loss: 0.06406332552433014, data time: 0.027159929275512695\n",
      "step: 692196, loss: 0.06236603856086731, data time: 0.024884007193825462\n",
      "step: 692197, loss: 0.06541068851947784, data time: 0.022990286350250244\n",
      "step: 692198, loss: 0.06242556497454643, data time: 0.021386366624098558\n",
      "step: 692199, loss: 0.05905607342720032, data time: 0.0200061457497733\n",
      "step: 692200, loss: 0.06019413098692894, data time: 0.018809922536214194\n",
      "step: 692201, loss: 0.0634603351354599, data time: 0.017764508724212646\n",
      "step: 692202, loss: 0.05820438265800476, data time: 0.016837498720954445\n",
      "step: 692203, loss: 0.05646007880568504, data time: 0.016007714801364474\n",
      "step: 692204, loss: 0.05724161118268967, data time: 0.015270998603419253\n",
      "step: 692205, loss: 0.06316712498664856, data time: 0.014619410037994385\n",
      "step: 692206, loss: 0.05992702767252922, data time: 0.014027277628580729\n",
      "step: 692207, loss: 0.06287181377410889, data time: 0.013485106554898348\n",
      "step: 692208, loss: 0.0636172965168953, data time: 0.012992962546970533\n",
      "step: 692209, loss: 0.06237378716468811, data time: 0.012536863485972086\n",
      "step: 692210, loss: 0.06375276297330856, data time: 0.01211771011352539\n",
      "step: 692211, loss: 0.05818726122379303, data time: 0.011740418580862192\n",
      "step: 692212, loss: 0.05955752730369568, data time: 0.01137766131648311\n",
      "step: 692213, loss: 0.058743368834257126, data time: 0.011044749191829137\n",
      "step: 692214, loss: 0.06429678201675415, data time: 0.010736901184608197\n",
      "step: 692215, loss: 0.06585649400949478, data time: 0.010449433326721191\n",
      "step: 692216, loss: 0.06125196814537048, data time: 0.010181673111454133\n",
      "step: 692217, loss: 0.0638866126537323, data time: 0.00993441790342331\n",
      "step: 692218, loss: 0.06505623459815979, data time: 0.009691830837365353\n",
      "step: 692219, loss: 0.057780902832746506, data time: 0.00946348554947797\n",
      "step: 692220, loss: 0.05461999773979187, data time: 0.009247268949236189\n",
      "step: 692221, loss: 0.056589752435684204, data time: 0.009041700098249648\n",
      "step: 692222, loss: 0.06641975045204163, data time: 0.008849272856841216\n",
      "step: 692223, loss: 0.06068548560142517, data time: 0.008669639888562654\n",
      "step: 692224, loss: 0.06274785846471786, data time: 0.00850067383203751\n",
      "step: 692225, loss: 0.062354784458875656, data time: 0.008338534832000732\n",
      "step: 692226, loss: 0.06849263608455658, data time: 0.2667708396911621\n",
      "step: 692227, loss: 0.062359511852264404, data time: 0.1341770887374878\n",
      "step: 692228, loss: 0.06638213992118835, data time: 0.08999435106913249\n",
      "step: 692229, loss: 0.06235506758093834, data time: 0.06834173202514648\n",
      "step: 692230, loss: 0.05546347051858902, data time: 0.05496635437011719\n",
      "step: 692231, loss: 0.06943850964307785, data time: 0.046039978663126625\n",
      "step: 692232, loss: 0.05656723305583, data time: 0.0396683897290911\n",
      "step: 692233, loss: 0.0686216652393341, data time: 0.034976303577423096\n",
      "step: 692234, loss: 0.0626523345708847, data time: 0.031240834130181208\n",
      "step: 692235, loss: 0.057369232177734375, data time: 0.028319549560546876\n",
      "step: 692236, loss: 0.05522477626800537, data time: 0.025952035730535335\n",
      "step: 692237, loss: 0.06229765713214874, data time: 0.023962974548339844\n",
      "step: 692238, loss: 0.05687330290675163, data time: 0.022280197877150316\n",
      "step: 692239, loss: 0.06713774055242538, data time: 0.02083407129560198\n",
      "step: 692240, loss: 0.06236764416098595, data time: 0.01957850456237793\n",
      "step: 692241, loss: 0.05754656344652176, data time: 0.018481269478797913\n",
      "step: 692242, loss: 0.05896139144897461, data time: 0.017546415328979492\n",
      "step: 692243, loss: 0.06071874499320984, data time: 0.016701671812269423\n",
      "step: 692244, loss: 0.06213100999593735, data time: 0.015948885365536337\n",
      "step: 692245, loss: 0.06112540885806084, data time: 0.015278041362762451\n",
      "step: 692246, loss: 0.06493907421827316, data time: 0.01466928209577288\n",
      "step: 692247, loss: 0.06272025406360626, data time: 0.014119104905561968\n",
      "step: 692248, loss: 0.060395002365112305, data time: 0.013610010561735733\n",
      "step: 692249, loss: 0.05934520065784454, data time: 0.01314107577006022\n",
      "step: 692250, loss: 0.057146959006786346, data time: 0.012714309692382813\n",
      "step: 692251, loss: 0.056621864438056946, data time: 0.01231997746687669\n",
      "step: 692252, loss: 0.06348489969968796, data time: 0.011950272100943106\n",
      "step: 692253, loss: 0.06064627692103386, data time: 0.011608260018484933\n",
      "step: 692254, loss: 0.06437238305807114, data time: 0.011295137734248721\n",
      "step: 692255, loss: 0.06311722844839096, data time: 0.011005632082621257\n",
      "step: 692256, loss: 0.05742073059082031, data time: 0.010731366372877551\n",
      "step: 692257, loss: 0.05448645353317261, data time: 0.010476768016815186\n",
      "step: 692258, loss: 0.058357492089271545, data time: 0.010220939462835138\n",
      "step: 692259, loss: 0.06463027000427246, data time: 0.009981337715597713\n",
      "step: 692260, loss: 0.05602390691637993, data time: 0.009755870274135044\n",
      "step: 692261, loss: 0.05808679759502411, data time: 0.009539153840806749\n",
      "step: 692262, loss: 0.06596913933753967, data time: 0.00933660043252481\n",
      "step: 692263, loss: 0.062380995601415634, data time: 0.00914702289982846\n",
      "step: 692264, loss: 0.06292006373405457, data time: 0.008967454616840068\n",
      "step: 692265, loss: 0.06421028822660446, data time: 0.008796781301498413\n",
      "step: 692266, loss: 0.06128507852554321, data time: 0.26285290718078613\n",
      "step: 692267, loss: 0.06539633125066757, data time: 0.13219749927520752\n",
      "step: 692268, loss: 0.0675555020570755, data time: 0.08863973617553711\n",
      "step: 692269, loss: 0.06014954671263695, data time: 0.0672408938407898\n",
      "step: 692270, loss: 0.060163915157318115, data time: 0.054085159301757814\n",
      "step: 692271, loss: 0.0636601448059082, data time: 0.04530254999796549\n",
      "step: 692272, loss: 0.06397406756877899, data time: 0.03903429848807199\n",
      "step: 692273, loss: 0.059028856456279755, data time: 0.03441727161407471\n",
      "step: 692274, loss: 0.06281308829784393, data time: 0.0307417975531684\n",
      "step: 692275, loss: 0.06882674992084503, data time: 0.027872395515441895\n",
      "step: 692276, loss: 0.06751157343387604, data time: 0.02554373307661577\n",
      "step: 692277, loss: 0.057232242077589035, data time: 0.02359803517659505\n",
      "step: 692278, loss: 0.06583012640476227, data time: 0.021950740080613356\n",
      "step: 692279, loss: 0.06314831227064133, data time: 0.020533067839486257\n",
      "step: 692280, loss: 0.0599883571267128, data time: 0.01930238405863444\n",
      "step: 692281, loss: 0.06098754331469536, data time: 0.018223896622657776\n",
      "step: 692282, loss: 0.05944313108921051, data time: 0.017279218224918142\n",
      "step: 692283, loss: 0.06616055965423584, data time: 0.016426483790079754\n",
      "step: 692284, loss: 0.06027654930949211, data time: 0.01567002346641139\n",
      "step: 692285, loss: 0.0601855032145977, data time: 0.01499413251876831\n",
      "step: 692286, loss: 0.0664244070649147, data time: 0.014386585780552455\n",
      "step: 692287, loss: 0.0604415163397789, data time: 0.013831507075916637\n",
      "step: 692288, loss: 0.06517395377159119, data time: 0.013320943583612856\n",
      "step: 692289, loss: 0.060753773897886276, data time: 0.012850910425186157\n",
      "step: 692290, loss: 0.0616525262594223, data time: 0.012416753768920898\n",
      "step: 692291, loss: 0.059927798807621, data time: 0.012020743810213529\n",
      "step: 692292, loss: 0.0608021579682827, data time: 0.01165292881153248\n",
      "step: 692293, loss: 0.06054957956075668, data time: 0.011309691837855749\n",
      "step: 692294, loss: 0.05826806649565697, data time: 0.010993612223658068\n",
      "step: 692295, loss: 0.06157451122999191, data time: 0.010700535774230958\n",
      "step: 692296, loss: 0.062367022037506104, data time: 0.01042562146340647\n",
      "step: 692297, loss: 0.07172277569770813, data time: 0.0101751908659935\n",
      "step: 692298, loss: 0.06529098004102707, data time: 0.009929476362286192\n",
      "step: 692299, loss: 0.05664336681365967, data time: 0.009694681448094985\n",
      "step: 692300, loss: 0.06346343457698822, data time: 0.009476218904767717\n",
      "step: 692301, loss: 0.06654483079910278, data time: 0.009272469414605035\n",
      "step: 692302, loss: 0.060659270733594894, data time: 0.00907307057767301\n",
      "step: 692303, loss: 0.06170877814292908, data time: 0.00888867127267938\n",
      "step: 692304, loss: 0.06413637101650238, data time: 0.008713673322628707\n",
      "step: 692305, loss: 0.05297020077705383, data time: 0.008547723293304443\n",
      "step: 692306, loss: 0.06637129187583923, data time: 0.2708754539489746\n",
      "step: 692307, loss: 0.06057917699217796, data time: 0.13654685020446777\n",
      "step: 692308, loss: 0.06408510357141495, data time: 0.09246047337849934\n",
      "step: 692309, loss: 0.05982240289449692, data time: 0.06971794366836548\n",
      "step: 692310, loss: 0.05940711125731468, data time: 0.05605745315551758\n",
      "step: 692311, loss: 0.06054497882723808, data time: 0.04697159926096598\n",
      "step: 692312, loss: 0.05335134267807007, data time: 0.040571859904697964\n",
      "step: 692313, loss: 0.0674104318022728, data time: 0.03567424416542053\n",
      "step: 692314, loss: 0.05966001749038696, data time: 0.03187674946255154\n",
      "step: 692315, loss: 0.061512745916843414, data time: 0.028884315490722658\n",
      "step: 692316, loss: 0.06322804093360901, data time: 0.026458436792547054\n",
      "step: 692317, loss: 0.05818522721529007, data time: 0.0244293212890625\n",
      "step: 692318, loss: 0.06097792088985443, data time: 0.022717677629910983\n",
      "step: 692319, loss: 0.06255504488945007, data time: 0.021236521857125417\n",
      "step: 692320, loss: 0.05921010300517082, data time: 0.019962501525878907\n",
      "step: 692321, loss: 0.06367012113332748, data time: 0.01884375512599945\n",
      "step: 692322, loss: 0.06030842289328575, data time: 0.017851759405697092\n",
      "step: 692323, loss: 0.06045754998922348, data time: 0.01697001192304823\n",
      "step: 692324, loss: 0.05964822694659233, data time: 0.016181305835121555\n",
      "step: 692325, loss: 0.06519803404808044, data time: 0.015478384494781495\n",
      "step: 692326, loss: 0.0712064728140831, data time: 0.014843077886672247\n",
      "step: 692327, loss: 0.06429938226938248, data time: 0.014264908703890715\n",
      "step: 692328, loss: 0.0583304725587368, data time: 0.013731521108876104\n",
      "step: 692329, loss: 0.063998743891716, data time: 0.013246903816858927\n",
      "step: 692330, loss: 0.059800323098897934, data time: 0.012800273895263671\n",
      "step: 692331, loss: 0.06322935223579407, data time: 0.01238576265481802\n",
      "step: 692332, loss: 0.05760018154978752, data time: 0.012000543099862558\n",
      "step: 692333, loss: 0.062238626182079315, data time: 0.011642379420144218\n",
      "step: 692334, loss: 0.06264546513557434, data time: 0.011315937699942753\n",
      "step: 692335, loss: 0.05641218274831772, data time: 0.011010352770487468\n",
      "step: 692336, loss: 0.061767321079969406, data time: 0.010723244759344285\n",
      "step: 692337, loss: 0.06588612496852875, data time: 0.010460048913955688\n",
      "step: 692338, loss: 0.0660489946603775, data time: 0.01020346265850645\n",
      "step: 692339, loss: 0.05945289134979248, data time: 0.009960160535924575\n",
      "step: 692340, loss: 0.07012239098548889, data time: 0.009729228700910296\n",
      "step: 692341, loss: 0.06600263714790344, data time: 0.00951360993915134\n",
      "step: 692342, loss: 0.05902353674173355, data time: 0.009308338165283203\n",
      "step: 692343, loss: 0.06367851793766022, data time: 0.009116969610515394\n",
      "step: 692344, loss: 0.06141436845064163, data time: 0.008934338887532553\n",
      "step: 692345, loss: 0.08930041640996933, data time: 0.008762109279632568\n",
      "step: 692346, loss: 0.06311576813459396, data time: 0.27915382385253906\n",
      "step: 692347, loss: 0.06604766100645065, data time: 0.1403719186782837\n",
      "step: 692348, loss: 0.06439771503210068, data time: 0.09447749455769856\n",
      "step: 692349, loss: 0.06230917200446129, data time: 0.07152634859085083\n",
      "step: 692350, loss: 0.05569048225879669, data time: 0.05751681327819824\n",
      "step: 692351, loss: 0.060974977910518646, data time: 0.04818050066630045\n",
      "step: 692352, loss: 0.06252998113632202, data time: 0.04150325911385672\n",
      "step: 692353, loss: 0.06366610527038574, data time: 0.03656992316246033\n",
      "step: 692354, loss: 0.06454940885305405, data time: 0.03274276521470812\n",
      "step: 692355, loss: 0.0662623941898346, data time: 0.029667592048645018\n",
      "step: 692356, loss: 0.05864189192652702, data time: 0.027168425646695225\n",
      "step: 692357, loss: 0.06806632876396179, data time: 0.025091866652170818\n",
      "step: 692358, loss: 0.06613491475582123, data time: 0.023333989656888522\n",
      "step: 692359, loss: 0.0671449676156044, data time: 0.021815998213631765\n",
      "step: 692360, loss: 0.0604778528213501, data time: 0.020505205790201823\n",
      "step: 692361, loss: 0.061498671770095825, data time: 0.01936490833759308\n",
      "step: 692362, loss: 0.06266562640666962, data time: 0.018342186422909006\n",
      "step: 692363, loss: 0.059084683656692505, data time: 0.017434451315138075\n",
      "step: 692364, loss: 0.06512680649757385, data time: 0.016621225758602743\n",
      "step: 692365, loss: 0.05607667565345764, data time: 0.015903007984161378\n",
      "step: 692366, loss: 0.06607469171285629, data time: 0.015247344970703125\n",
      "step: 692367, loss: 0.05793340131640434, data time: 0.014650171453302557\n",
      "step: 692368, loss: 0.06281490623950958, data time: 0.014101899188497791\n",
      "step: 692369, loss: 0.05514032766222954, data time: 0.013602932294209799\n",
      "step: 692370, loss: 0.06255093216896057, data time: 0.01314004898071289\n",
      "step: 692371, loss: 0.0634385347366333, data time: 0.012712597846984863\n",
      "step: 692372, loss: 0.06269841641187668, data time: 0.012315891407154224\n",
      "step: 692373, loss: 0.06309720128774643, data time: 0.011950271470206124\n",
      "step: 692374, loss: 0.06843707710504532, data time: 0.011610433973115066\n",
      "step: 692375, loss: 0.0655265599489212, data time: 0.011295485496520995\n",
      "step: 692376, loss: 0.058430563658475876, data time: 0.011002079133064516\n",
      "step: 692377, loss: 0.05984475463628769, data time: 0.010729186236858368\n",
      "step: 692378, loss: 0.06472799181938171, data time: 0.010469321048620975\n",
      "step: 692379, loss: 0.057902853935956955, data time: 0.010218234623179716\n",
      "step: 692380, loss: 0.06137530505657196, data time: 0.009981564113071986\n",
      "step: 692381, loss: 0.05421300232410431, data time: 0.009758459197150337\n",
      "step: 692382, loss: 0.06508051604032516, data time: 0.009545332676655537\n",
      "step: 692383, loss: 0.06091444194316864, data time: 0.009346497686285721\n",
      "step: 692384, loss: 0.05909644812345505, data time: 0.009159723917643229\n",
      "step: 692385, loss: 0.06684525310993195, data time: 0.008981961011886596\n",
      "step: 692386, loss: 0.06725487112998962, data time: 0.26821279525756836\n",
      "step: 692387, loss: 0.0616493746638298, data time: 0.13486874103546143\n",
      "step: 692388, loss: 0.06264488399028778, data time: 0.09042890866597493\n",
      "step: 692389, loss: 0.05802401155233383, data time: 0.06869745254516602\n",
      "step: 692390, loss: 0.05891243740916252, data time: 0.055247163772583006\n",
      "step: 692391, loss: 0.056579675525426865, data time: 0.04628249009450277\n",
      "step: 692392, loss: 0.06657075136899948, data time: 0.03987683568681989\n",
      "step: 692393, loss: 0.05935467407107353, data time: 0.035151153802871704\n",
      "step: 692394, loss: 0.06200047954916954, data time: 0.031405634350246854\n",
      "step: 692395, loss: 0.06154407560825348, data time: 0.028465914726257324\n",
      "step: 692396, loss: 0.05546053498983383, data time: 0.026080044833096592\n",
      "step: 692397, loss: 0.05359005928039551, data time: 0.02408440907796224\n",
      "step: 692398, loss: 0.059558264911174774, data time: 0.022396637843205378\n",
      "step: 692399, loss: 0.061676692217588425, data time: 0.020947115761893138\n",
      "step: 692400, loss: 0.06390088051557541, data time: 0.01968803405761719\n",
      "step: 692401, loss: 0.0574500635266304, data time: 0.018590450286865234\n",
      "step: 692402, loss: 0.06620337814092636, data time: 0.017613607294419232\n",
      "step: 692403, loss: 0.052720487117767334, data time: 0.016750613848368328\n",
      "step: 692404, loss: 0.06420042365789413, data time: 0.015977420304950914\n",
      "step: 692405, loss: 0.06717021763324738, data time: 0.015290486812591552\n",
      "step: 692406, loss: 0.061054229736328125, data time: 0.014664547783987862\n",
      "step: 692407, loss: 0.057536184787750244, data time: 0.014094027605923739\n",
      "step: 692408, loss: 0.06826188415288925, data time: 0.01357033978337827\n",
      "step: 692409, loss: 0.06252985447645187, data time: 0.013092190027236938\n",
      "step: 692410, loss: 0.06364887207746506, data time: 0.012651891708374023\n",
      "step: 692411, loss: 0.061248861253261566, data time: 0.012244637195880596\n",
      "step: 692412, loss: 0.06111305207014084, data time: 0.011869607148347077\n",
      "step: 692413, loss: 0.06493428349494934, data time: 0.011518844536372594\n",
      "step: 692414, loss: 0.06596007198095322, data time: 0.011196004933324354\n",
      "step: 692415, loss: 0.05710074305534363, data time: 0.01089621384938558\n",
      "step: 692416, loss: 0.06279498338699341, data time: 0.01061507963365124\n",
      "step: 692417, loss: 0.06553660333156586, data time: 0.010354653000831604\n",
      "step: 692418, loss: 0.06061626598238945, data time: 0.01009862350695061\n",
      "step: 692419, loss: 0.055898066610097885, data time: 0.009861167739419377\n",
      "step: 692420, loss: 0.05819769948720932, data time: 0.009636851719447545\n",
      "step: 692421, loss: 0.06296052783727646, data time: 0.009421421421898736\n",
      "step: 692422, loss: 0.0640447586774826, data time: 0.009218989191828546\n",
      "step: 692423, loss: 0.05814134329557419, data time: 0.009029344508522436\n",
      "step: 692424, loss: 0.0630398839712143, data time: 0.008848459292680789\n",
      "step: 692425, loss: 0.07114365696907043, data time: 0.008677607774734497\n",
      "step: 692426, loss: 0.06122267246246338, data time: 0.26801228523254395\n",
      "step: 692427, loss: 0.06004001945257187, data time: 0.13579189777374268\n",
      "step: 692428, loss: 0.05992196500301361, data time: 0.09104188283284505\n",
      "step: 692429, loss: 0.06301858276128769, data time: 0.0691562294960022\n",
      "step: 692430, loss: 0.06461532413959503, data time: 0.05561075210571289\n",
      "step: 692431, loss: 0.06433188915252686, data time: 0.04658647378285726\n",
      "step: 692432, loss: 0.0640595480799675, data time: 0.040127617972237725\n",
      "step: 692433, loss: 0.06536877900362015, data time: 0.03536754846572876\n",
      "step: 692434, loss: 0.06166289746761322, data time: 0.03158850140041775\n",
      "step: 692435, loss: 0.06597515940666199, data time: 0.028629517555236815\n",
      "step: 692436, loss: 0.06724657863378525, data time: 0.026223291050304066\n",
      "step: 692437, loss: 0.06510510295629501, data time: 0.024223705132802326\n",
      "step: 692438, loss: 0.06927214562892914, data time: 0.022526135811438926\n",
      "step: 692439, loss: 0.061972104012966156, data time: 0.021067346845354353\n",
      "step: 692440, loss: 0.0655047595500946, data time: 0.01981067657470703\n",
      "step: 692441, loss: 0.06464427709579468, data time: 0.01869809627532959\n",
      "step: 692442, loss: 0.05941039323806763, data time: 0.01771710900699391\n",
      "step: 692443, loss: 0.0631025955080986, data time: 0.016838696267869737\n",
      "step: 692444, loss: 0.05820634961128235, data time: 0.016057227787218596\n",
      "step: 692445, loss: 0.061269611120224, data time: 0.015366744995117188\n",
      "step: 692446, loss: 0.06334660947322845, data time: 0.014740319479079474\n",
      "step: 692447, loss: 0.0614987313747406, data time: 0.014176574620333586\n",
      "step: 692448, loss: 0.06058952212333679, data time: 0.013646374578061312\n",
      "step: 692449, loss: 0.062302231788635254, data time: 0.013165911038716635\n",
      "step: 692450, loss: 0.06584950536489487, data time: 0.012722291946411134\n",
      "step: 692451, loss: 0.06528676301240921, data time: 0.01230934950021597\n",
      "step: 692452, loss: 0.0636090412735939, data time: 0.011926898249873408\n",
      "step: 692453, loss: 0.06244340538978577, data time: 0.011575511523655482\n",
      "step: 692454, loss: 0.06034957617521286, data time: 0.011252806104462722\n",
      "step: 692455, loss: 0.06501659005880356, data time: 0.010950390497843425\n",
      "step: 692456, loss: 0.0636696070432663, data time: 0.01066633193723617\n",
      "step: 692457, loss: 0.061948902904987335, data time: 0.010404065251350403\n",
      "step: 692458, loss: 0.0609745979309082, data time: 0.010149124896887577\n",
      "step: 692459, loss: 0.06529583781957626, data time: 0.009906376109403722\n",
      "step: 692460, loss: 0.05830826610326767, data time: 0.009679290226527623\n",
      "step: 692461, loss: 0.06449107825756073, data time: 0.009463959270053439\n",
      "step: 692462, loss: 0.06621549278497696, data time: 0.009263657234810494\n",
      "step: 692463, loss: 0.059173088520765305, data time: 0.009072880995901008\n",
      "step: 692464, loss: 0.058011822402477264, data time: 0.00889277458190918\n",
      "step: 692465, loss: 0.07907576113939285, data time: 0.008722829818725585\n",
      "step: 692466, loss: 0.05926761031150818, data time: 0.26257944107055664\n",
      "step: 692467, loss: 0.057675644755363464, data time: 0.13290369510650635\n",
      "step: 692468, loss: 0.06094808876514435, data time: 0.08924325307210286\n",
      "step: 692469, loss: 0.059242285788059235, data time: 0.06811010837554932\n",
      "step: 692470, loss: 0.060684286057949066, data time: 0.05481047630310058\n",
      "step: 692471, loss: 0.05665455386042595, data time: 0.04595645268758138\n",
      "step: 692472, loss: 0.05925814434885979, data time: 0.039635147367204936\n",
      "step: 692473, loss: 0.060836680233478546, data time: 0.034973353147506714\n",
      "step: 692474, loss: 0.06405384093523026, data time: 0.03125969568888346\n",
      "step: 692475, loss: 0.05803118646144867, data time: 0.02837636470794678\n",
      "step: 692476, loss: 0.06489899754524231, data time: 0.026033314791592686\n",
      "step: 692477, loss: 0.0653543770313263, data time: 0.02407282590866089\n",
      "step: 692478, loss: 0.058569662272930145, data time: 0.022414262478168193\n",
      "step: 692479, loss: 0.0662517249584198, data time: 0.020984666688101634\n",
      "step: 692480, loss: 0.05746326968073845, data time: 0.019746780395507812\n",
      "step: 692481, loss: 0.05750589072704315, data time: 0.01866552233695984\n",
      "step: 692482, loss: 0.06198146194219589, data time: 0.017706085653866038\n",
      "step: 692483, loss: 0.06118824705481529, data time: 0.016850855615403917\n",
      "step: 692484, loss: 0.06287088990211487, data time: 0.016091120870489823\n",
      "step: 692485, loss: 0.06413809955120087, data time: 0.015415680408477784\n",
      "step: 692486, loss: 0.06240364909172058, data time: 0.01480465843563988\n",
      "step: 692487, loss: 0.06522300839424133, data time: 0.014240459962324663\n",
      "step: 692488, loss: 0.0671824961900711, data time: 0.013729904008948284\n",
      "step: 692489, loss: 0.06196527183055878, data time: 0.013263126214345297\n",
      "step: 692490, loss: 0.061752282083034515, data time: 0.012817535400390625\n",
      "step: 692491, loss: 0.06578923761844635, data time: 0.012402901282677284\n",
      "step: 692492, loss: 0.0636637955904007, data time: 0.012016084459092882\n",
      "step: 692493, loss: 0.05877351760864258, data time: 0.01165926456451416\n",
      "step: 692494, loss: 0.0649811178445816, data time: 0.01134394777232203\n",
      "step: 692495, loss: 0.06240648031234741, data time: 0.011049501101175944\n",
      "step: 692496, loss: 0.06387590616941452, data time: 0.010773743352582377\n",
      "step: 692497, loss: 0.05781326815485954, data time: 0.01051931083202362\n",
      "step: 692498, loss: 0.05873764306306839, data time: 0.010264216047344786\n",
      "step: 692499, loss: 0.06046765297651291, data time: 0.01002494727863985\n",
      "step: 692500, loss: 0.06550394743680954, data time: 0.009797940935407366\n",
      "step: 692501, loss: 0.06403156369924545, data time: 0.009580565823449029\n",
      "step: 692502, loss: 0.06954032182693481, data time: 0.00937721535966203\n",
      "step: 692503, loss: 0.06677137315273285, data time: 0.009186167466013055\n",
      "step: 692504, loss: 0.06137953698635101, data time: 0.009005876687856821\n",
      "step: 692505, loss: 0.0749877318739891, data time: 0.008833873271942138\n",
      "step: 692506, loss: 0.06068861484527588, data time: 0.25717854499816895\n",
      "step: 692507, loss: 0.0631292313337326, data time: 0.12974250316619873\n",
      "step: 692508, loss: 0.06336560845375061, data time: 0.08753005663553874\n",
      "step: 692509, loss: 0.06099630519747734, data time: 0.06644803285598755\n",
      "step: 692510, loss: 0.06354319304227829, data time: 0.05344686508178711\n",
      "step: 692511, loss: 0.057089097797870636, data time: 0.04478971163431803\n",
      "step: 692512, loss: 0.0694134458899498, data time: 0.038601432527814596\n",
      "step: 692513, loss: 0.06359165906906128, data time: 0.03402939438819885\n",
      "step: 692514, loss: 0.06297285109758377, data time: 0.030401362313164607\n",
      "step: 692515, loss: 0.05985094606876373, data time: 0.02756528854370117\n",
      "step: 692516, loss: 0.060316335409879684, data time: 0.025256243619051846\n",
      "step: 692517, loss: 0.05823962017893791, data time: 0.023343026638031006\n",
      "step: 692518, loss: 0.060219906270504, data time: 0.02172116132882925\n",
      "step: 692519, loss: 0.06122159957885742, data time: 0.020314676421029226\n",
      "step: 692520, loss: 0.06311403214931488, data time: 0.01910088857014974\n",
      "step: 692521, loss: 0.06048154458403587, data time: 0.018036112189292908\n",
      "step: 692522, loss: 0.06484048068523407, data time: 0.01709149865543141\n",
      "step: 692523, loss: 0.0587976835668087, data time: 0.016254001193576388\n",
      "step: 692524, loss: 0.06480363011360168, data time: 0.015504598617553711\n",
      "step: 692525, loss: 0.059822551906108856, data time: 0.014838755130767822\n",
      "step: 692526, loss: 0.056548308581113815, data time: 0.014231477464948381\n",
      "step: 692527, loss: 0.06252624094486237, data time: 0.013680913231589577\n",
      "step: 692528, loss: 0.06415974348783493, data time: 0.013175093609353771\n",
      "step: 692529, loss: 0.05496717616915703, data time: 0.012712568044662476\n",
      "step: 692530, loss: 0.06169701740145683, data time: 0.012285699844360351\n",
      "step: 692531, loss: 0.06317487359046936, data time: 0.01189573911520151\n",
      "step: 692532, loss: 0.06819208711385727, data time: 0.011532218367965133\n",
      "step: 692533, loss: 0.06108499690890312, data time: 0.01119215999330793\n",
      "step: 692534, loss: 0.0530303530395031, data time: 0.01088255027244831\n",
      "step: 692535, loss: 0.06602393090724945, data time: 0.010591451327006023\n",
      "step: 692536, loss: 0.06203322857618332, data time: 0.010319963578254945\n",
      "step: 692537, loss: 0.06085889786481857, data time: 0.010067693889141083\n",
      "step: 692538, loss: 0.057020168751478195, data time: 0.009823120001590612\n",
      "step: 692539, loss: 0.05981616675853729, data time: 0.009592112372903264\n",
      "step: 692540, loss: 0.057387977838516235, data time: 0.00937206404549735\n",
      "step: 692541, loss: 0.0623885914683342, data time: 0.009164849917093912\n",
      "step: 692542, loss: 0.05517590790987015, data time: 0.008967657346983213\n",
      "step: 692543, loss: 0.06265281140804291, data time: 0.008784130999916479\n",
      "step: 692544, loss: 0.06456577777862549, data time: 0.008614069376236353\n",
      "step: 692545, loss: 0.03378073871135712, data time: 0.008450275659561158\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.4092717170715332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 692546, loss: 0.06619905680418015, data time: 0.2604944705963135\n",
      "step: 692547, loss: 0.05983078479766846, data time: 0.13167285919189453\n",
      "step: 692548, loss: 0.056061189621686935, data time: 0.08882904052734375\n",
      "step: 692549, loss: 0.06422903388738632, data time: 0.06728839874267578\n",
      "step: 692550, loss: 0.06064101308584213, data time: 0.05409054756164551\n",
      "step: 692551, loss: 0.05496811866760254, data time: 0.045318802197774254\n",
      "step: 692552, loss: 0.06710506230592728, data time: 0.039051873343331475\n",
      "step: 692553, loss: 0.05897320434451103, data time: 0.03447842597961426\n",
      "step: 692554, loss: 0.06364455819129944, data time: 0.030792819129096136\n",
      "step: 692555, loss: 0.05964314192533493, data time: 0.027907776832580566\n",
      "step: 692556, loss: 0.06570371985435486, data time: 0.025603857907381924\n",
      "step: 692557, loss: 0.062058769166469574, data time: 0.02368287245432536\n",
      "step: 692558, loss: 0.061090271919965744, data time: 0.02204693280733549\n",
      "step: 692559, loss: 0.06516812741756439, data time: 0.020607914243425642\n",
      "step: 692560, loss: 0.06454901397228241, data time: 0.019370047251383464\n",
      "step: 692561, loss: 0.062184207141399384, data time: 0.018285289406776428\n",
      "step: 692562, loss: 0.06357651948928833, data time: 0.017328346476835364\n",
      "step: 692563, loss: 0.06450909376144409, data time: 0.01646753152211507\n",
      "step: 692564, loss: 0.058476757258176804, data time: 0.01570279974686472\n",
      "step: 692565, loss: 0.06547856330871582, data time: 0.015027475357055665\n",
      "step: 692566, loss: 0.06090158969163895, data time: 0.0144129821232387\n",
      "step: 692567, loss: 0.062403298914432526, data time: 0.013852065259760077\n",
      "step: 692568, loss: 0.06876815855503082, data time: 0.013335538947063944\n",
      "step: 692569, loss: 0.06530814617872238, data time: 0.012864261865615845\n",
      "step: 692570, loss: 0.06055055558681488, data time: 0.01243180274963379\n",
      "step: 692571, loss: 0.06410224735736847, data time: 0.012030528141902043\n",
      "step: 692572, loss: 0.06462060660123825, data time: 0.011654818499529804\n",
      "step: 692573, loss: 0.05773609131574631, data time: 0.011310398578643799\n",
      "step: 692574, loss: 0.0569135844707489, data time: 0.01100086343699488\n",
      "step: 692575, loss: 0.06236795336008072, data time: 0.0107069730758667\n",
      "step: 692576, loss: 0.06131184101104736, data time: 0.010428636304793818\n",
      "step: 692577, loss: 0.05991901457309723, data time: 0.010169751942157745\n",
      "step: 692578, loss: 0.061819545924663544, data time: 0.009919665076515892\n",
      "step: 692579, loss: 0.056468527764081955, data time: 0.009684008710524616\n",
      "step: 692580, loss: 0.07119330018758774, data time: 0.009463582720075334\n",
      "step: 692581, loss: 0.06401588767766953, data time: 0.009249243471357558\n",
      "step: 692582, loss: 0.0568714365363121, data time: 0.009049647563212627\n",
      "step: 692583, loss: 0.05559013411402702, data time: 0.008864948624058774\n",
      "step: 692584, loss: 0.06146964430809021, data time: 0.00869299815251277\n",
      "step: 692585, loss: 0.06284027546644211, data time: 0.00852709412574768\n",
      "step: 692586, loss: 0.06347669661045074, data time: 0.2796149253845215\n",
      "step: 692587, loss: 0.06089409440755844, data time: 0.14059317111968994\n",
      "step: 692588, loss: 0.06448222696781158, data time: 0.09427086512247722\n",
      "step: 692589, loss: 0.06247597932815552, data time: 0.0714566707611084\n",
      "step: 692590, loss: 0.060655735433101654, data time: 0.05744462013244629\n",
      "step: 692591, loss: 0.06482376158237457, data time: 0.048111399014790855\n",
      "step: 692592, loss: 0.05987001582980156, data time: 0.04143377712794712\n",
      "step: 692593, loss: 0.0682772696018219, data time: 0.03650766611099243\n",
      "step: 692594, loss: 0.05978464335203171, data time: 0.032598416010538735\n",
      "step: 692595, loss: 0.0618479959666729, data time: 0.029539942741394043\n",
      "step: 692596, loss: 0.06341800093650818, data time: 0.027053139426491478\n",
      "step: 692597, loss: 0.0582272931933403, data time: 0.024978160858154297\n",
      "step: 692598, loss: 0.06237386167049408, data time: 0.023220355694110576\n",
      "step: 692599, loss: 0.06330318003892899, data time: 0.02171604973929269\n",
      "step: 692600, loss: 0.05334296450018883, data time: 0.02040888468424479\n",
      "step: 692601, loss: 0.06070619076490402, data time: 0.019271358847618103\n",
      "step: 692602, loss: 0.06580586731433868, data time: 0.018256061217364145\n",
      "step: 692603, loss: 0.06473329663276672, data time: 0.01734782589806451\n",
      "step: 692604, loss: 0.06287138909101486, data time: 0.016537653772454513\n",
      "step: 692605, loss: 0.05982808396220207, data time: 0.015819907188415527\n",
      "step: 692606, loss: 0.06036125868558884, data time: 0.015169313975742884\n",
      "step: 692607, loss: 0.06856396794319153, data time: 0.014574321833523836\n",
      "step: 692608, loss: 0.062001969665288925, data time: 0.014027274173239002\n",
      "step: 692609, loss: 0.05711847543716431, data time: 0.013532082239786783\n",
      "step: 692610, loss: 0.06400163471698761, data time: 0.013072128295898438\n",
      "step: 692611, loss: 0.05965524911880493, data time: 0.012647344515873836\n",
      "step: 692612, loss: 0.06583546102046967, data time: 0.012250361619172272\n",
      "step: 692613, loss: 0.058344386518001556, data time: 0.011883386543818883\n",
      "step: 692614, loss: 0.0610528364777565, data time: 0.011549250832919416\n",
      "step: 692615, loss: 0.062061261385679245, data time: 0.011240975062052409\n",
      "step: 692616, loss: 0.05877523869276047, data time: 0.010943720417637979\n",
      "step: 692617, loss: 0.054812055081129074, data time: 0.010676458477973938\n",
      "step: 692618, loss: 0.05539567023515701, data time: 0.010411862171057499\n",
      "step: 692619, loss: 0.060125868767499924, data time: 0.010164653553682216\n",
      "step: 692620, loss: 0.06764091551303864, data time: 0.009927967616489956\n",
      "step: 692621, loss: 0.06280291080474854, data time: 0.009702894422743056\n",
      "step: 692622, loss: 0.05787324905395508, data time: 0.009491011903092667\n",
      "step: 692623, loss: 0.06509774923324585, data time: 0.009295262788471422\n",
      "step: 692624, loss: 0.05879216641187668, data time: 0.0091094909570156\n",
      "step: 692625, loss: 0.06560171395540237, data time: 0.008933860063552856\n",
      "step: 692626, loss: 0.06325595080852509, data time: 0.27012062072753906\n",
      "step: 692627, loss: 0.05692758411169052, data time: 0.1358577013015747\n",
      "step: 692628, loss: 0.06728629767894745, data time: 0.0910789966583252\n",
      "step: 692629, loss: 0.05616992712020874, data time: 0.06918394565582275\n",
      "step: 692630, loss: 0.06018051505088806, data time: 0.055625295639038085\n",
      "step: 692631, loss: 0.0661359578371048, data time: 0.046590566635131836\n",
      "step: 692632, loss: 0.059729211032390594, data time: 0.04014529500688825\n",
      "step: 692633, loss: 0.06309382617473602, data time: 0.03537517786026001\n",
      "step: 692634, loss: 0.0635518953204155, data time: 0.03158659405178494\n",
      "step: 692635, loss: 0.06308001279830933, data time: 0.02863001823425293\n",
      "step: 692636, loss: 0.06360587477684021, data time: 0.026226954026655716\n",
      "step: 692637, loss: 0.057321276515722275, data time: 0.02423006296157837\n",
      "step: 692638, loss: 0.06678049266338348, data time: 0.02252782308138334\n",
      "step: 692639, loss: 0.060474805533885956, data time: 0.021062578473772322\n",
      "step: 692640, loss: 0.05839395523071289, data time: 0.019792620340983072\n",
      "step: 692641, loss: 0.057085491716861725, data time: 0.018681183457374573\n",
      "step: 692642, loss: 0.06388472020626068, data time: 0.0177002654356115\n",
      "step: 692643, loss: 0.06330172717571259, data time: 0.016829636361863878\n",
      "step: 692644, loss: 0.0632222518324852, data time: 0.01604839375144557\n",
      "step: 692645, loss: 0.06400971114635468, data time: 0.015353310108184814\n",
      "step: 692646, loss: 0.06217370554804802, data time: 0.01472140493847075\n",
      "step: 692647, loss: 0.06237560510635376, data time: 0.014150110158053312\n",
      "step: 692648, loss: 0.05951675772666931, data time: 0.013631063958872919\n",
      "step: 692649, loss: 0.0631568431854248, data time: 0.013153056303660074\n",
      "step: 692650, loss: 0.06763136386871338, data time: 0.01271254539489746\n",
      "step: 692651, loss: 0.0624236986041069, data time: 0.012299601848308857\n",
      "step: 692652, loss: 0.058937832713127136, data time: 0.011916513796205874\n",
      "step: 692653, loss: 0.06840164959430695, data time: 0.01156095096043178\n",
      "step: 692654, loss: 0.062396369874477386, data time: 0.011235845500025255\n",
      "step: 692655, loss: 0.06450443714857101, data time: 0.010932524998982748\n",
      "step: 692656, loss: 0.06538116931915283, data time: 0.010648881235430318\n",
      "step: 692657, loss: 0.0621834322810173, data time: 0.010385431349277496\n",
      "step: 692658, loss: 0.059167616069316864, data time: 0.010128693147139116\n",
      "step: 692659, loss: 0.061739902943372726, data time: 0.009888775208417107\n",
      "step: 692660, loss: 0.06710198521614075, data time: 0.009660325731549945\n",
      "step: 692661, loss: 0.06095292419195175, data time: 0.009441322750515409\n",
      "step: 692662, loss: 0.05958613380789757, data time: 0.009236516179265204\n",
      "step: 692663, loss: 0.06832098215818405, data time: 0.009050714342217697\n",
      "step: 692664, loss: 0.05492373928427696, data time: 0.008870558860974435\n",
      "step: 692665, loss: 0.05525839328765869, data time: 0.008700251579284668\n",
      "step: 692666, loss: 0.05918034166097641, data time: 0.26668667793273926\n",
      "step: 692667, loss: 0.06570607423782349, data time: 0.1341099739074707\n",
      "step: 692668, loss: 0.05724896490573883, data time: 0.08992449442545573\n",
      "step: 692669, loss: 0.06368684768676758, data time: 0.06829440593719482\n",
      "step: 692670, loss: 0.06400379538536072, data time: 0.05492715835571289\n",
      "step: 692671, loss: 0.058976709842681885, data time: 0.04600389798482259\n",
      "step: 692672, loss: 0.058749109506607056, data time: 0.03963419369288853\n",
      "step: 692673, loss: 0.060125932097435, data time: 0.03492778539657593\n",
      "step: 692674, loss: 0.058923911303281784, data time: 0.031193971633911133\n",
      "step: 692675, loss: 0.05618802830576897, data time: 0.028282546997070314\n",
      "step: 692676, loss: 0.05639226734638214, data time: 0.025905435735529118\n",
      "step: 692677, loss: 0.06173114478588104, data time: 0.023929059505462646\n",
      "step: 692678, loss: 0.06307901442050934, data time: 0.022254265271700345\n",
      "step: 692679, loss: 0.06452610343694687, data time: 0.020807743072509766\n",
      "step: 692680, loss: 0.0648019015789032, data time: 0.019584496815999348\n",
      "step: 692681, loss: 0.06298255920410156, data time: 0.018488198518753052\n",
      "step: 692682, loss: 0.06186984106898308, data time: 0.01751871669993681\n",
      "step: 692683, loss: 0.06001706048846245, data time: 0.016652226448059082\n",
      "step: 692684, loss: 0.06682981550693512, data time: 0.015882316388581928\n",
      "step: 692685, loss: 0.06292775273323059, data time: 0.015198206901550293\n",
      "step: 692686, loss: 0.055782221257686615, data time: 0.014575731186639695\n",
      "step: 692687, loss: 0.056876759976148605, data time: 0.014008207754655317\n",
      "step: 692688, loss: 0.05740901827812195, data time: 0.013485908508300781\n",
      "step: 692689, loss: 0.059074994176626205, data time: 0.01301199197769165\n",
      "step: 692690, loss: 0.057355113327503204, data time: 0.012580909729003907\n",
      "step: 692691, loss: 0.06431762874126434, data time: 0.012175743396465596\n",
      "step: 692692, loss: 0.06992244720458984, data time: 0.011798487769232856\n",
      "step: 692693, loss: 0.05904250964522362, data time: 0.011448511055537633\n",
      "step: 692694, loss: 0.06140805408358574, data time: 0.011127554137131264\n",
      "step: 692695, loss: 0.05974019691348076, data time: 0.010828852653503418\n",
      "step: 692696, loss: 0.061497025191783905, data time: 0.010551175763530115\n",
      "step: 692697, loss: 0.058934055268764496, data time: 0.010297328233718872\n",
      "step: 692698, loss: 0.059049662202596664, data time: 0.010042956381133108\n",
      "step: 692699, loss: 0.06389402598142624, data time: 0.0098038631327012\n",
      "step: 692700, loss: 0.06405690312385559, data time: 0.009577492305210659\n",
      "step: 692701, loss: 0.062265992164611816, data time: 0.009361584981282553\n",
      "step: 692702, loss: 0.057852596044540405, data time: 0.009159494090724635\n",
      "step: 692703, loss: 0.06022905930876732, data time: 0.008971452713012695\n",
      "step: 692704, loss: 0.06070460379123688, data time: 0.008796056111653646\n",
      "step: 692705, loss: 0.0716579481959343, data time: 0.008626919984817506\n",
      "step: 692706, loss: 0.06299985945224762, data time: 0.26161956787109375\n",
      "step: 692707, loss: 0.06650209426879883, data time: 0.13157451152801514\n",
      "step: 692708, loss: 0.06499812006950378, data time: 0.08865062395731609\n",
      "step: 692709, loss: 0.06343082338571548, data time: 0.06731528043746948\n",
      "step: 692710, loss: 0.05533144995570183, data time: 0.054124069213867185\n",
      "step: 692711, loss: 0.06143816560506821, data time: 0.045367161432902016\n",
      "step: 692712, loss: 0.0542030856013298, data time: 0.039094550268990655\n",
      "step: 692713, loss: 0.05878379940986633, data time: 0.03446519374847412\n",
      "step: 692714, loss: 0.06186414510011673, data time: 0.030782222747802734\n",
      "step: 692715, loss: 0.05696627497673035, data time: 0.027916741371154786\n",
      "step: 692716, loss: 0.06665702164173126, data time: 0.025578780607743698\n",
      "step: 692717, loss: 0.06288494169712067, data time: 0.023625969886779785\n",
      "step: 692718, loss: 0.05702972412109375, data time: 0.02197720454289363\n",
      "step: 692719, loss: 0.05685931071639061, data time: 0.020553231239318848\n",
      "step: 692720, loss: 0.05995243787765503, data time: 0.01931870778401693\n",
      "step: 692721, loss: 0.05987612158060074, data time: 0.018234938383102417\n",
      "step: 692722, loss: 0.061990171670913696, data time: 0.017282345715691063\n",
      "step: 692723, loss: 0.0633516013622284, data time: 0.01642914613087972\n",
      "step: 692724, loss: 0.06310941278934479, data time: 0.015669621919330797\n",
      "step: 692725, loss: 0.05350540578365326, data time: 0.014992821216583251\n",
      "step: 692726, loss: 0.06155385076999664, data time: 0.0143826234908331\n",
      "step: 692727, loss: 0.06383813917636871, data time: 0.01382722637870095\n",
      "step: 692728, loss: 0.06113585829734802, data time: 0.013312163560286812\n",
      "step: 692729, loss: 0.06378675997257233, data time: 0.012839744488398233\n",
      "step: 692730, loss: 0.06183784455060959, data time: 0.012406187057495117\n",
      "step: 692731, loss: 0.06357686221599579, data time: 0.012006301146287184\n",
      "step: 692732, loss: 0.06260424852371216, data time: 0.011636054074322735\n",
      "step: 692733, loss: 0.06251315772533417, data time: 0.01129622118813651\n",
      "step: 692734, loss: 0.06622727960348129, data time: 0.010981140465571963\n",
      "step: 692735, loss: 0.0645524337887764, data time: 0.01068865458170573\n",
      "step: 692736, loss: 0.05972412973642349, data time: 0.01041348518863801\n",
      "step: 692737, loss: 0.059833310544490814, data time: 0.010158099234104156\n",
      "step: 692738, loss: 0.06178775429725647, data time: 0.00991072076739687\n",
      "step: 692739, loss: 0.06792610883712769, data time: 0.009673777748556697\n",
      "step: 692740, loss: 0.05907207727432251, data time: 0.009453821182250976\n",
      "step: 692741, loss: 0.06263355910778046, data time: 0.009244885709550645\n",
      "step: 692742, loss: 0.06414049863815308, data time: 0.00904708939629632\n",
      "step: 692743, loss: 0.05517727881669998, data time: 0.008862049956070749\n",
      "step: 692744, loss: 0.06195398420095444, data time: 0.008688144194774138\n",
      "step: 692745, loss: 0.061708204448223114, data time: 0.008521980047225952\n",
      "step: 692746, loss: 0.06207820773124695, data time: 0.26973605155944824\n",
      "step: 692747, loss: 0.05685865134000778, data time: 0.13563168048858643\n",
      "step: 692748, loss: 0.060332369059324265, data time: 0.09131844838460286\n",
      "step: 692749, loss: 0.05892699956893921, data time: 0.0692785382270813\n",
      "step: 692750, loss: 0.06854268163442612, data time: 0.05568509101867676\n",
      "step: 692751, loss: 0.05855034291744232, data time: 0.0466299851735433\n",
      "step: 692752, loss: 0.0586029589176178, data time: 0.040169204984392436\n",
      "step: 692753, loss: 0.05763398855924606, data time: 0.035407066345214844\n",
      "step: 692754, loss: 0.06852893531322479, data time: 0.03161920441521539\n",
      "step: 692755, loss: 0.06606920063495636, data time: 0.028654265403747558\n",
      "step: 692756, loss: 0.06225484609603882, data time: 0.02623969858342951\n",
      "step: 692757, loss: 0.06179596111178398, data time: 0.02422809600830078\n",
      "step: 692758, loss: 0.053857848048210144, data time: 0.02252820821908804\n",
      "step: 692759, loss: 0.06323059648275375, data time: 0.021060517856052945\n",
      "step: 692760, loss: 0.05854269117116928, data time: 0.0197903315226237\n",
      "step: 692761, loss: 0.05260006710886955, data time: 0.018682748079299927\n",
      "step: 692762, loss: 0.059886634349823, data time: 0.017700882518992704\n",
      "step: 692763, loss: 0.06838037818670273, data time: 0.01682330502404107\n",
      "step: 692764, loss: 0.059492677450180054, data time: 0.016046059759039628\n",
      "step: 692765, loss: 0.05927899479866028, data time: 0.015357303619384765\n",
      "step: 692766, loss: 0.05990830063819885, data time: 0.014727683294387091\n",
      "step: 692767, loss: 0.0588916651904583, data time: 0.014156439087607643\n",
      "step: 692768, loss: 0.06598448753356934, data time: 0.01362653400586999\n",
      "step: 692769, loss: 0.06610371172428131, data time: 0.01314242680867513\n",
      "step: 692770, loss: 0.05869675800204277, data time: 0.012697362899780273\n",
      "step: 692771, loss: 0.06501616537570953, data time: 0.012288671273451585\n",
      "step: 692772, loss: 0.059996142983436584, data time: 0.011907692308779116\n",
      "step: 692773, loss: 0.05735784024000168, data time: 0.011559358664921351\n",
      "step: 692774, loss: 0.058131273835897446, data time: 0.011233921708731815\n",
      "step: 692775, loss: 0.06781984865665436, data time: 0.01093142827351888\n",
      "step: 692776, loss: 0.06822872161865234, data time: 0.010646773922827936\n",
      "step: 692777, loss: 0.061635978519916534, data time: 0.01038350909948349\n",
      "step: 692778, loss: 0.06120697408914566, data time: 0.010127240961248224\n",
      "step: 692779, loss: 0.06788040697574615, data time: 0.009885549545288086\n",
      "step: 692780, loss: 0.05803166329860687, data time: 0.009657512392316546\n",
      "step: 692781, loss: 0.062099263072013855, data time: 0.009440633985731337\n",
      "step: 692782, loss: 0.05901859328150749, data time: 0.009236748154098922\n",
      "step: 692783, loss: 0.05937425419688225, data time: 0.009047131789358039\n",
      "step: 692784, loss: 0.056517358869314194, data time: 0.00886688476953751\n",
      "step: 692785, loss: 0.052143923938274384, data time: 0.00869516134262085\n",
      "step: 692786, loss: 0.055625852197408676, data time: 0.2825343608856201\n",
      "step: 692787, loss: 0.0647285133600235, data time: 0.1420269012451172\n",
      "step: 692788, loss: 0.060648709535598755, data time: 0.09518114725748698\n",
      "step: 692789, loss: 0.06734006106853485, data time: 0.07212775945663452\n",
      "step: 692790, loss: 0.06501434743404388, data time: 0.05797529220581055\n",
      "step: 692791, loss: 0.0580299012362957, data time: 0.04856757322947184\n",
      "step: 692792, loss: 0.05841473862528801, data time: 0.041831799915858676\n",
      "step: 692793, loss: 0.05547100305557251, data time: 0.036857396364212036\n",
      "step: 692794, loss: 0.05801941081881523, data time: 0.03290677070617676\n",
      "step: 692795, loss: 0.06511932611465454, data time: 0.029818916320800783\n",
      "step: 692796, loss: 0.05904631316661835, data time: 0.027299772609363903\n",
      "step: 692797, loss: 0.05410344526171684, data time: 0.025200029214223225\n",
      "step: 692798, loss: 0.06604988873004913, data time: 0.023426184287438027\n",
      "step: 692799, loss: 0.06409208476543427, data time: 0.021893790790012906\n",
      "step: 692800, loss: 0.059621453285217285, data time: 0.020572996139526366\n",
      "step: 692801, loss: 0.06119881197810173, data time: 0.019413605332374573\n",
      "step: 692802, loss: 0.052640896290540695, data time: 0.018398677601533776\n",
      "step: 692803, loss: 0.0632806345820427, data time: 0.017484015888637967\n",
      "step: 692804, loss: 0.05557837337255478, data time: 0.01666836989553351\n",
      "step: 692805, loss: 0.060648076236248016, data time: 0.015941178798675536\n",
      "step: 692806, loss: 0.056024543941020966, data time: 0.015284436089651925\n",
      "step: 692807, loss: 0.05711403116583824, data time: 0.014701247215270996\n",
      "step: 692808, loss: 0.060245297849178314, data time: 0.01414774811786154\n",
      "step: 692809, loss: 0.056602220982313156, data time: 0.013645052909851074\n",
      "step: 692810, loss: 0.06484731286764145, data time: 0.013182516098022462\n",
      "step: 692811, loss: 0.062349505722522736, data time: 0.012753009796142578\n",
      "step: 692812, loss: 0.062144260853528976, data time: 0.01235838289614077\n",
      "step: 692813, loss: 0.06188173219561577, data time: 0.011990768568856376\n",
      "step: 692814, loss: 0.06056841462850571, data time: 0.011652576512303846\n",
      "step: 692815, loss: 0.05760137364268303, data time: 0.011339402198791504\n",
      "step: 692816, loss: 0.059273652732372284, data time: 0.0110418027447116\n",
      "step: 692817, loss: 0.06052946671843529, data time: 0.010767526924610138\n",
      "step: 692818, loss: 0.06218823790550232, data time: 0.01049845146410393\n",
      "step: 692819, loss: 0.059360310435295105, data time: 0.010244972565594842\n",
      "step: 692820, loss: 0.0590934231877327, data time: 0.010006611687796457\n",
      "step: 692821, loss: 0.05694037303328514, data time: 0.009779102272457547\n",
      "step: 692822, loss: 0.06459977477788925, data time: 0.009566448830269478\n",
      "step: 692823, loss: 0.06519109010696411, data time: 0.00936886511350933\n",
      "step: 692824, loss: 0.06509565562009811, data time: 0.009180649732932067\n",
      "step: 692825, loss: 0.043908026069402695, data time: 0.009002500772476196\n",
      "step: 692826, loss: 0.06199859082698822, data time: 0.2742321491241455\n",
      "step: 692827, loss: 0.06657149642705917, data time: 0.13849520683288574\n",
      "step: 692828, loss: 0.05715477094054222, data time: 0.09321180979410808\n",
      "step: 692829, loss: 0.06037294864654541, data time: 0.07069474458694458\n",
      "step: 692830, loss: 0.06428538262844086, data time: 0.05684356689453125\n",
      "step: 692831, loss: 0.0651165097951889, data time: 0.047602454821268715\n",
      "step: 692832, loss: 0.059045225381851196, data time: 0.04101252555847168\n",
      "step: 692833, loss: 0.06018572300672531, data time: 0.03614974021911621\n",
      "step: 692834, loss: 0.06319582462310791, data time: 0.032281213336520724\n",
      "step: 692835, loss: 0.06612706184387207, data time: 0.029258918762207032\n",
      "step: 692836, loss: 0.05683497339487076, data time: 0.026792504570700905\n",
      "step: 692837, loss: 0.062072139233350754, data time: 0.024742424488067627\n",
      "step: 692838, loss: 0.057749759405851364, data time: 0.023007998099693887\n",
      "step: 692839, loss: 0.06730964779853821, data time: 0.021510294505528042\n",
      "step: 692840, loss: 0.06446227431297302, data time: 0.020210091272989908\n",
      "step: 692841, loss: 0.05742814391851425, data time: 0.019073456525802612\n",
      "step: 692842, loss: 0.06171858310699463, data time: 0.01807999610900879\n",
      "step: 692843, loss: 0.059018053114414215, data time: 0.017185767491658527\n",
      "step: 692844, loss: 0.06284886598587036, data time: 0.016407326648109836\n",
      "step: 692845, loss: 0.06614851206541061, data time: 0.01571251153945923\n",
      "step: 692846, loss: 0.06368862092494965, data time: 0.015085992358979724\n",
      "step: 692847, loss: 0.06535162031650543, data time: 0.014515454118902033\n",
      "step: 692848, loss: 0.05911732465028763, data time: 0.013984814934108568\n",
      "step: 692849, loss: 0.05912558361887932, data time: 0.013500471909840902\n",
      "step: 692850, loss: 0.058873843401670456, data time: 0.013056917190551758\n",
      "step: 692851, loss: 0.05928037688136101, data time: 0.012647353685819186\n",
      "step: 692852, loss: 0.061072349548339844, data time: 0.012266521100644712\n",
      "step: 692853, loss: 0.058612458407878876, data time: 0.011913248470851354\n",
      "step: 692854, loss: 0.06579764187335968, data time: 0.011587603338833513\n",
      "step: 692855, loss: 0.06874299794435501, data time: 0.011284295717875164\n",
      "step: 692856, loss: 0.06132610887289047, data time: 0.01100059478513656\n",
      "step: 692857, loss: 0.06630419194698334, data time: 0.010738469660282135\n",
      "step: 692858, loss: 0.06524227559566498, data time: 0.010474819125551166\n",
      "step: 692859, loss: 0.06794250011444092, data time: 0.010227098184473375\n",
      "step: 692860, loss: 0.057409629225730896, data time: 0.009993083136422293\n",
      "step: 692861, loss: 0.06436009705066681, data time: 0.009770366880628798\n",
      "step: 692862, loss: 0.06438499689102173, data time: 0.009560990977931666\n",
      "step: 692863, loss: 0.05790823698043823, data time: 0.009365702930249666\n",
      "step: 692864, loss: 0.06506572663784027, data time: 0.009180472447321964\n",
      "step: 692865, loss: 0.06925656646490097, data time: 0.009004932641983033\n",
      "step: 692866, loss: 0.0658324658870697, data time: 0.28333210945129395\n",
      "step: 692867, loss: 0.06695102155208588, data time: 0.14245450496673584\n",
      "step: 692868, loss: 0.06216543912887573, data time: 0.09586286544799805\n",
      "step: 692869, loss: 0.06893093883991241, data time: 0.07244986295700073\n",
      "step: 692870, loss: 0.05898958072066307, data time: 0.058241653442382815\n",
      "step: 692871, loss: 0.06720782816410065, data time: 0.048786322275797524\n",
      "step: 692872, loss: 0.06604376435279846, data time: 0.04211677823747907\n",
      "step: 692873, loss: 0.0647679790854454, data time: 0.03711336851119995\n",
      "step: 692874, loss: 0.06343337148427963, data time: 0.03326325946384006\n",
      "step: 692875, loss: 0.057830583304166794, data time: 0.03016810417175293\n",
      "step: 692876, loss: 0.061239466071128845, data time: 0.02765224196694114\n",
      "step: 692877, loss: 0.06683236360549927, data time: 0.025556484858194988\n",
      "step: 692878, loss: 0.057683080434799194, data time: 0.02379017609816331\n",
      "step: 692879, loss: 0.05853334069252014, data time: 0.02226299898965018\n",
      "step: 692880, loss: 0.06574438512325287, data time: 0.020942115783691408\n",
      "step: 692881, loss: 0.06294149905443192, data time: 0.019784480333328247\n",
      "step: 692882, loss: 0.06414958089590073, data time: 0.01876090554630055\n",
      "step: 692883, loss: 0.06849809736013412, data time: 0.017850610944959853\n",
      "step: 692884, loss: 0.06181871145963669, data time: 0.017030690845690276\n",
      "step: 692885, loss: 0.0617833137512207, data time: 0.01630610227584839\n",
      "step: 692886, loss: 0.06183693930506706, data time: 0.015647922243390764\n",
      "step: 692887, loss: 0.0590200275182724, data time: 0.015052708712491121\n",
      "step: 692888, loss: 0.05911435931921005, data time: 0.014504567436549974\n",
      "step: 692889, loss: 0.06452086567878723, data time: 0.014001995325088501\n",
      "step: 692890, loss: 0.06593076139688492, data time: 0.013536710739135743\n",
      "step: 692891, loss: 0.06019784137606621, data time: 0.013107675772446852\n",
      "step: 692892, loss: 0.06133304536342621, data time: 0.012710739065099645\n",
      "step: 692893, loss: 0.05655370652675629, data time: 0.012339396136147636\n",
      "step: 692894, loss: 0.06137850508093834, data time: 0.01199990305407294\n",
      "step: 692895, loss: 0.06105011701583862, data time: 0.011684163411458334\n",
      "step: 692896, loss: 0.06183212250471115, data time: 0.011391232090611611\n",
      "step: 692897, loss: 0.06210636347532272, data time: 0.011115968227386475\n",
      "step: 692898, loss: 0.05701964348554611, data time: 0.010840712171612364\n",
      "step: 692899, loss: 0.06170568987727165, data time: 0.010580876294304343\n",
      "step: 692900, loss: 0.055230967700481415, data time: 0.010336412702287946\n",
      "step: 692901, loss: 0.06093680486083031, data time: 0.010106086730957031\n",
      "step: 692902, loss: 0.0640915036201477, data time: 0.009886065044918575\n",
      "step: 692903, loss: 0.06133350357413292, data time: 0.009681620095905504\n",
      "step: 692904, loss: 0.06519420444965363, data time: 0.009487787882486979\n",
      "step: 692905, loss: 0.05069083720445633, data time: 0.009305518865585328\n",
      "step: 692906, loss: 0.05364463850855827, data time: 0.2687540054321289\n",
      "step: 692907, loss: 0.06075931712985039, data time: 0.13619351387023926\n",
      "step: 692908, loss: 0.06081350892782211, data time: 0.09132671356201172\n",
      "step: 692909, loss: 0.06311960518360138, data time: 0.06926298141479492\n",
      "step: 692910, loss: 0.06182984635233879, data time: 0.05569667816162109\n",
      "step: 692911, loss: 0.06131778284907341, data time: 0.04665259520212809\n",
      "step: 692912, loss: 0.05966581031680107, data time: 0.04020295824323382\n",
      "step: 692913, loss: 0.056194793432950974, data time: 0.035428255796432495\n",
      "step: 692914, loss: 0.06242380663752556, data time: 0.03165419896443685\n",
      "step: 692915, loss: 0.059356532990932465, data time: 0.028686118125915528\n",
      "step: 692916, loss: 0.05743476748466492, data time: 0.026273033835671165\n",
      "step: 692917, loss: 0.05743147060275078, data time: 0.024265507857004803\n",
      "step: 692918, loss: 0.06591539829969406, data time: 0.022564172744750977\n",
      "step: 692919, loss: 0.059467293322086334, data time: 0.021098409380231584\n",
      "step: 692920, loss: 0.06655430048704147, data time: 0.019826587041219076\n",
      "step: 692921, loss: 0.06442679464817047, data time: 0.01871289312839508\n",
      "step: 692922, loss: 0.0665225088596344, data time: 0.017733321470372817\n",
      "step: 692923, loss: 0.06272321939468384, data time: 0.016863226890563965\n",
      "step: 692924, loss: 0.061683010309934616, data time: 0.0160923255117316\n",
      "step: 692925, loss: 0.060472168028354645, data time: 0.015393197536468506\n",
      "step: 692926, loss: 0.06030956655740738, data time: 0.014761323020571754\n",
      "step: 692927, loss: 0.06538920104503632, data time: 0.01418513601476496\n",
      "step: 692928, loss: 0.07103433459997177, data time: 0.013659228449282438\n",
      "step: 692929, loss: 0.06270737946033478, data time: 0.013174662987391153\n",
      "step: 692930, loss: 0.06295463442802429, data time: 0.01272944450378418\n",
      "step: 692931, loss: 0.06330001354217529, data time: 0.012318712014418382\n",
      "step: 692932, loss: 0.06298569589853287, data time: 0.011933547479135019\n",
      "step: 692933, loss: 0.06327773630619049, data time: 0.011578832353864397\n",
      "step: 692934, loss: 0.06245476007461548, data time: 0.011265162763924435\n",
      "step: 692935, loss: 0.061186112463474274, data time: 0.010960809389750163\n",
      "step: 692936, loss: 0.0642823576927185, data time: 0.01068165994459583\n",
      "step: 692937, loss: 0.05913129448890686, data time: 0.010417953133583069\n",
      "step: 692938, loss: 0.06237015128135681, data time: 0.010159947655417702\n",
      "step: 692939, loss: 0.05907478183507919, data time: 0.0099183110629811\n",
      "step: 692940, loss: 0.06277845799922943, data time: 0.009688479559762138\n",
      "step: 692941, loss: 0.060259681195020676, data time: 0.00946987337536282\n",
      "step: 692942, loss: 0.05953900143504143, data time: 0.009266582695213525\n",
      "step: 692943, loss: 0.06658944487571716, data time: 0.009076356887817383\n",
      "step: 692944, loss: 0.0635821521282196, data time: 0.008896821584457006\n",
      "step: 692945, loss: 0.04975268244743347, data time: 0.008725440502166748\n",
      "step: 692946, loss: 0.06136636063456535, data time: 0.2602088451385498\n",
      "step: 692947, loss: 0.05867437273263931, data time: 0.13122999668121338\n",
      "step: 692948, loss: 0.06079313904047012, data time: 0.08862423896789551\n",
      "step: 692949, loss: 0.06168629229068756, data time: 0.06714075803756714\n",
      "step: 692950, loss: 0.06887847185134888, data time: 0.05398216247558594\n",
      "step: 692951, loss: 0.06408949196338654, data time: 0.045229434967041016\n",
      "step: 692952, loss: 0.06335040926933289, data time: 0.0389627388545445\n",
      "step: 692953, loss: 0.06254049390554428, data time: 0.034350812435150146\n",
      "step: 692954, loss: 0.057420939207077026, data time: 0.030688921610514324\n",
      "step: 692955, loss: 0.055870186537504196, data time: 0.027821969985961915\n",
      "step: 692956, loss: 0.0608871728181839, data time: 0.025488961826671253\n",
      "step: 692957, loss: 0.058423351496458054, data time: 0.023552556832631428\n",
      "step: 692958, loss: 0.06061263009905815, data time: 0.021907696357140176\n",
      "step: 692959, loss: 0.05921279639005661, data time: 0.020491089139665877\n",
      "step: 692960, loss: 0.058601610362529755, data time: 0.019258546829223632\n",
      "step: 692961, loss: 0.06241905689239502, data time: 0.0181797593832016\n",
      "step: 692962, loss: 0.07016381621360779, data time: 0.01723044058855842\n",
      "step: 692963, loss: 0.06082470342516899, data time: 0.01638231012556288\n",
      "step: 692964, loss: 0.06345367431640625, data time: 0.015635753932752107\n",
      "step: 692965, loss: 0.06855691969394684, data time: 0.01496049165725708\n",
      "step: 692966, loss: 0.06349536776542664, data time: 0.014352037793114072\n",
      "step: 692967, loss: 0.062448933720588684, data time: 0.01379726149819114\n",
      "step: 692968, loss: 0.06271347403526306, data time: 0.013282921003258747\n",
      "step: 692969, loss: 0.06329852342605591, data time: 0.012814521789550781\n",
      "step: 692970, loss: 0.05806232988834381, data time: 0.012384004592895508\n",
      "step: 692971, loss: 0.06454823911190033, data time: 0.011990492160503682\n",
      "step: 692972, loss: 0.06600676476955414, data time: 0.011623188301368995\n",
      "step: 692973, loss: 0.06297619640827179, data time: 0.011280945369175501\n",
      "step: 692974, loss: 0.06245046108961105, data time: 0.010965799463206324\n",
      "step: 692975, loss: 0.06580009311437607, data time: 0.010672752062479656\n",
      "step: 692976, loss: 0.061689332127571106, data time: 0.010396234450801727\n",
      "step: 692977, loss: 0.06254050135612488, data time: 0.010142192244529724\n",
      "step: 692978, loss: 0.057983484119176865, data time: 0.009893337885538736\n",
      "step: 692979, loss: 0.06431131064891815, data time: 0.009657775654512294\n",
      "step: 692980, loss: 0.06008772552013397, data time: 0.009437622342790877\n",
      "step: 692981, loss: 0.058691903948783875, data time: 0.009226030773586698\n",
      "step: 692982, loss: 0.05838128179311752, data time: 0.009027932141278242\n",
      "step: 692983, loss: 0.06212763115763664, data time: 0.008843095679032175\n",
      "step: 692984, loss: 0.06284281611442566, data time: 0.008669290787134415\n",
      "step: 692985, loss: 0.07053296267986298, data time: 0.008503568172454835\n",
      "step: 692986, loss: 0.06234533339738846, data time: 0.2764618396759033\n",
      "step: 692987, loss: 0.0620071217417717, data time: 0.13900697231292725\n",
      "step: 692988, loss: 0.05856574699282646, data time: 0.09317382176717122\n",
      "step: 692989, loss: 0.06277382373809814, data time: 0.07061856985092163\n",
      "step: 692990, loss: 0.05881138890981674, data time: 0.05678668022155762\n",
      "step: 692991, loss: 0.06409390270709991, data time: 0.04755322138468424\n",
      "step: 692992, loss: 0.06326113641262054, data time: 0.040965488978794644\n",
      "step: 692993, loss: 0.06489213556051254, data time: 0.03609707951545715\n",
      "step: 692994, loss: 0.0691642165184021, data time: 0.032235304514567055\n",
      "step: 692995, loss: 0.061469003558158875, data time: 0.02921128273010254\n",
      "step: 692996, loss: 0.05899526923894882, data time: 0.026754010807384144\n",
      "step: 692997, loss: 0.06013869494199753, data time: 0.02470328410466512\n",
      "step: 692998, loss: 0.06536269187927246, data time: 0.022973959262554463\n",
      "step: 692999, loss: 0.05762661620974541, data time: 0.02149088042122977\n",
      "step: 693000, loss: 0.05864609032869339, data time: 0.02019351323445638\n",
      "step: 693001, loss: 0.06414321810007095, data time: 0.019068509340286255\n",
      "step: 693002, loss: 0.06535539031028748, data time: 0.018067219678093407\n",
      "step: 693003, loss: 0.061345361173152924, data time: 0.017170230547587078\n",
      "step: 693004, loss: 0.058120038360357285, data time: 0.016370283929925216\n",
      "step: 693005, loss: 0.06135859712958336, data time: 0.015660715103149415\n",
      "step: 693006, loss: 0.058874815702438354, data time: 0.015021256038120814\n",
      "step: 693007, loss: 0.059533119201660156, data time: 0.014436190778558905\n",
      "step: 693008, loss: 0.05790480971336365, data time: 0.01390323431595512\n",
      "step: 693009, loss: 0.06557700037956238, data time: 0.013406097888946533\n",
      "step: 693010, loss: 0.05754508823156357, data time: 0.012952413558959961\n",
      "step: 693011, loss: 0.05751575157046318, data time: 0.012530647791348971\n",
      "step: 693012, loss: 0.06234532594680786, data time: 0.012139435167665835\n",
      "step: 693013, loss: 0.06671321392059326, data time: 0.011778618608202254\n",
      "step: 693014, loss: 0.06042347848415375, data time: 0.01144648420399633\n",
      "step: 693015, loss: 0.06736601889133453, data time: 0.011135633786519368\n",
      "step: 693016, loss: 0.06362812221050262, data time: 0.010847806930541992\n",
      "step: 693017, loss: 0.06386049091815948, data time: 0.010582037270069122\n",
      "step: 693018, loss: 0.0639238953590393, data time: 0.010318105871027166\n",
      "step: 693019, loss: 0.06774815171957016, data time: 0.010071130359874052\n",
      "step: 693020, loss: 0.06242794916033745, data time: 0.009838465281895229\n",
      "step: 693021, loss: 0.057197801768779755, data time: 0.009620779090457492\n",
      "step: 693022, loss: 0.05984767898917198, data time: 0.009411721616177945\n",
      "step: 693023, loss: 0.06651397794485092, data time: 0.0092174442190873\n",
      "step: 693024, loss: 0.06390015035867691, data time: 0.009035202173086314\n",
      "step: 693025, loss: 0.06226681172847748, data time: 0.008861202001571655\n",
      "step: 693026, loss: 0.06231601536273956, data time: 0.26468992233276367\n",
      "step: 693027, loss: 0.05916668102145195, data time: 0.1331092119216919\n",
      "step: 693028, loss: 0.06408344954252243, data time: 0.08948636054992676\n",
      "step: 693029, loss: 0.060096144676208496, data time: 0.06798708438873291\n",
      "step: 693030, loss: 0.06828659772872925, data time: 0.05466737747192383\n",
      "step: 693031, loss: 0.06436089426279068, data time: 0.0457911491394043\n",
      "step: 693032, loss: 0.06163061782717705, data time: 0.03947104726518903\n",
      "step: 693033, loss: 0.05737975239753723, data time: 0.03480306267738342\n",
      "step: 693034, loss: 0.05905231088399887, data time: 0.031081332100762263\n",
      "step: 693035, loss: 0.06172239035367966, data time: 0.028177905082702636\n",
      "step: 693036, loss: 0.06061440706253052, data time: 0.02581080523404208\n",
      "step: 693037, loss: 0.062297239899635315, data time: 0.023875077565511067\n",
      "step: 693038, loss: 0.0649026557803154, data time: 0.022234678268432617\n",
      "step: 693039, loss: 0.06417011469602585, data time: 0.02081329481942313\n",
      "step: 693040, loss: 0.06250171363353729, data time: 0.01961061159769694\n",
      "step: 693041, loss: 0.06603538244962692, data time: 0.01853741705417633\n",
      "step: 693042, loss: 0.06560228019952774, data time: 0.01758705868440516\n",
      "step: 693043, loss: 0.061797987669706345, data time: 0.016741500960456\n",
      "step: 693044, loss: 0.06541236490011215, data time: 0.015989190653750773\n",
      "step: 693045, loss: 0.06214918941259384, data time: 0.015313923358917236\n",
      "step: 693046, loss: 0.05664924904704094, data time: 0.014701650256202334\n",
      "step: 693047, loss: 0.06250744313001633, data time: 0.01414810527454723\n",
      "step: 693048, loss: 0.06391957402229309, data time: 0.013636371363764223\n",
      "step: 693049, loss: 0.06279388815164566, data time: 0.013168235619862875\n",
      "step: 693050, loss: 0.06235005706548691, data time: 0.012738161087036133\n",
      "step: 693051, loss: 0.06024345010519028, data time: 0.012339353561401367\n",
      "step: 693052, loss: 0.06344398111104965, data time: 0.011971261766221788\n",
      "step: 693053, loss: 0.06680157780647278, data time: 0.011629087584359305\n",
      "step: 693054, loss: 0.059838902205228806, data time: 0.011314852484341326\n",
      "step: 693055, loss: 0.05655122548341751, data time: 0.01102303663889567\n",
      "step: 693056, loss: 0.05176064372062683, data time: 0.010750655205019059\n",
      "step: 693057, loss: 0.058150213211774826, data time: 0.010494880378246307\n",
      "step: 693058, loss: 0.06378454715013504, data time: 0.010237679337010239\n",
      "step: 693059, loss: 0.0636179968714714, data time: 0.009996764800127815\n",
      "step: 693060, loss: 0.0566510371863842, data time: 0.009769746235438756\n",
      "step: 693061, loss: 0.062119048088788986, data time: 0.009562141365475126\n",
      "step: 693062, loss: 0.06356728076934814, data time: 0.00935799366719014\n",
      "step: 693063, loss: 0.05559345707297325, data time: 0.009168242153368499\n",
      "step: 693064, loss: 0.0579240545630455, data time: 0.008989095687866211\n",
      "step: 693065, loss: 0.03693991154432297, data time: 0.008818882703781127\n",
      "step: 693066, loss: 0.05981741100549698, data time: 0.26801323890686035\n",
      "step: 693067, loss: 0.06209563463926315, data time: 0.13477611541748047\n",
      "step: 693068, loss: 0.06609649956226349, data time: 0.09035356839497884\n",
      "step: 693069, loss: 0.06051678583025932, data time: 0.06862252950668335\n",
      "step: 693070, loss: 0.05674850568175316, data time: 0.05517458915710449\n",
      "step: 693071, loss: 0.0664491057395935, data time: 0.04623560110727946\n",
      "step: 693072, loss: 0.06125418841838837, data time: 0.03982792581830706\n",
      "step: 693073, loss: 0.06174495443701744, data time: 0.035105735063552856\n",
      "step: 693074, loss: 0.05857992172241211, data time: 0.03134968545701769\n",
      "step: 693075, loss: 0.06594334542751312, data time: 0.028422951698303223\n",
      "step: 693076, loss: 0.059725113213062286, data time: 0.02603240446610884\n",
      "step: 693077, loss: 0.06519809365272522, data time: 0.024044156074523926\n",
      "step: 693078, loss: 0.05892249196767807, data time: 0.02236626698420598\n",
      "step: 693079, loss: 0.05744081735610962, data time: 0.02091632570539202\n",
      "step: 693080, loss: 0.05978897213935852, data time: 0.019654877980550132\n",
      "step: 693081, loss: 0.06811949610710144, data time: 0.018550843000411987\n",
      "step: 693082, loss: 0.059263940900564194, data time: 0.017580368939568016\n",
      "step: 693083, loss: 0.06756830215454102, data time: 0.016713367568122015\n",
      "step: 693084, loss: 0.06086715683341026, data time: 0.015940126619840924\n",
      "step: 693085, loss: 0.0699334666132927, data time: 0.015248143672943115\n",
      "step: 693086, loss: 0.06507384777069092, data time: 0.014622960771833147\n",
      "step: 693087, loss: 0.062201112508773804, data time: 0.0140553279356523\n",
      "step: 693088, loss: 0.05794644355773926, data time: 0.013535261154174805\n",
      "step: 693089, loss: 0.06671807169914246, data time: 0.01306078831354777\n",
      "step: 693090, loss: 0.06505312770605087, data time: 0.012623252868652344\n",
      "step: 693091, loss: 0.06334712356328964, data time: 0.012215513449448805\n",
      "step: 693092, loss: 0.07306718081235886, data time: 0.011837835665102358\n",
      "step: 693093, loss: 0.06670257449150085, data time: 0.011485559599740165\n",
      "step: 693094, loss: 0.06697824597358704, data time: 0.011161943961834085\n",
      "step: 693095, loss: 0.06175064668059349, data time: 0.010862278938293456\n",
      "step: 693096, loss: 0.05803275108337402, data time: 0.010581554905060799\n",
      "step: 693097, loss: 0.06427820026874542, data time: 0.010323531925678253\n",
      "step: 693098, loss: 0.06721313297748566, data time: 0.010067397897893732\n",
      "step: 693099, loss: 0.06202148646116257, data time: 0.009826961685629451\n",
      "step: 693100, loss: 0.05704446882009506, data time: 0.009602764674595424\n",
      "step: 693101, loss: 0.06121271848678589, data time: 0.009387049410078261\n",
      "step: 693102, loss: 0.06375183910131454, data time: 0.009183084642564928\n",
      "step: 693103, loss: 0.06005457043647766, data time: 0.008995300845095986\n",
      "step: 693104, loss: 0.06276259571313858, data time: 0.008817159212552585\n",
      "step: 693105, loss: 0.05288153514266014, data time: 0.00864795446395874\n",
      "step: 693106, loss: 0.06546854227781296, data time: 0.27744364738464355\n",
      "step: 693107, loss: 0.06621565669775009, data time: 0.1394968032836914\n",
      "step: 693108, loss: 0.060912858694791794, data time: 0.09352755546569824\n",
      "step: 693109, loss: 0.06695929169654846, data time: 0.07089871168136597\n",
      "step: 693110, loss: 0.05856581777334213, data time: 0.05701313018798828\n",
      "step: 693111, loss: 0.06136717647314072, data time: 0.047749082247416176\n",
      "step: 693112, loss: 0.06368088722229004, data time: 0.041138104030064175\n",
      "step: 693113, loss: 0.0650038868188858, data time: 0.03626587986946106\n",
      "step: 693114, loss: 0.05561896413564682, data time: 0.03238707118564182\n",
      "step: 693115, loss: 0.07211713492870331, data time: 0.029350090026855468\n",
      "step: 693116, loss: 0.06142463535070419, data time: 0.026878270235928623\n",
      "step: 693117, loss: 0.062468551099300385, data time: 0.024816075960795086\n",
      "step: 693118, loss: 0.061647459864616394, data time: 0.023073599888728216\n",
      "step: 693119, loss: 0.058042339980602264, data time: 0.02156719139644078\n",
      "step: 693120, loss: 0.05953891575336456, data time: 0.020264132817586263\n",
      "step: 693121, loss: 0.05986572057008743, data time: 0.019124582409858704\n",
      "step: 693122, loss: 0.05650503560900688, data time: 0.01812744140625\n",
      "step: 693123, loss: 0.06385024636983871, data time: 0.017228696081373427\n",
      "step: 693124, loss: 0.05901931971311569, data time: 0.01642693971332751\n",
      "step: 693125, loss: 0.06202428787946701, data time: 0.015712881088256837\n",
      "step: 693126, loss: 0.05600137636065483, data time: 0.01506585166567848\n",
      "step: 693127, loss: 0.05888764560222626, data time: 0.014480742541226473\n",
      "step: 693128, loss: 0.06438858062028885, data time: 0.013940852621327276\n",
      "step: 693129, loss: 0.06110440939664841, data time: 0.01345127820968628\n",
      "step: 693130, loss: 0.05895209312438965, data time: 0.012993545532226562\n",
      "step: 693131, loss: 0.06988456845283508, data time: 0.01257623158968412\n",
      "step: 693132, loss: 0.06383013725280762, data time: 0.012185087910404912\n",
      "step: 693133, loss: 0.06200721859931946, data time: 0.011820427009037562\n",
      "step: 693134, loss: 0.0690281018614769, data time: 0.011485806826887459\n",
      "step: 693135, loss: 0.062314506620168686, data time: 0.011174488067626952\n",
      "step: 693136, loss: 0.05987618863582611, data time: 0.010885084829022807\n",
      "step: 693137, loss: 0.060670897364616394, data time: 0.010615572333335876\n",
      "step: 693138, loss: 0.061476659029722214, data time: 0.010351224379106001\n",
      "step: 693139, loss: 0.060262203216552734, data time: 0.01010306442485136\n",
      "step: 693140, loss: 0.05703244358301163, data time: 0.009869888850620815\n",
      "step: 693141, loss: 0.06395082920789719, data time: 0.00964897871017456\n",
      "step: 693142, loss: 0.06347215175628662, data time: 0.009439500602515968\n",
      "step: 693143, loss: 0.05637985095381737, data time: 0.009244423163564582\n",
      "step: 693144, loss: 0.06243356689810753, data time: 0.009060278916970277\n",
      "step: 693145, loss: 0.03766678273677826, data time: 0.008887207508087159\n",
      "step: 693146, loss: 0.061447761952877045, data time: 0.2702960968017578\n",
      "step: 693147, loss: 0.05510625243186951, data time: 0.13647699356079102\n",
      "step: 693148, loss: 0.06422075629234314, data time: 0.09148232142130534\n",
      "step: 693149, loss: 0.06469690799713135, data time: 0.06937944889068604\n",
      "step: 693150, loss: 0.06256286054849625, data time: 0.055779027938842776\n",
      "step: 693151, loss: 0.05753539502620697, data time: 0.04671398798624674\n",
      "step: 693152, loss: 0.06121602654457092, data time: 0.04025752203805106\n",
      "step: 693153, loss: 0.06361672282218933, data time: 0.035483747720718384\n",
      "step: 693154, loss: 0.0596274733543396, data time: 0.031695551342434354\n",
      "step: 693155, loss: 0.062197595834732056, data time: 0.028728389739990236\n",
      "step: 693156, loss: 0.059366464614868164, data time: 0.02631224285472523\n",
      "step: 693157, loss: 0.0645037293434143, data time: 0.02430357535680135\n",
      "step: 693158, loss: 0.06268230080604553, data time: 0.0225981932419997\n",
      "step: 693159, loss: 0.060806941241025925, data time: 0.021130272320338657\n",
      "step: 693160, loss: 0.059779465198516846, data time: 0.019860013326009115\n",
      "step: 693161, loss: 0.06195493042469025, data time: 0.018745332956314087\n",
      "step: 693162, loss: 0.05688459798693657, data time: 0.017771524541518268\n",
      "step: 693163, loss: 0.06140517443418503, data time: 0.016898592313130695\n",
      "step: 693164, loss: 0.057817406952381134, data time: 0.016115226243671617\n",
      "step: 693165, loss: 0.05437372252345085, data time: 0.015418922901153565\n",
      "step: 693166, loss: 0.0585690401494503, data time: 0.014788922809419177\n",
      "step: 693167, loss: 0.05809802561998367, data time: 0.014214624058116566\n",
      "step: 693168, loss: 0.06319021433591843, data time: 0.013685464859008789\n",
      "step: 693169, loss: 0.06471778452396393, data time: 0.013200004895528158\n",
      "step: 693170, loss: 0.061234526336193085, data time: 0.012755231857299805\n",
      "step: 693171, loss: 0.06509087234735489, data time: 0.012344424541179951\n",
      "step: 693172, loss: 0.06602852046489716, data time: 0.011963685353597006\n",
      "step: 693173, loss: 0.06385688483715057, data time: 0.011607800211225237\n",
      "step: 693174, loss: 0.06098520755767822, data time: 0.011280462659638503\n",
      "step: 693175, loss: 0.07054364681243896, data time: 0.010976608594258625\n",
      "step: 693176, loss: 0.05931082367897034, data time: 0.010690419904647334\n",
      "step: 693177, loss: 0.061884209513664246, data time: 0.010436974465847015\n",
      "step: 693178, loss: 0.058645233511924744, data time: 0.010181275281039152\n",
      "step: 693179, loss: 0.05884287878870964, data time: 0.009941739194533405\n",
      "step: 693180, loss: 0.06766998767852783, data time: 0.009716265542166574\n",
      "step: 693181, loss: 0.06169041991233826, data time: 0.009502444002363417\n",
      "step: 693182, loss: 0.06181124970316887, data time: 0.009301076064238677\n",
      "step: 693183, loss: 0.0552646666765213, data time: 0.009112414560819926\n",
      "step: 693184, loss: 0.05979515239596367, data time: 0.008933501365857247\n",
      "step: 693185, loss: 0.048175230622291565, data time: 0.008763223886489868\n",
      "step: 693186, loss: 0.06041167303919792, data time: 0.27794480323791504\n",
      "step: 693187, loss: 0.06045457720756531, data time: 0.14051997661590576\n",
      "step: 693188, loss: 0.06045231968164444, data time: 0.0942079226175944\n",
      "step: 693189, loss: 0.06442704051733017, data time: 0.07150697708129883\n",
      "step: 693190, loss: 0.061795931309461594, data time: 0.057486438751220705\n",
      "step: 693191, loss: 0.06754667311906815, data time: 0.04813822110493978\n",
      "step: 693192, loss: 0.0665736198425293, data time: 0.04145700590951102\n",
      "step: 693193, loss: 0.0645255371928215, data time: 0.03654521703720093\n",
      "step: 693194, loss: 0.06182580813765526, data time: 0.03263661596510145\n",
      "step: 693195, loss: 0.05924764275550842, data time: 0.02958369255065918\n",
      "step: 693196, loss: 0.06094100698828697, data time: 0.027087580073963512\n",
      "step: 693197, loss: 0.06218334287405014, data time: 0.02501465876897176\n",
      "step: 693198, loss: 0.066229447722435, data time: 0.023255183146550104\n",
      "step: 693199, loss: 0.062010593712329865, data time: 0.021745102746146067\n",
      "step: 693200, loss: 0.06174428388476372, data time: 0.020431105295817056\n",
      "step: 693201, loss: 0.061682891100645065, data time: 0.019282162189483643\n",
      "step: 693202, loss: 0.06390412151813507, data time: 0.01827422310324276\n",
      "step: 693203, loss: 0.05769903212785721, data time: 0.01736495229932997\n",
      "step: 693204, loss: 0.057318948209285736, data time: 0.016556150034854288\n",
      "step: 693205, loss: 0.06172490119934082, data time: 0.015841877460479735\n",
      "step: 693206, loss: 0.06125415116548538, data time: 0.015191055479503814\n",
      "step: 693207, loss: 0.06773369759321213, data time: 0.01460065624930642\n",
      "step: 693208, loss: 0.06710608303546906, data time: 0.01405955397564432\n",
      "step: 693209, loss: 0.059765785932540894, data time: 0.013559808333714804\n",
      "step: 693210, loss: 0.06010235846042633, data time: 0.013102855682373047\n",
      "step: 693211, loss: 0.05502001568675041, data time: 0.012678705728971042\n",
      "step: 693212, loss: 0.060970209538936615, data time: 0.01228099399142795\n",
      "step: 693213, loss: 0.0620168000459671, data time: 0.011916203158242362\n",
      "step: 693214, loss: 0.05830831453204155, data time: 0.011580163034899482\n",
      "step: 693215, loss: 0.05861082673072815, data time: 0.011265397071838379\n",
      "step: 693216, loss: 0.06369414180517197, data time: 0.010971384663735666\n",
      "step: 693217, loss: 0.06033758446574211, data time: 0.01070106029510498\n",
      "step: 693218, loss: 0.06572510302066803, data time: 0.010434714230624113\n",
      "step: 693219, loss: 0.05744117498397827, data time: 0.010187282281763414\n",
      "step: 693220, loss: 0.058376412838697433, data time: 0.009951639175415038\n",
      "step: 693221, loss: 0.0661870539188385, data time: 0.009725623660617404\n",
      "step: 693222, loss: 0.06113877147436142, data time: 0.009515388591869458\n",
      "step: 693223, loss: 0.061843644827604294, data time: 0.009317818440889058\n",
      "step: 693224, loss: 0.06194441020488739, data time: 0.009130575718023838\n",
      "step: 693225, loss: 0.06002790853381157, data time: 0.00895707607269287\n",
      "step: 693226, loss: 0.062036167830228806, data time: 0.27173519134521484\n",
      "step: 693227, loss: 0.05681554600596428, data time: 0.13698828220367432\n",
      "step: 693228, loss: 0.06482188403606415, data time: 0.09183088938395183\n",
      "step: 693229, loss: 0.06150862202048302, data time: 0.06974214315414429\n",
      "step: 693230, loss: 0.06661167740821838, data time: 0.05608320236206055\n",
      "step: 693231, loss: 0.05850914865732193, data time: 0.04696770509084066\n",
      "step: 693232, loss: 0.06311757862567902, data time: 0.04046143804277692\n",
      "step: 693233, loss: 0.05859684944152832, data time: 0.03566285967826843\n",
      "step: 693234, loss: 0.06115065515041351, data time: 0.03184808625115289\n",
      "step: 693235, loss: 0.06110048294067383, data time: 0.02886221408843994\n",
      "step: 693236, loss: 0.060582831501960754, data time: 0.026434920050881126\n",
      "step: 693237, loss: 0.06299303472042084, data time: 0.02442067861557007\n",
      "step: 693238, loss: 0.06221380829811096, data time: 0.022710011555598333\n",
      "step: 693239, loss: 0.06320159137248993, data time: 0.02123284339904785\n",
      "step: 693240, loss: 0.05938749760389328, data time: 0.01995695432027181\n",
      "step: 693241, loss: 0.062226906418800354, data time: 0.01883503794670105\n",
      "step: 693242, loss: 0.05797342583537102, data time: 0.017850805731380686\n",
      "step: 693243, loss: 0.060363173484802246, data time: 0.016965945561726887\n",
      "step: 693244, loss: 0.05620930716395378, data time: 0.016183514344064814\n",
      "step: 693245, loss: 0.06221747398376465, data time: 0.015482699871063233\n",
      "step: 693246, loss: 0.06117439270019531, data time: 0.014850514275687081\n",
      "step: 693247, loss: 0.06304339319467545, data time: 0.0142744779586792\n",
      "step: 693248, loss: 0.06668604910373688, data time: 0.013744219489719557\n",
      "step: 693249, loss: 0.052954524755477905, data time: 0.013256977001825968\n",
      "step: 693250, loss: 0.0612308606505394, data time: 0.01281357765197754\n",
      "step: 693251, loss: 0.06507259607315063, data time: 0.012400388717651367\n",
      "step: 693252, loss: 0.06152825802564621, data time: 0.012012932035658095\n",
      "step: 693253, loss: 0.06680077314376831, data time: 0.01165637799671718\n",
      "step: 693254, loss: 0.055307354778051376, data time: 0.011327949063531283\n",
      "step: 693255, loss: 0.06937067955732346, data time: 0.011020843187967937\n",
      "step: 693256, loss: 0.060349076986312866, data time: 0.010735088779080299\n",
      "step: 693257, loss: 0.059980958700180054, data time: 0.010470740497112274\n",
      "step: 693258, loss: 0.06067821383476257, data time: 0.010214220393787731\n",
      "step: 693259, loss: 0.05823107808828354, data time: 0.009975727866677678\n",
      "step: 693260, loss: 0.06321966648101807, data time: 0.00974693979535784\n",
      "step: 693261, loss: 0.06083967536687851, data time: 0.00952756404876709\n",
      "step: 693262, loss: 0.062304966151714325, data time: 0.009320645718961148\n",
      "step: 693263, loss: 0.059897445142269135, data time: 0.009127472576342131\n",
      "step: 693264, loss: 0.06889081001281738, data time: 0.008946100870768229\n",
      "step: 693265, loss: 0.09876051545143127, data time: 0.008774137496948243\n",
      "step: 693266, loss: 0.06825478374958038, data time: 0.2686452865600586\n",
      "step: 693267, loss: 0.07213132083415985, data time: 0.1355801820755005\n",
      "step: 693268, loss: 0.06803859025239944, data time: 0.09089891115824382\n",
      "step: 693269, loss: 0.06052947789430618, data time: 0.06913584470748901\n",
      "step: 693270, loss: 0.059085097163915634, data time: 0.05559802055358887\n",
      "step: 693271, loss: 0.06150774657726288, data time: 0.04657614231109619\n",
      "step: 693272, loss: 0.06613864004611969, data time: 0.04014015197753906\n",
      "step: 693273, loss: 0.06089957058429718, data time: 0.03538748621940613\n",
      "step: 693274, loss: 0.060422610491514206, data time: 0.03161271413167318\n",
      "step: 693275, loss: 0.05757442116737366, data time: 0.028669047355651855\n",
      "step: 693276, loss: 0.0636872798204422, data time: 0.026260462674227627\n",
      "step: 693277, loss: 0.06444434821605682, data time: 0.02424699068069458\n",
      "step: 693278, loss: 0.06413330882787704, data time: 0.02254322858957144\n",
      "step: 693279, loss: 0.06143641099333763, data time: 0.02107705388750349\n",
      "step: 693280, loss: 0.05861319601535797, data time: 0.019808864593505858\n",
      "step: 693281, loss: 0.06079758703708649, data time: 0.018696755170822144\n",
      "step: 693282, loss: 0.06414525955915451, data time: 0.017720909679637235\n",
      "step: 693283, loss: 0.06537429243326187, data time: 0.01684378253089057\n",
      "step: 693284, loss: 0.056741319596767426, data time: 0.016063025123194644\n",
      "step: 693285, loss: 0.06345521658658981, data time: 0.015365338325500489\n",
      "step: 693286, loss: 0.05902280658483505, data time: 0.014735755466279529\n",
      "step: 693287, loss: 0.0586075484752655, data time: 0.014166181737726385\n",
      "step: 693288, loss: 0.0610315203666687, data time: 0.013639408609141474\n",
      "step: 693289, loss: 0.06090553104877472, data time: 0.01317631204922994\n",
      "step: 693290, loss: 0.06083882972598076, data time: 0.01274658203125\n",
      "step: 693291, loss: 0.05760747939348221, data time: 0.012347872440631572\n",
      "step: 693292, loss: 0.06955074518918991, data time: 0.011976851357354058\n",
      "step: 693293, loss: 0.06520842015743256, data time: 0.011634843690054757\n",
      "step: 693294, loss: 0.060520391911268234, data time: 0.011318872714864796\n",
      "step: 693295, loss: 0.057815924286842346, data time: 0.011026875178019205\n",
      "step: 693296, loss: 0.05807357281446457, data time: 0.01075240873521374\n",
      "step: 693297, loss: 0.060390036553144455, data time: 0.01049976795911789\n",
      "step: 693298, loss: 0.06988860666751862, data time: 0.010244116638645981\n",
      "step: 693299, loss: 0.06155623495578766, data time: 0.010001925861134249\n",
      "step: 693300, loss: 0.06504489481449127, data time: 0.009775713511875698\n",
      "step: 693301, loss: 0.06197511404752731, data time: 0.009558565086788602\n",
      "step: 693302, loss: 0.06452834606170654, data time: 0.00935535817532926\n",
      "step: 693303, loss: 0.060783058404922485, data time: 0.009165098792628237\n",
      "step: 693304, loss: 0.06652047485113144, data time: 0.008984773586957883\n",
      "step: 693305, loss: 0.052760154008865356, data time: 0.008813732862472534\n",
      "step: 693306, loss: 0.06593167036771774, data time: 0.2595028877258301\n",
      "step: 693307, loss: 0.06382879614830017, data time: 0.13112413883209229\n",
      "step: 693308, loss: 0.05729746073484421, data time: 0.08832462628682454\n",
      "step: 693309, loss: 0.059115421026945114, data time: 0.06701761484146118\n",
      "step: 693310, loss: 0.058798789978027344, data time: 0.05390615463256836\n",
      "step: 693311, loss: 0.060609571635723114, data time: 0.04517924785614014\n",
      "step: 693312, loss: 0.061792582273483276, data time: 0.03894271169389997\n",
      "step: 693313, loss: 0.06307759881019592, data time: 0.03437548875808716\n",
      "step: 693314, loss: 0.05987105891108513, data time: 0.030711889266967773\n",
      "step: 693315, loss: 0.05763484165072441, data time: 0.02784562110900879\n",
      "step: 693316, loss: 0.05491011589765549, data time: 0.025509899312799626\n",
      "step: 693317, loss: 0.06583599746227264, data time: 0.02357041835784912\n",
      "step: 693318, loss: 0.06011243909597397, data time: 0.02192060764019306\n",
      "step: 693319, loss: 0.06251576542854309, data time: 0.020502465111868724\n",
      "step: 693320, loss: 0.0681639313697815, data time: 0.019278526306152344\n",
      "step: 693321, loss: 0.06588470935821533, data time: 0.018197521567344666\n",
      "step: 693322, loss: 0.060359030961990356, data time: 0.017250622020048255\n",
      "step: 693323, loss: 0.058748792856931686, data time: 0.016405555937025283\n",
      "step: 693324, loss: 0.06788819283246994, data time: 0.015647800345169872\n",
      "step: 693325, loss: 0.06101887673139572, data time: 0.014974188804626466\n",
      "step: 693326, loss: 0.06216002628207207, data time: 0.01436548005966913\n",
      "step: 693327, loss: 0.06676163524389267, data time: 0.013815359635786577\n",
      "step: 693328, loss: 0.06428144872188568, data time: 0.013307208600251572\n",
      "step: 693329, loss: 0.0555923655629158, data time: 0.01284061868985494\n",
      "step: 693330, loss: 0.06466379016637802, data time: 0.012409629821777344\n",
      "step: 693331, loss: 0.054654598236083984, data time: 0.012011977342458872\n",
      "step: 693332, loss: 0.06667309999465942, data time: 0.011638341126618561\n",
      "step: 693333, loss: 0.05704479664564133, data time: 0.011294594832829066\n",
      "step: 693334, loss: 0.06753237545490265, data time: 0.010985489549307987\n",
      "step: 693335, loss: 0.06602393090724945, data time: 0.010693120956420898\n",
      "step: 693336, loss: 0.0662570595741272, data time: 0.010416730757682555\n",
      "step: 693337, loss: 0.06182868406176567, data time: 0.010171093046665192\n",
      "step: 693338, loss: 0.059757016599178314, data time: 0.009927843556259617\n",
      "step: 693339, loss: 0.06308045238256454, data time: 0.009695193346808939\n",
      "step: 693340, loss: 0.059816405177116394, data time: 0.009476716177804129\n",
      "step: 693341, loss: 0.06614173203706741, data time: 0.009268078539106581\n",
      "step: 693342, loss: 0.06133812665939331, data time: 0.009072683952950142\n",
      "step: 693343, loss: 0.05771886557340622, data time: 0.008889750430458471\n",
      "step: 693344, loss: 0.06379096210002899, data time: 0.008716931709876427\n",
      "step: 693345, loss: 0.07183384895324707, data time: 0.00855320692062378\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40917181968688965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 693346, loss: 0.05714372918009758, data time: 0.2612881660461426\n",
      "step: 693347, loss: 0.06673332303762436, data time: 0.13199841976165771\n",
      "step: 693348, loss: 0.05646892637014389, data time: 0.08894069989522298\n",
      "step: 693349, loss: 0.06311991065740585, data time: 0.06754744052886963\n",
      "step: 693350, loss: 0.06299758702516556, data time: 0.05437979698181152\n",
      "step: 693351, loss: 0.05940621346235275, data time: 0.045610626538594566\n",
      "step: 693352, loss: 0.06152128428220749, data time: 0.03932891573224749\n",
      "step: 693353, loss: 0.0588846430182457, data time: 0.034704387187957764\n",
      "step: 693354, loss: 0.060933277010917664, data time: 0.031023184458414715\n",
      "step: 693355, loss: 0.06586045026779175, data time: 0.028156208992004394\n",
      "step: 693356, loss: 0.06294113397598267, data time: 0.025826042348688297\n",
      "step: 693357, loss: 0.05817504972219467, data time: 0.023882548014322918\n",
      "step: 693358, loss: 0.06200646609067917, data time: 0.02223346783564641\n",
      "step: 693359, loss: 0.06285232305526733, data time: 0.020812835012163435\n",
      "step: 693360, loss: 0.06501029431819916, data time: 0.01958889961242676\n",
      "step: 693361, loss: 0.05932658165693283, data time: 0.018514543771743774\n",
      "step: 693362, loss: 0.07229875028133392, data time: 0.01756738213931813\n",
      "step: 693363, loss: 0.05463310703635216, data time: 0.016721500290764704\n",
      "step: 693364, loss: 0.05595407634973526, data time: 0.015967544756437604\n",
      "step: 693365, loss: 0.05742305517196655, data time: 0.015297234058380127\n",
      "step: 693366, loss: 0.06356792151927948, data time: 0.014684188933599563\n",
      "step: 693367, loss: 0.06929189711809158, data time: 0.014125639742070978\n",
      "step: 693368, loss: 0.06442846357822418, data time: 0.0136129441468612\n",
      "step: 693369, loss: 0.058427594602108, data time: 0.013145367304484049\n",
      "step: 693370, loss: 0.06660372018814087, data time: 0.012715997695922852\n",
      "step: 693371, loss: 0.06432965397834778, data time: 0.012321664736821102\n",
      "step: 693372, loss: 0.061847083270549774, data time: 0.01195296534785518\n",
      "step: 693373, loss: 0.06122390553355217, data time: 0.01160970755985805\n",
      "step: 693374, loss: 0.06720064580440521, data time: 0.011294833545027107\n",
      "step: 693375, loss: 0.06330975145101547, data time: 0.011000903447469075\n",
      "step: 693376, loss: 0.057572364807128906, data time: 0.010723552396220545\n",
      "step: 693377, loss: 0.05917323753237724, data time: 0.010469317436218262\n",
      "step: 693378, loss: 0.0655810683965683, data time: 0.010214213168982304\n",
      "step: 693379, loss: 0.07162001729011536, data time: 0.009974002838134766\n",
      "step: 693380, loss: 0.06154925748705864, data time: 0.009746783120291574\n",
      "step: 693381, loss: 0.05959787219762802, data time: 0.00953111383650038\n",
      "step: 693382, loss: 0.06524058431386948, data time: 0.009327218339249894\n",
      "step: 693383, loss: 0.05872197076678276, data time: 0.009138201412401702\n",
      "step: 693384, loss: 0.06441112607717514, data time: 0.00895993526165302\n",
      "step: 693385, loss: 0.0452960841357708, data time: 0.008786344528198242\n",
      "step: 693386, loss: 0.06478306651115417, data time: 0.2745511531829834\n",
      "step: 693387, loss: 0.06292349100112915, data time: 0.13804996013641357\n",
      "step: 693388, loss: 0.057277947664260864, data time: 0.09307344754536946\n",
      "step: 693389, loss: 0.05840642377734184, data time: 0.07033687829971313\n",
      "step: 693390, loss: 0.0626879334449768, data time: 0.05654778480529785\n",
      "step: 693391, loss: 0.06318110227584839, data time: 0.047360857327779136\n",
      "step: 693392, loss: 0.06041352078318596, data time: 0.040899651391165595\n",
      "step: 693393, loss: 0.06621472537517548, data time: 0.03605106472969055\n",
      "step: 693394, loss: 0.06493973731994629, data time: 0.03219268057081434\n",
      "step: 693395, loss: 0.062442343682050705, data time: 0.029170584678649903\n",
      "step: 693396, loss: 0.06534193456172943, data time: 0.026714411648837002\n",
      "step: 693397, loss: 0.054429929703474045, data time: 0.024667223294576008\n",
      "step: 693398, loss: 0.06690166890621185, data time: 0.022924349858210638\n",
      "step: 693399, loss: 0.0650707483291626, data time: 0.021445035934448242\n",
      "step: 693400, loss: 0.06336327642202377, data time: 0.020148817698160806\n",
      "step: 693401, loss: 0.06194409728050232, data time: 0.01901620626449585\n",
      "step: 693402, loss: 0.06648697704076767, data time: 0.018020882326013902\n",
      "step: 693403, loss: 0.059186309576034546, data time: 0.017131143146091037\n",
      "step: 693404, loss: 0.05448814481496811, data time: 0.01633429527282715\n",
      "step: 693405, loss: 0.06574174761772156, data time: 0.015622687339782716\n",
      "step: 693406, loss: 0.05976672098040581, data time: 0.014980509167625791\n",
      "step: 693407, loss: 0.06781459599733353, data time: 0.014391660690307617\n",
      "step: 693408, loss: 0.063985176384449, data time: 0.013854586559793224\n",
      "step: 693409, loss: 0.060185447335243225, data time: 0.013361583153406778\n",
      "step: 693410, loss: 0.061457082629203796, data time: 0.012914829254150391\n",
      "step: 693411, loss: 0.06062541529536247, data time: 0.012496709823608398\n",
      "step: 693412, loss: 0.06262939423322678, data time: 0.012109456238923249\n",
      "step: 693413, loss: 0.06065898388624191, data time: 0.011747113295963832\n",
      "step: 693414, loss: 0.0642830953001976, data time: 0.01141966622451256\n",
      "step: 693415, loss: 0.05864294618368149, data time: 0.011110250155131023\n",
      "step: 693416, loss: 0.05745206028223038, data time: 0.010818581427297285\n",
      "step: 693417, loss: 0.058011215180158615, data time: 0.010551244020462036\n",
      "step: 693418, loss: 0.05969047546386719, data time: 0.010288968230738785\n",
      "step: 693419, loss: 0.06147569417953491, data time: 0.010041615542243509\n",
      "step: 693420, loss: 0.05711536109447479, data time: 0.009808833258492605\n",
      "step: 693421, loss: 0.06034570187330246, data time: 0.009586956765916612\n",
      "step: 693422, loss: 0.06573663651943207, data time: 0.009376828734939164\n",
      "step: 693423, loss: 0.06313461065292358, data time: 0.009183256249678763\n",
      "step: 693424, loss: 0.06144247576594353, data time: 0.009003419142503005\n",
      "step: 693425, loss: 0.05742357671260834, data time: 0.008827537298202515\n",
      "step: 693426, loss: 0.0677027702331543, data time: 0.28470325469970703\n",
      "step: 693427, loss: 0.06277135014533997, data time: 0.14311325550079346\n",
      "step: 693428, loss: 0.06377162784337997, data time: 0.09677751859029134\n",
      "step: 693429, loss: 0.05481068417429924, data time: 0.07296478748321533\n",
      "step: 693430, loss: 0.058854661881923676, data time: 0.058662796020507814\n",
      "step: 693431, loss: 0.05812288075685501, data time: 0.04912412166595459\n",
      "step: 693432, loss: 0.06362812221050262, data time: 0.04239858899797712\n",
      "step: 693433, loss: 0.06503613293170929, data time: 0.03727221488952637\n",
      "step: 693434, loss: 0.06146911904215813, data time: 0.03335478570726183\n",
      "step: 693435, loss: 0.0562908798456192, data time: 0.030214715003967284\n",
      "step: 693436, loss: 0.06422613561153412, data time: 0.027673396197232334\n",
      "step: 693437, loss: 0.06547931581735611, data time: 0.025550365447998047\n",
      "step: 693438, loss: 0.06301610916852951, data time: 0.02374401459327111\n",
      "step: 693439, loss: 0.06439218670129776, data time: 0.02219911984034947\n",
      "step: 693440, loss: 0.06293236464262009, data time: 0.02088503837585449\n",
      "step: 693441, loss: 0.06536619365215302, data time: 0.01970610022544861\n",
      "step: 693442, loss: 0.06511829793453217, data time: 0.018666716182933134\n",
      "step: 693443, loss: 0.0604499988257885, data time: 0.017744329240587022\n",
      "step: 693444, loss: 0.058516960591077805, data time: 0.01691764279415733\n",
      "step: 693445, loss: 0.060543935745954514, data time: 0.016181182861328126\n",
      "step: 693446, loss: 0.06085578724741936, data time: 0.015516065415881929\n",
      "step: 693447, loss: 0.06311963498592377, data time: 0.01490539854223078\n",
      "step: 693448, loss: 0.06198621541261673, data time: 0.014348714247993801\n",
      "step: 693449, loss: 0.06574107706546783, data time: 0.0138356884320577\n",
      "step: 693450, loss: 0.064182810485363, data time: 0.013364105224609376\n",
      "step: 693451, loss: 0.05945820361375809, data time: 0.012930182310251089\n",
      "step: 693452, loss: 0.06117977946996689, data time: 0.012525337713736075\n",
      "step: 693453, loss: 0.057987041771411896, data time: 0.012149734156472343\n",
      "step: 693454, loss: 0.056310784071683884, data time: 0.011804893099028489\n",
      "step: 693455, loss: 0.06685560196638107, data time: 0.011486570040384928\n",
      "step: 693456, loss: 0.06263992190361023, data time: 0.011186499749460527\n",
      "step: 693457, loss: 0.059033580124378204, data time: 0.010912440717220306\n",
      "step: 693458, loss: 0.057130366563797, data time: 0.010640830704660126\n",
      "step: 693459, loss: 0.059093959629535675, data time: 0.010385681601131664\n",
      "step: 693460, loss: 0.06538694351911545, data time: 0.010144853591918945\n",
      "step: 693461, loss: 0.05687563866376877, data time: 0.009916199578179253\n",
      "step: 693462, loss: 0.05882158875465393, data time: 0.009697430842631572\n",
      "step: 693463, loss: 0.05863898992538452, data time: 0.009496004957901804\n",
      "step: 693464, loss: 0.05690907686948776, data time: 0.00930458460098658\n",
      "step: 693465, loss: 0.06212794780731201, data time: 0.009121179580688477\n",
      "step: 693466, loss: 0.06084189936518669, data time: 0.2593681812286377\n",
      "step: 693467, loss: 0.06294326484203339, data time: 0.13132166862487793\n",
      "step: 693468, loss: 0.05988381430506706, data time: 0.08804726600646973\n",
      "step: 693469, loss: 0.053534261882305145, data time: 0.0668899416923523\n",
      "step: 693470, loss: 0.05805876478552818, data time: 0.05377683639526367\n",
      "step: 693471, loss: 0.06089269742369652, data time: 0.04505626360575358\n",
      "step: 693472, loss: 0.05901017040014267, data time: 0.03881556647164481\n",
      "step: 693473, loss: 0.06315966695547104, data time: 0.034212857484817505\n",
      "step: 693474, loss: 0.061722930520772934, data time: 0.030565288331773546\n",
      "step: 693475, loss: 0.06480293720960617, data time: 0.027715063095092772\n",
      "step: 693476, loss: 0.0655779168009758, data time: 0.02539870955727317\n",
      "step: 693477, loss: 0.06884109973907471, data time: 0.02346781889597575\n",
      "step: 693478, loss: 0.06406885385513306, data time: 0.021828321310190055\n",
      "step: 693479, loss: 0.061319898813962936, data time: 0.02040937968662807\n",
      "step: 693480, loss: 0.06351803988218307, data time: 0.019179868698120116\n",
      "step: 693481, loss: 0.059246450662612915, data time: 0.01810447871685028\n",
      "step: 693482, loss: 0.06948448717594147, data time: 0.017159490024342257\n",
      "step: 693483, loss: 0.05994492024183273, data time: 0.016313579347398546\n",
      "step: 693484, loss: 0.06290075927972794, data time: 0.015567340348896227\n",
      "step: 693485, loss: 0.06491255760192871, data time: 0.01489427089691162\n",
      "step: 693486, loss: 0.06282657384872437, data time: 0.014292853219168527\n",
      "step: 693487, loss: 0.06041114777326584, data time: 0.013742934573780407\n",
      "step: 693488, loss: 0.07066434621810913, data time: 0.01323148478632388\n",
      "step: 693489, loss: 0.06346293538808823, data time: 0.01276099681854248\n",
      "step: 693490, loss: 0.05913852900266647, data time: 0.012332162857055663\n",
      "step: 693491, loss: 0.05772317573428154, data time: 0.011937554065997783\n",
      "step: 693492, loss: 0.05906921625137329, data time: 0.011568943659464518\n",
      "step: 693493, loss: 0.0656091570854187, data time: 0.01122891902923584\n",
      "step: 693494, loss: 0.06093955039978027, data time: 0.010914185951495993\n",
      "step: 693495, loss: 0.05933567136526108, data time: 0.010622827212015788\n",
      "step: 693496, loss: 0.06331858783960342, data time: 0.010354188180738879\n",
      "step: 693497, loss: 0.06274526566267014, data time: 0.010100513696670532\n",
      "step: 693498, loss: 0.06358806788921356, data time: 0.009852011998494467\n",
      "step: 693499, loss: 0.05871190130710602, data time: 0.009617272545309627\n",
      "step: 693500, loss: 0.0632217526435852, data time: 0.009396655218941825\n",
      "step: 693501, loss: 0.06497557461261749, data time: 0.00918758577770657\n",
      "step: 693502, loss: 0.06400873512029648, data time: 0.008990558418067725\n",
      "step: 693503, loss: 0.05828452110290527, data time: 0.008807019183510229\n",
      "step: 693504, loss: 0.06435093283653259, data time: 0.008634127103365384\n",
      "step: 693505, loss: 0.0691882073879242, data time: 0.008468705415725707\n",
      "step: 693506, loss: 0.05764002725481987, data time: 0.2787618637084961\n",
      "step: 693507, loss: 0.06919577717781067, data time: 0.14012420177459717\n",
      "step: 693508, loss: 0.06014835089445114, data time: 0.09396298726399739\n",
      "step: 693509, loss: 0.056733857840299606, data time: 0.07125258445739746\n",
      "step: 693510, loss: 0.06356579065322876, data time: 0.05727720260620117\n",
      "step: 693511, loss: 0.06074991077184677, data time: 0.04797414938608805\n",
      "step: 693512, loss: 0.06479553133249283, data time: 0.0413247857775007\n",
      "step: 693513, loss: 0.06432636082172394, data time: 0.03641057014465332\n",
      "step: 693514, loss: 0.059346944093704224, data time: 0.032509962717692055\n",
      "step: 693515, loss: 0.06234008073806763, data time: 0.029456663131713866\n",
      "step: 693516, loss: 0.06956824660301208, data time: 0.026974569667469372\n",
      "step: 693517, loss: 0.06459078937768936, data time: 0.024910370508829754\n",
      "step: 693518, loss: 0.06146746128797531, data time: 0.02315864196190467\n",
      "step: 693519, loss: 0.06090056523680687, data time: 0.02166083880833217\n",
      "step: 693520, loss: 0.06664314866065979, data time: 0.020376046498616535\n",
      "step: 693521, loss: 0.06240756809711456, data time: 0.019251689314842224\n",
      "step: 693522, loss: 0.0641227588057518, data time: 0.018262400346643785\n",
      "step: 693523, loss: 0.06452804058790207, data time: 0.01737417115105523\n",
      "step: 693524, loss: 0.058149367570877075, data time: 0.016584233233803196\n",
      "step: 693525, loss: 0.060890913009643555, data time: 0.015877902507781982\n",
      "step: 693526, loss: 0.06598400324583054, data time: 0.015223684765043714\n",
      "step: 693527, loss: 0.06336233019828796, data time: 0.01463018764149059\n",
      "step: 693528, loss: 0.06763823330402374, data time: 0.014084992201431938\n",
      "step: 693529, loss: 0.05783892795443535, data time: 0.013583372036616007\n",
      "step: 693530, loss: 0.06377904862165451, data time: 0.013120880126953125\n",
      "step: 693531, loss: 0.06388526409864426, data time: 0.012697348227867713\n",
      "step: 693532, loss: 0.06014879047870636, data time: 0.012304403163768627\n",
      "step: 693533, loss: 0.06102211773395538, data time: 0.011942148208618164\n",
      "step: 693534, loss: 0.06299521028995514, data time: 0.011604679041895372\n",
      "step: 693535, loss: 0.06800535321235657, data time: 0.011288332939147949\n",
      "step: 693536, loss: 0.062008798122406006, data time: 0.010993026917980563\n",
      "step: 693537, loss: 0.06471066921949387, data time: 0.010720796883106232\n",
      "step: 693538, loss: 0.06430232524871826, data time: 0.010453130259658352\n",
      "step: 693539, loss: 0.06069386005401611, data time: 0.01020050048828125\n",
      "step: 693540, loss: 0.06767924875020981, data time: 0.009962327139718192\n",
      "step: 693541, loss: 0.0652257651090622, data time: 0.009736511442396376\n",
      "step: 693542, loss: 0.07144656777381897, data time: 0.009524255185513883\n",
      "step: 693543, loss: 0.06497214734554291, data time: 0.00932576154407702\n",
      "step: 693544, loss: 0.06901317089796066, data time: 0.00913706192603478\n",
      "step: 693545, loss: 0.04949135705828667, data time: 0.008959627151489258\n",
      "step: 693546, loss: 0.06015470251441002, data time: 0.27927494049072266\n",
      "step: 693547, loss: 0.05572929233312607, data time: 0.14039218425750732\n",
      "step: 693548, loss: 0.05708685517311096, data time: 0.09409737586975098\n",
      "step: 693549, loss: 0.056291595101356506, data time: 0.07126641273498535\n",
      "step: 693550, loss: 0.05678490549325943, data time: 0.057293701171875\n",
      "step: 693551, loss: 0.06406809389591217, data time: 0.04798257350921631\n",
      "step: 693552, loss: 0.060749154537916183, data time: 0.04133084842136928\n",
      "step: 693553, loss: 0.05875878781080246, data time: 0.0364115834236145\n",
      "step: 693554, loss: 0.06152450665831566, data time: 0.03258742226494683\n",
      "step: 693555, loss: 0.06393198668956757, data time: 0.029520797729492187\n",
      "step: 693556, loss: 0.06532589346170425, data time: 0.02703426100990989\n",
      "step: 693557, loss: 0.060620538890361786, data time: 0.024957517782847088\n",
      "step: 693558, loss: 0.0687563493847847, data time: 0.02320276773892916\n",
      "step: 693559, loss: 0.06348743289709091, data time: 0.02168839318411691\n",
      "step: 693560, loss: 0.06456293165683746, data time: 0.020375442504882813\n",
      "step: 693561, loss: 0.06191679835319519, data time: 0.01922912895679474\n",
      "step: 693562, loss: 0.06849326193332672, data time: 0.01821775997386259\n",
      "step: 693563, loss: 0.06419142335653305, data time: 0.017319891187879775\n",
      "step: 693564, loss: 0.06841658055782318, data time: 0.016509708605314557\n",
      "step: 693565, loss: 0.0597568154335022, data time: 0.015791118144989014\n",
      "step: 693566, loss: 0.05852183699607849, data time: 0.01514103299095517\n",
      "step: 693567, loss: 0.06267273426055908, data time: 0.014574939554387873\n",
      "step: 693568, loss: 0.06132345646619797, data time: 0.014029948607735012\n",
      "step: 693569, loss: 0.0663568302989006, data time: 0.01353079080581665\n",
      "step: 693570, loss: 0.061707817018032074, data time: 0.013077764511108399\n",
      "step: 693571, loss: 0.06853793561458588, data time: 0.01265280980330247\n",
      "step: 693572, loss: 0.057312265038490295, data time: 0.012257487685592086\n",
      "step: 693573, loss: 0.0611698180437088, data time: 0.011889074529920305\n",
      "step: 693574, loss: 0.05692159757018089, data time: 0.011554257623080549\n",
      "step: 693575, loss: 0.062341488897800446, data time: 0.011240649223327636\n",
      "step: 693576, loss: 0.06254126131534576, data time: 0.010947819679014145\n",
      "step: 693577, loss: 0.06595371663570404, data time: 0.010676249861717224\n",
      "step: 693578, loss: 0.06814837455749512, data time: 0.010413892341382576\n",
      "step: 693579, loss: 0.06320848315954208, data time: 0.010164681602926814\n",
      "step: 693580, loss: 0.060447629541158676, data time: 0.009928321838378907\n",
      "step: 693581, loss: 0.06010650098323822, data time: 0.00970630513297187\n",
      "step: 693582, loss: 0.062481969594955444, data time: 0.009493292988957586\n",
      "step: 693583, loss: 0.06330432742834091, data time: 0.009296875250966926\n",
      "step: 693584, loss: 0.06481859087944031, data time: 0.009110835882333608\n",
      "step: 693585, loss: 0.056083571165800095, data time: 0.00893552303314209\n",
      "step: 693586, loss: 0.0667925775051117, data time: 0.26337146759033203\n",
      "step: 693587, loss: 0.06968456506729126, data time: 0.13302993774414062\n",
      "step: 693588, loss: 0.05892651528120041, data time: 0.09007906913757324\n",
      "step: 693589, loss: 0.06026202440261841, data time: 0.06821244955062866\n",
      "step: 693590, loss: 0.05783332511782646, data time: 0.054932689666748045\n",
      "step: 693591, loss: 0.06375601887702942, data time: 0.046059091885884605\n",
      "step: 693592, loss: 0.06064828485250473, data time: 0.039902993610927036\n",
      "step: 693593, loss: 0.06235751137137413, data time: 0.03513067960739136\n",
      "step: 693594, loss: 0.06405310332775116, data time: 0.031398614247639976\n",
      "step: 693595, loss: 0.060922347009181976, data time: 0.028452634811401367\n",
      "step: 693596, loss: 0.05972197651863098, data time: 0.02606409246271307\n",
      "step: 693597, loss: 0.05754665657877922, data time: 0.024076978365580242\n",
      "step: 693598, loss: 0.0585063211619854, data time: 0.022388476591843825\n",
      "step: 693599, loss: 0.05708829686045647, data time: 0.020944527217320034\n",
      "step: 693600, loss: 0.06359803676605225, data time: 0.019688177108764648\n",
      "step: 693601, loss: 0.061469003558158875, data time: 0.018582910299301147\n",
      "step: 693602, loss: 0.06211226433515549, data time: 0.017614897559670842\n",
      "step: 693603, loss: 0.06292784959077835, data time: 0.01674550109439426\n",
      "step: 693604, loss: 0.07134315371513367, data time: 0.015975826664974813\n",
      "step: 693605, loss: 0.06501666456460953, data time: 0.0152923583984375\n",
      "step: 693606, loss: 0.06755271553993225, data time: 0.014671427862984794\n",
      "step: 693607, loss: 0.05777312442660332, data time: 0.014101678674871271\n",
      "step: 693608, loss: 0.06607857346534729, data time: 0.013576839281165081\n",
      "step: 693609, loss: 0.0623825266957283, data time: 0.01309467355410258\n",
      "step: 693610, loss: 0.06172358617186546, data time: 0.01265181541442871\n",
      "step: 693611, loss: 0.061033785343170166, data time: 0.012241996251619779\n",
      "step: 693612, loss: 0.058365996927022934, data time: 0.01186263119732892\n",
      "step: 693613, loss: 0.0633411705493927, data time: 0.011520343167441232\n",
      "step: 693614, loss: 0.055245157331228256, data time: 0.011196613311767578\n",
      "step: 693615, loss: 0.06130698323249817, data time: 0.010896078745524089\n",
      "step: 693616, loss: 0.0600740909576416, data time: 0.010615748743857107\n",
      "step: 693617, loss: 0.06407444179058075, data time: 0.010354280471801758\n",
      "step: 693618, loss: 0.060511231422424316, data time: 0.010097908251213305\n",
      "step: 693619, loss: 0.05671417713165283, data time: 0.009861560428843778\n",
      "step: 693620, loss: 0.05824890360236168, data time: 0.009637723650251116\n",
      "step: 693621, loss: 0.06668372452259064, data time: 0.009422765837775337\n",
      "step: 693622, loss: 0.06468100845813751, data time: 0.009222153070810679\n",
      "step: 693623, loss: 0.06851383298635483, data time: 0.009036076696295487\n",
      "step: 693624, loss: 0.060350410640239716, data time: 0.008858601252237955\n",
      "step: 693625, loss: 0.048278775066137314, data time: 0.008691126108169555\n",
      "step: 693626, loss: 0.06202627345919609, data time: 0.27802062034606934\n",
      "step: 693627, loss: 0.06298195570707321, data time: 0.13980603218078613\n",
      "step: 693628, loss: 0.06222481653094292, data time: 0.09371415774027507\n",
      "step: 693629, loss: 0.0641893595457077, data time: 0.07113564014434814\n",
      "step: 693630, loss: 0.05916048586368561, data time: 0.057194852828979494\n",
      "step: 693631, loss: 0.06717953830957413, data time: 0.04790814717610677\n",
      "step: 693632, loss: 0.06223928928375244, data time: 0.04126824651445661\n",
      "step: 693633, loss: 0.06680759787559509, data time: 0.03637498617172241\n",
      "step: 693634, loss: 0.05746544525027275, data time: 0.03248416052924262\n",
      "step: 693635, loss: 0.06239507347345352, data time: 0.0294464111328125\n",
      "step: 693636, loss: 0.06472659856081009, data time: 0.026974071155894886\n",
      "step: 693637, loss: 0.06062048673629761, data time: 0.02491066853205363\n",
      "step: 693638, loss: 0.06334125995635986, data time: 0.023157523228571966\n",
      "step: 693639, loss: 0.06055276840925217, data time: 0.021645120212009976\n",
      "step: 693640, loss: 0.06253194063901901, data time: 0.020347134272257487\n",
      "step: 693641, loss: 0.06138112396001816, data time: 0.019203394651412964\n",
      "step: 693642, loss: 0.05495025962591171, data time: 0.018196694991167855\n",
      "step: 693643, loss: 0.06121276319026947, data time: 0.01729260550604926\n",
      "step: 693644, loss: 0.06832592189311981, data time: 0.016486167907714844\n",
      "step: 693645, loss: 0.05820547044277191, data time: 0.015767645835876466\n",
      "step: 693646, loss: 0.06030036509037018, data time: 0.015119507199242002\n",
      "step: 693647, loss: 0.062293559312820435, data time: 0.014532934535633434\n",
      "step: 693648, loss: 0.06623639911413193, data time: 0.013988266820492952\n",
      "step: 693649, loss: 0.059768445789813995, data time: 0.013489464918772379\n",
      "step: 693650, loss: 0.05726765841245651, data time: 0.013030099868774413\n",
      "step: 693651, loss: 0.05492478236556053, data time: 0.01260811548966628\n",
      "step: 693652, loss: 0.05984184145927429, data time: 0.01221450169881185\n",
      "step: 693653, loss: 0.05520879477262497, data time: 0.011852085590362549\n",
      "step: 693654, loss: 0.06137854978442192, data time: 0.01151764803919299\n",
      "step: 693655, loss: 0.06330712884664536, data time: 0.011203885078430176\n",
      "step: 693656, loss: 0.05800008401274681, data time: 0.010910980163082\n",
      "step: 693657, loss: 0.061607711017131805, data time: 0.010639563202857971\n",
      "step: 693658, loss: 0.06374694406986237, data time: 0.010379892407041607\n",
      "step: 693659, loss: 0.06601207703351974, data time: 0.01013048957375919\n",
      "step: 693660, loss: 0.0658661350607872, data time: 0.009894629887172154\n",
      "step: 693661, loss: 0.058628927916288376, data time: 0.009671615229712592\n",
      "step: 693662, loss: 0.061190806329250336, data time: 0.009461944167678421\n",
      "step: 693663, loss: 0.06391322612762451, data time: 0.009268566181785181\n",
      "step: 693664, loss: 0.0654335767030716, data time: 0.009083851789816832\n",
      "step: 693665, loss: 0.05785461887717247, data time: 0.008908158540725708\n",
      "step: 693666, loss: 0.06599165499210358, data time: 0.2860405445098877\n",
      "step: 693667, loss: 0.06886425614356995, data time: 0.1441819667816162\n",
      "step: 693668, loss: 0.0621611587703228, data time: 0.09715970357259114\n",
      "step: 693669, loss: 0.06281475722789764, data time: 0.07366389036178589\n",
      "step: 693670, loss: 0.06225228309631348, data time: 0.05922374725341797\n",
      "step: 693671, loss: 0.06327363103628159, data time: 0.04959003130594889\n",
      "step: 693672, loss: 0.06315247714519501, data time: 0.042704922812325616\n",
      "step: 693673, loss: 0.06457964330911636, data time: 0.03761830925941467\n",
      "step: 693674, loss: 0.05902429670095444, data time: 0.03358430332607693\n",
      "step: 693675, loss: 0.05835895985364914, data time: 0.030426836013793944\n",
      "step: 693676, loss: 0.062127839773893356, data time: 0.027860077944668857\n",
      "step: 693677, loss: 0.05966570973396301, data time: 0.02572937806447347\n",
      "step: 693678, loss: 0.06745149195194244, data time: 0.023915180793175332\n",
      "step: 693679, loss: 0.05662459135055542, data time: 0.022354858262198313\n",
      "step: 693680, loss: 0.062464095652103424, data time: 0.021002976099650066\n",
      "step: 693681, loss: 0.06011315435171127, data time: 0.01981484889984131\n",
      "step: 693682, loss: 0.06070796772837639, data time: 0.018772405736586627\n",
      "step: 693683, loss: 0.06868164241313934, data time: 0.017840716573927138\n",
      "step: 693684, loss: 0.06526218354701996, data time: 0.017008141467445774\n",
      "step: 693685, loss: 0.05757945030927658, data time: 0.016272914409637452\n",
      "step: 693686, loss: 0.06484508514404297, data time: 0.015598433358328683\n",
      "step: 693687, loss: 0.06653116643428802, data time: 0.014986645091663708\n",
      "step: 693688, loss: 0.055013835430145264, data time: 0.0144247697747272\n",
      "step: 693689, loss: 0.06135645508766174, data time: 0.013909210761388143\n",
      "step: 693690, loss: 0.059414129704236984, data time: 0.013433361053466797\n",
      "step: 693691, loss: 0.06184796616435051, data time: 0.012997966546278734\n",
      "step: 693692, loss: 0.05993937700986862, data time: 0.012588624601010923\n",
      "step: 693693, loss: 0.06074298545718193, data time: 0.012210275445665632\n",
      "step: 693694, loss: 0.06187842786312103, data time: 0.01188035668997929\n",
      "step: 693695, loss: 0.06503152847290039, data time: 0.011558620134989421\n",
      "step: 693696, loss: 0.05770990252494812, data time: 0.011256564048028762\n",
      "step: 693697, loss: 0.05686337500810623, data time: 0.01097937673330307\n",
      "step: 693698, loss: 0.05514002591371536, data time: 0.010705052000103575\n",
      "step: 693699, loss: 0.05461273714900017, data time: 0.010447460062363568\n",
      "step: 693700, loss: 0.059923261404037476, data time: 0.010203238895961217\n",
      "step: 693701, loss: 0.05796866863965988, data time: 0.009970870282914903\n",
      "step: 693702, loss: 0.06098451465368271, data time: 0.009753162796432907\n",
      "step: 693703, loss: 0.06589166820049286, data time: 0.009550245184647409\n",
      "step: 693704, loss: 0.06522315740585327, data time: 0.009357128387842422\n",
      "step: 693705, loss: 0.03638670593500137, data time: 0.009174460172653198\n",
      "step: 693706, loss: 0.06416738033294678, data time: 0.269866943359375\n",
      "step: 693707, loss: 0.06365081667900085, data time: 0.13607323169708252\n",
      "step: 693708, loss: 0.05880635231733322, data time: 0.09182262420654297\n",
      "step: 693709, loss: 0.06061149761080742, data time: 0.06963938474655151\n",
      "step: 693710, loss: 0.06063089892268181, data time: 0.056000709533691406\n",
      "step: 693711, loss: 0.06519308686256409, data time: 0.046911279360453285\n",
      "step: 693712, loss: 0.06167256459593773, data time: 0.04042165619986398\n",
      "step: 693713, loss: 0.062180034816265106, data time: 0.03562438488006592\n",
      "step: 693714, loss: 0.056586869060993195, data time: 0.03182490666707357\n",
      "step: 693715, loss: 0.06307937204837799, data time: 0.0288485050201416\n",
      "step: 693716, loss: 0.061954908072948456, data time: 0.026420398191972214\n",
      "step: 693717, loss: 0.05877786502242088, data time: 0.02440541982650757\n",
      "step: 693718, loss: 0.05991266667842865, data time: 0.0226901677938608\n",
      "step: 693719, loss: 0.05773107334971428, data time: 0.02121227128165109\n",
      "step: 693720, loss: 0.056981295347213745, data time: 0.019933096567789712\n",
      "step: 693721, loss: 0.06114761158823967, data time: 0.01881273090839386\n",
      "step: 693722, loss: 0.0574963316321373, data time: 0.01782387845656451\n",
      "step: 693723, loss: 0.06134562939405441, data time: 0.0169472164577908\n",
      "step: 693724, loss: 0.059139274060726166, data time: 0.01615937132584421\n",
      "step: 693725, loss: 0.06615667790174484, data time: 0.015474343299865722\n",
      "step: 693726, loss: 0.0586555115878582, data time: 0.014860164551507859\n",
      "step: 693727, loss: 0.06273545324802399, data time: 0.014296011491255327\n",
      "step: 693728, loss: 0.058237507939338684, data time: 0.01377689320108165\n",
      "step: 693729, loss: 0.06404713541269302, data time: 0.0133055051167806\n",
      "step: 693730, loss: 0.06037551909685135, data time: 0.012869606018066406\n",
      "step: 693731, loss: 0.06486746668815613, data time: 0.012467090900127705\n",
      "step: 693732, loss: 0.06704597175121307, data time: 0.012092970035694263\n",
      "step: 693733, loss: 0.05948524922132492, data time: 0.011744941983904158\n",
      "step: 693734, loss: 0.06719838082790375, data time: 0.011431751580073916\n",
      "step: 693735, loss: 0.056234341114759445, data time: 0.011135745048522949\n",
      "step: 693736, loss: 0.06342259049415588, data time: 0.01085762054689469\n",
      "step: 693737, loss: 0.06447216868400574, data time: 0.010599888861179352\n",
      "step: 693738, loss: 0.05959882587194443, data time: 0.01033963579120058\n",
      "step: 693739, loss: 0.0647740587592125, data time: 0.010091984973234288\n",
      "step: 693740, loss: 0.06416547298431396, data time: 0.009860188620431082\n",
      "step: 693741, loss: 0.05993248149752617, data time: 0.00963918368021647\n",
      "step: 693742, loss: 0.06474673748016357, data time: 0.00943107862730284\n",
      "step: 693743, loss: 0.06724957376718521, data time: 0.009235909110621401\n",
      "step: 693744, loss: 0.061538033187389374, data time: 0.009050638247758914\n",
      "step: 693745, loss: 0.07300437986850739, data time: 0.008874613046646117\n",
      "step: 693746, loss: 0.0602274015545845, data time: 0.2781708240509033\n",
      "step: 693747, loss: 0.06225308030843735, data time: 0.14065217971801758\n",
      "step: 693748, loss: 0.060035645961761475, data time: 0.09427833557128906\n",
      "step: 693749, loss: 0.06185251474380493, data time: 0.07158219814300537\n",
      "step: 693750, loss: 0.06670783460140228, data time: 0.057552671432495116\n",
      "step: 693751, loss: 0.05886427313089371, data time: 0.04820370674133301\n",
      "step: 693752, loss: 0.05952155590057373, data time: 0.041516780853271484\n",
      "step: 693753, loss: 0.05841919779777527, data time: 0.03658333420753479\n",
      "step: 693754, loss: 0.0633225366473198, data time: 0.03266988860236274\n",
      "step: 693755, loss: 0.05741097778081894, data time: 0.029596233367919923\n",
      "step: 693756, loss: 0.06118520349264145, data time: 0.027110056443647904\n",
      "step: 693757, loss: 0.060806624591350555, data time: 0.02503792444864909\n",
      "step: 693758, loss: 0.06649493426084518, data time: 0.02327247766348032\n",
      "step: 693759, loss: 0.060911890119314194, data time: 0.021758317947387695\n",
      "step: 693760, loss: 0.05731889605522156, data time: 0.020442803700764973\n",
      "step: 693761, loss: 0.06580473482608795, data time: 0.019289284944534302\n",
      "step: 693762, loss: 0.06377214938402176, data time: 0.01827563959009507\n",
      "step: 693763, loss: 0.06751970946788788, data time: 0.017373906241522893\n",
      "step: 693764, loss: 0.0702027827501297, data time: 0.016565360521015367\n",
      "step: 693765, loss: 0.0613536462187767, data time: 0.015866315364837645\n",
      "step: 693766, loss: 0.0618806928396225, data time: 0.015230439958118257\n",
      "step: 693767, loss: 0.06194121390581131, data time: 0.014647928151217375\n",
      "step: 693768, loss: 0.0654357373714447, data time: 0.014115882956463358\n",
      "step: 693769, loss: 0.06966917216777802, data time: 0.013627469539642334\n",
      "step: 693770, loss: 0.0605531744658947, data time: 0.013178396224975585\n",
      "step: 693771, loss: 0.06243012472987175, data time: 0.012767626689030575\n",
      "step: 693772, loss: 0.05847012624144554, data time: 0.012384079120777271\n",
      "step: 693773, loss: 0.06211640685796738, data time: 0.012025577681405204\n",
      "step: 693774, loss: 0.0626990795135498, data time: 0.011701485206340921\n",
      "step: 693775, loss: 0.06496032327413559, data time: 0.011394643783569336\n",
      "step: 693776, loss: 0.06585633009672165, data time: 0.011108952183877268\n",
      "step: 693777, loss: 0.06223175674676895, data time: 0.010844014585018158\n",
      "step: 693778, loss: 0.058162473142147064, data time: 0.01057746916106253\n",
      "step: 693779, loss: 0.06310233473777771, data time: 0.010327360209296732\n",
      "step: 693780, loss: 0.06447029113769531, data time: 0.010091659000941685\n",
      "step: 693781, loss: 0.05986201390624046, data time: 0.009865661462148031\n",
      "step: 693782, loss: 0.05701832100749016, data time: 0.009652563043542811\n",
      "step: 693783, loss: 0.06645447760820389, data time: 0.009455624379609761\n",
      "step: 693784, loss: 0.05559147521853447, data time: 0.009268876833793445\n",
      "step: 693785, loss: 0.040367983281612396, data time: 0.00909152626991272\n",
      "step: 693786, loss: 0.06913215667009354, data time: 0.27834081649780273\n",
      "step: 693787, loss: 0.059036582708358765, data time: 0.13994896411895752\n",
      "step: 693788, loss: 0.06240161508321762, data time: 0.09420402844746907\n",
      "step: 693789, loss: 0.06396319717168808, data time: 0.07140249013900757\n",
      "step: 693790, loss: 0.05921371281147003, data time: 0.05741648674011231\n",
      "step: 693791, loss: 0.07206861674785614, data time: 0.0480948289235433\n",
      "step: 693792, loss: 0.06327857077121735, data time: 0.04143064362662179\n",
      "step: 693793, loss: 0.06089121103286743, data time: 0.036510318517684937\n",
      "step: 693794, loss: 0.05873655527830124, data time: 0.03260805871751574\n",
      "step: 693795, loss: 0.06181926652789116, data time: 0.02955319881439209\n",
      "step: 693796, loss: 0.059880997985601425, data time: 0.027072277936068447\n",
      "step: 693797, loss: 0.061456285417079926, data time: 0.02500989039738973\n",
      "step: 693798, loss: 0.057068876922130585, data time: 0.023254559590266302\n",
      "step: 693799, loss: 0.058366745710372925, data time: 0.021734867777143205\n",
      "step: 693800, loss: 0.06890390068292618, data time: 0.020425971349080405\n",
      "step: 693801, loss: 0.059719018638134, data time: 0.019282430410385132\n",
      "step: 693802, loss: 0.06176137179136276, data time: 0.018267224816715017\n",
      "step: 693803, loss: 0.06169154495000839, data time: 0.017367336485120986\n",
      "step: 693804, loss: 0.06407979130744934, data time: 0.016559374959845292\n",
      "step: 693805, loss: 0.06642082333564758, data time: 0.01584033966064453\n",
      "step: 693806, loss: 0.06238170713186264, data time: 0.015194665817987351\n",
      "step: 693807, loss: 0.06930410116910934, data time: 0.014599388295953924\n",
      "step: 693808, loss: 0.06266134232282639, data time: 0.014052505078523056\n",
      "step: 693809, loss: 0.05557318776845932, data time: 0.013566205898920694\n",
      "step: 693810, loss: 0.06225641816854477, data time: 0.013116979598999023\n",
      "step: 693811, loss: 0.06263059377670288, data time: 0.012707774455730732\n",
      "step: 693812, loss: 0.06059664487838745, data time: 0.012312403431645146\n",
      "step: 693813, loss: 0.061042170971632004, data time: 0.011944711208343506\n",
      "step: 693814, loss: 0.06692571938037872, data time: 0.01160962828274431\n",
      "step: 693815, loss: 0.056949254125356674, data time: 0.01129450003306071\n",
      "step: 693816, loss: 0.06788884103298187, data time: 0.010999594965288717\n",
      "step: 693817, loss: 0.06236220896244049, data time: 0.010725922882556915\n",
      "step: 693818, loss: 0.06395896524190903, data time: 0.010459032925692472\n",
      "step: 693819, loss: 0.06153377890586853, data time: 0.01020967960357666\n",
      "step: 693820, loss: 0.0608501099050045, data time: 0.009973430633544922\n",
      "step: 693821, loss: 0.0600380003452301, data time: 0.00976028045018514\n",
      "step: 693822, loss: 0.06087775155901909, data time: 0.009547716862446553\n",
      "step: 693823, loss: 0.06112832948565483, data time: 0.00934997985237523\n",
      "step: 693824, loss: 0.0598866268992424, data time: 0.009164070471739158\n",
      "step: 693825, loss: 0.07053172588348389, data time: 0.00898612141609192\n",
      "step: 693826, loss: 0.058802634477615356, data time: 0.2715878486633301\n",
      "step: 693827, loss: 0.06921159476041794, data time: 0.137626051902771\n",
      "step: 693828, loss: 0.060067664831876755, data time: 0.09239395459493001\n",
      "step: 693829, loss: 0.06385332345962524, data time: 0.07020586729049683\n",
      "step: 693830, loss: 0.06480199098587036, data time: 0.056485223770141604\n",
      "step: 693831, loss: 0.05900224670767784, data time: 0.047345598538716636\n",
      "step: 693832, loss: 0.0579565167427063, data time: 0.040830680302211216\n",
      "step: 693833, loss: 0.06653561443090439, data time: 0.036031752824783325\n",
      "step: 693834, loss: 0.06309927254915237, data time: 0.032234377331203885\n",
      "step: 693835, loss: 0.0566013902425766, data time: 0.029245781898498534\n",
      "step: 693836, loss: 0.06684748083353043, data time: 0.026821136474609375\n",
      "step: 693837, loss: 0.059625934809446335, data time: 0.024796684583028156\n",
      "step: 693838, loss: 0.06446889042854309, data time: 0.023081504381619967\n",
      "step: 693839, loss: 0.06552833318710327, data time: 0.02160278388432094\n",
      "step: 693840, loss: 0.06540565192699432, data time: 0.020323816935221353\n",
      "step: 693841, loss: 0.06294938176870346, data time: 0.019203126430511475\n",
      "step: 693842, loss: 0.05787203833460808, data time: 0.018214772729312673\n",
      "step: 693843, loss: 0.06511183828115463, data time: 0.017345746358235676\n",
      "step: 693844, loss: 0.057591915130615234, data time: 0.01653518174823962\n",
      "step: 693845, loss: 0.061190031468868256, data time: 0.015814733505249024\n",
      "step: 693846, loss: 0.060508161783218384, data time: 0.015168871198381697\n",
      "step: 693847, loss: 0.06319504231214523, data time: 0.014583349227905273\n",
      "step: 693848, loss: 0.0632103830575943, data time: 0.014043486636617909\n",
      "step: 693849, loss: 0.05738413333892822, data time: 0.013542254765828451\n",
      "step: 693850, loss: 0.061190225183963776, data time: 0.01308481216430664\n",
      "step: 693851, loss: 0.05879393219947815, data time: 0.012656972958491398\n",
      "step: 693852, loss: 0.0643707662820816, data time: 0.012266512270326968\n",
      "step: 693853, loss: 0.06294914335012436, data time: 0.011898355824606759\n",
      "step: 693854, loss: 0.05926964804530144, data time: 0.01156398345684183\n",
      "step: 693855, loss: 0.05990973860025406, data time: 0.011249407132466634\n",
      "step: 693856, loss: 0.061457760632038116, data time: 0.010952757250878119\n",
      "step: 693857, loss: 0.06192545220255852, data time: 0.010680332779884338\n",
      "step: 693858, loss: 0.05990075320005417, data time: 0.010413769519690311\n",
      "step: 693859, loss: 0.06317128241062164, data time: 0.01016397335950066\n",
      "step: 693860, loss: 0.06447918713092804, data time: 0.009926407677786691\n",
      "step: 693861, loss: 0.06383390724658966, data time: 0.009706430964999728\n",
      "step: 693862, loss: 0.05982361361384392, data time: 0.009494214444547086\n",
      "step: 693863, loss: 0.06544869393110275, data time: 0.009297998327957956\n",
      "step: 693864, loss: 0.06412937492132187, data time: 0.009114711712568235\n",
      "step: 693865, loss: 0.05286534130573273, data time: 0.008936375379562378\n",
      "step: 693866, loss: 0.056530289351940155, data time: 0.2834746837615967\n",
      "step: 693867, loss: 0.05838456004858017, data time: 0.14284229278564453\n",
      "step: 693868, loss: 0.06778557598590851, data time: 0.09573650360107422\n",
      "step: 693869, loss: 0.06285735219717026, data time: 0.07265114784240723\n",
      "step: 693870, loss: 0.06521846354007721, data time: 0.058396530151367185\n",
      "step: 693871, loss: 0.062266357243061066, data time: 0.04889472325642904\n",
      "step: 693872, loss: 0.05848069116473198, data time: 0.04211344037737165\n",
      "step: 693873, loss: 0.06115689128637314, data time: 0.037101566791534424\n",
      "step: 693874, loss: 0.0649629682302475, data time: 0.033154302173190646\n",
      "step: 693875, loss: 0.06430674344301224, data time: 0.030047345161437988\n",
      "step: 693876, loss: 0.05957610160112381, data time: 0.027507565238259056\n",
      "step: 693877, loss: 0.06540252268314362, data time: 0.02539229393005371\n",
      "step: 693878, loss: 0.06598849594593048, data time: 0.023605218300452598\n",
      "step: 693879, loss: 0.06203056126832962, data time: 0.022062812532697405\n",
      "step: 693880, loss: 0.052551209926605225, data time: 0.020731035868326822\n",
      "step: 693881, loss: 0.06729255616664886, data time: 0.019561052322387695\n",
      "step: 693882, loss: 0.05855049937963486, data time: 0.01852895231807933\n",
      "step: 693883, loss: 0.06670915335416794, data time: 0.017621159553527832\n",
      "step: 693884, loss: 0.06461171805858612, data time: 0.01679932443719161\n",
      "step: 693885, loss: 0.06169212982058525, data time: 0.01606771945953369\n",
      "step: 693886, loss: 0.06190454959869385, data time: 0.015405757086617606\n",
      "step: 693887, loss: 0.06763815879821777, data time: 0.014802152460271662\n",
      "step: 693888, loss: 0.06850185990333557, data time: 0.01424848515054454\n",
      "step: 693889, loss: 0.06110215187072754, data time: 0.013738473256429037\n",
      "step: 693890, loss: 0.0566897839307785, data time: 0.013269739151000976\n",
      "step: 693891, loss: 0.06451287865638733, data time: 0.012836327919593224\n",
      "step: 693892, loss: 0.06515368819236755, data time: 0.012439975032099971\n",
      "step: 693893, loss: 0.05677584931254387, data time: 0.01207350833075387\n",
      "step: 693894, loss: 0.06547367572784424, data time: 0.011732158989741885\n",
      "step: 693895, loss: 0.0698205828666687, data time: 0.011415664354960125\n",
      "step: 693896, loss: 0.06510423868894577, data time: 0.011115912468202652\n",
      "step: 693897, loss: 0.06599895656108856, data time: 0.010837055742740631\n",
      "step: 693898, loss: 0.058744609355926514, data time: 0.010565858898740826\n",
      "step: 693899, loss: 0.06433127820491791, data time: 0.010310860241160673\n",
      "step: 693900, loss: 0.06198565289378166, data time: 0.010071073259626116\n",
      "step: 693901, loss: 0.060757413506507874, data time: 0.009848137696584066\n",
      "step: 693902, loss: 0.06802202016115189, data time: 0.009634662318874049\n",
      "step: 693903, loss: 0.058524470776319504, data time: 0.00943496352747867\n",
      "step: 693904, loss: 0.06258562952280045, data time: 0.009245921403933793\n",
      "step: 693905, loss: 0.07893943786621094, data time: 0.009064590930938721\n",
      "step: 693906, loss: 0.06062600016593933, data time: 0.2729039192199707\n",
      "step: 693907, loss: 0.06375367939472198, data time: 0.13723671436309814\n",
      "step: 693908, loss: 0.06509570777416229, data time: 0.09229516983032227\n",
      "step: 693909, loss: 0.06388074159622192, data time: 0.07007461786270142\n",
      "step: 693910, loss: 0.06672360748052597, data time: 0.05634040832519531\n",
      "step: 693911, loss: 0.06121907755732536, data time: 0.04719225565592448\n",
      "step: 693912, loss: 0.0610555037856102, data time: 0.040660040719168525\n",
      "step: 693913, loss: 0.057115744799375534, data time: 0.03584492206573486\n",
      "step: 693914, loss: 0.06906428933143616, data time: 0.03203696674770779\n",
      "step: 693915, loss: 0.057905785739421844, data time: 0.029041409492492676\n",
      "step: 693916, loss: 0.05991310626268387, data time: 0.02659687128933993\n",
      "step: 693917, loss: 0.06413377821445465, data time: 0.024557610352834065\n",
      "step: 693918, loss: 0.055795229971408844, data time: 0.022828413889958307\n",
      "step: 693919, loss: 0.061031293123960495, data time: 0.021349413054330007\n",
      "step: 693920, loss: 0.06462027132511139, data time: 0.020064560572306316\n",
      "step: 693921, loss: 0.053866781294345856, data time: 0.018946364521980286\n",
      "step: 693922, loss: 0.06525640189647675, data time: 0.017951348248650047\n",
      "step: 693923, loss: 0.05773518979549408, data time: 0.017083618375990126\n",
      "step: 693924, loss: 0.06443099677562714, data time: 0.016295269915932102\n",
      "step: 693925, loss: 0.06908786296844482, data time: 0.01559586524963379\n",
      "step: 693926, loss: 0.05461994931101799, data time: 0.014960924784342447\n",
      "step: 693927, loss: 0.060386158525943756, data time: 0.014375383203679865\n",
      "step: 693928, loss: 0.061712972819805145, data time: 0.013840965602708899\n",
      "step: 693929, loss: 0.06901945918798447, data time: 0.013349980115890503\n",
      "step: 693930, loss: 0.06540479511022568, data time: 0.012897996902465821\n",
      "step: 693931, loss: 0.06008981913328171, data time: 0.012482056250939002\n",
      "step: 693932, loss: 0.06213395297527313, data time: 0.012100855509440104\n",
      "step: 693933, loss: 0.05907950550317764, data time: 0.011745546545301164\n",
      "step: 693934, loss: 0.06889929622411728, data time: 0.011415514452704069\n",
      "step: 693935, loss: 0.06109042093157768, data time: 0.011107786496480306\n",
      "step: 693936, loss: 0.058680981397628784, data time: 0.010818696791125883\n",
      "step: 693937, loss: 0.06689061969518661, data time: 0.010554559528827667\n",
      "step: 693938, loss: 0.05666990205645561, data time: 0.010295463330817945\n",
      "step: 693939, loss: 0.06924629211425781, data time: 0.010048410471747904\n",
      "step: 693940, loss: 0.06537631154060364, data time: 0.009824316842215401\n",
      "step: 693941, loss: 0.06214561685919762, data time: 0.009608255492316352\n",
      "step: 693942, loss: 0.06116765737533569, data time: 0.009398866344142604\n",
      "step: 693943, loss: 0.06410301476716995, data time: 0.009206263642562064\n",
      "step: 693944, loss: 0.05897141993045807, data time: 0.009024210465260042\n",
      "step: 693945, loss: 0.07012660801410675, data time: 0.008849161863327026\n",
      "step: 693946, loss: 0.06523434072732925, data time: 0.26779747009277344\n",
      "step: 693947, loss: 0.0680035948753357, data time: 0.13549864292144775\n",
      "step: 693948, loss: 0.07101617753505707, data time: 0.09128546714782715\n",
      "step: 693949, loss: 0.06773512065410614, data time: 0.06924360990524292\n",
      "step: 693950, loss: 0.06705224514007568, data time: 0.055687904357910156\n",
      "step: 693951, loss: 0.06386619806289673, data time: 0.04665482044219971\n",
      "step: 693952, loss: 0.05959097668528557, data time: 0.040196623120989115\n",
      "step: 693953, loss: 0.06737977266311646, data time: 0.035420000553131104\n",
      "step: 693954, loss: 0.06339842826128006, data time: 0.03165705998738607\n",
      "step: 693955, loss: 0.06029282137751579, data time: 0.028737425804138184\n",
      "step: 693956, loss: 0.07184861600399017, data time: 0.026354486292058773\n",
      "step: 693957, loss: 0.06404470652341843, data time: 0.024370114008585613\n",
      "step: 693958, loss: 0.05189405009150505, data time: 0.022682795157799356\n",
      "step: 693959, loss: 0.05885528028011322, data time: 0.021236181259155273\n",
      "step: 693960, loss: 0.06320425122976303, data time: 0.019984674453735352\n",
      "step: 693961, loss: 0.060984015464782715, data time: 0.0188857764005661\n",
      "step: 693962, loss: 0.06162163242697716, data time: 0.017914659836713004\n",
      "step: 693963, loss: 0.06163838133215904, data time: 0.017060478528340656\n",
      "step: 693964, loss: 0.06788522005081177, data time: 0.016290953284815737\n",
      "step: 693965, loss: 0.06522437930107117, data time: 0.015602242946624757\n",
      "step: 693966, loss: 0.06154882162809372, data time: 0.014979782558622815\n",
      "step: 693967, loss: 0.06505602598190308, data time: 0.014412641525268555\n",
      "step: 693968, loss: 0.06672649830579758, data time: 0.01389208047286324\n",
      "step: 693969, loss: 0.06905096769332886, data time: 0.013412654399871826\n",
      "step: 693970, loss: 0.06381377577781677, data time: 0.012973012924194336\n",
      "step: 693971, loss: 0.06046532839536667, data time: 0.012566795715918908\n",
      "step: 693972, loss: 0.06034519523382187, data time: 0.012196699778238932\n",
      "step: 693973, loss: 0.06834109127521515, data time: 0.011846431664058141\n",
      "step: 693974, loss: 0.0610460564494133, data time: 0.011524052455507475\n",
      "step: 693975, loss: 0.06849300861358643, data time: 0.011224079132080077\n",
      "step: 693976, loss: 0.06139146536588669, data time: 0.010940659430719191\n",
      "step: 693977, loss: 0.06597088277339935, data time: 0.010682784020900726\n",
      "step: 693978, loss: 0.06050047278404236, data time: 0.01042204914671002\n",
      "step: 693979, loss: 0.058766499161720276, data time: 0.010175270192763385\n",
      "step: 693980, loss: 0.05846167728304863, data time: 0.009943110602242606\n",
      "step: 693981, loss: 0.05918402597308159, data time: 0.009727550877465142\n",
      "step: 693982, loss: 0.05996199697256088, data time: 0.009519009976773648\n",
      "step: 693983, loss: 0.06482498347759247, data time: 0.0093258493825009\n",
      "step: 693984, loss: 0.06528618186712265, data time: 0.009142759518745618\n",
      "step: 693985, loss: 0.07427255809307098, data time: 0.00896785855293274\n",
      "step: 693986, loss: 0.06360401213169098, data time: 0.2682676315307617\n",
      "step: 693987, loss: 0.058451540768146515, data time: 0.13550543785095215\n",
      "step: 693988, loss: 0.057363223284482956, data time: 0.09160820643107097\n",
      "step: 693989, loss: 0.06429179757833481, data time: 0.06906765699386597\n",
      "step: 693990, loss: 0.06917205452919006, data time: 0.05553832054138184\n",
      "step: 693991, loss: 0.0645964965224266, data time: 0.046518405278523765\n",
      "step: 693992, loss: 0.06725713610649109, data time: 0.04021716117858887\n",
      "step: 693993, loss: 0.06763739138841629, data time: 0.03538200259208679\n",
      "step: 693994, loss: 0.06539282202720642, data time: 0.031625297334459096\n",
      "step: 693995, loss: 0.06221584603190422, data time: 0.028663039207458496\n",
      "step: 693996, loss: 0.06534396857023239, data time: 0.026252724907614967\n",
      "step: 693997, loss: 0.06941170990467072, data time: 0.02424156665802002\n",
      "step: 693998, loss: 0.06278207153081894, data time: 0.02253899207481971\n",
      "step: 693999, loss: 0.06538857519626617, data time: 0.021077990531921387\n",
      "step: 694000, loss: 0.060173340141773224, data time: 0.0198088010152181\n",
      "step: 694001, loss: 0.06573313474655151, data time: 0.018705308437347412\n",
      "step: 694002, loss: 0.06104950234293938, data time: 0.017730376299689796\n",
      "step: 694003, loss: 0.06553131341934204, data time: 0.016865650812784832\n",
      "step: 694004, loss: 0.06505947560071945, data time: 0.01608720578645405\n",
      "step: 694005, loss: 0.05983912944793701, data time: 0.015394866466522217\n",
      "step: 694006, loss: 0.057206518948078156, data time: 0.014767885208129883\n",
      "step: 694007, loss: 0.05742855370044708, data time: 0.014193621548739347\n",
      "step: 694008, loss: 0.0601535439491272, data time: 0.013671294502590014\n",
      "step: 694009, loss: 0.05900656804442406, data time: 0.013189852237701416\n",
      "step: 694010, loss: 0.060398392379283905, data time: 0.012744264602661133\n",
      "step: 694011, loss: 0.06154940649867058, data time: 0.012335896492004395\n",
      "step: 694012, loss: 0.0631893128156662, data time: 0.0119595174436216\n",
      "step: 694013, loss: 0.0574398934841156, data time: 0.011606948716299874\n",
      "step: 694014, loss: 0.057232245802879333, data time: 0.011281868507122171\n",
      "step: 694015, loss: 0.05886651203036308, data time: 0.010980693499247234\n",
      "step: 694016, loss: 0.058410435914993286, data time: 0.010698826082291142\n",
      "step: 694017, loss: 0.05727626383304596, data time: 0.010436013340950012\n",
      "step: 694018, loss: 0.058448925614356995, data time: 0.010178103591456558\n",
      "step: 694019, loss: 0.06069337576627731, data time: 0.009935834828545065\n",
      "step: 694020, loss: 0.062494970858097076, data time: 0.009710741043090821\n",
      "step: 694021, loss: 0.05577777326107025, data time: 0.009500006834665934\n",
      "step: 694022, loss: 0.06106458231806755, data time: 0.009295888849206874\n",
      "step: 694023, loss: 0.06155749410390854, data time: 0.009105167890849867\n",
      "step: 694024, loss: 0.06825714558362961, data time: 0.008923958509396285\n",
      "step: 694025, loss: 0.08064299076795578, data time: 0.00874972939491272\n",
      "step: 694026, loss: 0.06044068932533264, data time: 0.28467559814453125\n",
      "step: 694027, loss: 0.05819406360387802, data time: 0.14415550231933594\n",
      "step: 694028, loss: 0.06021883338689804, data time: 0.09660482406616211\n",
      "step: 694029, loss: 0.06167032942175865, data time: 0.07321959733963013\n",
      "step: 694030, loss: 0.06277769804000854, data time: 0.058864259719848634\n",
      "step: 694031, loss: 0.05853765830397606, data time: 0.04929546515146891\n",
      "step: 694032, loss: 0.05914686247706413, data time: 0.042450053351266046\n",
      "step: 694033, loss: 0.06501301378011703, data time: 0.037412017583847046\n",
      "step: 694034, loss: 0.06410308182239532, data time: 0.03344119919670953\n",
      "step: 694035, loss: 0.05937575548887253, data time: 0.030310511589050293\n",
      "step: 694036, loss: 0.05714711546897888, data time: 0.027753569863059303\n",
      "step: 694037, loss: 0.06207007169723511, data time: 0.025621155897776287\n",
      "step: 694038, loss: 0.06445255130529404, data time: 0.023812312346238356\n",
      "step: 694039, loss: 0.05878034234046936, data time: 0.02225661277770996\n",
      "step: 694040, loss: 0.06332279741764069, data time: 0.020906654993693034\n",
      "step: 694041, loss: 0.060004036873579025, data time: 0.019735634326934814\n",
      "step: 694042, loss: 0.06434204429388046, data time: 0.018716966404634362\n",
      "step: 694043, loss: 0.06335356831550598, data time: 0.017817762162950303\n",
      "step: 694044, loss: 0.05873081833124161, data time: 0.01700742621170847\n",
      "step: 694045, loss: 0.05847237631678581, data time: 0.016286373138427734\n",
      "step: 694046, loss: 0.06224223971366882, data time: 0.015630642573038738\n",
      "step: 694047, loss: 0.06778081506490707, data time: 0.015031088482249867\n",
      "step: 694048, loss: 0.05834090709686279, data time: 0.014482498168945312\n",
      "step: 694049, loss: 0.06524480134248734, data time: 0.013982037703196207\n",
      "step: 694050, loss: 0.06062335893511772, data time: 0.013521146774291993\n",
      "step: 694051, loss: 0.05948951467871666, data time: 0.013092710421635555\n",
      "step: 694052, loss: 0.06809233129024506, data time: 0.012703869077894423\n",
      "step: 694053, loss: 0.06450510770082474, data time: 0.012335283415658134\n",
      "step: 694054, loss: 0.06155618280172348, data time: 0.01199911380636281\n",
      "step: 694055, loss: 0.062268152832984924, data time: 0.01168367862701416\n",
      "step: 694056, loss: 0.0698990598320961, data time: 0.011390216888919953\n",
      "step: 694057, loss: 0.06405916810035706, data time: 0.01111513376235962\n",
      "step: 694058, loss: 0.06121532618999481, data time: 0.010840704946806936\n",
      "step: 694059, loss: 0.06517769396305084, data time: 0.010582026313332951\n",
      "step: 694060, loss: 0.06420595198869705, data time: 0.010338578905378069\n",
      "step: 694061, loss: 0.06315325200557709, data time: 0.010111894872453477\n",
      "step: 694062, loss: 0.06759139150381088, data time: 0.009894267932788746\n",
      "step: 694063, loss: 0.059052854776382446, data time: 0.009690109052156148\n",
      "step: 694064, loss: 0.06504445523023605, data time: 0.009497245152791342\n",
      "step: 694065, loss: 0.0484001487493515, data time: 0.009312468767166137\n",
      "step: 694066, loss: 0.06168565899133682, data time: 0.2883429527282715\n",
      "step: 694067, loss: 0.0661420002579689, data time: 0.14489412307739258\n",
      "step: 694068, loss: 0.06331796944141388, data time: 0.09710407257080078\n",
      "step: 694069, loss: 0.06077961623668671, data time: 0.07361578941345215\n",
      "step: 694070, loss: 0.06618069857358932, data time: 0.05916643142700195\n",
      "step: 694071, loss: 0.060477569699287415, data time: 0.049540082613627114\n",
      "step: 694072, loss: 0.06251326948404312, data time: 0.04266858100891113\n",
      "step: 694073, loss: 0.0586765818297863, data time: 0.0376373827457428\n",
      "step: 694074, loss: 0.05783945322036743, data time: 0.033632225460476346\n",
      "step: 694075, loss: 0.06713201105594635, data time: 0.030473971366882326\n",
      "step: 694076, loss: 0.06325829029083252, data time: 0.027896664359352806\n",
      "step: 694077, loss: 0.060374461114406586, data time: 0.025759299596150715\n",
      "step: 694078, loss: 0.0616905614733696, data time: 0.023944744696983926\n",
      "step: 694079, loss: 0.06558119505643845, data time: 0.022377525057111467\n",
      "step: 694080, loss: 0.060147106647491455, data time: 0.021025975545247395\n",
      "step: 694081, loss: 0.0606139600276947, data time: 0.01983824372291565\n",
      "step: 694082, loss: 0.058905865997076035, data time: 0.01878958589890424\n",
      "step: 694083, loss: 0.06648463010787964, data time: 0.017865339914957683\n",
      "step: 694084, loss: 0.06457363069057465, data time: 0.017032133905511154\n",
      "step: 694085, loss: 0.06147228926420212, data time: 0.01629023551940918\n",
      "step: 694086, loss: 0.0626380667090416, data time: 0.015632583981468565\n",
      "step: 694087, loss: 0.0618954598903656, data time: 0.015041492202065208\n",
      "step: 694088, loss: 0.06572604924440384, data time: 0.014493880064591118\n",
      "step: 694089, loss: 0.0668470710515976, data time: 0.013991326093673706\n",
      "step: 694090, loss: 0.058311671018600464, data time: 0.013526077270507813\n",
      "step: 694091, loss: 0.06263374537229538, data time: 0.013098881794856144\n",
      "step: 694092, loss: 0.06857823580503464, data time: 0.01270692436783402\n",
      "step: 694093, loss: 0.06116541475057602, data time: 0.012338783059801375\n",
      "step: 694094, loss: 0.060854073613882065, data time: 0.011997847721494478\n",
      "step: 694095, loss: 0.06430666893720627, data time: 0.011681326230367025\n",
      "step: 694096, loss: 0.06915836036205292, data time: 0.011387194356610697\n",
      "step: 694097, loss: 0.055708907544612885, data time: 0.01111508160829544\n",
      "step: 694098, loss: 0.05964851379394531, data time: 0.010842410000887785\n",
      "step: 694099, loss: 0.060149580240249634, data time: 0.010584473609924316\n",
      "step: 694100, loss: 0.05759891867637634, data time: 0.010340888159615653\n",
      "step: 694101, loss: 0.0632731020450592, data time: 0.01011600759294298\n",
      "step: 694102, loss: 0.06535781919956207, data time: 0.009898578798448717\n",
      "step: 694103, loss: 0.05741749703884125, data time: 0.009694588811774003\n",
      "step: 694104, loss: 0.061969876289367676, data time: 0.009500405727288662\n",
      "step: 694105, loss: 0.061413370072841644, data time: 0.009316772222518921\n",
      "step: 694106, loss: 0.055744707584381104, data time: 0.2822251319885254\n",
      "step: 694107, loss: 0.05405987426638603, data time: 0.14253580570220947\n",
      "step: 694108, loss: 0.05993425473570824, data time: 0.09595123926798503\n",
      "step: 694109, loss: 0.06057536602020264, data time: 0.07273894548416138\n",
      "step: 694110, loss: 0.06284765899181366, data time: 0.05849919319152832\n",
      "step: 694111, loss: 0.061354465782642365, data time: 0.04899362723032633\n",
      "step: 694112, loss: 0.06676938384771347, data time: 0.042192697525024414\n",
      "step: 694113, loss: 0.06415417790412903, data time: 0.03718233108520508\n",
      "step: 694114, loss: 0.061788976192474365, data time: 0.033216502931382924\n",
      "step: 694115, loss: 0.06285302340984344, data time: 0.0300947904586792\n",
      "step: 694116, loss: 0.06462327390909195, data time: 0.02755130421031605\n",
      "step: 694117, loss: 0.06218116730451584, data time: 0.025436679522196453\n",
      "step: 694118, loss: 0.06166224926710129, data time: 0.023645621079664964\n",
      "step: 694119, loss: 0.06230394169688225, data time: 0.0221076352255685\n",
      "step: 694120, loss: 0.05613498389720917, data time: 0.020768165588378906\n",
      "step: 694121, loss: 0.05581703409552574, data time: 0.019595324993133545\n",
      "step: 694122, loss: 0.06294330954551697, data time: 0.01856411204618566\n",
      "step: 694123, loss: 0.07080990076065063, data time: 0.017652273178100586\n",
      "step: 694124, loss: 0.06295288354158401, data time: 0.016828210730301708\n",
      "step: 694125, loss: 0.05886722356081009, data time: 0.016092610359191895\n",
      "step: 694126, loss: 0.06181634962558746, data time: 0.01542753265017555\n",
      "step: 694127, loss: 0.06299483776092529, data time: 0.014822504737160423\n",
      "step: 694128, loss: 0.05564427375793457, data time: 0.014273031898166822\n",
      "step: 694129, loss: 0.06428514420986176, data time: 0.013764719168345133\n",
      "step: 694130, loss: 0.05590395629405975, data time: 0.013297758102416991\n",
      "step: 694131, loss: 0.0659896731376648, data time: 0.012865139887883114\n",
      "step: 694132, loss: 0.06406070291996002, data time: 0.012468134915387188\n",
      "step: 694133, loss: 0.06465936452150345, data time: 0.012095868587493896\n",
      "step: 694134, loss: 0.06424237787723541, data time: 0.011751142041436556\n",
      "step: 694135, loss: 0.06491613388061523, data time: 0.011431026458740234\n",
      "step: 694136, loss: 0.06443627923727036, data time: 0.011133317024477066\n",
      "step: 694137, loss: 0.06928299367427826, data time: 0.01086176186800003\n",
      "step: 694138, loss: 0.05988064035773277, data time: 0.010591340787482985\n",
      "step: 694139, loss: 0.06159127876162529, data time: 0.01033636401681339\n",
      "step: 694140, loss: 0.06171467900276184, data time: 0.010094812938145228\n",
      "step: 694141, loss: 0.06016802415251732, data time: 0.009870462947421603\n",
      "step: 694142, loss: 0.05854138359427452, data time: 0.009653999998762802\n",
      "step: 694143, loss: 0.06047424301505089, data time: 0.009454137400576943\n",
      "step: 694144, loss: 0.06125771999359131, data time: 0.009263943403195113\n",
      "step: 694145, loss: 0.0697089433670044, data time: 0.009086066484451294\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.40792226791381836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 694146, loss: 0.05732495337724686, data time: 0.2739865779876709\n",
      "step: 694147, loss: 0.06688503921031952, data time: 0.13833999633789062\n",
      "step: 694148, loss: 0.07387818396091461, data time: 0.09326791763305664\n",
      "step: 694149, loss: 0.06468217074871063, data time: 0.07059293985366821\n",
      "step: 694150, loss: 0.06432801485061646, data time: 0.05674290657043457\n",
      "step: 694151, loss: 0.06219610199332237, data time: 0.04751793543497721\n",
      "step: 694152, loss: 0.06323559582233429, data time: 0.04093711716788156\n",
      "step: 694153, loss: 0.0594855360686779, data time: 0.03608086705207825\n",
      "step: 694154, loss: 0.06280969828367233, data time: 0.03221975432501899\n",
      "step: 694155, loss: 0.06253080815076828, data time: 0.029209446907043458\n",
      "step: 694156, loss: 0.06119748204946518, data time: 0.026743607087568802\n",
      "step: 694157, loss: 0.06812756508588791, data time: 0.024687965710957844\n",
      "step: 694158, loss: 0.06985689699649811, data time: 0.02294457875765287\n",
      "step: 694159, loss: 0.06446614861488342, data time: 0.02144723279135568\n",
      "step: 694160, loss: 0.06460562348365784, data time: 0.020151456197102863\n",
      "step: 694161, loss: 0.06442596018314362, data time: 0.01901879906654358\n",
      "step: 694162, loss: 0.0634864792227745, data time: 0.01802374334896312\n",
      "step: 694163, loss: 0.05615541338920593, data time: 0.017131898138258193\n",
      "step: 694164, loss: 0.06940890103578568, data time: 0.016344246111418072\n",
      "step: 694165, loss: 0.06109654903411865, data time: 0.015629339218139648\n",
      "step: 694166, loss: 0.06488785892724991, data time: 0.014983426956903367\n",
      "step: 694167, loss: 0.06334523856639862, data time: 0.014397480271079323\n",
      "step: 694168, loss: 0.056302499026060104, data time: 0.01385733355646548\n",
      "step: 694169, loss: 0.06614935398101807, data time: 0.013363569974899292\n",
      "step: 694170, loss: 0.06095067784190178, data time: 0.012909164428710937\n",
      "step: 694171, loss: 0.06297346949577332, data time: 0.012492161530714769\n",
      "step: 694172, loss: 0.05861864611506462, data time: 0.012101844505027489\n",
      "step: 694173, loss: 0.05808991193771362, data time: 0.01174790518624442\n",
      "step: 694174, loss: 0.06042776629328728, data time: 0.011415678879310345\n",
      "step: 694175, loss: 0.0685119554400444, data time: 0.011104075113932292\n",
      "step: 694176, loss: 0.06278026849031448, data time: 0.010813843819402879\n",
      "step: 694177, loss: 0.06420968472957611, data time: 0.010551832616329193\n",
      "step: 694178, loss: 0.05880420655012131, data time: 0.010291085098728989\n",
      "step: 694179, loss: 0.060197532176971436, data time: 0.010048319311702953\n",
      "step: 694180, loss: 0.06074798107147217, data time: 0.009820583888462612\n",
      "step: 694181, loss: 0.06695284694433212, data time: 0.00959916247261895\n",
      "step: 694182, loss: 0.06264090538024902, data time: 0.009393588916675464\n",
      "step: 694183, loss: 0.05829507112503052, data time: 0.009197423332615903\n",
      "step: 694184, loss: 0.06593871116638184, data time: 0.009011427561442057\n",
      "step: 694185, loss: 0.06601475179195404, data time: 0.008834099769592286\n",
      "step: 694186, loss: 0.052771177142858505, data time: 0.2887890338897705\n",
      "step: 694187, loss: 0.06327527016401291, data time: 0.14509892463684082\n",
      "step: 694188, loss: 0.06386594474315643, data time: 0.09722463289896648\n",
      "step: 694189, loss: 0.06168720871210098, data time: 0.07364636659622192\n",
      "step: 694190, loss: 0.05938613414764404, data time: 0.059211206436157224\n",
      "step: 694191, loss: 0.06188129633665085, data time: 0.049595157305399575\n",
      "step: 694192, loss: 0.07277604937553406, data time: 0.04271101951599121\n",
      "step: 694193, loss: 0.05371605604887009, data time: 0.03763771057128906\n",
      "step: 694194, loss: 0.06346143782138824, data time: 0.03360393312242296\n",
      "step: 694195, loss: 0.06574295461177826, data time: 0.03045048713684082\n",
      "step: 694196, loss: 0.06245522201061249, data time: 0.027862830595536667\n",
      "step: 694197, loss: 0.056906722486019135, data time: 0.02571350336074829\n",
      "step: 694198, loss: 0.06614580750465393, data time: 0.02389676754291241\n",
      "step: 694199, loss: 0.05985107272863388, data time: 0.022337419646126882\n",
      "step: 694200, loss: 0.05892374739050865, data time: 0.021022828420003255\n",
      "step: 694201, loss: 0.06251615285873413, data time: 0.019836097955703735\n",
      "step: 694202, loss: 0.06310443580150604, data time: 0.01878979626823874\n",
      "step: 694203, loss: 0.058718498796224594, data time: 0.017854915724860296\n",
      "step: 694204, loss: 0.06260514259338379, data time: 0.01703116768284848\n",
      "step: 694205, loss: 0.06005305051803589, data time: 0.016285932064056395\n",
      "step: 694206, loss: 0.06278272718191147, data time: 0.015609298433576311\n",
      "step: 694207, loss: 0.060687851160764694, data time: 0.014991576021367853\n",
      "step: 694208, loss: 0.06333358585834503, data time: 0.014429755832837976\n",
      "step: 694209, loss: 0.06343802809715271, data time: 0.013914783795674643\n",
      "step: 694210, loss: 0.06218181550502777, data time: 0.01344127655029297\n",
      "step: 694211, loss: 0.06315996497869492, data time: 0.013005944398733286\n",
      "step: 694212, loss: 0.05958349630236626, data time: 0.01259764035542806\n",
      "step: 694213, loss: 0.05729949101805687, data time: 0.012221847261701311\n",
      "step: 694214, loss: 0.06300213932991028, data time: 0.011870014256444471\n",
      "step: 694215, loss: 0.06097293645143509, data time: 0.011541064580281575\n",
      "step: 694216, loss: 0.06317248195409775, data time: 0.011233552809684508\n",
      "step: 694217, loss: 0.06849696487188339, data time: 0.01095900684595108\n",
      "step: 694218, loss: 0.06631875038146973, data time: 0.01068719228108724\n",
      "step: 694219, loss: 0.06227210909128189, data time: 0.010430777774137609\n",
      "step: 694220, loss: 0.057082127779722214, data time: 0.010188354764665876\n",
      "step: 694221, loss: 0.05895745009183884, data time: 0.009958803653717041\n",
      "step: 694222, loss: 0.06462061405181885, data time: 0.009742833472586967\n",
      "step: 694223, loss: 0.056851089000701904, data time: 0.009536247504384894\n",
      "step: 694224, loss: 0.0662497729063034, data time: 0.009342804933205629\n",
      "step: 694225, loss: 0.04922698065638542, data time: 0.009157383441925048\n",
      "step: 694226, loss: 0.06125566363334656, data time: 0.2826375961303711\n",
      "step: 694227, loss: 0.0682031437754631, data time: 0.14202308654785156\n",
      "step: 694228, loss: 0.0596223883330822, data time: 0.09514443079630534\n",
      "step: 694229, loss: 0.06231445074081421, data time: 0.0721861720085144\n",
      "step: 694230, loss: 0.05950532481074333, data time: 0.058046722412109376\n",
      "step: 694231, loss: 0.06319290399551392, data time: 0.04861835638682047\n",
      "step: 694232, loss: 0.062477223575115204, data time: 0.041871138981410434\n",
      "step: 694233, loss: 0.06405229866504669, data time: 0.03690469264984131\n",
      "step: 694234, loss: 0.06666714698076248, data time: 0.03294955359564887\n",
      "step: 694235, loss: 0.059795599430799484, data time: 0.029863739013671876\n",
      "step: 694236, loss: 0.06281662732362747, data time: 0.02732970497824929\n",
      "step: 694237, loss: 0.05897020921111107, data time: 0.02522114912668864\n",
      "step: 694238, loss: 0.061835676431655884, data time: 0.02347397804260254\n",
      "step: 694239, loss: 0.06382792443037033, data time: 0.021945204053606306\n",
      "step: 694240, loss: 0.06142730638384819, data time: 0.020620950063069663\n",
      "step: 694241, loss: 0.06618368625640869, data time: 0.019462406635284424\n",
      "step: 694242, loss: 0.06384515762329102, data time: 0.018442869186401367\n",
      "step: 694243, loss: 0.06102508306503296, data time: 0.017528441217210557\n",
      "step: 694244, loss: 0.06290342658758163, data time: 0.01671767234802246\n",
      "step: 694245, loss: 0.06205558776855469, data time: 0.01598484516143799\n",
      "step: 694246, loss: 0.059841930866241455, data time: 0.015319415501185827\n",
      "step: 694247, loss: 0.0579664409160614, data time: 0.014724471352317116\n",
      "step: 694248, loss: 0.0613744854927063, data time: 0.01417950961900794\n",
      "step: 694249, loss: 0.06751211732625961, data time: 0.013673007488250732\n",
      "step: 694250, loss: 0.062108688056468964, data time: 0.013206729888916016\n",
      "step: 694251, loss: 0.06681935489177704, data time: 0.012779630147493802\n",
      "step: 694252, loss: 0.05782456696033478, data time: 0.012380953188295718\n",
      "step: 694253, loss: 0.06598387658596039, data time: 0.012012285845620292\n",
      "step: 694254, loss: 0.062268681824207306, data time: 0.011666322576588598\n",
      "step: 694255, loss: 0.06510691344738007, data time: 0.011345116297403972\n",
      "step: 694256, loss: 0.06408974528312683, data time: 0.011045555914601973\n",
      "step: 694257, loss: 0.059325650334358215, data time: 0.010773047804832458\n",
      "step: 694258, loss: 0.06067844480276108, data time: 0.01050597248655377\n",
      "step: 694259, loss: 0.061765313148498535, data time: 0.010257756008821376\n",
      "step: 694260, loss: 0.05944040045142174, data time: 0.010024942670549665\n",
      "step: 694261, loss: 0.05660221725702286, data time: 0.009797718789842393\n",
      "step: 694262, loss: 0.05930953472852707, data time: 0.009588273795875343\n",
      "step: 694263, loss: 0.06383135914802551, data time: 0.009386746506941946\n",
      "step: 694264, loss: 0.0624786838889122, data time: 0.009195236059335561\n",
      "step: 694265, loss: 0.09032945334911346, data time: 0.009014815092086792\n",
      "step: 694266, loss: 0.06452135741710663, data time: 0.2747488021850586\n",
      "step: 694267, loss: 0.06719329953193665, data time: 0.1384674310684204\n",
      "step: 694268, loss: 0.05736362561583519, data time: 0.0931696097056071\n",
      "step: 694269, loss: 0.062365829944610596, data time: 0.07076185941696167\n",
      "step: 694270, loss: 0.05916190147399902, data time: 0.056896638870239255\n",
      "step: 694271, loss: 0.05958620831370354, data time: 0.04763797918955485\n",
      "step: 694272, loss: 0.06075428053736687, data time: 0.041040897369384766\n",
      "step: 694273, loss: 0.057797275483608246, data time: 0.03619566559791565\n",
      "step: 694274, loss: 0.06239350140094757, data time: 0.03232198291354709\n",
      "step: 694275, loss: 0.06555567681789398, data time: 0.029306554794311525\n",
      "step: 694276, loss: 0.06067843735218048, data time: 0.02683026140386408\n",
      "step: 694277, loss: 0.05700760334730148, data time: 0.024764796098073322\n",
      "step: 694278, loss: 0.060196470469236374, data time: 0.023025677754328802\n",
      "step: 694279, loss: 0.06134422868490219, data time: 0.0215245144707816\n",
      "step: 694280, loss: 0.05841520056128502, data time: 0.020223633448282877\n",
      "step: 694281, loss: 0.06344862282276154, data time: 0.01911139488220215\n",
      "step: 694282, loss: 0.0641811266541481, data time: 0.01813633301678826\n",
      "step: 694283, loss: 0.06057734414935112, data time: 0.017259822951422796\n",
      "step: 694284, loss: 0.06192327290773392, data time: 0.01648712158203125\n",
      "step: 694285, loss: 0.05907296761870384, data time: 0.015783023834228516\n",
      "step: 694286, loss: 0.06327539682388306, data time: 0.015145540237426758\n",
      "step: 694287, loss: 0.06543977558612823, data time: 0.014548550952564587\n",
      "step: 694288, loss: 0.06742583960294724, data time: 0.014003121334573498\n",
      "step: 694289, loss: 0.06519468873739243, data time: 0.013501813014348349\n",
      "step: 694290, loss: 0.06592480838298798, data time: 0.013042831420898437\n",
      "step: 694291, loss: 0.06019144505262375, data time: 0.012624703920804538\n",
      "step: 694292, loss: 0.068577341735363, data time: 0.012231111526489258\n",
      "step: 694293, loss: 0.056244246661663055, data time: 0.01187046936580113\n",
      "step: 694294, loss: 0.06082908436655998, data time: 0.011530358215858197\n",
      "step: 694295, loss: 0.05714812129735947, data time: 0.011214121182759603\n",
      "step: 694296, loss: 0.06352019309997559, data time: 0.010916856027418567\n",
      "step: 694297, loss: 0.057001322507858276, data time: 0.01064559817314148\n",
      "step: 694298, loss: 0.06593213975429535, data time: 0.010380130825620709\n",
      "step: 694299, loss: 0.06172247976064682, data time: 0.010132417959325454\n",
      "step: 694300, loss: 0.06792163848876953, data time: 0.009902252469744001\n",
      "step: 694301, loss: 0.059697799384593964, data time: 0.009680337376064725\n",
      "step: 694302, loss: 0.059419237077236176, data time: 0.009474986308329814\n",
      "step: 694303, loss: 0.05854110047221184, data time: 0.00927759471692537\n",
      "step: 694304, loss: 0.062568798661232, data time: 0.009089109225150866\n",
      "step: 694305, loss: 0.06406284868717194, data time: 0.00890958309173584\n",
      "step: 694306, loss: 0.06102057546377182, data time: 0.28578686714172363\n",
      "step: 694307, loss: 0.0612603984773159, data time: 0.144179105758667\n",
      "step: 694308, loss: 0.05860705301165581, data time: 0.09697405497233073\n",
      "step: 694309, loss: 0.056183263659477234, data time: 0.07338064908981323\n",
      "step: 694310, loss: 0.05574537441134453, data time: 0.059009122848510745\n",
      "step: 694311, loss: 0.05965094268321991, data time: 0.0494083563486735\n",
      "step: 694312, loss: 0.06327065080404282, data time: 0.04266582216535296\n",
      "step: 694313, loss: 0.06258802860975266, data time: 0.03760620951652527\n",
      "step: 694314, loss: 0.06639218330383301, data time: 0.03359593285454644\n",
      "step: 694315, loss: 0.06018806993961334, data time: 0.030443644523620604\n",
      "step: 694316, loss: 0.06044372543692589, data time: 0.027857433665882458\n",
      "step: 694317, loss: 0.06248265132308006, data time: 0.025705536206563313\n",
      "step: 694318, loss: 0.060204021632671356, data time: 0.02388475491450383\n",
      "step: 694319, loss: 0.06272195279598236, data time: 0.022337896483285085\n",
      "step: 694320, loss: 0.06226586550474167, data time: 0.02098840077718099\n",
      "step: 694321, loss: 0.06928141415119171, data time: 0.01980164647102356\n",
      "step: 694322, loss: 0.06371328979730606, data time: 0.018762027516084558\n",
      "step: 694323, loss: 0.05385246500372887, data time: 0.017828570471869573\n",
      "step: 694324, loss: 0.06449513882398605, data time: 0.01699753811484889\n",
      "step: 694325, loss: 0.06169503927230835, data time: 0.016248226165771484\n",
      "step: 694326, loss: 0.06475143134593964, data time: 0.015570844922746931\n",
      "step: 694327, loss: 0.061360396444797516, data time: 0.014978788115761497\n",
      "step: 694328, loss: 0.06266975402832031, data time: 0.014421245326166567\n",
      "step: 694329, loss: 0.056973785161972046, data time: 0.01390761137008667\n",
      "step: 694330, loss: 0.06030324846506119, data time: 0.013429479598999023\n",
      "step: 694331, loss: 0.05998764932155609, data time: 0.012996710263765775\n",
      "step: 694332, loss: 0.06440675258636475, data time: 0.012591061768708405\n",
      "step: 694333, loss: 0.06111304089426994, data time: 0.012213451521737235\n",
      "step: 694334, loss: 0.06100892275571823, data time: 0.011864752605043608\n",
      "step: 694335, loss: 0.06000556796789169, data time: 0.011537893613179525\n",
      "step: 694336, loss: 0.0606953501701355, data time: 0.01123240686232044\n",
      "step: 694337, loss: 0.05865311622619629, data time: 0.010958127677440643\n",
      "step: 694338, loss: 0.058731745928525925, data time: 0.010687813614353989\n",
      "step: 694339, loss: 0.06244444474577904, data time: 0.010428954573238598\n",
      "step: 694340, loss: 0.06411169469356537, data time: 0.010190262113298688\n",
      "step: 694341, loss: 0.06723693013191223, data time: 0.009959916273752848\n",
      "step: 694342, loss: 0.06481271982192993, data time: 0.009742994566221495\n",
      "step: 694343, loss: 0.05584476515650749, data time: 0.009536053005017732\n",
      "step: 694344, loss: 0.06445356458425522, data time: 0.009339711604974208\n",
      "step: 694345, loss: 0.1026390939950943, data time: 0.009153854846954346\n",
      "step: 694346, loss: 0.0676671713590622, data time: 0.28251004219055176\n",
      "step: 694347, loss: 0.05863669142127037, data time: 0.14237797260284424\n",
      "step: 694348, loss: 0.0657399520277977, data time: 0.095915953318278\n",
      "step: 694349, loss: 0.05723985284566879, data time: 0.07270723581314087\n",
      "step: 694350, loss: 0.06286986917257309, data time: 0.05846872329711914\n",
      "step: 694351, loss: 0.0647096335887909, data time: 0.0489809513092041\n",
      "step: 694352, loss: 0.06069127470254898, data time: 0.042177370616367886\n",
      "step: 694353, loss: 0.06140473112463951, data time: 0.037192732095718384\n",
      "step: 694354, loss: 0.06337042152881622, data time: 0.03320948282877604\n",
      "step: 694355, loss: 0.06181194633245468, data time: 0.03009519577026367\n",
      "step: 694356, loss: 0.054681967943906784, data time: 0.02753793109547008\n",
      "step: 694357, loss: 0.056623369455337524, data time: 0.025419175624847412\n",
      "step: 694358, loss: 0.0638292133808136, data time: 0.023622476137601413\n",
      "step: 694359, loss: 0.06060495600104332, data time: 0.022090554237365723\n",
      "step: 694360, loss: 0.06173921748995781, data time: 0.02075223922729492\n",
      "step: 694361, loss: 0.05790521204471588, data time: 0.01957818865776062\n",
      "step: 694362, loss: 0.058804702013731, data time: 0.018560297348920035\n",
      "step: 694363, loss: 0.06006172299385071, data time: 0.01763827270931668\n",
      "step: 694364, loss: 0.0697675347328186, data time: 0.01681964020980032\n",
      "step: 694365, loss: 0.06441612541675568, data time: 0.01607961654663086\n",
      "step: 694366, loss: 0.06244707852602005, data time: 0.015412194388253348\n",
      "step: 694367, loss: 0.06144937127828598, data time: 0.014802282506769354\n",
      "step: 694368, loss: 0.05986350029706955, data time: 0.014248910157576851\n",
      "step: 694369, loss: 0.05842253193259239, data time: 0.013737758000691732\n",
      "step: 694370, loss: 0.06174333021044731, data time: 0.013266754150390626\n",
      "step: 694371, loss: 0.06392215192317963, data time: 0.012845534544724684\n",
      "step: 694372, loss: 0.0601242296397686, data time: 0.012448566931265372\n",
      "step: 694373, loss: 0.06072508916258812, data time: 0.012077297483171736\n",
      "step: 694374, loss: 0.05968880653381348, data time: 0.0117324056296513\n",
      "step: 694375, loss: 0.06646431237459183, data time: 0.011408623059590657\n",
      "step: 694376, loss: 0.061650849878787994, data time: 0.011106391106882402\n",
      "step: 694377, loss: 0.06386299431324005, data time: 0.010831885039806366\n",
      "step: 694378, loss: 0.06642564386129379, data time: 0.01056140119379217\n",
      "step: 694379, loss: 0.05862508714199066, data time: 0.01030569918015424\n",
      "step: 694380, loss: 0.059960585087537766, data time: 0.01007305553981236\n",
      "step: 694381, loss: 0.06595520675182343, data time: 0.009845422373877631\n",
      "step: 694382, loss: 0.06605552136898041, data time: 0.009632149258175411\n",
      "step: 694383, loss: 0.0643630251288414, data time: 0.009428482306630988\n",
      "step: 694384, loss: 0.06829291582107544, data time: 0.009236598626161233\n",
      "step: 694385, loss: 0.030824586749076843, data time: 0.009052938222885132\n",
      "step: 694386, loss: 0.06262856721878052, data time: 0.27846837043762207\n",
      "step: 694387, loss: 0.06386993825435638, data time: 0.14054346084594727\n",
      "step: 694388, loss: 0.06746186316013336, data time: 0.0944524606068929\n",
      "step: 694389, loss: 0.05799170583486557, data time: 0.07168537378311157\n",
      "step: 694390, loss: 0.05943116545677185, data time: 0.057623291015625\n",
      "step: 694391, loss: 0.06251716613769531, data time: 0.0482555627822876\n",
      "step: 694392, loss: 0.06465865671634674, data time: 0.04157430785042899\n",
      "step: 694393, loss: 0.05789486691355705, data time: 0.036651402711868286\n",
      "step: 694394, loss: 0.06361530721187592, data time: 0.032724221547444664\n",
      "step: 694395, loss: 0.05920473858714104, data time: 0.02967219352722168\n",
      "step: 694396, loss: 0.0597681850194931, data time: 0.027164459228515625\n",
      "step: 694397, loss: 0.06571590900421143, data time: 0.02508393923441569\n",
      "step: 694398, loss: 0.05972711369395256, data time: 0.023313283920288086\n",
      "step: 694399, loss: 0.0659213587641716, data time: 0.021790810993739536\n",
      "step: 694400, loss: 0.060817115008831024, data time: 0.02047100067138672\n",
      "step: 694401, loss: 0.05935750529170036, data time: 0.019322708249092102\n",
      "step: 694402, loss: 0.06010285019874573, data time: 0.018318891525268555\n",
      "step: 694403, loss: 0.061895377933979034, data time: 0.01741186777750651\n",
      "step: 694404, loss: 0.06567075103521347, data time: 0.016609580893265575\n",
      "step: 694405, loss: 0.061897195875644684, data time: 0.015882050991058348\n",
      "step: 694406, loss: 0.06010463088750839, data time: 0.015226704733712333\n",
      "step: 694407, loss: 0.06164557859301567, data time: 0.014625473455949263\n",
      "step: 694408, loss: 0.06089000031352043, data time: 0.014077922572260317\n",
      "step: 694409, loss: 0.058788150548934937, data time: 0.013579070568084717\n",
      "step: 694410, loss: 0.06309255957603455, data time: 0.013118562698364257\n",
      "step: 694411, loss: 0.06175191327929497, data time: 0.012698861268850474\n",
      "step: 694412, loss: 0.06023850291967392, data time: 0.012301286061604818\n",
      "step: 694413, loss: 0.06032908707857132, data time: 0.011937132903507777\n",
      "step: 694414, loss: 0.06635313481092453, data time: 0.01159740316456762\n",
      "step: 694415, loss: 0.06452074646949768, data time: 0.011279010772705078\n",
      "step: 694416, loss: 0.06432702392339706, data time: 0.01098353632034794\n",
      "step: 694417, loss: 0.06006376072764397, data time: 0.010713078081607819\n",
      "step: 694418, loss: 0.06279800087213516, data time: 0.010445320244991419\n",
      "step: 694419, loss: 0.05599002167582512, data time: 0.01019615285536822\n",
      "step: 694420, loss: 0.061962004750967026, data time: 0.009966625486101423\n",
      "step: 694421, loss: 0.061007630079984665, data time: 0.009741425514221191\n",
      "step: 694422, loss: 0.05944505333900452, data time: 0.009533843478640995\n",
      "step: 694423, loss: 0.06111949682235718, data time: 0.009333459954512747\n",
      "step: 694424, loss: 0.06268272548913956, data time: 0.009147986387595152\n",
      "step: 694425, loss: 0.08833712339401245, data time: 0.008970260620117188\n",
      "step: 694426, loss: 0.056364722549915314, data time: 0.2853243350982666\n",
      "step: 694427, loss: 0.06695391982793808, data time: 0.14397215843200684\n",
      "step: 694428, loss: 0.06212344393134117, data time: 0.09718640645345052\n",
      "step: 694429, loss: 0.06239674612879753, data time: 0.0736464262008667\n",
      "step: 694430, loss: 0.060830842703580856, data time: 0.059244823455810544\n",
      "step: 694431, loss: 0.06323552131652832, data time: 0.0496373176574707\n",
      "step: 694432, loss: 0.06382760405540466, data time: 0.04279671396527972\n",
      "step: 694433, loss: 0.06387437880039215, data time: 0.03776475787162781\n",
      "step: 694434, loss: 0.06468226760625839, data time: 0.033754958046807185\n",
      "step: 694435, loss: 0.06265613436698914, data time: 0.030623316764831543\n",
      "step: 694436, loss: 0.059835284948349, data time: 0.028061541644009678\n",
      "step: 694437, loss: 0.0598389208316803, data time: 0.025917470455169678\n",
      "step: 694438, loss: 0.0644911602139473, data time: 0.024111087505634014\n",
      "step: 694439, loss: 0.05934309959411621, data time: 0.02256035804748535\n",
      "step: 694440, loss: 0.05956364795565605, data time: 0.021218283971150716\n",
      "step: 694441, loss: 0.061079226434230804, data time: 0.020046383142471313\n",
      "step: 694442, loss: 0.07220404595136642, data time: 0.019018018946928138\n",
      "step: 694443, loss: 0.06575936079025269, data time: 0.018069664637247723\n",
      "step: 694444, loss: 0.05946362018585205, data time: 0.017229067651849044\n",
      "step: 694445, loss: 0.06170438975095749, data time: 0.016475987434387208\n",
      "step: 694446, loss: 0.0636659562587738, data time: 0.015791030157180058\n",
      "step: 694447, loss: 0.06909217685461044, data time: 0.01516359502618963\n",
      "step: 694448, loss: 0.06302094459533691, data time: 0.014593715253083603\n",
      "step: 694449, loss: 0.05714709684252739, data time: 0.01406921943028768\n",
      "step: 694450, loss: 0.06690370291471481, data time: 0.01359053611755371\n",
      "step: 694451, loss: 0.06573289632797241, data time: 0.01315148977132944\n",
      "step: 694452, loss: 0.059523507952690125, data time: 0.012734545601738824\n",
      "step: 694453, loss: 0.06604351848363876, data time: 0.012352347373962402\n",
      "step: 694454, loss: 0.05919171869754791, data time: 0.012009406911915746\n",
      "step: 694455, loss: 0.07061433792114258, data time: 0.011676478385925292\n",
      "step: 694456, loss: 0.057408880442380905, data time: 0.011364913755847562\n",
      "step: 694457, loss: 0.06490131467580795, data time: 0.01108156144618988\n",
      "step: 694458, loss: 0.06384985893964767, data time: 0.010805130004882812\n",
      "step: 694459, loss: 0.05699094757437706, data time: 0.010546586092780618\n",
      "step: 694460, loss: 0.06199149414896965, data time: 0.010304192134312221\n",
      "step: 694461, loss: 0.06272485852241516, data time: 0.010072151819864908\n",
      "step: 694462, loss: 0.06269420683383942, data time: 0.009861198631492821\n",
      "step: 694463, loss: 0.0614960603415966, data time: 0.009651058598568565\n",
      "step: 694464, loss: 0.05488376319408417, data time: 0.009452556952452049\n",
      "step: 694465, loss: 0.05345340073108673, data time: 0.009264081716537476\n",
      "step: 694466, loss: 0.06552715599536896, data time: 0.283322811126709\n",
      "step: 694467, loss: 0.064904123544693, data time: 0.14275014400482178\n",
      "step: 694468, loss: 0.06125582382082939, data time: 0.09563295046488444\n",
      "step: 694469, loss: 0.060680922120809555, data time: 0.07255995273590088\n",
      "step: 694470, loss: 0.056689053773880005, data time: 0.058334970474243165\n",
      "step: 694471, loss: 0.06221597641706467, data time: 0.048871755599975586\n",
      "step: 694472, loss: 0.06082598492503166, data time: 0.042119809559413364\n",
      "step: 694473, loss: 0.0640520304441452, data time: 0.037113308906555176\n",
      "step: 694474, loss: 0.06007503718137741, data time: 0.03313586446974012\n",
      "step: 694475, loss: 0.06540745496749878, data time: 0.030029106140136718\n",
      "step: 694476, loss: 0.06074022874236107, data time: 0.027480905706232243\n",
      "step: 694477, loss: 0.07131652534008026, data time: 0.025359749794006348\n",
      "step: 694478, loss: 0.06176777929067612, data time: 0.023568061681894157\n",
      "step: 694479, loss: 0.06196039170026779, data time: 0.022034900529044017\n",
      "step: 694480, loss: 0.05973045155405998, data time: 0.020713218053181968\n",
      "step: 694481, loss: 0.06606578081846237, data time: 0.01955658197402954\n",
      "step: 694482, loss: 0.05988691747188568, data time: 0.01852909256430233\n",
      "step: 694483, loss: 0.06051330268383026, data time: 0.01760775513119168\n",
      "step: 694484, loss: 0.06280464679002762, data time: 0.016788005828857422\n",
      "step: 694485, loss: 0.06485441327095032, data time: 0.016049718856811522\n",
      "step: 694486, loss: 0.061240654438734055, data time: 0.015381960641770135\n",
      "step: 694487, loss: 0.061164744198322296, data time: 0.014776099811900745\n",
      "step: 694488, loss: 0.06823363900184631, data time: 0.014222538989523182\n",
      "step: 694489, loss: 0.06025950238108635, data time: 0.013737728198369345\n",
      "step: 694490, loss: 0.06494943797588348, data time: 0.013290214538574218\n",
      "step: 694491, loss: 0.06710805743932724, data time: 0.012873291969299316\n",
      "step: 694492, loss: 0.056931495666503906, data time: 0.01248541584721318\n",
      "step: 694493, loss: 0.05878177285194397, data time: 0.012126352105821882\n",
      "step: 694494, loss: 0.06665536761283875, data time: 0.011792404898281756\n",
      "step: 694495, loss: 0.06067325919866562, data time: 0.011480188369750977\n",
      "step: 694496, loss: 0.06270882487297058, data time: 0.011187668769590316\n",
      "step: 694497, loss: 0.06462573260068893, data time: 0.01090899109840393\n",
      "step: 694498, loss: 0.0606837272644043, data time: 0.010643135417591442\n",
      "step: 694499, loss: 0.06601148843765259, data time: 0.010390569182003246\n",
      "step: 694500, loss: 0.05764518678188324, data time: 0.010148116520472936\n",
      "step: 694501, loss: 0.05920664221048355, data time: 0.00992106729083591\n",
      "step: 694502, loss: 0.06592336297035217, data time: 0.009705324430723448\n",
      "step: 694503, loss: 0.06838297843933105, data time: 0.009501733277973375\n",
      "step: 694504, loss: 0.06398110836744308, data time: 0.009310685671292819\n",
      "step: 694505, loss: 0.07737410068511963, data time: 0.009126120805740356\n",
      "step: 694506, loss: 0.05895553156733513, data time: 0.2783210277557373\n",
      "step: 694507, loss: 0.06614363193511963, data time: 0.1407790184020996\n",
      "step: 694508, loss: 0.0563429556787014, data time: 0.09433301289876302\n",
      "step: 694509, loss: 0.06313811242580414, data time: 0.07165127992630005\n",
      "step: 694510, loss: 0.06657297909259796, data time: 0.05760946273803711\n",
      "step: 694511, loss: 0.059246864169836044, data time: 0.04826184113820394\n",
      "step: 694512, loss: 0.06457482278347015, data time: 0.04160073825291225\n",
      "step: 694513, loss: 0.06062750145792961, data time: 0.03666454553604126\n",
      "step: 694514, loss: 0.06173451244831085, data time: 0.032755904727511935\n",
      "step: 694515, loss: 0.06983928382396698, data time: 0.02968764305114746\n",
      "step: 694516, loss: 0.06391409039497375, data time: 0.027169682762839577\n",
      "step: 694517, loss: 0.05827140063047409, data time: 0.025074342886606853\n",
      "step: 694518, loss: 0.06023011356592178, data time: 0.023300024179311898\n",
      "step: 694519, loss: 0.06430336087942123, data time: 0.021778873034885952\n",
      "step: 694520, loss: 0.063190758228302, data time: 0.020470205942789713\n",
      "step: 694521, loss: 0.0648300051689148, data time: 0.01933637261390686\n",
      "step: 694522, loss: 0.06080666184425354, data time: 0.018318821402157053\n",
      "step: 694523, loss: 0.06313958764076233, data time: 0.017410569720798068\n",
      "step: 694524, loss: 0.06617088615894318, data time: 0.016601474661576122\n",
      "step: 694525, loss: 0.06484054028987885, data time: 0.01588926315307617\n",
      "step: 694526, loss: 0.07075954973697662, data time: 0.015230019887288412\n",
      "step: 694527, loss: 0.061263054609298706, data time: 0.014644438570195978\n",
      "step: 694528, loss: 0.05748917907476425, data time: 0.014095420422761337\n",
      "step: 694529, loss: 0.06507157534360886, data time: 0.01359647512435913\n",
      "step: 694530, loss: 0.06390105187892914, data time: 0.013143148422241211\n",
      "step: 694531, loss: 0.06683953106403351, data time: 0.012716724322392391\n",
      "step: 694532, loss: 0.059556059539318085, data time: 0.012317984192459672\n",
      "step: 694533, loss: 0.06714752316474915, data time: 0.011952178818838937\n",
      "step: 694534, loss: 0.05805540084838867, data time: 0.011610984802246094\n",
      "step: 694535, loss: 0.06188258156180382, data time: 0.011297059059143067\n",
      "step: 694536, loss: 0.06053430959582329, data time: 0.010997910653391191\n",
      "step: 694537, loss: 0.05577414482831955, data time: 0.010724909603595734\n",
      "step: 694538, loss: 0.058080337941646576, data time: 0.010462103467999083\n",
      "step: 694539, loss: 0.06487265974283218, data time: 0.010216025745167452\n",
      "step: 694540, loss: 0.06122475862503052, data time: 0.009983342034476143\n",
      "step: 694541, loss: 0.05969260260462761, data time: 0.009757737318674723\n",
      "step: 694542, loss: 0.06089925020933151, data time: 0.009548303243276235\n",
      "step: 694543, loss: 0.06014227867126465, data time: 0.009349678692064788\n",
      "step: 694544, loss: 0.062107574194669724, data time: 0.009158855829483423\n",
      "step: 694545, loss: 0.048008229583501816, data time: 0.00897786021232605\n",
      "step: 694546, loss: 0.059115514159202576, data time: 0.29226016998291016\n",
      "step: 694547, loss: 0.06034976989030838, data time: 0.14683640003204346\n",
      "step: 694548, loss: 0.05865306407213211, data time: 0.09835076332092285\n",
      "step: 694549, loss: 0.06253308057785034, data time: 0.07446986436843872\n",
      "step: 694550, loss: 0.06056123971939087, data time: 0.05985913276672363\n",
      "step: 694551, loss: 0.06703709810972214, data time: 0.05014538764953613\n",
      "step: 694552, loss: 0.06497593224048615, data time: 0.04320829255240304\n",
      "step: 694553, loss: 0.05767209082841873, data time: 0.038076311349868774\n",
      "step: 694554, loss: 0.06233648955821991, data time: 0.03399321768018934\n",
      "step: 694555, loss: 0.06279636919498444, data time: 0.03080592155456543\n",
      "step: 694556, loss: 0.0569424144923687, data time: 0.02819147976962003\n",
      "step: 694557, loss: 0.0654262900352478, data time: 0.02601357301076253\n",
      "step: 694558, loss: 0.06683193147182465, data time: 0.02416535524221567\n",
      "step: 694559, loss: 0.06335213035345078, data time: 0.022582275526864187\n",
      "step: 694560, loss: 0.06526432931423187, data time: 0.021223036448160808\n",
      "step: 694561, loss: 0.0665326938033104, data time: 0.02003711462020874\n",
      "step: 694562, loss: 0.06711357831954956, data time: 0.01898478059207692\n",
      "step: 694563, loss: 0.06989017874002457, data time: 0.01804020669725206\n",
      "step: 694564, loss: 0.06250688433647156, data time: 0.017195751792506167\n",
      "step: 694565, loss: 0.06254058331251144, data time: 0.016437506675720213\n",
      "step: 694566, loss: 0.06070660054683685, data time: 0.015749091193789526\n",
      "step: 694567, loss: 0.0597187802195549, data time: 0.015125870704650879\n",
      "step: 694568, loss: 0.0691651925444603, data time: 0.014556781105373217\n",
      "step: 694569, loss: 0.06407096236944199, data time: 0.014040788014729818\n",
      "step: 694570, loss: 0.05794965848326683, data time: 0.013572597503662109\n",
      "step: 694571, loss: 0.05758994072675705, data time: 0.01313020632817195\n",
      "step: 694572, loss: 0.05883638188242912, data time: 0.012715763515896268\n",
      "step: 694573, loss: 0.06095343455672264, data time: 0.01233375072479248\n",
      "step: 694574, loss: 0.061803653836250305, data time: 0.011978535816587251\n",
      "step: 694575, loss: 0.07190805673599243, data time: 0.011648845672607423\n",
      "step: 694576, loss: 0.06340271234512329, data time: 0.011337449473719443\n",
      "step: 694577, loss: 0.0566905252635479, data time: 0.011056311428546906\n",
      "step: 694578, loss: 0.060205407440662384, data time: 0.010785045045794863\n",
      "step: 694579, loss: 0.06336668133735657, data time: 0.010529714472153607\n",
      "step: 694580, loss: 0.061857860535383224, data time: 0.010284880229404995\n",
      "step: 694581, loss: 0.06163116544485092, data time: 0.010061277283562554\n",
      "step: 694582, loss: 0.05925662815570831, data time: 0.00984283395715662\n",
      "step: 694583, loss: 0.0675763338804245, data time: 0.009633660316467285\n",
      "step: 694584, loss: 0.05642596632242203, data time: 0.009434999563755134\n",
      "step: 694585, loss: 0.06204995512962341, data time: 0.009246045351028442\n",
      "step: 694586, loss: 0.061161383986473083, data time: 0.28446173667907715\n",
      "step: 694587, loss: 0.057535570114851, data time: 0.1433093547821045\n",
      "step: 694588, loss: 0.06363918632268906, data time: 0.09599566459655762\n",
      "step: 694589, loss: 0.06601101160049438, data time: 0.07281816005706787\n",
      "step: 694590, loss: 0.057758353650569916, data time: 0.0585331916809082\n",
      "step: 694591, loss: 0.07264477014541626, data time: 0.04906038443247477\n",
      "step: 694592, loss: 0.06764797866344452, data time: 0.042281150817871094\n",
      "step: 694593, loss: 0.05668637156486511, data time: 0.03725114464759827\n",
      "step: 694594, loss: 0.05747530981898308, data time: 0.03325502077738444\n",
      "step: 694595, loss: 0.06411273777484894, data time: 0.030144262313842773\n",
      "step: 694596, loss: 0.06618896126747131, data time: 0.02759120681069114\n",
      "step: 694597, loss: 0.05976013466715813, data time: 0.02546292543411255\n",
      "step: 694598, loss: 0.05875169485807419, data time: 0.023667225470909707\n",
      "step: 694599, loss: 0.059550683945417404, data time: 0.022121242114475796\n",
      "step: 694600, loss: 0.06569592654705048, data time: 0.020791896184285483\n",
      "step: 694601, loss: 0.06442456692457199, data time: 0.0196315199136734\n",
      "step: 694602, loss: 0.0610310435295105, data time: 0.01859866871553309\n",
      "step: 694603, loss: 0.06490984559059143, data time: 0.017674750751919217\n",
      "step: 694604, loss: 0.0588383786380291, data time: 0.016860497625250565\n",
      "step: 694605, loss: 0.06778430193662643, data time: 0.01611713171005249\n",
      "step: 694606, loss: 0.06027528643608093, data time: 0.015446140652611143\n",
      "step: 694607, loss: 0.062214963138103485, data time: 0.014834685759110884\n",
      "step: 694608, loss: 0.06271693110466003, data time: 0.014280163723489513\n",
      "step: 694609, loss: 0.06728518009185791, data time: 0.013781934976577759\n",
      "step: 694610, loss: 0.05918660759925842, data time: 0.013323707580566406\n",
      "step: 694611, loss: 0.06212368607521057, data time: 0.012889724511366624\n",
      "step: 694612, loss: 0.06212123483419418, data time: 0.012483914693196615\n",
      "step: 694613, loss: 0.06390303373336792, data time: 0.012114184243338448\n",
      "step: 694614, loss: 0.06494113057851791, data time: 0.011768965885556978\n",
      "step: 694615, loss: 0.05921099707484245, data time: 0.011449686686197917\n",
      "step: 694616, loss: 0.061789534986019135, data time: 0.011145091825915922\n",
      "step: 694617, loss: 0.06317798793315887, data time: 0.010868020355701447\n",
      "step: 694618, loss: 0.05621396750211716, data time: 0.010601484414302942\n",
      "step: 694619, loss: 0.05778989940881729, data time: 0.010353922843933105\n",
      "step: 694620, loss: 0.062065523117780685, data time: 0.01011507851736886\n",
      "step: 694621, loss: 0.061108436435461044, data time: 0.009888880782657199\n",
      "step: 694622, loss: 0.056005191057920456, data time: 0.009674355790421769\n",
      "step: 694623, loss: 0.05869682878255844, data time: 0.009470374960648386\n",
      "step: 694624, loss: 0.059946950525045395, data time: 0.009276567361293694\n",
      "step: 694625, loss: 0.05270152911543846, data time: 0.009090667963027954\n",
      "step: 694626, loss: 0.05683282017707825, data time: 0.283306360244751\n",
      "step: 694627, loss: 0.05888776481151581, data time: 0.14315927028656006\n",
      "step: 694628, loss: 0.05452462285757065, data time: 0.0959018071492513\n",
      "step: 694629, loss: 0.062445998191833496, data time: 0.07275468111038208\n",
      "step: 694630, loss: 0.056010253727436066, data time: 0.05849466323852539\n",
      "step: 694631, loss: 0.06687752902507782, data time: 0.048999508221944175\n",
      "step: 694632, loss: 0.06865447014570236, data time: 0.04223070825849261\n",
      "step: 694633, loss: 0.06532913446426392, data time: 0.03722184896469116\n",
      "step: 694634, loss: 0.06234438344836235, data time: 0.0332345167795817\n",
      "step: 694635, loss: 0.07072117179632187, data time: 0.030115795135498048\n",
      "step: 694636, loss: 0.06239502876996994, data time: 0.027559952302412552\n",
      "step: 694637, loss: 0.06039321422576904, data time: 0.025449176629384358\n",
      "step: 694638, loss: 0.0574272945523262, data time: 0.02364343863267165\n",
      "step: 694639, loss: 0.06262977421283722, data time: 0.022099154336111888\n",
      "step: 694640, loss: 0.06313662976026535, data time: 0.020766655604044598\n",
      "step: 694641, loss: 0.05990707874298096, data time: 0.019609108567237854\n",
      "step: 694642, loss: 0.06424930691719055, data time: 0.0185848123887006\n",
      "step: 694643, loss: 0.06278635561466217, data time: 0.0176624854405721\n",
      "step: 694644, loss: 0.06103070080280304, data time: 0.016837120056152344\n",
      "step: 694645, loss: 0.0655449777841568, data time: 0.016099536418914796\n",
      "step: 694646, loss: 0.06320662051439285, data time: 0.015429871422903878\n",
      "step: 694647, loss: 0.06384953111410141, data time: 0.014820998365228827\n",
      "step: 694648, loss: 0.05730515718460083, data time: 0.014268626337466032\n",
      "step: 694649, loss: 0.05724194645881653, data time: 0.013767540454864502\n",
      "step: 694650, loss: 0.06424172222614288, data time: 0.013311252593994141\n",
      "step: 694651, loss: 0.0643647164106369, data time: 0.012882333535414476\n",
      "step: 694652, loss: 0.06277887523174286, data time: 0.012477397918701172\n",
      "step: 694653, loss: 0.06914226710796356, data time: 0.012106903961726598\n",
      "step: 694654, loss: 0.05952825769782066, data time: 0.011761204949740705\n",
      "step: 694655, loss: 0.060868941247463226, data time: 0.01143956979115804\n",
      "step: 694656, loss: 0.06495358049869537, data time: 0.011134339917090631\n",
      "step: 694657, loss: 0.06048589572310448, data time: 0.01085592806339264\n",
      "step: 694658, loss: 0.061435312032699585, data time: 0.010587258772416548\n",
      "step: 694659, loss: 0.06994807720184326, data time: 0.010341574163997875\n",
      "step: 694660, loss: 0.05811039358377457, data time: 0.010104247501918249\n",
      "step: 694661, loss: 0.06300178170204163, data time: 0.009875337282816568\n",
      "step: 694662, loss: 0.06436026096343994, data time: 0.00966104945620975\n",
      "step: 694663, loss: 0.06225050613284111, data time: 0.009457161552027651\n",
      "step: 694664, loss: 0.05724848061800003, data time: 0.00926666382031563\n",
      "step: 694665, loss: 0.050161391496658325, data time: 0.009082704782485962\n",
      "step: 694666, loss: 0.061550822108983994, data time: 0.2836928367614746\n",
      "step: 694667, loss: 0.06264612823724747, data time: 0.14257192611694336\n",
      "step: 694668, loss: 0.06215701624751091, data time: 0.09553670883178711\n",
      "step: 694669, loss: 0.058560561388731, data time: 0.07248169183731079\n",
      "step: 694670, loss: 0.05862679332494736, data time: 0.05826973915100098\n",
      "step: 694671, loss: 0.06688816100358963, data time: 0.04883106549580892\n",
      "step: 694672, loss: 0.056917592883110046, data time: 0.04208483014787946\n",
      "step: 694673, loss: 0.06029341369867325, data time: 0.037098586559295654\n",
      "step: 694674, loss: 0.057908058166503906, data time: 0.03312510914272732\n",
      "step: 694675, loss: 0.06270278990268707, data time: 0.030010008811950685\n",
      "step: 694676, loss: 0.06607180088758469, data time: 0.027461528778076172\n",
      "step: 694677, loss: 0.05814497917890549, data time: 0.025345981121063232\n",
      "step: 694678, loss: 0.06010788679122925, data time: 0.023551225662231445\n",
      "step: 694679, loss: 0.06303609907627106, data time: 0.022013323647635325\n",
      "step: 694680, loss: 0.06387369334697723, data time: 0.020688517888387045\n",
      "step: 694681, loss: 0.05945071578025818, data time: 0.019535720348358154\n",
      "step: 694682, loss: 0.06318412721157074, data time: 0.01851675089667825\n",
      "step: 694683, loss: 0.05944538116455078, data time: 0.017613450686136883\n",
      "step: 694684, loss: 0.06197847053408623, data time: 0.01679507054780659\n",
      "step: 694685, loss: 0.05681758373975754, data time: 0.01605648994445801\n",
      "step: 694686, loss: 0.06510119140148163, data time: 0.015387444269089471\n",
      "step: 694687, loss: 0.06451800465583801, data time: 0.014780532230030407\n",
      "step: 694688, loss: 0.06508569419384003, data time: 0.014227877492490023\n",
      "step: 694689, loss: 0.05910512059926987, data time: 0.013723999261856079\n",
      "step: 694690, loss: 0.06746469438076019, data time: 0.013266677856445313\n",
      "step: 694691, loss: 0.06796503067016602, data time: 0.012840784513033353\n",
      "step: 694692, loss: 0.05989767611026764, data time: 0.01243820896855107\n",
      "step: 694693, loss: 0.058893606066703796, data time: 0.012065785271780831\n",
      "step: 694694, loss: 0.0676746517419815, data time: 0.011721668572261416\n",
      "step: 694695, loss: 0.06725233048200607, data time: 0.011400039990743\n",
      "step: 694696, loss: 0.06521323323249817, data time: 0.011095000851538873\n",
      "step: 694697, loss: 0.05771060287952423, data time: 0.010819688439369202\n",
      "step: 694698, loss: 0.06444765627384186, data time: 0.010552695303252249\n",
      "step: 694699, loss: 0.06079092621803284, data time: 0.010306260165046243\n",
      "step: 694700, loss: 0.059298090636730194, data time: 0.010072496959141322\n",
      "step: 694701, loss: 0.05661465972661972, data time: 0.009846687316894531\n",
      "step: 694702, loss: 0.060354676097631454, data time: 0.009633676425830738\n",
      "step: 694703, loss: 0.05984566733241081, data time: 0.009430646896362305\n",
      "step: 694704, loss: 0.062427401542663574, data time: 0.009237937438182341\n",
      "step: 694705, loss: 0.0612010583281517, data time: 0.009053927659988404\n",
      "step: 694706, loss: 0.0631110817193985, data time: 0.28055381774902344\n",
      "step: 694707, loss: 0.05907348915934563, data time: 0.14226436614990234\n",
      "step: 694708, loss: 0.06464605033397675, data time: 0.09531871477762859\n",
      "step: 694709, loss: 0.0623694472014904, data time: 0.07223057746887207\n",
      "step: 694710, loss: 0.06524544209241867, data time: 0.05807948112487793\n",
      "step: 694711, loss: 0.06410951912403107, data time: 0.048689842224121094\n",
      "step: 694712, loss: 0.0575675368309021, data time: 0.04196054594857352\n",
      "step: 694713, loss: 0.06427565217018127, data time: 0.037001848220825195\n",
      "step: 694714, loss: 0.06581385433673859, data time: 0.03304208649529351\n",
      "step: 694715, loss: 0.06220326945185661, data time: 0.029942846298217772\n",
      "step: 694716, loss: 0.06441862881183624, data time: 0.027405370365489613\n",
      "step: 694717, loss: 0.057204704731702805, data time: 0.025297085444132488\n",
      "step: 694718, loss: 0.06662793457508087, data time: 0.023514710939847507\n",
      "step: 694719, loss: 0.06605006754398346, data time: 0.0219989504132952\n",
      "step: 694720, loss: 0.06231745332479477, data time: 0.020686006546020506\n",
      "step: 694721, loss: 0.06106875091791153, data time: 0.019527986645698547\n",
      "step: 694722, loss: 0.06401169300079346, data time: 0.018511239220114314\n",
      "step: 694723, loss: 0.06551996618509293, data time: 0.017594218254089355\n",
      "step: 694724, loss: 0.05856063961982727, data time: 0.01677696328414114\n",
      "step: 694725, loss: 0.058599360287189484, data time: 0.016042661666870118\n",
      "step: 694726, loss: 0.06398418545722961, data time: 0.015376465661185128\n",
      "step: 694727, loss: 0.05859505385160446, data time: 0.014769814231178978\n",
      "step: 694728, loss: 0.05892275273799896, data time: 0.014229111049486242\n",
      "step: 694729, loss: 0.05927741527557373, data time: 0.013745248317718506\n",
      "step: 694730, loss: 0.05915149673819542, data time: 0.013298120498657227\n",
      "step: 694731, loss: 0.05688587576150894, data time: 0.012890302217923678\n",
      "step: 694732, loss: 0.058890145272016525, data time: 0.012501778425993744\n",
      "step: 694733, loss: 0.05998130887746811, data time: 0.012144190924508231\n",
      "step: 694734, loss: 0.06306226551532745, data time: 0.011809694355931776\n",
      "step: 694735, loss: 0.06649845093488693, data time: 0.011491481463114421\n",
      "step: 694736, loss: 0.059439271688461304, data time: 0.011192244868124685\n",
      "step: 694737, loss: 0.06549971550703049, data time: 0.010913245379924774\n",
      "step: 694738, loss: 0.06129245087504387, data time: 0.01064613371184378\n",
      "step: 694739, loss: 0.06454592198133469, data time: 0.0103989839553833\n",
      "step: 694740, loss: 0.054566048085689545, data time: 0.010162101473127093\n",
      "step: 694741, loss: 0.05908501148223877, data time: 0.009931570953792997\n",
      "step: 694742, loss: 0.06615778803825378, data time: 0.009715331567300332\n",
      "step: 694743, loss: 0.0562426894903183, data time: 0.009509764219585218\n",
      "step: 694744, loss: 0.06965526193380356, data time: 0.009319177040686974\n",
      "step: 694745, loss: 0.05760258063673973, data time: 0.009136611223220825\n",
      "step: 694746, loss: 0.05976326763629913, data time: 0.2772562503814697\n",
      "step: 694747, loss: 0.058254167437553406, data time: 0.1397087574005127\n",
      "step: 694748, loss: 0.05967087671160698, data time: 0.09417064984639485\n",
      "step: 694749, loss: 0.0577859990298748, data time: 0.07138562202453613\n",
      "step: 694750, loss: 0.06338897347450256, data time: 0.05739402770996094\n",
      "step: 694751, loss: 0.05635688826441765, data time: 0.04811370372772217\n",
      "step: 694752, loss: 0.0628199577331543, data time: 0.04146766662597656\n",
      "step: 694753, loss: 0.05987919121980667, data time: 0.03655913472175598\n",
      "step: 694754, loss: 0.06199204921722412, data time: 0.03265698750813802\n",
      "step: 694755, loss: 0.0630250871181488, data time: 0.029594826698303222\n",
      "step: 694756, loss: 0.06264153122901917, data time: 0.027086929841475052\n",
      "step: 694757, loss: 0.07190890610218048, data time: 0.02500540018081665\n",
      "step: 694758, loss: 0.0676921010017395, data time: 0.023247462052565355\n",
      "step: 694759, loss: 0.06440852582454681, data time: 0.021734680448259627\n",
      "step: 694760, loss: 0.06253698468208313, data time: 0.020438973108927408\n",
      "step: 694761, loss: 0.057455502450466156, data time: 0.0192984938621521\n",
      "step: 694762, loss: 0.06160062551498413, data time: 0.01829094045302447\n",
      "step: 694763, loss: 0.06528924405574799, data time: 0.017383204566107854\n",
      "step: 694764, loss: 0.06076058745384216, data time: 0.016584521845767374\n",
      "step: 694765, loss: 0.05896497890353203, data time: 0.015859770774841308\n",
      "step: 694766, loss: 0.07399284839630127, data time: 0.015202386038643973\n",
      "step: 694767, loss: 0.061015501618385315, data time: 0.01460751620205966\n",
      "step: 694768, loss: 0.05921171233057976, data time: 0.01406109851339589\n",
      "step: 694769, loss: 0.0675235316157341, data time: 0.01357373595237732\n",
      "step: 694770, loss: 0.06140626221895218, data time: 0.013121328353881835\n",
      "step: 694771, loss: 0.06724634766578674, data time: 0.0126954225393442\n",
      "step: 694772, loss: 0.06127091497182846, data time: 0.01229832790516041\n",
      "step: 694773, loss: 0.0635003000497818, data time: 0.011933003153119768\n",
      "step: 694774, loss: 0.06499967724084854, data time: 0.01159172222532075\n",
      "step: 694775, loss: 0.06355741620063782, data time: 0.011272859573364259\n",
      "step: 694776, loss: 0.06992285698652267, data time: 0.010976468363115865\n",
      "step: 694777, loss: 0.06280066072940826, data time: 0.0107058584690094\n",
      "step: 694778, loss: 0.06294919550418854, data time: 0.010449373360836145\n",
      "step: 694779, loss: 0.05833711847662926, data time: 0.01020350175745347\n",
      "step: 694780, loss: 0.06268783658742905, data time: 0.009967803955078125\n",
      "step: 694781, loss: 0.0651836171746254, data time: 0.009740995036231147\n",
      "step: 694782, loss: 0.059963636100292206, data time: 0.009530067443847656\n",
      "step: 694783, loss: 0.05636599659919739, data time: 0.009332443538464998\n",
      "step: 694784, loss: 0.06146199628710747, data time: 0.009144031084500827\n",
      "step: 694785, loss: 0.05083254724740982, data time: 0.008964234590530395\n",
      "step: 694786, loss: 0.05948824808001518, data time: 0.28220176696777344\n",
      "step: 694787, loss: 0.06275413930416107, data time: 0.14182400703430176\n",
      "step: 694788, loss: 0.060316406190395355, data time: 0.09565496444702148\n",
      "step: 694789, loss: 0.05797863006591797, data time: 0.07241374254226685\n",
      "step: 694790, loss: 0.06435462087392807, data time: 0.058245611190795896\n",
      "step: 694791, loss: 0.06137406826019287, data time: 0.048801581064860024\n",
      "step: 694792, loss: 0.06804506480693817, data time: 0.04204978261675153\n",
      "step: 694793, loss: 0.06382660567760468, data time: 0.037068456411361694\n",
      "step: 694794, loss: 0.06197964400053024, data time: 0.03309051195780436\n",
      "step: 694795, loss: 0.05494420975446701, data time: 0.029979515075683593\n",
      "step: 694796, loss: 0.06843375414609909, data time: 0.02745985984802246\n",
      "step: 694797, loss: 0.05282728001475334, data time: 0.025340994199117024\n",
      "step: 694798, loss: 0.057123929262161255, data time: 0.02356714468735915\n",
      "step: 694799, loss: 0.059401195496320724, data time: 0.022040929113115584\n",
      "step: 694800, loss: 0.05457887798547745, data time: 0.020721276601155598\n",
      "step: 694801, loss: 0.06116372346878052, data time: 0.01956029236316681\n",
      "step: 694802, loss: 0.05922132730484009, data time: 0.018535571939804974\n",
      "step: 694803, loss: 0.061884503811597824, data time: 0.017617026964823406\n",
      "step: 694804, loss: 0.06553561985492706, data time: 0.016798910341764752\n",
      "step: 694805, loss: 0.06077614054083824, data time: 0.016058480739593504\n",
      "step: 694806, loss: 0.06622862070798874, data time: 0.01539241699945359\n",
      "step: 694807, loss: 0.06009819358587265, data time: 0.014789256182583895\n",
      "step: 694808, loss: 0.06510215252637863, data time: 0.014232448909593664\n",
      "step: 694809, loss: 0.06225020810961723, data time: 0.013737738132476807\n",
      "step: 694810, loss: 0.056498147547245026, data time: 0.01327566146850586\n",
      "step: 694811, loss: 0.05976684018969536, data time: 0.01284648821904109\n",
      "step: 694812, loss: 0.057601965963840485, data time: 0.012441891211050528\n",
      "step: 694813, loss: 0.06526236981153488, data time: 0.012067973613739014\n",
      "step: 694814, loss: 0.06660157442092896, data time: 0.011721002644505995\n",
      "step: 694815, loss: 0.06251376122236252, data time: 0.011396487553914389\n",
      "step: 694816, loss: 0.05830336734652519, data time: 0.011100146078294324\n",
      "step: 694817, loss: 0.06113630533218384, data time: 0.01082579791545868\n",
      "step: 694818, loss: 0.06067071482539177, data time: 0.010561054403131659\n",
      "step: 694819, loss: 0.06129040569067001, data time: 0.010311470312230727\n",
      "step: 694820, loss: 0.06436163187026978, data time: 0.010074574606759208\n",
      "step: 694821, loss: 0.05774606764316559, data time: 0.009845111105177138\n",
      "step: 694822, loss: 0.06132424995303154, data time: 0.009630673640483135\n",
      "step: 694823, loss: 0.06598886847496033, data time: 0.009429053256386205\n",
      "step: 694824, loss: 0.06438520550727844, data time: 0.00923593227679913\n",
      "step: 694825, loss: 0.05461858958005905, data time: 0.009057509899139404\n",
      "step: 694826, loss: 0.06334125250577927, data time: 0.27169203758239746\n",
      "step: 694827, loss: 0.06466633826494217, data time: 0.13713300228118896\n",
      "step: 694828, loss: 0.06496082246303558, data time: 0.09241064389546712\n",
      "step: 694829, loss: 0.06839717924594879, data time: 0.07006549835205078\n",
      "step: 694830, loss: 0.061446189880371094, data time: 0.05633525848388672\n",
      "step: 694831, loss: 0.054232560098171234, data time: 0.047220309575398765\n",
      "step: 694832, loss: 0.061340443789958954, data time: 0.04071007456098284\n",
      "step: 694833, loss: 0.06619808077812195, data time: 0.03593403100967407\n",
      "step: 694834, loss: 0.05984794348478317, data time: 0.03208695517645942\n",
      "step: 694835, loss: 0.06048227474093437, data time: 0.02908961772918701\n",
      "step: 694836, loss: 0.059104204177856445, data time: 0.026638616215098988\n",
      "step: 694837, loss: 0.06644999235868454, data time: 0.02458365758260091\n",
      "step: 694838, loss: 0.060335732996463776, data time: 0.022867826315072868\n",
      "step: 694839, loss: 0.07224179804325104, data time: 0.021378227642604282\n",
      "step: 694840, loss: 0.06203765794634819, data time: 0.020115486780802407\n",
      "step: 694841, loss: 0.05908869951963425, data time: 0.018995240330696106\n",
      "step: 694842, loss: 0.057026833295822144, data time: 0.01799900391522576\n",
      "step: 694843, loss: 0.06151062250137329, data time: 0.01711322201622857\n",
      "step: 694844, loss: 0.06546428054571152, data time: 0.016320228576660156\n",
      "step: 694845, loss: 0.0597173273563385, data time: 0.015609323978424072\n",
      "step: 694846, loss: 0.06000149995088577, data time: 0.014964444296700614\n",
      "step: 694847, loss: 0.05816172808408737, data time: 0.014386686411770907\n",
      "step: 694848, loss: 0.05970269441604614, data time: 0.013853384100872538\n",
      "step: 694849, loss: 0.06174417585134506, data time: 0.013369212547938028\n",
      "step: 694850, loss: 0.06760285794734955, data time: 0.01292013168334961\n",
      "step: 694851, loss: 0.05932775139808655, data time: 0.012500707919781025\n",
      "step: 694852, loss: 0.0632316917181015, data time: 0.012110445234510634\n",
      "step: 694853, loss: 0.05726584047079086, data time: 0.011750962053026472\n",
      "step: 694854, loss: 0.05826263129711151, data time: 0.01141881120615992\n",
      "step: 694855, loss: 0.06256996095180511, data time: 0.011107508341471355\n",
      "step: 694856, loss: 0.059544336050748825, data time: 0.010821311704574092\n",
      "step: 694857, loss: 0.06129830330610275, data time: 0.01055559515953064\n",
      "step: 694858, loss: 0.05781663581728935, data time: 0.010300860260472153\n",
      "step: 694859, loss: 0.05713583528995514, data time: 0.010058417039759019\n",
      "step: 694860, loss: 0.05903910845518112, data time: 0.009826557976858956\n",
      "step: 694861, loss: 0.06240782141685486, data time: 0.009605063332451714\n",
      "step: 694862, loss: 0.06420356035232544, data time: 0.009397255407797324\n",
      "step: 694863, loss: 0.05894837900996208, data time: 0.009199606744866622\n",
      "step: 694864, loss: 0.058770131319761276, data time: 0.009011782132662259\n",
      "step: 694865, loss: 0.05054938420653343, data time: 0.008841335773468018\n",
      "step: 694866, loss: 0.061578139662742615, data time: 0.29098987579345703\n",
      "step: 694867, loss: 0.05692608281970024, data time: 0.14627456665039062\n",
      "step: 694868, loss: 0.06754893064498901, data time: 0.09797811508178711\n",
      "step: 694869, loss: 0.06333307176828384, data time: 0.07437026500701904\n",
      "step: 694870, loss: 0.06005008518695831, data time: 0.059798908233642575\n",
      "step: 694871, loss: 0.05989013612270355, data time: 0.050074378649393715\n",
      "step: 694872, loss: 0.06347702443599701, data time: 0.04315355845860073\n",
      "step: 694873, loss: 0.062188856303691864, data time: 0.038030266761779785\n",
      "step: 694874, loss: 0.06467310339212418, data time: 0.03394990497165256\n",
      "step: 694875, loss: 0.061026789247989655, data time: 0.03075375556945801\n",
      "step: 694876, loss: 0.06550773978233337, data time: 0.028158382935957474\n",
      "step: 694877, loss: 0.06194622442126274, data time: 0.025992095470428467\n",
      "step: 694878, loss: 0.065372996032238, data time: 0.024165226862980768\n",
      "step: 694879, loss: 0.06163925677537918, data time: 0.022589751652308872\n",
      "step: 694880, loss: 0.0632108524441719, data time: 0.021224164962768556\n",
      "step: 694881, loss: 0.06095645949244499, data time: 0.020039379596710205\n",
      "step: 694882, loss: 0.06034940108656883, data time: 0.018995060640222886\n",
      "step: 694883, loss: 0.06029481813311577, data time: 0.01805024676852756\n",
      "step: 694884, loss: 0.06174808368086815, data time: 0.017206606112028424\n",
      "step: 694885, loss: 0.056134384125471115, data time: 0.016458463668823243\n",
      "step: 694886, loss: 0.06234118342399597, data time: 0.015771105175926572\n",
      "step: 694887, loss: 0.06265150010585785, data time: 0.015148509632457386\n",
      "step: 694888, loss: 0.058134790509939194, data time: 0.014580674793409265\n",
      "step: 694889, loss: 0.060504250228405, data time: 0.014060288667678833\n",
      "step: 694890, loss: 0.05928051471710205, data time: 0.013588304519653321\n",
      "step: 694891, loss: 0.05547692999243736, data time: 0.013146794759310208\n",
      "step: 694892, loss: 0.06850218027830124, data time: 0.012753168741861979\n",
      "step: 694893, loss: 0.06026093289256096, data time: 0.012368031910487584\n",
      "step: 694894, loss: 0.06089650094509125, data time: 0.012016493698646283\n",
      "step: 694895, loss: 0.0621420256793499, data time: 0.011684242884318035\n",
      "step: 694896, loss: 0.06121475622057915, data time: 0.01137538110056231\n",
      "step: 694897, loss: 0.06290692090988159, data time: 0.011091873049736023\n",
      "step: 694898, loss: 0.06582504510879517, data time: 0.010814529476743755\n",
      "step: 694899, loss: 0.05885779857635498, data time: 0.010558142381555894\n",
      "step: 694900, loss: 0.05386115238070488, data time: 0.010315922328404017\n",
      "step: 694901, loss: 0.06698843836784363, data time: 0.010081741544935439\n",
      "step: 694902, loss: 0.06028100103139877, data time: 0.009863453942376215\n",
      "step: 694903, loss: 0.0559992752969265, data time: 0.009658248800980417\n",
      "step: 694904, loss: 0.05818649381399155, data time: 0.009459244899260692\n",
      "step: 694905, loss: 0.06766984611749649, data time: 0.009276801347732544\n",
      "step: 694906, loss: 0.0655813217163086, data time: 0.28447818756103516\n",
      "step: 694907, loss: 0.057994477450847626, data time: 0.1429610252380371\n",
      "step: 694908, loss: 0.05897057056427002, data time: 0.09578529993693034\n",
      "step: 694909, loss: 0.0597771555185318, data time: 0.07274943590164185\n",
      "step: 694910, loss: 0.06542614102363586, data time: 0.05847377777099609\n",
      "step: 694911, loss: 0.05841406434774399, data time: 0.04898818333943685\n",
      "step: 694912, loss: 0.05970590561628342, data time: 0.04220386913844517\n",
      "step: 694913, loss: 0.06363765895366669, data time: 0.03719210624694824\n",
      "step: 694914, loss: 0.06909213215112686, data time: 0.03320309850904676\n",
      "step: 694915, loss: 0.0568821094930172, data time: 0.030088591575622558\n",
      "step: 694916, loss: 0.05993321165442467, data time: 0.027536240491000088\n",
      "step: 694917, loss: 0.061488084495067596, data time: 0.02541526158650716\n",
      "step: 694918, loss: 0.05664005130529404, data time: 0.02363115090590257\n",
      "step: 694919, loss: 0.06148790568113327, data time: 0.022085632596697127\n",
      "step: 694920, loss: 0.05606532841920853, data time: 0.02075169881184896\n",
      "step: 694921, loss: 0.056770771741867065, data time: 0.019589290022850037\n",
      "step: 694922, loss: 0.06194093078374863, data time: 0.018564630957210764\n",
      "step: 694923, loss: 0.05675872415304184, data time: 0.017647663752237957\n",
      "step: 694924, loss: 0.0647047907114029, data time: 0.0168219992988988\n",
      "step: 694925, loss: 0.05951046943664551, data time: 0.01608142852783203\n",
      "step: 694926, loss: 0.05991493910551071, data time: 0.015410729816981725\n",
      "step: 694927, loss: 0.05674702674150467, data time: 0.014817118644714355\n",
      "step: 694928, loss: 0.05954815074801445, data time: 0.01426281099734099\n",
      "step: 694929, loss: 0.05639047548174858, data time: 0.013755311568578085\n",
      "step: 694930, loss: 0.060744576156139374, data time: 0.013291511535644531\n",
      "step: 694931, loss: 0.06450928002595901, data time: 0.012862847401545597\n",
      "step: 694932, loss: 0.06474664807319641, data time: 0.01246032891450105\n",
      "step: 694933, loss: 0.06380034238100052, data time: 0.01208695343562535\n",
      "step: 694934, loss: 0.05523157864809036, data time: 0.011747763074677566\n",
      "step: 694935, loss: 0.06220325455069542, data time: 0.011424883206685384\n",
      "step: 694936, loss: 0.05956321954727173, data time: 0.01112987918238486\n",
      "step: 694937, loss: 0.055764924734830856, data time: 0.01085314154624939\n",
      "step: 694938, loss: 0.06518248468637466, data time: 0.010584751764933268\n",
      "step: 694939, loss: 0.06224853917956352, data time: 0.010333818547865924\n",
      "step: 694940, loss: 0.06216423958539963, data time: 0.010101706641060965\n",
      "step: 694941, loss: 0.06220947206020355, data time: 0.009871900081634521\n",
      "step: 694942, loss: 0.0583496168255806, data time: 0.009657215427707982\n",
      "step: 694943, loss: 0.061932142823934555, data time: 0.009455693395514237\n",
      "step: 694944, loss: 0.058670513331890106, data time: 0.009261565330700997\n",
      "step: 694945, loss: 0.08785246312618256, data time: 0.00908295512199402\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.41091394424438477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 694946, loss: 0.062053583562374115, data time: 0.2930481433868408\n",
      "step: 694947, loss: 0.0681658536195755, data time: 0.14732575416564941\n",
      "step: 694948, loss: 0.06126577779650688, data time: 0.09875297546386719\n",
      "step: 694949, loss: 0.06427469104528427, data time: 0.07482808828353882\n",
      "step: 694950, loss: 0.06004997342824936, data time: 0.06013345718383789\n",
      "step: 694951, loss: 0.06610924750566483, data time: 0.05034403006235758\n",
      "step: 694952, loss: 0.06291487067937851, data time: 0.043368986674717495\n",
      "step: 694953, loss: 0.0634545236825943, data time: 0.03819555044174194\n",
      "step: 694954, loss: 0.061420001089572906, data time: 0.034092982610066734\n",
      "step: 694955, loss: 0.06115791201591492, data time: 0.03087594509124756\n",
      "step: 694956, loss: 0.06424706429243088, data time: 0.02827232534235174\n",
      "step: 694957, loss: 0.0594479963183403, data time: 0.026095330715179443\n",
      "step: 694958, loss: 0.06769804656505585, data time: 0.024265876183143027\n",
      "step: 694959, loss: 0.057819437235593796, data time: 0.02267176764351981\n",
      "step: 694960, loss: 0.05555165931582451, data time: 0.02133185068766276\n",
      "step: 694961, loss: 0.06296823918819427, data time: 0.020123615860939026\n",
      "step: 694962, loss: 0.06209705397486687, data time: 0.01906261724584243\n",
      "step: 694963, loss: 0.05734969303011894, data time: 0.018117070198059082\n",
      "step: 694964, loss: 0.05529582500457764, data time: 0.017268820812827664\n",
      "step: 694965, loss: 0.06151878088712692, data time: 0.01651676893234253\n",
      "step: 694966, loss: 0.06527788937091827, data time: 0.01583332107180641\n",
      "step: 694967, loss: 0.0633619949221611, data time: 0.015211397951299494\n",
      "step: 694968, loss: 0.06534899771213531, data time: 0.014642000198364258\n",
      "step: 694969, loss: 0.06091408431529999, data time: 0.014116446177164713\n",
      "step: 694970, loss: 0.06332619488239288, data time: 0.01363739013671875\n",
      "step: 694971, loss: 0.05912376567721367, data time: 0.013190792157099797\n",
      "step: 694972, loss: 0.06092553585767746, data time: 0.012774538110803675\n",
      "step: 694973, loss: 0.06077825278043747, data time: 0.012387582233973913\n",
      "step: 694974, loss: 0.06081186980009079, data time: 0.012038091133380759\n",
      "step: 694975, loss: 0.06047863885760307, data time: 0.011710508664449056\n",
      "step: 694976, loss: 0.05922689288854599, data time: 0.011404983458980437\n",
      "step: 694977, loss: 0.06636106222867966, data time: 0.011119291186332703\n",
      "step: 694978, loss: 0.05345718190073967, data time: 0.010841326280073687\n",
      "step: 694979, loss: 0.057625576853752136, data time: 0.010579859509187587\n",
      "step: 694980, loss: 0.06839894503355026, data time: 0.01033367429460798\n",
      "step: 694981, loss: 0.05837087333202362, data time: 0.01009588771396213\n",
      "step: 694982, loss: 0.05452212691307068, data time: 0.009875207333951383\n",
      "step: 694983, loss: 0.06442797183990479, data time: 0.00967197669179816\n",
      "step: 694984, loss: 0.0636710599064827, data time: 0.0094756652147342\n",
      "step: 694985, loss: 0.06323888897895813, data time: 0.00928945541381836\n",
      "step: 694986, loss: 0.06638597697019577, data time: 0.29256129264831543\n",
      "step: 694987, loss: 0.060972146689891815, data time: 0.14705336093902588\n",
      "step: 694988, loss: 0.06620825827121735, data time: 0.09856001536051433\n",
      "step: 694989, loss: 0.06473422050476074, data time: 0.074698805809021\n",
      "step: 694990, loss: 0.0576191209256649, data time: 0.06008090972900391\n",
      "step: 694991, loss: 0.06086675450205803, data time: 0.05031847953796387\n",
      "step: 694992, loss: 0.05555720254778862, data time: 0.04333492687770298\n",
      "step: 694993, loss: 0.06472079455852509, data time: 0.03817254304885864\n",
      "step: 694994, loss: 0.06224595010280609, data time: 0.03407936626010471\n",
      "step: 694995, loss: 0.06397396326065063, data time: 0.03086857795715332\n",
      "step: 694996, loss: 0.06420323252677917, data time: 0.02826146645979448\n",
      "step: 694997, loss: 0.061061035841703415, data time: 0.02608416477839152\n",
      "step: 694998, loss: 0.06625580042600632, data time: 0.024248434947087213\n",
      "step: 694999, loss: 0.059680335223674774, data time: 0.02265707084110805\n",
      "step: 695000, loss: 0.06654888391494751, data time: 0.021291780471801757\n",
      "step: 695001, loss: 0.06228569149971008, data time: 0.02009013295173645\n",
      "step: 695002, loss: 0.0643300786614418, data time: 0.01903094964868882\n",
      "step: 695003, loss: 0.06478407979011536, data time: 0.01807990339067247\n",
      "step: 695004, loss: 0.060898423194885254, data time: 0.01723338428296541\n",
      "step: 695005, loss: 0.06011207029223442, data time: 0.016482269763946532\n",
      "step: 695006, loss: 0.06254532188177109, data time: 0.015801656813848586\n",
      "step: 695007, loss: 0.060177695006132126, data time: 0.015182126652110706\n",
      "step: 695008, loss: 0.06378109753131866, data time: 0.014611368593962296\n",
      "step: 695009, loss: 0.06415455788373947, data time: 0.014087319374084473\n",
      "step: 695010, loss: 0.05952874943614006, data time: 0.013604135513305663\n",
      "step: 695011, loss: 0.06490927189588547, data time: 0.013161952678973857\n",
      "step: 695012, loss: 0.06393545866012573, data time: 0.012749557141904478\n",
      "step: 695013, loss: 0.06443712115287781, data time: 0.012365221977233887\n",
      "step: 695014, loss: 0.05766035616397858, data time: 0.012026655262914198\n",
      "step: 695015, loss: 0.06444679200649261, data time: 0.011707703272501627\n",
      "step: 695016, loss: 0.060700565576553345, data time: 0.011401038016042402\n",
      "step: 695017, loss: 0.06202516704797745, data time: 0.011117503046989441\n",
      "step: 695018, loss: 0.06043307110667229, data time: 0.010838400233875622\n",
      "step: 695019, loss: 0.06135158985853195, data time: 0.010578008259043974\n",
      "step: 695020, loss: 0.06571078300476074, data time: 0.010330636160714286\n",
      "step: 695021, loss: 0.05689138174057007, data time: 0.010094510184393989\n",
      "step: 695022, loss: 0.060593195259571075, data time: 0.009871953242533916\n",
      "step: 695023, loss: 0.05511710047721863, data time: 0.009668601186651932\n",
      "step: 695024, loss: 0.05912550911307335, data time: 0.00947275528540978\n",
      "step: 695025, loss: 0.051802512258291245, data time: 0.009287422895431519\n",
      "step: 695026, loss: 0.06818122416734695, data time: 0.2910187244415283\n",
      "step: 695027, loss: 0.0614900067448616, data time: 0.1470789909362793\n",
      "step: 695028, loss: 0.061886709183454514, data time: 0.09856979052225749\n",
      "step: 695029, loss: 0.061470042914152145, data time: 0.07475429773330688\n",
      "step: 695030, loss: 0.06567241996526718, data time: 0.06008634567260742\n",
      "step: 695031, loss: 0.06478515267372131, data time: 0.050318241119384766\n",
      "step: 695032, loss: 0.06138693541288376, data time: 0.043327774320329936\n",
      "step: 695033, loss: 0.060140460729599, data time: 0.038170039653778076\n",
      "step: 695034, loss: 0.06470577418804169, data time: 0.034072293175591364\n",
      "step: 695035, loss: 0.06424084305763245, data time: 0.030864429473876954\n",
      "step: 695036, loss: 0.0576813668012619, data time: 0.02826868404041637\n",
      "step: 695037, loss: 0.06762552261352539, data time: 0.02609982093175252\n",
      "step: 695038, loss: 0.06726016104221344, data time: 0.024257898330688477\n",
      "step: 695039, loss: 0.06106709688901901, data time: 0.022666028567722867\n",
      "step: 695040, loss: 0.05906796455383301, data time: 0.021289793650309245\n",
      "step: 695041, loss: 0.06266777962446213, data time: 0.020082786679267883\n",
      "step: 695042, loss: 0.06817200779914856, data time: 0.01902113241307876\n",
      "step: 695043, loss: 0.06467057764530182, data time: 0.018076909912957087\n",
      "step: 695044, loss: 0.05770878493785858, data time: 0.017230510711669922\n",
      "step: 695045, loss: 0.06275413930416107, data time: 0.016483497619628907\n",
      "step: 695046, loss: 0.05930512398481369, data time: 0.015804949260893323\n",
      "step: 695047, loss: 0.062178608030080795, data time: 0.01518470590764826\n",
      "step: 695048, loss: 0.05780739337205887, data time: 0.014615463173907736\n",
      "step: 695049, loss: 0.06011545658111572, data time: 0.014092137416203817\n",
      "step: 695050, loss: 0.06549669802188873, data time: 0.013611669540405274\n",
      "step: 695051, loss: 0.06928660720586777, data time: 0.013166097494272085\n",
      "step: 695052, loss: 0.06009744107723236, data time: 0.012749830881754557\n",
      "step: 695053, loss: 0.06084301695227623, data time: 0.012369104794093541\n",
      "step: 695054, loss: 0.05962508171796799, data time: 0.012020423494536301\n",
      "step: 695055, loss: 0.06055545434355736, data time: 0.011690918604532878\n",
      "step: 695056, loss: 0.06395253539085388, data time: 0.011395585152410691\n",
      "step: 695057, loss: 0.056337952613830566, data time: 0.011119812726974487\n",
      "step: 695058, loss: 0.06067401170730591, data time: 0.010843565969756155\n",
      "step: 695059, loss: 0.06461305171251297, data time: 0.010583779391120462\n",
      "step: 695060, loss: 0.06494130939245224, data time: 0.01034097671508789\n",
      "step: 695061, loss: 0.06455926597118378, data time: 0.010107762283749051\n",
      "step: 695062, loss: 0.06277835369110107, data time: 0.009888455674454972\n",
      "step: 695063, loss: 0.06123250350356102, data time: 0.009687059804012901\n",
      "step: 695064, loss: 0.06881975382566452, data time: 0.009494353563357621\n",
      "step: 695065, loss: 0.07806359231472015, data time: 0.00931081771850586\n",
      "step: 695066, loss: 0.0634540244936943, data time: 0.2916581630706787\n",
      "step: 695067, loss: 0.0643666535615921, data time: 0.14709341526031494\n",
      "step: 695068, loss: 0.05906394124031067, data time: 0.09860094388326009\n",
      "step: 695069, loss: 0.05530722066760063, data time: 0.07481110095977783\n",
      "step: 695070, loss: 0.060464099049568176, data time: 0.060124874114990234\n",
      "step: 695071, loss: 0.05839923769235611, data time: 0.05033449331919352\n",
      "step: 695072, loss: 0.06040825694799423, data time: 0.043378591537475586\n",
      "step: 695073, loss: 0.054647140204906464, data time: 0.03825950622558594\n",
      "step: 695074, loss: 0.06964440643787384, data time: 0.034181303448147245\n",
      "step: 695075, loss: 0.05964791029691696, data time: 0.03099391460418701\n",
      "step: 695076, loss: 0.06189598888158798, data time: 0.028416438536210495\n",
      "step: 695077, loss: 0.053973060101270676, data time: 0.026265323162078857\n",
      "step: 695078, loss: 0.06511413305997849, data time: 0.024443351305448092\n",
      "step: 695079, loss: 0.06005193293094635, data time: 0.022866691861833845\n",
      "step: 695080, loss: 0.06621134281158447, data time: 0.02150270144144694\n",
      "step: 695081, loss: 0.058766841888427734, data time: 0.02030685544013977\n",
      "step: 695082, loss: 0.05654839798808098, data time: 0.01925500701455509\n",
      "step: 695083, loss: 0.060177914798259735, data time: 0.018318030569288466\n",
      "step: 695084, loss: 0.06565186381340027, data time: 0.017485204495881732\n",
      "step: 695085, loss: 0.06461332738399506, data time: 0.01673910617828369\n",
      "step: 695086, loss: 0.06617747992277145, data time: 0.01606123788016183\n",
      "step: 695087, loss: 0.059920668601989746, data time: 0.01546124978498979\n",
      "step: 695088, loss: 0.05627405643463135, data time: 0.014893790949945864\n",
      "step: 695089, loss: 0.05996178835630417, data time: 0.01437510053316752\n",
      "step: 695090, loss: 0.0658588856458664, data time: 0.013895750045776367\n",
      "step: 695091, loss: 0.05760018527507782, data time: 0.013454932432908278\n",
      "step: 695092, loss: 0.06060120463371277, data time: 0.013042600066573531\n",
      "step: 695093, loss: 0.061538100242614746, data time: 0.01265967743737357\n",
      "step: 695094, loss: 0.056629814207553864, data time: 0.012312675344532934\n",
      "step: 695095, loss: 0.06324367970228195, data time: 0.011989967028299967\n",
      "step: 695096, loss: 0.06289903819561005, data time: 0.011683787069013041\n",
      "step: 695097, loss: 0.058522146195173264, data time: 0.011399753391742706\n",
      "step: 695098, loss: 0.06504151225090027, data time: 0.01112144643610174\n",
      "step: 695099, loss: 0.06423094123601913, data time: 0.010853311594794779\n",
      "step: 695100, loss: 0.05909278243780136, data time: 0.010603645869663784\n",
      "step: 695101, loss: 0.058107078075408936, data time: 0.010364578829871284\n",
      "step: 695102, loss: 0.06316425651311874, data time: 0.010149002075195312\n",
      "step: 695103, loss: 0.06764263659715652, data time: 0.009940574043675474\n",
      "step: 695104, loss: 0.05626588314771652, data time: 0.009743170860486153\n",
      "step: 695105, loss: 0.08477193117141724, data time: 0.009553462266921997\n",
      "step: 695106, loss: 0.06610693037509918, data time: 0.2959439754486084\n",
      "step: 695107, loss: 0.06203343719244003, data time: 0.14875948429107666\n",
      "step: 695108, loss: 0.060923319309949875, data time: 0.0997021993001302\n",
      "step: 695109, loss: 0.062446385622024536, data time: 0.0755537748336792\n",
      "step: 695110, loss: 0.05931205302476883, data time: 0.06071305274963379\n",
      "step: 695111, loss: 0.061412513256073, data time: 0.050829529762268066\n",
      "step: 695112, loss: 0.06558720767498016, data time: 0.04376578330993652\n",
      "step: 695113, loss: 0.05417411029338837, data time: 0.038554221391677856\n",
      "step: 695114, loss: 0.060548268258571625, data time: 0.03441521856519911\n",
      "step: 695115, loss: 0.06249028444290161, data time: 0.03116791248321533\n",
      "step: 695116, loss: 0.057985685765743256, data time: 0.0285368182442405\n",
      "step: 695117, loss: 0.05926644802093506, data time: 0.02633790175120036\n",
      "step: 695118, loss: 0.06643714010715485, data time: 0.02447986602783203\n",
      "step: 695119, loss: 0.0626760795712471, data time: 0.02288494791303362\n",
      "step: 695120, loss: 0.05878223106265068, data time: 0.02149802843729655\n",
      "step: 695121, loss: 0.05938177555799484, data time: 0.02027648687362671\n",
      "step: 695122, loss: 0.062313102185726166, data time: 0.01920622937819537\n",
      "step: 695123, loss: 0.05720827728509903, data time: 0.01824384265475803\n",
      "step: 695124, loss: 0.059950750321149826, data time: 0.017389598645662006\n",
      "step: 695125, loss: 0.05899786949157715, data time: 0.016631615161895753\n",
      "step: 695126, loss: 0.062415629625320435, data time: 0.015943266096569243\n",
      "step: 695127, loss: 0.0601620189845562, data time: 0.015313907103105024\n",
      "step: 695128, loss: 0.06157294660806656, data time: 0.014736299929411514\n",
      "step: 695129, loss: 0.06406349688768387, data time: 0.014210095008214315\n",
      "step: 695130, loss: 0.06357044726610184, data time: 0.013720216751098633\n",
      "step: 695131, loss: 0.0679725781083107, data time: 0.013271597715524526\n",
      "step: 695132, loss: 0.058893024921417236, data time: 0.01285492049323188\n",
      "step: 695133, loss: 0.0654028058052063, data time: 0.012467009680611747\n",
      "step: 695134, loss: 0.05775972455739975, data time: 0.012113464289698107\n",
      "step: 695135, loss: 0.06741920113563538, data time: 0.011785594622294109\n",
      "step: 695136, loss: 0.06301625818014145, data time: 0.011476470578101373\n",
      "step: 695137, loss: 0.0634191483259201, data time: 0.011186644434928894\n",
      "step: 695138, loss: 0.05906950682401657, data time: 0.010906811916466915\n",
      "step: 695139, loss: 0.06435093283653259, data time: 0.010640368742101333\n",
      "step: 695140, loss: 0.060875631868839264, data time: 0.010393973759242467\n",
      "step: 695141, loss: 0.0639304667711258, data time: 0.010157412952846952\n",
      "step: 695142, loss: 0.0630863830447197, data time: 0.009933684323285077\n",
      "step: 695143, loss: 0.062140993773937225, data time: 0.009728977554722837\n",
      "step: 695144, loss: 0.06415707617998123, data time: 0.009533924934191581\n",
      "step: 695145, loss: 0.051663488149642944, data time: 0.009345835447311402\n",
      "step: 695146, loss: 0.06488193571567535, data time: 0.29576873779296875\n",
      "step: 695147, loss: 0.06154165789484978, data time: 0.14928066730499268\n",
      "step: 695148, loss: 0.057570070028305054, data time: 0.10061756769816081\n",
      "step: 695149, loss: 0.061067841947078705, data time: 0.07614767551422119\n",
      "step: 695150, loss: 0.0620698556303978, data time: 0.06120510101318359\n",
      "step: 695151, loss: 0.0630376935005188, data time: 0.05123583475748698\n",
      "step: 695152, loss: 0.06320368498563766, data time: 0.04411428315298898\n",
      "step: 695153, loss: 0.06197032332420349, data time: 0.0388721227645874\n",
      "step: 695154, loss: 0.061668671667575836, data time: 0.03470987743801541\n",
      "step: 695155, loss: 0.06719415634870529, data time: 0.031455183029174806\n",
      "step: 695156, loss: 0.05438750609755516, data time: 0.028798601844094017\n",
      "step: 695157, loss: 0.06568526476621628, data time: 0.02658190329869588\n",
      "step: 695158, loss: 0.0693555474281311, data time: 0.024700403213500977\n",
      "step: 695159, loss: 0.065643809735775, data time: 0.023077300616673062\n",
      "step: 695160, loss: 0.061118416488170624, data time: 0.021680196126302082\n",
      "step: 695161, loss: 0.06143103167414665, data time: 0.020449072122573853\n",
      "step: 695162, loss: 0.06395890563726425, data time: 0.019360934986787683\n",
      "step: 695163, loss: 0.06345358490943909, data time: 0.018396615982055664\n",
      "step: 695164, loss: 0.05939982086420059, data time: 0.017535912363152755\n",
      "step: 695165, loss: 0.05901471525430679, data time: 0.01676720380783081\n",
      "step: 695166, loss: 0.06189785897731781, data time: 0.01607787041437058\n",
      "step: 695167, loss: 0.06487434357404709, data time: 0.015443281693892046\n",
      "step: 695168, loss: 0.062390051782131195, data time: 0.014859976975814156\n",
      "step: 695169, loss: 0.05993126332759857, data time: 0.014326353867848715\n",
      "step: 695170, loss: 0.05936477705836296, data time: 0.013837003707885742\n",
      "step: 695171, loss: 0.06662043184041977, data time: 0.013382654923659105\n",
      "step: 695172, loss: 0.05746575444936752, data time: 0.012958588423552337\n",
      "step: 695173, loss: 0.061465539038181305, data time: 0.01256617477961949\n",
      "step: 695174, loss: 0.05663705989718437, data time: 0.0122091687958816\n",
      "step: 695175, loss: 0.06004326790571213, data time: 0.011879412333170573\n",
      "step: 695176, loss: 0.06081617623567581, data time: 0.011567608002693422\n",
      "step: 695177, loss: 0.06229455769062042, data time: 0.011275224387645721\n",
      "step: 695178, loss: 0.06379424035549164, data time: 0.010994289860580906\n",
      "step: 695179, loss: 0.05845622345805168, data time: 0.010728078729966107\n",
      "step: 695180, loss: 0.07230593264102936, data time: 0.01047558103288923\n",
      "step: 695181, loss: 0.053824156522750854, data time: 0.010234998332129585\n",
      "step: 695182, loss: 0.06212177500128746, data time: 0.010009804287472286\n",
      "step: 695183, loss: 0.059703629463911057, data time: 0.00980259242810701\n",
      "step: 695184, loss: 0.06024455651640892, data time: 0.009607901939978966\n",
      "step: 695185, loss: 0.0680016577243805, data time: 0.009420138597488404\n",
      "step: 695186, loss: 0.059945374727249146, data time: 0.2930481433868408\n",
      "step: 695187, loss: 0.05708660930395126, data time: 0.14735424518585205\n",
      "step: 695188, loss: 0.06205321103334427, data time: 0.09873827298482259\n",
      "step: 695189, loss: 0.06832835078239441, data time: 0.07491391897201538\n",
      "step: 695190, loss: 0.0600406751036644, data time: 0.0602017879486084\n",
      "step: 695191, loss: 0.06282342970371246, data time: 0.05040280024210612\n",
      "step: 695192, loss: 0.0609065406024456, data time: 0.0434157167162214\n",
      "step: 695193, loss: 0.05756243318319321, data time: 0.03825885057449341\n",
      "step: 695194, loss: 0.05816563591361046, data time: 0.034160243140326604\n",
      "step: 695195, loss: 0.06623886525630951, data time: 0.030935335159301757\n",
      "step: 695196, loss: 0.059018753468990326, data time: 0.028326576406305485\n",
      "step: 695197, loss: 0.05883276090025902, data time: 0.02614470322926839\n",
      "step: 695198, loss: 0.058064766228199005, data time: 0.024303564658531777\n",
      "step: 695199, loss: 0.06311126053333282, data time: 0.022707121712820872\n",
      "step: 695200, loss: 0.06004902720451355, data time: 0.0213286558787028\n",
      "step: 695201, loss: 0.05908555909991264, data time: 0.020124003291130066\n",
      "step: 695202, loss: 0.06481459736824036, data time: 0.019081410239724553\n",
      "step: 695203, loss: 0.06939180195331573, data time: 0.018146342701382108\n",
      "step: 695204, loss: 0.05732553452253342, data time: 0.017315726531179326\n",
      "step: 695205, loss: 0.07325172424316406, data time: 0.016576325893402098\n",
      "step: 695206, loss: 0.0595046691596508, data time: 0.015903711318969727\n",
      "step: 695207, loss: 0.06346936523914337, data time: 0.015295473012057218\n",
      "step: 695208, loss: 0.05961506813764572, data time: 0.014734599901282269\n",
      "step: 695209, loss: 0.06293964385986328, data time: 0.01421902577082316\n",
      "step: 695210, loss: 0.05305875465273857, data time: 0.013745965957641602\n",
      "step: 695211, loss: 0.06212633103132248, data time: 0.013310056466322679\n",
      "step: 695212, loss: 0.05919038504362106, data time: 0.012904061211480035\n",
      "step: 695213, loss: 0.05995924398303032, data time: 0.012528036321912493\n",
      "step: 695214, loss: 0.06217872351408005, data time: 0.012197749368075666\n",
      "step: 695215, loss: 0.06429355591535568, data time: 0.011874063809712728\n",
      "step: 695216, loss: 0.059399377554655075, data time: 0.011575698852539062\n",
      "step: 695217, loss: 0.06247158348560333, data time: 0.011293299496173859\n",
      "step: 695218, loss: 0.05960152670741081, data time: 0.011011658292828184\n",
      "step: 695219, loss: 0.06260441243648529, data time: 0.010749459266662598\n",
      "step: 695220, loss: 0.06256452947854996, data time: 0.010501623153686523\n",
      "step: 695221, loss: 0.05977056547999382, data time: 0.010264628463321261\n",
      "step: 695222, loss: 0.06107253581285477, data time: 0.010041417302312079\n",
      "step: 695223, loss: 0.061136823147535324, data time: 0.00983457188857229\n",
      "step: 695224, loss: 0.06089964509010315, data time: 0.00963744139059996\n",
      "step: 695225, loss: 0.052061256021261215, data time: 0.00945190191268921\n",
      "step: 695226, loss: 0.06454731523990631, data time: 0.28482890129089355\n",
      "step: 695227, loss: 0.05513560771942139, data time: 0.143163800239563\n",
      "step: 695228, loss: 0.06506314873695374, data time: 0.09598644574483235\n",
      "step: 695229, loss: 0.06366195529699326, data time: 0.07286679744720459\n",
      "step: 695230, loss: 0.058930933475494385, data time: 0.0585667610168457\n",
      "step: 695231, loss: 0.05859258770942688, data time: 0.04904627799987793\n",
      "step: 695232, loss: 0.06062128394842148, data time: 0.042247465678623745\n",
      "step: 695233, loss: 0.05965828895568848, data time: 0.037212491035461426\n",
      "step: 695234, loss: 0.06644278019666672, data time: 0.03322972191704644\n",
      "step: 695235, loss: 0.06544217467308044, data time: 0.03010425567626953\n",
      "step: 695236, loss: 0.054693907499313354, data time: 0.02756478569724343\n",
      "step: 695237, loss: 0.06250440329313278, data time: 0.025447348753611248\n",
      "step: 695238, loss: 0.06591753661632538, data time: 0.023660898208618164\n",
      "step: 695239, loss: 0.061868250370025635, data time: 0.022115213530404226\n",
      "step: 695240, loss: 0.06480883806943893, data time: 0.020774078369140626\n",
      "step: 695241, loss: 0.06629734486341476, data time: 0.019607320427894592\n",
      "step: 695242, loss: 0.05990775674581528, data time: 0.01859283447265625\n",
      "step: 695243, loss: 0.06078098714351654, data time: 0.017688910166422527\n",
      "step: 695244, loss: 0.06615358591079712, data time: 0.016881541201942844\n",
      "step: 695245, loss: 0.060576822608709335, data time: 0.01616690158843994\n",
      "step: 695246, loss: 0.06078792363405228, data time: 0.015517791112263998\n",
      "step: 695247, loss: 0.06445635855197906, data time: 0.014922727238048206\n",
      "step: 695248, loss: 0.06307805329561234, data time: 0.014380268428636633\n",
      "step: 695249, loss: 0.06514549255371094, data time: 0.01388274629910787\n",
      "step: 695250, loss: 0.06378082931041718, data time: 0.013426361083984375\n",
      "step: 695251, loss: 0.05945640802383423, data time: 0.013001836263216458\n",
      "step: 695252, loss: 0.06208755075931549, data time: 0.0126072124198631\n",
      "step: 695253, loss: 0.06144201382994652, data time: 0.012240103312901087\n",
      "step: 695254, loss: 0.05983280763030052, data time: 0.011903680604079673\n",
      "step: 695255, loss: 0.06240076571702957, data time: 0.011589606602986654\n",
      "step: 695256, loss: 0.056389495730400085, data time: 0.011294972512029832\n",
      "step: 695257, loss: 0.05773359164595604, data time: 0.011022090911865234\n",
      "step: 695258, loss: 0.055634450167417526, data time: 0.010749499003092447\n",
      "step: 695259, loss: 0.06618084013462067, data time: 0.010494680965647978\n",
      "step: 695260, loss: 0.059907328337430954, data time: 0.010250043869018555\n",
      "step: 695261, loss: 0.06079874560236931, data time: 0.010016189681159126\n",
      "step: 695262, loss: 0.05919605493545532, data time: 0.009795833278346705\n",
      "step: 695263, loss: 0.05772187560796738, data time: 0.009591039858366313\n",
      "step: 695264, loss: 0.056842707097530365, data time: 0.009397457807491988\n",
      "step: 695265, loss: 0.06253461539745331, data time: 0.009213614463806152\n",
      "step: 695266, loss: 0.06090261787176132, data time: 0.2934262752532959\n",
      "step: 695267, loss: 0.058339428156614304, data time: 0.1474705934524536\n",
      "step: 695268, loss: 0.06205087527632713, data time: 0.0993808110555013\n",
      "step: 695269, loss: 0.059650540351867676, data time: 0.07530361413955688\n",
      "step: 695270, loss: 0.06662865728139877, data time: 0.06051268577575684\n",
      "step: 695271, loss: 0.06233271211385727, data time: 0.050675153732299805\n",
      "step: 695272, loss: 0.06187973544001579, data time: 0.04363884244646345\n",
      "step: 695273, loss: 0.06149781495332718, data time: 0.03843331336975098\n",
      "step: 695274, loss: 0.056568220257759094, data time: 0.03430891036987305\n",
      "step: 695275, loss: 0.05390128493309021, data time: 0.031075286865234374\n",
      "step: 695276, loss: 0.0627339705824852, data time: 0.02845419536937367\n",
      "step: 695277, loss: 0.06338699907064438, data time: 0.026264746983846027\n",
      "step: 695278, loss: 0.06766390800476074, data time: 0.02440951420710637\n",
      "step: 695279, loss: 0.06140302121639252, data time: 0.022804226194109236\n",
      "step: 695280, loss: 0.05892329663038254, data time: 0.02141722043355306\n",
      "step: 695281, loss: 0.06144994497299194, data time: 0.02020837366580963\n",
      "step: 695282, loss: 0.0574372336268425, data time: 0.019143244799445656\n",
      "step: 695283, loss: 0.06827449798583984, data time: 0.01819500658247206\n",
      "step: 695284, loss: 0.06601150333881378, data time: 0.017346507624575968\n",
      "step: 695285, loss: 0.06461408734321594, data time: 0.016591155529022218\n",
      "step: 695286, loss: 0.062346167862415314, data time: 0.015907321657453264\n",
      "step: 695287, loss: 0.061194103211164474, data time: 0.015288331291892311\n",
      "step: 695288, loss: 0.05561075732111931, data time: 0.014712043430494226\n",
      "step: 695289, loss: 0.05521799623966217, data time: 0.014196236928304037\n",
      "step: 695290, loss: 0.06219449266791344, data time: 0.013726377487182617\n",
      "step: 695291, loss: 0.0667189508676529, data time: 0.013291413967426006\n",
      "step: 695292, loss: 0.05778661370277405, data time: 0.012886797940289532\n",
      "step: 695293, loss: 0.05948498100042343, data time: 0.012512956346784319\n",
      "step: 695294, loss: 0.05956302210688591, data time: 0.012167511315181338\n",
      "step: 695295, loss: 0.06314302980899811, data time: 0.01184844175974528\n",
      "step: 695296, loss: 0.059663183987140656, data time: 0.0115462887671686\n",
      "step: 695297, loss: 0.0586482398211956, data time: 0.011263914406299591\n",
      "step: 695298, loss: 0.058583326637744904, data time: 0.010983568249326763\n",
      "step: 695299, loss: 0.05977632850408554, data time: 0.010721262763528264\n",
      "step: 695300, loss: 0.06248648092150688, data time: 0.01047285624912807\n",
      "step: 695301, loss: 0.0652027353644371, data time: 0.010237501727210151\n",
      "step: 695302, loss: 0.057185035198926926, data time: 0.010015410345953863\n",
      "step: 695303, loss: 0.06425279378890991, data time: 0.009809519115247224\n",
      "step: 695304, loss: 0.06327372789382935, data time: 0.00961463267986591\n",
      "step: 695305, loss: 0.07626324146986008, data time: 0.00942782759666443\n",
      "step: 695306, loss: 0.07360416650772095, data time: 0.29576992988586426\n",
      "step: 695307, loss: 0.05911324545741081, data time: 0.1486433744430542\n",
      "step: 695308, loss: 0.06535150110721588, data time: 0.09961557388305664\n",
      "step: 695309, loss: 0.06214803457260132, data time: 0.07562023401260376\n",
      "step: 695310, loss: 0.06478047370910645, data time: 0.060770940780639646\n",
      "step: 695311, loss: 0.06229139864444733, data time: 0.05088388919830322\n",
      "step: 695312, loss: 0.06172860413789749, data time: 0.04382995196751186\n",
      "step: 695313, loss: 0.05975522845983505, data time: 0.038602739572525024\n",
      "step: 695314, loss: 0.05958261340856552, data time: 0.034457392162746854\n",
      "step: 695315, loss: 0.06283847987651825, data time: 0.031211280822753908\n",
      "step: 695316, loss: 0.06573913991451263, data time: 0.028570240194147282\n",
      "step: 695317, loss: 0.06518696248531342, data time: 0.026366313298543293\n",
      "step: 695318, loss: 0.06284162402153015, data time: 0.024504606540386494\n",
      "step: 695319, loss: 0.06513676047325134, data time: 0.022895319121224538\n",
      "step: 695320, loss: 0.0599730908870697, data time: 0.02150382995605469\n",
      "step: 695321, loss: 0.05573447048664093, data time: 0.020293205976486206\n",
      "step: 695322, loss: 0.0561799518764019, data time: 0.019226859597598806\n",
      "step: 695323, loss: 0.06145266816020012, data time: 0.01827110184563531\n",
      "step: 695324, loss: 0.059215933084487915, data time: 0.017415034143548263\n",
      "step: 695325, loss: 0.06849148869514465, data time: 0.01665174961090088\n",
      "step: 695326, loss: 0.05350089073181152, data time: 0.015963304610479446\n",
      "step: 695327, loss: 0.061858080327510834, data time: 0.015335993333296343\n",
      "step: 695328, loss: 0.060601573437452316, data time: 0.014754222786944845\n",
      "step: 695329, loss: 0.061544667929410934, data time: 0.014221509297688803\n",
      "step: 695330, loss: 0.05619245767593384, data time: 0.013736953735351562\n",
      "step: 695331, loss: 0.05791640281677246, data time: 0.01328727832207313\n",
      "step: 695332, loss: 0.05971803888678551, data time: 0.01287202481870298\n",
      "step: 695333, loss: 0.06292421370744705, data time: 0.012483179569244385\n",
      "step: 695334, loss: 0.06204073876142502, data time: 0.012126758180815598\n",
      "step: 695335, loss: 0.05930941551923752, data time: 0.011797920862833659\n",
      "step: 695336, loss: 0.05308977887034416, data time: 0.011490629565331244\n",
      "step: 695337, loss: 0.06620822846889496, data time: 0.011200547218322754\n",
      "step: 695338, loss: 0.056528620421886444, data time: 0.010918082612933535\n",
      "step: 695339, loss: 0.06705605983734131, data time: 0.01065509459551643\n",
      "step: 695340, loss: 0.06123709678649902, data time: 0.010404559544154576\n",
      "step: 695341, loss: 0.06649956107139587, data time: 0.010177128844790988\n",
      "step: 695342, loss: 0.05901430547237396, data time: 0.009951675260389174\n",
      "step: 695343, loss: 0.05663682520389557, data time: 0.009744041844418174\n",
      "step: 695344, loss: 0.06233970448374748, data time: 0.009549342668973483\n",
      "step: 695345, loss: 0.0695282369852066, data time: 0.009365499019622803\n",
      "step: 695346, loss: 0.06050942838191986, data time: 0.29845309257507324\n",
      "step: 695347, loss: 0.06057528778910637, data time: 0.14998912811279297\n",
      "step: 695348, loss: 0.05827770754694939, data time: 0.1009058157602946\n",
      "step: 695349, loss: 0.056539468467235565, data time: 0.07646775245666504\n",
      "step: 695350, loss: 0.06280882656574249, data time: 0.06144871711730957\n",
      "step: 695351, loss: 0.056910496205091476, data time: 0.051445206006368004\n",
      "step: 695352, loss: 0.05647987127304077, data time: 0.04429459571838379\n",
      "step: 695353, loss: 0.058358751237392426, data time: 0.0390036404132843\n",
      "step: 695354, loss: 0.059229545295238495, data time: 0.034814410739474826\n",
      "step: 695355, loss: 0.06458818167448044, data time: 0.03153729438781738\n",
      "step: 695356, loss: 0.06120800971984863, data time: 0.028873421929099342\n",
      "step: 695357, loss: 0.056239888072013855, data time: 0.026647528012593586\n",
      "step: 695358, loss: 0.05834727734327316, data time: 0.0247647578899677\n",
      "step: 695359, loss: 0.0557238832116127, data time: 0.023152027811322893\n",
      "step: 695360, loss: 0.05700675770640373, data time: 0.02174523671468099\n",
      "step: 695361, loss: 0.06234335899353027, data time: 0.02051439881324768\n",
      "step: 695362, loss: 0.063789501786232, data time: 0.019429262946633732\n",
      "step: 695363, loss: 0.060467906296253204, data time: 0.018460618125067815\n",
      "step: 695364, loss: 0.0617765411734581, data time: 0.017590924313193874\n",
      "step: 695365, loss: 0.07016819715499878, data time: 0.01682252883911133\n",
      "step: 695366, loss: 0.059183888137340546, data time: 0.016126553217569988\n",
      "step: 695367, loss: 0.06323143094778061, data time: 0.015491604804992676\n",
      "step: 695368, loss: 0.059961941093206406, data time: 0.014906986899997877\n",
      "step: 695369, loss: 0.06278707832098007, data time: 0.014376779397328695\n",
      "step: 695370, loss: 0.06103486567735672, data time: 0.013887548446655273\n",
      "step: 695371, loss: 0.06070004403591156, data time: 0.013430347809424767\n",
      "step: 695372, loss: 0.05890756845474243, data time: 0.013005936587298358\n",
      "step: 695373, loss: 0.06814861297607422, data time: 0.012615621089935303\n",
      "step: 695374, loss: 0.06346329301595688, data time: 0.012255857730733937\n",
      "step: 695375, loss: 0.06148349493741989, data time: 0.011921151479085287\n",
      "step: 695376, loss: 0.053691815584897995, data time: 0.011609569672615297\n",
      "step: 695377, loss: 0.06198414787650108, data time: 0.011317066848278046\n",
      "step: 695378, loss: 0.05866770073771477, data time: 0.011034698197335907\n",
      "step: 695379, loss: 0.06132303178310394, data time: 0.010765804963953355\n",
      "step: 695380, loss: 0.06766626238822937, data time: 0.010511548178536551\n",
      "step: 695381, loss: 0.06459522992372513, data time: 0.010273502932654487\n",
      "step: 695382, loss: 0.06803321838378906, data time: 0.010049272227931666\n",
      "step: 695383, loss: 0.058171242475509644, data time: 0.009843506311115465\n",
      "step: 695384, loss: 0.06632356345653534, data time: 0.009646910887498122\n",
      "step: 695385, loss: 0.07025536894798279, data time: 0.009460866451263428\n",
      "step: 695386, loss: 0.05879328399896622, data time: 0.29105710983276367\n",
      "step: 695387, loss: 0.057744111865758896, data time: 0.1466618776321411\n",
      "step: 695388, loss: 0.06235289201140404, data time: 0.09828861554463704\n",
      "step: 695389, loss: 0.061222463846206665, data time: 0.07460892200469971\n",
      "step: 695390, loss: 0.06152046471834183, data time: 0.05996413230895996\n",
      "step: 695391, loss: 0.05809895694255829, data time: 0.05020336310068766\n",
      "step: 695392, loss: 0.0630548894405365, data time: 0.043231112616402764\n",
      "step: 695393, loss: 0.056576769798994064, data time: 0.03807783126831055\n",
      "step: 695394, loss: 0.058490853756666183, data time: 0.0339968204498291\n",
      "step: 695395, loss: 0.0590456984937191, data time: 0.030810213088989256\n",
      "step: 695396, loss: 0.060740649700164795, data time: 0.028209556232799183\n",
      "step: 695397, loss: 0.0639348030090332, data time: 0.026036242643992107\n",
      "step: 695398, loss: 0.057824473828077316, data time: 0.024198201986459587\n",
      "step: 695399, loss: 0.06179894506931305, data time: 0.022611635071890696\n",
      "step: 695400, loss: 0.05626894161105156, data time: 0.021236928304036458\n",
      "step: 695401, loss: 0.06524321436882019, data time: 0.02003943920135498\n",
      "step: 695402, loss: 0.06278876215219498, data time: 0.01898504705990062\n",
      "step: 695403, loss: 0.06336040049791336, data time: 0.01803730593787299\n",
      "step: 695404, loss: 0.0615588054060936, data time: 0.017194346377724094\n",
      "step: 695405, loss: 0.057528238743543625, data time: 0.016439378261566162\n",
      "step: 695406, loss: 0.06368610262870789, data time: 0.01576103482927595\n",
      "step: 695407, loss: 0.06147215887904167, data time: 0.015150698748501864\n",
      "step: 695408, loss: 0.06624472141265869, data time: 0.014578684516575026\n",
      "step: 695409, loss: 0.0653228759765625, data time: 0.014053483804066976\n",
      "step: 695410, loss: 0.06483229249715805, data time: 0.013572015762329102\n",
      "step: 695411, loss: 0.06953630596399307, data time: 0.013129115104675293\n",
      "step: 695412, loss: 0.06569474935531616, data time: 0.012716019595110858\n",
      "step: 695413, loss: 0.06381478160619736, data time: 0.012331485748291016\n",
      "step: 695414, loss: 0.06260943412780762, data time: 0.011980410279898808\n",
      "step: 695415, loss: 0.05399535223841667, data time: 0.011651817957560222\n",
      "step: 695416, loss: 0.060148775577545166, data time: 0.011345955633348035\n",
      "step: 695417, loss: 0.061122700572013855, data time: 0.01106308400630951\n",
      "step: 695418, loss: 0.06910255551338196, data time: 0.010784965572935162\n",
      "step: 695419, loss: 0.06264683604240417, data time: 0.010524160721722771\n",
      "step: 695420, loss: 0.055602800101041794, data time: 0.01027853148324149\n",
      "step: 695421, loss: 0.05891309306025505, data time: 0.01004562775293986\n",
      "step: 695422, loss: 0.06179600954055786, data time: 0.009824321076676652\n",
      "step: 695423, loss: 0.06575512886047363, data time: 0.009619185799046567\n",
      "step: 695424, loss: 0.06476202607154846, data time: 0.009424435786711864\n",
      "step: 695425, loss: 0.036642711609601974, data time: 0.009240341186523438\n",
      "step: 695426, loss: 0.06203716993331909, data time: 0.29694604873657227\n",
      "step: 695427, loss: 0.06566284596920013, data time: 0.14922809600830078\n",
      "step: 695428, loss: 0.06965772807598114, data time: 0.1000043551127116\n",
      "step: 695429, loss: 0.06813071668148041, data time: 0.07576388120651245\n",
      "step: 695430, loss: 0.05540180951356888, data time: 0.060898447036743165\n",
      "step: 695431, loss: 0.059423595666885376, data time: 0.05098179976145426\n",
      "step: 695432, loss: 0.06626564264297485, data time: 0.0439150333404541\n",
      "step: 695433, loss: 0.06260405480861664, data time: 0.038693130016326904\n",
      "step: 695434, loss: 0.052786730229854584, data time: 0.03454142146640354\n",
      "step: 695435, loss: 0.06350848078727722, data time: 0.03128185272216797\n",
      "step: 695436, loss: 0.06512241065502167, data time: 0.028629974885420365\n",
      "step: 695437, loss: 0.06540349125862122, data time: 0.0264320174853007\n",
      "step: 695438, loss: 0.060349322855472565, data time: 0.02456366098844088\n",
      "step: 695439, loss: 0.062242500483989716, data time: 0.022953016417367116\n",
      "step: 695440, loss: 0.06172676384449005, data time: 0.021555471420288085\n",
      "step: 695441, loss: 0.05699294060468674, data time: 0.020339295268058777\n",
      "step: 695442, loss: 0.05985496565699577, data time: 0.019262846778420842\n",
      "step: 695443, loss: 0.0660061463713646, data time: 0.018302824762132432\n",
      "step: 695444, loss: 0.05490759015083313, data time: 0.017446969684801604\n",
      "step: 695445, loss: 0.06007623299956322, data time: 0.01668144464492798\n",
      "step: 695446, loss: 0.05459658429026604, data time: 0.01599066598074777\n",
      "step: 695447, loss: 0.05823899060487747, data time: 0.015361753377047453\n",
      "step: 695448, loss: 0.05893082916736603, data time: 0.014781848244045092\n",
      "step: 695449, loss: 0.0659511461853981, data time: 0.014250357945760092\n",
      "step: 695450, loss: 0.06636800616979599, data time: 0.01376338005065918\n",
      "step: 695451, loss: 0.06468556076288223, data time: 0.013314393850473257\n",
      "step: 695452, loss: 0.0638112872838974, data time: 0.01289457745022244\n",
      "step: 695453, loss: 0.06762945652008057, data time: 0.012505863394056047\n",
      "step: 695454, loss: 0.057891976088285446, data time: 0.012148955772662985\n",
      "step: 695455, loss: 0.06092303618788719, data time: 0.011820340156555175\n",
      "step: 695456, loss: 0.05714556202292442, data time: 0.011512548692764775\n",
      "step: 695457, loss: 0.06250858306884766, data time: 0.011225029826164246\n",
      "step: 695458, loss: 0.06968565285205841, data time: 0.010943875168309067\n",
      "step: 695459, loss: 0.05823087692260742, data time: 0.01067824223462273\n",
      "step: 695460, loss: 0.06772881746292114, data time: 0.010428687504359654\n",
      "step: 695461, loss: 0.0589434951543808, data time: 0.010189228587680392\n",
      "step: 695462, loss: 0.06207243725657463, data time: 0.009963969926576357\n",
      "step: 695463, loss: 0.06366895884275436, data time: 0.009755034195749383\n",
      "step: 695464, loss: 0.06611126661300659, data time: 0.009557613959679237\n",
      "step: 695465, loss: 0.052930839359760284, data time: 0.00936928391456604\n",
      "step: 695466, loss: 0.05945558845996857, data time: 0.2922782897949219\n",
      "step: 695467, loss: 0.0621480755507946, data time: 0.14773941040039062\n",
      "step: 695468, loss: 0.057619787752628326, data time: 0.09954333305358887\n",
      "step: 695469, loss: 0.060378871858119965, data time: 0.07533711194992065\n",
      "step: 695470, loss: 0.06582088768482208, data time: 0.060559654235839845\n",
      "step: 695471, loss: 0.05796905606985092, data time: 0.050707618395487465\n",
      "step: 695472, loss: 0.06332038342952728, data time: 0.04366534096854074\n",
      "step: 695473, loss: 0.06546643376350403, data time: 0.03845864534378052\n",
      "step: 695474, loss: 0.06887556612491608, data time: 0.03433166609870063\n",
      "step: 695475, loss: 0.061859577894210815, data time: 0.03110649585723877\n",
      "step: 695476, loss: 0.06178605556488037, data time: 0.028475718064741654\n",
      "step: 695477, loss: 0.058338068425655365, data time: 0.02628546953201294\n",
      "step: 695478, loss: 0.05536237359046936, data time: 0.024428587693434495\n",
      "step: 695479, loss: 0.05778588354587555, data time: 0.022828749247959683\n",
      "step: 695480, loss: 0.05801643058657646, data time: 0.02144150733947754\n",
      "step: 695481, loss: 0.06047256290912628, data time: 0.020226940512657166\n",
      "step: 695482, loss: 0.06201136112213135, data time: 0.019153160207411823\n",
      "step: 695483, loss: 0.062234386801719666, data time: 0.018197377522786457\n",
      "step: 695484, loss: 0.06385069340467453, data time: 0.017348201651322216\n",
      "step: 695485, loss: 0.06639709323644638, data time: 0.016595053672790527\n",
      "step: 695486, loss: 0.06616159528493881, data time: 0.015908309391566684\n",
      "step: 695487, loss: 0.06008119881153107, data time: 0.015284213152798739\n",
      "step: 695488, loss: 0.05992724001407623, data time: 0.014705740887185802\n",
      "step: 695489, loss: 0.06251628696918488, data time: 0.014176269372304281\n",
      "step: 695490, loss: 0.06286291778087616, data time: 0.013689336776733398\n",
      "step: 695491, loss: 0.06571812927722931, data time: 0.013240245672372671\n",
      "step: 695492, loss: 0.05576377362012863, data time: 0.012821621365017362\n",
      "step: 695493, loss: 0.06398097425699234, data time: 0.012437283992767334\n",
      "step: 695494, loss: 0.06055140495300293, data time: 0.012081614856062264\n",
      "step: 695495, loss: 0.06591613590717316, data time: 0.011752009391784668\n",
      "step: 695496, loss: 0.06105604022741318, data time: 0.011441399974207725\n",
      "step: 695497, loss: 0.06142544001340866, data time: 0.011155813932418823\n",
      "step: 695498, loss: 0.05838136002421379, data time: 0.010876287113536488\n",
      "step: 695499, loss: 0.05801897495985031, data time: 0.010614878991070916\n",
      "step: 695500, loss: 0.06252534687519073, data time: 0.010365751811436244\n",
      "step: 695501, loss: 0.06203845143318176, data time: 0.01012906101014879\n",
      "step: 695502, loss: 0.0636693686246872, data time: 0.009908205754048115\n",
      "step: 695503, loss: 0.05840049311518669, data time: 0.009702826801099275\n",
      "step: 695504, loss: 0.06226889416575432, data time: 0.009506005507249098\n",
      "step: 695505, loss: 0.051251452416181564, data time: 0.009320080280303955\n",
      "step: 695506, loss: 0.06398816406726837, data time: 0.2958984375\n",
      "step: 695507, loss: 0.06257548928260803, data time: 0.14869678020477295\n",
      "step: 695508, loss: 0.05734970420598984, data time: 0.10016290346781413\n",
      "step: 695509, loss: 0.06258778274059296, data time: 0.07589930295944214\n",
      "step: 695510, loss: 0.05750250071287155, data time: 0.06099104881286621\n",
      "step: 695511, loss: 0.06013728678226471, data time: 0.05105753739674886\n",
      "step: 695512, loss: 0.05786503851413727, data time: 0.0439763069152832\n",
      "step: 695513, loss: 0.06582850217819214, data time: 0.038733839988708496\n",
      "step: 695514, loss: 0.06195668503642082, data time: 0.0345765749613444\n",
      "step: 695515, loss: 0.056145500391721725, data time: 0.03131437301635742\n",
      "step: 695516, loss: 0.06535477936267853, data time: 0.028659495440396397\n",
      "step: 695517, loss: 0.05522068589925766, data time: 0.0264474352200826\n",
      "step: 695518, loss: 0.06909564882516861, data time: 0.024578424600454476\n",
      "step: 695519, loss: 0.0643235445022583, data time: 0.022978152547563826\n",
      "step: 695520, loss: 0.06504945456981659, data time: 0.021584018071492513\n",
      "step: 695521, loss: 0.053533535450696945, data time: 0.0203646719455719\n",
      "step: 695522, loss: 0.059553541243076324, data time: 0.019289521610035616\n",
      "step: 695523, loss: 0.06300662457942963, data time: 0.01832515663570828\n",
      "step: 695524, loss: 0.06370658427476883, data time: 0.017465892590974506\n",
      "step: 695525, loss: 0.06137998029589653, data time: 0.01670241355895996\n",
      "step: 695526, loss: 0.06373453140258789, data time: 0.016007718585786365\n",
      "step: 695527, loss: 0.06362036615610123, data time: 0.01537976481697776\n",
      "step: 695528, loss: 0.05993485450744629, data time: 0.014800745507945185\n",
      "step: 695529, loss: 0.05999436974525452, data time: 0.014269868532816568\n",
      "step: 695530, loss: 0.06032325327396393, data time: 0.01378490447998047\n",
      "step: 695531, loss: 0.06789524853229523, data time: 0.013334934528057393\n",
      "step: 695532, loss: 0.06565248221158981, data time: 0.012913695088139287\n",
      "step: 695533, loss: 0.06330084055662155, data time: 0.012525941644396101\n",
      "step: 695534, loss: 0.06097005307674408, data time: 0.012168892498674064\n",
      "step: 695535, loss: 0.055745676159858704, data time: 0.011834939320882162\n",
      "step: 695536, loss: 0.06537362188100815, data time: 0.011524800331361832\n",
      "step: 695537, loss: 0.06436707079410553, data time: 0.011236123740673065\n",
      "step: 695538, loss: 0.07000572979450226, data time: 0.01095281947742809\n",
      "step: 695539, loss: 0.05723332613706589, data time: 0.01068897808299345\n",
      "step: 695540, loss: 0.055302441120147705, data time: 0.010439804622105189\n",
      "step: 695541, loss: 0.05986158177256584, data time: 0.010201010439130995\n",
      "step: 695542, loss: 0.05858300253748894, data time: 0.009980472358497413\n",
      "step: 695543, loss: 0.060589298605918884, data time: 0.009771209014089484\n",
      "step: 695544, loss: 0.060765720903873444, data time: 0.00957476175748385\n",
      "step: 695545, loss: 0.07767225801944733, data time: 0.009387075901031494\n",
      "step: 695546, loss: 0.05488061159849167, data time: 0.282146692276001\n",
      "step: 695547, loss: 0.061157818883657455, data time: 0.14224028587341309\n",
      "step: 695548, loss: 0.05825873464345932, data time: 0.09604724248250325\n",
      "step: 695549, loss: 0.0627492293715477, data time: 0.07273703813552856\n",
      "step: 695550, loss: 0.06383384764194489, data time: 0.058451080322265626\n",
      "step: 695551, loss: 0.058487650007009506, data time: 0.048944711685180664\n",
      "step: 695552, loss: 0.06380947679281235, data time: 0.042149135044642856\n",
      "step: 695553, loss: 0.057501405477523804, data time: 0.037132829427719116\n",
      "step: 695554, loss: 0.05547560378909111, data time: 0.03315975930955675\n",
      "step: 695555, loss: 0.07070636749267578, data time: 0.030055713653564454\n",
      "step: 695556, loss: 0.06039806827902794, data time: 0.02753340114246715\n",
      "step: 695557, loss: 0.06122323125600815, data time: 0.025418678919474285\n",
      "step: 695558, loss: 0.060057587921619415, data time: 0.023630435650165264\n",
      "step: 695559, loss: 0.06216473877429962, data time: 0.022081187793186734\n",
      "step: 695560, loss: 0.059151530265808105, data time: 0.020745070775349934\n",
      "step: 695561, loss: 0.059039097279310226, data time: 0.019578710198402405\n",
      "step: 695562, loss: 0.06532500684261322, data time: 0.018545697717105642\n",
      "step: 695563, loss: 0.05956562981009483, data time: 0.017625146441989474\n",
      "step: 695564, loss: 0.06107775494456291, data time: 0.016804393969084088\n",
      "step: 695565, loss: 0.060413312166929245, data time: 0.01607403755187988\n",
      "step: 695566, loss: 0.0668189749121666, data time: 0.015416179384504045\n",
      "step: 695567, loss: 0.06493330001831055, data time: 0.014817703853953968\n",
      "step: 695568, loss: 0.07032700628042221, data time: 0.01425822921421217\n",
      "step: 695569, loss: 0.06244107335805893, data time: 0.013748188813527426\n",
      "step: 695570, loss: 0.06633059680461884, data time: 0.013284978866577148\n",
      "step: 695571, loss: 0.06167249009013176, data time: 0.01285115572122427\n",
      "step: 695572, loss: 0.06065065413713455, data time: 0.012450430128309462\n",
      "step: 695573, loss: 0.06309255212545395, data time: 0.012078940868377686\n",
      "step: 695574, loss: 0.06176174804568291, data time: 0.011736902697332975\n",
      "step: 695575, loss: 0.06504839658737183, data time: 0.01141969362894694\n",
      "step: 695576, loss: 0.06172497197985649, data time: 0.011120188620782668\n",
      "step: 695577, loss: 0.06327429413795471, data time: 0.01084166020154953\n",
      "step: 695578, loss: 0.06347279995679855, data time: 0.010571414774114435\n",
      "step: 695579, loss: 0.06506025791168213, data time: 0.010321511941797593\n",
      "step: 695580, loss: 0.06870214641094208, data time: 0.010082428795950754\n",
      "step: 695581, loss: 0.06274265050888062, data time: 0.009853204091389975\n",
      "step: 695582, loss: 0.07002156972885132, data time: 0.009638586559811153\n",
      "step: 695583, loss: 0.05809043347835541, data time: 0.009439593867251747\n",
      "step: 695584, loss: 0.07012484967708588, data time: 0.009250463583530525\n",
      "step: 695585, loss: 0.0781855434179306, data time: 0.009074985980987549\n",
      "step: 695586, loss: 0.06049271672964096, data time: 0.2980682849884033\n",
      "step: 695587, loss: 0.06341441720724106, data time: 0.14977872371673584\n",
      "step: 695588, loss: 0.05896521732211113, data time: 0.10037843386332194\n",
      "step: 695589, loss: 0.062269553542137146, data time: 0.07604449987411499\n",
      "step: 695590, loss: 0.06306254863739014, data time: 0.06111965179443359\n",
      "step: 695591, loss: 0.059392593801021576, data time: 0.05116764704386393\n",
      "step: 695592, loss: 0.0642341896891594, data time: 0.044057982308523994\n",
      "step: 695593, loss: 0.06713780760765076, data time: 0.038859009742736816\n",
      "step: 695594, loss: 0.0670718401670456, data time: 0.03468873765733507\n",
      "step: 695595, loss: 0.06359045207500458, data time: 0.031415772438049314\n",
      "step: 695596, loss: 0.05544673651456833, data time: 0.0287544077092951\n",
      "step: 695597, loss: 0.05994325876235962, data time: 0.026537875334421795\n",
      "step: 695598, loss: 0.057690586894750595, data time: 0.02466273307800293\n",
      "step: 695599, loss: 0.0619574598968029, data time: 0.0230452333177839\n",
      "step: 695600, loss: 0.058851033449172974, data time: 0.02164765993754069\n",
      "step: 695601, loss: 0.06456731259822845, data time: 0.020425930619239807\n",
      "step: 695602, loss: 0.06403662264347076, data time: 0.019365969826193416\n",
      "step: 695603, loss: 0.06634674966335297, data time: 0.018420524067348905\n",
      "step: 695604, loss: 0.06796057522296906, data time: 0.017577485034340305\n",
      "step: 695605, loss: 0.05739569664001465, data time: 0.01682097911834717\n",
      "step: 695606, loss: 0.05665705353021622, data time: 0.01614245914277576\n",
      "step: 695607, loss: 0.05931839346885681, data time: 0.015523563731800426\n",
      "step: 695608, loss: 0.05729880556464195, data time: 0.01495180959286897\n",
      "step: 695609, loss: 0.06719779223203659, data time: 0.014428784449895224\n",
      "step: 695610, loss: 0.06315304338932037, data time: 0.01394744873046875\n",
      "step: 695611, loss: 0.057150572538375854, data time: 0.013503652352553148\n",
      "step: 695612, loss: 0.061861347407102585, data time: 0.013090168988263165\n",
      "step: 695613, loss: 0.06854306161403656, data time: 0.012706381934029716\n",
      "step: 695614, loss: 0.06624478101730347, data time: 0.012356511477766365\n",
      "step: 695615, loss: 0.06107349321246147, data time: 0.012029457092285156\n",
      "step: 695616, loss: 0.057524293661117554, data time: 0.011722680061094223\n",
      "step: 695617, loss: 0.06281539052724838, data time: 0.011436596512794495\n",
      "step: 695618, loss: 0.06357081979513168, data time: 0.011151833967729048\n",
      "step: 695619, loss: 0.06668310612440109, data time: 0.010883555692784926\n",
      "step: 695620, loss: 0.06308963894844055, data time: 0.010631309236798968\n",
      "step: 695621, loss: 0.05723581835627556, data time: 0.010390434000227187\n",
      "step: 695622, loss: 0.06217455863952637, data time: 0.010163068771362305\n",
      "step: 695623, loss: 0.05936214327812195, data time: 0.009950982896905197\n",
      "step: 695624, loss: 0.061038680374622345, data time: 0.009750922520955404\n",
      "step: 695625, loss: 0.05826320871710777, data time: 0.009560734033584595\n",
      "step: 695626, loss: 0.06365113705396652, data time: 0.3059506416320801\n",
      "step: 695627, loss: 0.05753421038389206, data time: 0.15436100959777832\n",
      "step: 695628, loss: 0.0642748549580574, data time: 0.10366113980611165\n",
      "step: 695629, loss: 0.05534900352358818, data time: 0.07841908931732178\n",
      "step: 695630, loss: 0.061997562646865845, data time: 0.06302733421325683\n",
      "step: 695631, loss: 0.05879485607147217, data time: 0.05289109547932943\n",
      "step: 695632, loss: 0.059280894696712494, data time: 0.04553822108677456\n",
      "step: 695633, loss: 0.06007876992225647, data time: 0.04010289907455444\n",
      "step: 695634, loss: 0.06426861882209778, data time: 0.035866366492377386\n",
      "step: 695635, loss: 0.06525950878858566, data time: 0.03247044086456299\n",
      "step: 695636, loss: 0.06503399461507797, data time: 0.02971350062977184\n",
      "step: 695637, loss: 0.06050997972488403, data time: 0.027425249417622883\n",
      "step: 695638, loss: 0.05806201696395874, data time: 0.025485937411968525\n",
      "step: 695639, loss: 0.054882027208805084, data time: 0.02381077834538051\n",
      "step: 695640, loss: 0.058524101972579956, data time: 0.02236515680948893\n",
      "step: 695641, loss: 0.06572473794221878, data time: 0.021093711256980896\n",
      "step: 695642, loss: 0.06808525323867798, data time: 0.019971679238712087\n",
      "step: 695643, loss: 0.06266069412231445, data time: 0.018975959883795843\n",
      "step: 695644, loss: 0.06378859281539917, data time: 0.01808207913448936\n",
      "step: 695645, loss: 0.05904238671064377, data time: 0.017292380332946777\n",
      "step: 695646, loss: 0.06567706167697906, data time: 0.016573133922758557\n",
      "step: 695647, loss: 0.059803854674100876, data time: 0.015924908898093483\n",
      "step: 695648, loss: 0.061788253486156464, data time: 0.01532950608626656\n",
      "step: 695649, loss: 0.06072641909122467, data time: 0.01478088895479838\n",
      "step: 695650, loss: 0.06373530626296997, data time: 0.0142716121673584\n",
      "step: 695651, loss: 0.06407521665096283, data time: 0.013799914946922889\n",
      "step: 695652, loss: 0.06233259290456772, data time: 0.013360535656964337\n",
      "step: 695653, loss: 0.06208493933081627, data time: 0.01295607430594308\n",
      "step: 695654, loss: 0.05743587017059326, data time: 0.01258467805796656\n",
      "step: 695655, loss: 0.06408501416444778, data time: 0.012239766120910645\n",
      "step: 695656, loss: 0.06195679306983948, data time: 0.011915391491305443\n",
      "step: 695657, loss: 0.05909700319170952, data time: 0.01161571592092514\n",
      "step: 695658, loss: 0.0600929856300354, data time: 0.011323011282718542\n",
      "step: 695659, loss: 0.05940654128789902, data time: 0.011045568129595588\n",
      "step: 695660, loss: 0.06385105848312378, data time: 0.01078336579459054\n",
      "step: 695661, loss: 0.06725621223449707, data time: 0.010534948772854276\n",
      "step: 695662, loss: 0.0551329180598259, data time: 0.010299244442501583\n",
      "step: 695663, loss: 0.05415663123130798, data time: 0.010083807142157303\n",
      "step: 695664, loss: 0.06690094619989395, data time: 0.009877663392287035\n",
      "step: 695665, loss: 0.09099217504262924, data time: 0.009682035446166993\n",
      "step: 695666, loss: 0.06458073854446411, data time: 0.2885129451751709\n",
      "step: 695667, loss: 0.06564295291900635, data time: 0.14608824253082275\n",
      "step: 695668, loss: 0.06417062878608704, data time: 0.09789649645487468\n",
      "step: 695669, loss: 0.06825288385152817, data time: 0.07419711351394653\n",
      "step: 695670, loss: 0.06093716248869896, data time: 0.05963211059570313\n",
      "step: 695671, loss: 0.062482401728630066, data time: 0.04992274443308512\n",
      "step: 695672, loss: 0.06228514760732651, data time: 0.04300458090645926\n",
      "step: 695673, loss: 0.055657532066106796, data time: 0.0378873348236084\n",
      "step: 695674, loss: 0.06105247884988785, data time: 0.0338233576880561\n",
      "step: 695675, loss: 0.06473059952259064, data time: 0.03064711093902588\n",
      "step: 695676, loss: 0.06344079226255417, data time: 0.028059157458218662\n",
      "step: 695677, loss: 0.06325940787792206, data time: 0.025901436805725098\n",
      "step: 695678, loss: 0.06206154450774193, data time: 0.024074297684889574\n",
      "step: 695679, loss: 0.06443018466234207, data time: 0.02249561037336077\n",
      "step: 695680, loss: 0.055683135986328125, data time: 0.02113348642985026\n",
      "step: 695681, loss: 0.0655822679400444, data time: 0.019939497113227844\n",
      "step: 695682, loss: 0.062092289328575134, data time: 0.01888722531935748\n",
      "step: 695683, loss: 0.05484950542449951, data time: 0.017958588070339628\n",
      "step: 695684, loss: 0.06247204542160034, data time: 0.017122406708566767\n",
      "step: 695685, loss: 0.06028467044234276, data time: 0.016371762752532958\n",
      "step: 695686, loss: 0.062066398561000824, data time: 0.015695299421037947\n",
      "step: 695687, loss: 0.06692258268594742, data time: 0.01507899977944114\n",
      "step: 695688, loss: 0.06699036806821823, data time: 0.014511595601620882\n",
      "step: 695689, loss: 0.059073250740766525, data time: 0.013990292946497599\n",
      "step: 695690, loss: 0.06083783134818077, data time: 0.013512210845947266\n",
      "step: 695691, loss: 0.05854625627398491, data time: 0.013071445318368765\n",
      "step: 695692, loss: 0.062497686594724655, data time: 0.012674428798534252\n",
      "step: 695693, loss: 0.06527979671955109, data time: 0.012306068624768938\n",
      "step: 695694, loss: 0.0597890242934227, data time: 0.011965521450700432\n",
      "step: 695695, loss: 0.06330183148384094, data time: 0.011649441719055176\n",
      "step: 695696, loss: 0.05986888334155083, data time: 0.011353277391003024\n",
      "step: 695697, loss: 0.06030161306262016, data time: 0.011077634990215302\n",
      "step: 695698, loss: 0.0626508891582489, data time: 0.010802854191173206\n",
      "step: 695699, loss: 0.06708374619483948, data time: 0.010547665988697726\n",
      "step: 695700, loss: 0.055791132152080536, data time: 0.010305091312953403\n",
      "step: 695701, loss: 0.06418207287788391, data time: 0.010075933403438993\n",
      "step: 695702, loss: 0.0658232718706131, data time: 0.009858601802104228\n",
      "step: 695703, loss: 0.05806713551282883, data time: 0.009656830837852076\n",
      "step: 695704, loss: 0.060505591332912445, data time: 0.00946512589087853\n",
      "step: 695705, loss: 0.05415492504835129, data time: 0.00928230881690979\n",
      "step: 695706, loss: 0.06218839809298515, data time: 0.29650211334228516\n",
      "step: 695707, loss: 0.06304445117712021, data time: 0.149361252784729\n",
      "step: 695708, loss: 0.058653078973293304, data time: 0.10061089197794597\n",
      "step: 695709, loss: 0.057761263102293015, data time: 0.07623928785324097\n",
      "step: 695710, loss: 0.06133032962679863, data time: 0.0612675666809082\n",
      "step: 695711, loss: 0.0632493793964386, data time: 0.0512996514638265\n",
      "step: 695712, loss: 0.061952076852321625, data time: 0.04416261400495257\n",
      "step: 695713, loss: 0.06191011518239975, data time: 0.03888964653015137\n",
      "step: 695714, loss: 0.05963296815752983, data time: 0.03471734788682726\n",
      "step: 695715, loss: 0.06592115014791489, data time: 0.03144392967224121\n",
      "step: 695716, loss: 0.06825491786003113, data time: 0.02878093719482422\n",
      "step: 695717, loss: 0.06195240840315819, data time: 0.026575744152069092\n",
      "step: 695718, loss: 0.06254730373620987, data time: 0.02470110012934758\n",
      "step: 695719, loss: 0.06809677183628082, data time: 0.02307934420449393\n",
      "step: 695720, loss: 0.06356023997068405, data time: 0.021697235107421876\n",
      "step: 695721, loss: 0.05656908079981804, data time: 0.02046462893486023\n",
      "step: 695722, loss: 0.06650041043758392, data time: 0.019386571996352252\n",
      "step: 695723, loss: 0.057923685759305954, data time: 0.018416762351989746\n",
      "step: 695724, loss: 0.07417662441730499, data time: 0.017553429854543584\n",
      "step: 695725, loss: 0.06128427013754845, data time: 0.016789746284484864\n",
      "step: 695726, loss: 0.0671197772026062, data time: 0.016092459360758465\n",
      "step: 695727, loss: 0.0637037605047226, data time: 0.015457760203968395\n",
      "step: 695728, loss: 0.06222730129957199, data time: 0.014871130818906038\n",
      "step: 695729, loss: 0.06323172152042389, data time: 0.014333715041478476\n",
      "step: 695730, loss: 0.058849990367889404, data time: 0.013845291137695313\n",
      "step: 695731, loss: 0.06891930103302002, data time: 0.013392998622013973\n",
      "step: 695732, loss: 0.06512071192264557, data time: 0.012973290902596933\n",
      "step: 695733, loss: 0.06782075762748718, data time: 0.012582710811070033\n",
      "step: 695734, loss: 0.06788191944360733, data time: 0.012224633118201947\n",
      "step: 695735, loss: 0.0634114220738411, data time: 0.011889449755350749\n",
      "step: 695736, loss: 0.061003148555755615, data time: 0.011575314306443738\n",
      "step: 695737, loss: 0.06562550365924835, data time: 0.011285647749900818\n",
      "step: 695738, loss: 0.06194001063704491, data time: 0.011000864433519768\n",
      "step: 695739, loss: 0.058784887194633484, data time: 0.010733422111062443\n",
      "step: 695740, loss: 0.05816177278757095, data time: 0.01048128945486886\n",
      "step: 695741, loss: 0.05803173780441284, data time: 0.010241660806867812\n",
      "step: 695742, loss: 0.06505132466554642, data time: 0.010017072832262193\n",
      "step: 695743, loss: 0.06368424743413925, data time: 0.00980716630032188\n",
      "step: 695744, loss: 0.060968901962041855, data time: 0.009608146471854968\n",
      "step: 695745, loss: 0.05042107030749321, data time: 0.009418106079101563\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.41267943382263184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 695746, loss: 0.05536074936389923, data time: 0.2804715633392334\n",
      "step: 695747, loss: 0.05965111404657364, data time: 0.14159953594207764\n",
      "step: 695748, loss: 0.06104312837123871, data time: 0.09548203150431316\n",
      "step: 695749, loss: 0.06753887236118317, data time: 0.07227391004562378\n",
      "step: 695750, loss: 0.06313551962375641, data time: 0.058077430725097655\n",
      "step: 695751, loss: 0.056395523250103, data time: 0.0486446221669515\n",
      "step: 695752, loss: 0.06440603733062744, data time: 0.041901043483189175\n",
      "step: 695753, loss: 0.06607969850301743, data time: 0.03691357374191284\n",
      "step: 695754, loss: 0.0547078475356102, data time: 0.03295164638095432\n",
      "step: 695755, loss: 0.06855805218219757, data time: 0.029855823516845702\n",
      "step: 695756, loss: 0.05921153351664543, data time: 0.02733501521023837\n",
      "step: 695757, loss: 0.06052738428115845, data time: 0.025279760360717773\n",
      "step: 695758, loss: 0.05530443415045738, data time: 0.023517737021813027\n",
      "step: 695759, loss: 0.062189579010009766, data time: 0.02200005735669817\n",
      "step: 695760, loss: 0.0626651793718338, data time: 0.020692793528238933\n",
      "step: 695761, loss: 0.0609455332159996, data time: 0.019549280405044556\n",
      "step: 695762, loss: 0.06385867297649384, data time: 0.018532710916855755\n",
      "step: 695763, loss: 0.06008216738700867, data time: 0.01762990156809489\n",
      "step: 695764, loss: 0.06462271511554718, data time: 0.016825964576319644\n",
      "step: 695765, loss: 0.0648631677031517, data time: 0.016093945503234862\n",
      "step: 695766, loss: 0.05735575407743454, data time: 0.015428997221447173\n",
      "step: 695767, loss: 0.05807296559214592, data time: 0.014820749109441584\n",
      "step: 695768, loss: 0.06808161735534668, data time: 0.01426305978194527\n",
      "step: 695769, loss: 0.06669183075428009, data time: 0.013756553332010904\n",
      "step: 695770, loss: 0.06354137510061264, data time: 0.013287668228149413\n",
      "step: 695771, loss: 0.06840049475431442, data time: 0.012850513825049767\n",
      "step: 695772, loss: 0.06090463325381279, data time: 0.012446120933250145\n",
      "step: 695773, loss: 0.06319516897201538, data time: 0.012074095862252372\n",
      "step: 695774, loss: 0.0613107904791832, data time: 0.011730029665190598\n",
      "step: 695775, loss: 0.0648002028465271, data time: 0.011407923698425294\n",
      "step: 695776, loss: 0.06111031770706177, data time: 0.011108806056361045\n",
      "step: 695777, loss: 0.058016758412122726, data time: 0.01083284616470337\n",
      "step: 695778, loss: 0.06166751682758331, data time: 0.010562058651086056\n",
      "step: 695779, loss: 0.05632064491510391, data time: 0.010307599516475903\n",
      "step: 695780, loss: 0.06516467034816742, data time: 0.010065909794398717\n",
      "step: 695781, loss: 0.06967860460281372, data time: 0.009835574362013075\n",
      "step: 695782, loss: 0.0618610680103302, data time: 0.00961997702315047\n",
      "step: 695783, loss: 0.06515568494796753, data time: 0.009419309465508712\n",
      "step: 695784, loss: 0.06353931874036789, data time: 0.009230797107403096\n",
      "step: 695785, loss: 0.06375992298126221, data time: 0.009047782421112061\n",
      "step: 695786, loss: 0.06480162590742111, data time: 0.29129505157470703\n",
      "step: 695787, loss: 0.057926811277866364, data time: 0.14695191383361816\n",
      "step: 695788, loss: 0.07270024716854095, data time: 0.09845995903015137\n",
      "step: 695789, loss: 0.06831681728363037, data time: 0.07465094327926636\n",
      "step: 695790, loss: 0.06213490292429924, data time: 0.05999245643615723\n",
      "step: 695791, loss: 0.059065382927656174, data time: 0.05024357636769613\n",
      "step: 695792, loss: 0.05955970287322998, data time: 0.04326115335736956\n",
      "step: 695793, loss: 0.06134004890918732, data time: 0.03811454772949219\n",
      "step: 695794, loss: 0.05971166491508484, data time: 0.034021218617757164\n",
      "step: 695795, loss: 0.06388247013092041, data time: 0.030811667442321777\n",
      "step: 695796, loss: 0.06393572688102722, data time: 0.028200214559381657\n",
      "step: 695797, loss: 0.06346195191144943, data time: 0.026032090187072754\n",
      "step: 695798, loss: 0.056450407952070236, data time: 0.024187179712148812\n",
      "step: 695799, loss: 0.055817924439907074, data time: 0.02262611048562186\n",
      "step: 695800, loss: 0.06340201199054718, data time: 0.021276394526163738\n",
      "step: 695801, loss: 0.05854300782084465, data time: 0.020094215869903564\n",
      "step: 695802, loss: 0.060150787234306335, data time: 0.019052898182588464\n",
      "step: 695803, loss: 0.06298750638961792, data time: 0.018121838569641113\n",
      "step: 695804, loss: 0.05851077288389206, data time: 0.017292700315776625\n",
      "step: 695805, loss: 0.05748918652534485, data time: 0.016549444198608397\n",
      "step: 695806, loss: 0.05387632176280022, data time: 0.01587856383550735\n",
      "step: 695807, loss: 0.05537518858909607, data time: 0.015246694738214666\n",
      "step: 695808, loss: 0.06361448764801025, data time: 0.014668744543324347\n",
      "step: 695809, loss: 0.06157722696661949, data time: 0.014142354329427084\n",
      "step: 695810, loss: 0.05683916062116623, data time: 0.013656539916992188\n",
      "step: 695811, loss: 0.05757899209856987, data time: 0.013213359392606296\n",
      "step: 695812, loss: 0.060722701251506805, data time: 0.012796666887071397\n",
      "step: 695813, loss: 0.059263285249471664, data time: 0.012410504477364677\n",
      "step: 695814, loss: 0.06487283110618591, data time: 0.012053851423592403\n",
      "step: 695815, loss: 0.06912924349308014, data time: 0.011723621686299642\n",
      "step: 695816, loss: 0.06433452665805817, data time: 0.011409367284467143\n",
      "step: 695817, loss: 0.060595087707042694, data time: 0.011122263967990875\n",
      "step: 695818, loss: 0.06508345901966095, data time: 0.010844505194461706\n",
      "step: 695819, loss: 0.06349843740463257, data time: 0.010583926649654614\n",
      "step: 695820, loss: 0.05902281031012535, data time: 0.010339648383004324\n",
      "step: 695821, loss: 0.06627707183361053, data time: 0.010104921129014757\n",
      "step: 695822, loss: 0.059902533888816833, data time: 0.009883674415382179\n",
      "step: 695823, loss: 0.06201202794909477, data time: 0.009674887908132453\n",
      "step: 695824, loss: 0.06228038668632507, data time: 0.009478024947337614\n",
      "step: 695825, loss: 0.07397782057523727, data time: 0.009288930892944336\n",
      "step: 695826, loss: 0.06508725881576538, data time: 0.2968275547027588\n",
      "step: 695827, loss: 0.05887063592672348, data time: 0.14966309070587158\n",
      "step: 695828, loss: 0.05944138020277023, data time: 0.10067812601725261\n",
      "step: 695829, loss: 0.059469860047101974, data time: 0.0763472318649292\n",
      "step: 695830, loss: 0.06417867541313171, data time: 0.06136178970336914\n",
      "step: 695831, loss: 0.06709311902523041, data time: 0.05139474074045817\n",
      "step: 695832, loss: 0.06431315839290619, data time: 0.0442577770778111\n",
      "step: 695833, loss: 0.06250110268592834, data time: 0.03898131847381592\n",
      "step: 695834, loss: 0.05713614076375961, data time: 0.034790992736816406\n",
      "step: 695835, loss: 0.06182920187711716, data time: 0.03150849342346192\n",
      "step: 695836, loss: 0.05965432524681091, data time: 0.028843099420720882\n",
      "step: 695837, loss: 0.05849070101976395, data time: 0.026638031005859375\n",
      "step: 695838, loss: 0.05734025686979294, data time: 0.024745812782874473\n",
      "step: 695839, loss: 0.06211491674184799, data time: 0.02312161241258894\n",
      "step: 695840, loss: 0.05597442388534546, data time: 0.021719598770141603\n",
      "step: 695841, loss: 0.05707010254263878, data time: 0.020491600036621094\n",
      "step: 695842, loss: 0.05549537390470505, data time: 0.01940570158116958\n",
      "step: 695843, loss: 0.05997956916689873, data time: 0.018432974815368652\n",
      "step: 695844, loss: 0.057580120861530304, data time: 0.01756915293241802\n",
      "step: 695845, loss: 0.06101159006357193, data time: 0.016802442073822022\n",
      "step: 695846, loss: 0.06041643023490906, data time: 0.01611009098234631\n",
      "step: 695847, loss: 0.060390960425138474, data time: 0.015492233363064852\n",
      "step: 695848, loss: 0.06272868812084198, data time: 0.01490510028341542\n",
      "step: 695849, loss: 0.058853745460510254, data time: 0.014369159936904907\n",
      "step: 695850, loss: 0.06397993862628937, data time: 0.013875980377197266\n",
      "step: 695851, loss: 0.06844161450862885, data time: 0.013419022926917443\n",
      "step: 695852, loss: 0.05928618833422661, data time: 0.012993680106268989\n",
      "step: 695853, loss: 0.06392132490873337, data time: 0.01260214192526681\n",
      "step: 695854, loss: 0.06548795104026794, data time: 0.012242530954295191\n",
      "step: 695855, loss: 0.05891963467001915, data time: 0.011913426717122396\n",
      "step: 695856, loss: 0.058488987386226654, data time: 0.011595018448368195\n",
      "step: 695857, loss: 0.06025637313723564, data time: 0.011305011808872223\n",
      "step: 695858, loss: 0.05740898847579956, data time: 0.011020306384924686\n",
      "step: 695859, loss: 0.06935569643974304, data time: 0.010751892538631664\n",
      "step: 695860, loss: 0.062390249222517014, data time: 0.01050001553126744\n",
      "step: 695861, loss: 0.0529438853263855, data time: 0.01025900575849745\n",
      "step: 695862, loss: 0.05814989656209946, data time: 0.01003208031525483\n",
      "step: 695863, loss: 0.05964898318052292, data time: 0.009824351260536596\n",
      "step: 695864, loss: 0.0625838190317154, data time: 0.009626003412099985\n",
      "step: 695865, loss: 0.037286337465047836, data time: 0.009433329105377197\n",
      "step: 695866, loss: 0.06326048821210861, data time: 0.2878251075744629\n",
      "step: 695867, loss: 0.057710662484169006, data time: 0.14567863941192627\n",
      "step: 695868, loss: 0.06904235482215881, data time: 0.09763034184773763\n",
      "step: 695869, loss: 0.06175978109240532, data time: 0.07397383451461792\n",
      "step: 695870, loss: 0.06128660961985588, data time: 0.059453058242797854\n",
      "step: 695871, loss: 0.06228628382086754, data time: 0.04979542891184489\n",
      "step: 695872, loss: 0.06423726677894592, data time: 0.0428856440952846\n",
      "step: 695873, loss: 0.06514191627502441, data time: 0.037779152393341064\n",
      "step: 695874, loss: 0.05864624306559563, data time: 0.03373238775465223\n",
      "step: 695875, loss: 0.06056106090545654, data time: 0.030574941635131837\n",
      "step: 695876, loss: 0.06046700477600098, data time: 0.027987003326416016\n",
      "step: 695877, loss: 0.064231738448143, data time: 0.025835196177164715\n",
      "step: 695878, loss: 0.059721142053604126, data time: 0.02402951167179988\n",
      "step: 695879, loss: 0.06085235998034477, data time: 0.022451298577444895\n",
      "step: 695880, loss: 0.0656927227973938, data time: 0.021087169647216797\n",
      "step: 695881, loss: 0.05740795657038689, data time: 0.019893482327461243\n",
      "step: 695882, loss: 0.05915427207946777, data time: 0.01886607618892894\n",
      "step: 695883, loss: 0.0653194934129715, data time: 0.017935620413886175\n",
      "step: 695884, loss: 0.06856559962034225, data time: 0.01709789978830438\n",
      "step: 695885, loss: 0.06539973616600037, data time: 0.016345036029815675\n",
      "step: 695886, loss: 0.05779653787612915, data time: 0.015671605155581518\n",
      "step: 695887, loss: 0.058474741876125336, data time: 0.015055634758689186\n",
      "step: 695888, loss: 0.06391274183988571, data time: 0.014486623846966288\n",
      "step: 695889, loss: 0.057583488523960114, data time: 0.013973206281661987\n",
      "step: 695890, loss: 0.06334292143583298, data time: 0.013501653671264649\n",
      "step: 695891, loss: 0.06369536370038986, data time: 0.013063485805804912\n",
      "step: 695892, loss: 0.05816902220249176, data time: 0.012651902657968027\n",
      "step: 695893, loss: 0.06055999919772148, data time: 0.012285138879503523\n",
      "step: 695894, loss: 0.0644940733909607, data time: 0.011946801481575802\n",
      "step: 695895, loss: 0.06026209518313408, data time: 0.0116317351659139\n",
      "step: 695896, loss: 0.057795535773038864, data time: 0.01133628814451156\n",
      "step: 695897, loss: 0.059341948479413986, data time: 0.011058598756790161\n",
      "step: 695898, loss: 0.06273449957370758, data time: 0.010785167867487127\n",
      "step: 695899, loss: 0.06348206102848053, data time: 0.010527176015517292\n",
      "step: 695900, loss: 0.05834861844778061, data time: 0.01028460775102888\n",
      "step: 695901, loss: 0.06284837424755096, data time: 0.010054250558217367\n",
      "step: 695902, loss: 0.06894369423389435, data time: 0.009837466317254144\n",
      "step: 695903, loss: 0.06222844123840332, data time: 0.009633302688598633\n",
      "step: 695904, loss: 0.062298886477947235, data time: 0.009442641184880184\n",
      "step: 695905, loss: 0.08530726283788681, data time: 0.009260344505310058\n",
      "step: 695906, loss: 0.06703125685453415, data time: 0.2936215400695801\n",
      "step: 695907, loss: 0.05712020397186279, data time: 0.14756298065185547\n",
      "step: 695908, loss: 0.05858439952135086, data time: 0.09887814521789551\n",
      "step: 695909, loss: 0.06074108928442001, data time: 0.07498884201049805\n",
      "step: 695910, loss: 0.05826738104224205, data time: 0.060262537002563475\n",
      "step: 695911, loss: 0.061269134283065796, data time: 0.05046486854553223\n",
      "step: 695912, loss: 0.0644252598285675, data time: 0.043457167489188056\n",
      "step: 695913, loss: 0.054873596876859665, data time: 0.038288414478302\n",
      "step: 695914, loss: 0.06018480658531189, data time: 0.03417812453375922\n",
      "step: 695915, loss: 0.05982249230146408, data time: 0.030959224700927733\n",
      "step: 695916, loss: 0.06734475493431091, data time: 0.028339407660744408\n",
      "step: 695917, loss: 0.06401453167200089, data time: 0.026153564453125\n",
      "step: 695918, loss: 0.05925172194838524, data time: 0.02429852118858924\n",
      "step: 695919, loss: 0.0661449134349823, data time: 0.022705657141549245\n",
      "step: 695920, loss: 0.05566929280757904, data time: 0.02132706642150879\n",
      "step: 695921, loss: 0.06180087849497795, data time: 0.020125314593315125\n",
      "step: 695922, loss: 0.059011735022068024, data time: 0.01907206984127269\n",
      "step: 695923, loss: 0.0599232017993927, data time: 0.018120514021979436\n",
      "step: 695924, loss: 0.06164216995239258, data time: 0.01727234689812911\n",
      "step: 695925, loss: 0.06073461472988129, data time: 0.01651463508605957\n",
      "step: 695926, loss: 0.06469354033470154, data time: 0.01583362760997954\n",
      "step: 695927, loss: 0.06460221856832504, data time: 0.015208883719010786\n",
      "step: 695928, loss: 0.0666639506816864, data time: 0.014633354933365532\n",
      "step: 695929, loss: 0.05870688706636429, data time: 0.014107157786687216\n",
      "step: 695930, loss: 0.056177735328674316, data time: 0.013623485565185547\n",
      "step: 695931, loss: 0.057792779058218, data time: 0.013176817160386305\n",
      "step: 695932, loss: 0.058570824563503265, data time: 0.012760321299235025\n",
      "step: 695933, loss: 0.06804877519607544, data time: 0.012375857148851668\n",
      "step: 695934, loss: 0.06482359766960144, data time: 0.012026474393647292\n",
      "step: 695935, loss: 0.05704518407583237, data time: 0.01169590950012207\n",
      "step: 695936, loss: 0.06070564687252045, data time: 0.011387348175048828\n",
      "step: 695937, loss: 0.05973440408706665, data time: 0.011100739240646362\n",
      "step: 695938, loss: 0.059023819863796234, data time: 0.010823098095980558\n",
      "step: 695939, loss: 0.05617392063140869, data time: 0.01056342966416303\n",
      "step: 695940, loss: 0.062846839427948, data time: 0.010316351481846401\n",
      "step: 695941, loss: 0.06773775815963745, data time: 0.010080840852525499\n",
      "step: 695942, loss: 0.060730110853910446, data time: 0.009858518033414273\n",
      "step: 695943, loss: 0.06925696134567261, data time: 0.009651867966902884\n",
      "step: 695944, loss: 0.06801742315292358, data time: 0.009456035418388171\n",
      "step: 695945, loss: 0.05254592001438141, data time: 0.009268635511398315\n",
      "step: 695946, loss: 0.07126935571432114, data time: 0.30197596549987793\n",
      "step: 695947, loss: 0.06208118423819542, data time: 0.15187561511993408\n",
      "step: 695948, loss: 0.0628485232591629, data time: 0.10230644543965657\n",
      "step: 695949, loss: 0.06308739632368088, data time: 0.0776442289352417\n",
      "step: 695950, loss: 0.06126704439520836, data time: 0.0624427318572998\n",
      "step: 695951, loss: 0.05781574174761772, data time: 0.05231316884358724\n",
      "step: 695952, loss: 0.05824815854430199, data time: 0.04507197652544294\n",
      "step: 695953, loss: 0.05724737048149109, data time: 0.039733827114105225\n",
      "step: 695954, loss: 0.05947142466902733, data time: 0.035492287741767034\n",
      "step: 695955, loss: 0.0563000850379467, data time: 0.03217556476593018\n",
      "step: 695956, loss: 0.0656871423125267, data time: 0.029478506608442825\n",
      "step: 695957, loss: 0.06246331334114075, data time: 0.027229130268096924\n",
      "step: 695958, loss: 0.0625104084610939, data time: 0.02532337262080266\n",
      "step: 695959, loss: 0.057394858449697495, data time: 0.023681998252868652\n",
      "step: 695960, loss: 0.050994496792554855, data time: 0.0222591241200765\n",
      "step: 695961, loss: 0.05993519723415375, data time: 0.021014511585235596\n",
      "step: 695962, loss: 0.062092140316963196, data time: 0.019919760086957145\n",
      "step: 695963, loss: 0.0573405995965004, data time: 0.01893783940209283\n",
      "step: 695964, loss: 0.05463920161128044, data time: 0.018065013383564196\n",
      "step: 695965, loss: 0.06293778121471405, data time: 0.017286217212677\n",
      "step: 695966, loss: 0.0650169849395752, data time: 0.016581751051403228\n",
      "step: 695967, loss: 0.0635041892528534, data time: 0.015941045501015404\n",
      "step: 695968, loss: 0.06296534836292267, data time: 0.015349522880885912\n",
      "step: 695969, loss: 0.05979043245315552, data time: 0.014811038970947266\n",
      "step: 695970, loss: 0.06563454866409302, data time: 0.014316396713256836\n",
      "step: 695971, loss: 0.06434078514575958, data time: 0.01385639264033391\n",
      "step: 695972, loss: 0.0630231648683548, data time: 0.013430092069837783\n",
      "step: 695973, loss: 0.058148741722106934, data time: 0.013034505503518241\n",
      "step: 695974, loss: 0.06496337056159973, data time: 0.01268436990935227\n",
      "step: 695975, loss: 0.05851124972105026, data time: 0.012346140543619792\n",
      "step: 695976, loss: 0.06789983808994293, data time: 0.012028009660782353\n",
      "step: 695977, loss: 0.05815684795379639, data time: 0.011732146143913269\n",
      "step: 695978, loss: 0.05885985493659973, data time: 0.01143886103774562\n",
      "step: 695979, loss: 0.05915652960538864, data time: 0.011162813972024357\n",
      "step: 695980, loss: 0.06588323414325714, data time: 0.010904346193586077\n",
      "step: 695981, loss: 0.059134673327207565, data time: 0.010655966069963243\n",
      "step: 695982, loss: 0.0620720200240612, data time: 0.010421437186163824\n",
      "step: 695983, loss: 0.06147381663322449, data time: 0.01020154827519467\n",
      "step: 695984, loss: 0.062036268413066864, data time: 0.009995099825736804\n",
      "step: 695985, loss: 0.06555811315774918, data time: 0.009797787666320801\n",
      "step: 695986, loss: 0.06401392817497253, data time: 0.29625821113586426\n",
      "step: 695987, loss: 0.06265473365783691, data time: 0.14992475509643555\n",
      "step: 695988, loss: 0.06441082060337067, data time: 0.10048143068949382\n",
      "step: 695989, loss: 0.058674003928899765, data time: 0.07612031698226929\n",
      "step: 695990, loss: 0.05617784708738327, data time: 0.061186695098876955\n",
      "step: 695991, loss: 0.05999859422445297, data time: 0.05121660232543945\n",
      "step: 695992, loss: 0.06847250461578369, data time: 0.044094630650111606\n",
      "step: 695993, loss: 0.06835628300905228, data time: 0.03883236646652222\n",
      "step: 695994, loss: 0.0702882707118988, data time: 0.03466876347859701\n",
      "step: 695995, loss: 0.06696256250143051, data time: 0.031396222114562986\n",
      "step: 695996, loss: 0.06523258984088898, data time: 0.028736114501953125\n",
      "step: 695997, loss: 0.062072545289993286, data time: 0.02651711304982503\n",
      "step: 695998, loss: 0.06049395725131035, data time: 0.024643439512986403\n",
      "step: 695999, loss: 0.06137343496084213, data time: 0.023031115531921387\n",
      "step: 696000, loss: 0.06424935907125473, data time: 0.02163702646891276\n",
      "step: 696001, loss: 0.05783805251121521, data time: 0.020410627126693726\n",
      "step: 696002, loss: 0.0603138729929924, data time: 0.019333811367259306\n",
      "step: 696003, loss: 0.061801355332136154, data time: 0.01836754216088189\n",
      "step: 696004, loss: 0.06073147803544998, data time: 0.01750529439825761\n",
      "step: 696005, loss: 0.06285761296749115, data time: 0.01673003435134888\n",
      "step: 696006, loss: 0.06002053618431091, data time: 0.016035761151994978\n",
      "step: 696007, loss: 0.05635450780391693, data time: 0.015404560349204323\n",
      "step: 696008, loss: 0.06563085317611694, data time: 0.014826422152311905\n",
      "step: 696009, loss: 0.06390485167503357, data time: 0.01429357131322225\n",
      "step: 696010, loss: 0.061237942427396774, data time: 0.013803253173828125\n",
      "step: 696011, loss: 0.06050007790327072, data time: 0.013349368022038387\n",
      "step: 696012, loss: 0.060466669499874115, data time: 0.012926242969654224\n",
      "step: 696013, loss: 0.056822746992111206, data time: 0.012534218175070626\n",
      "step: 696014, loss: 0.06358478218317032, data time: 0.012178996513629782\n",
      "step: 696015, loss: 0.06252535432577133, data time: 0.011845215161641439\n",
      "step: 696016, loss: 0.0665513426065445, data time: 0.011534675475089781\n",
      "step: 696017, loss: 0.057358380407094955, data time: 0.01124560832977295\n",
      "step: 696018, loss: 0.06615177541971207, data time: 0.010962486267089844\n",
      "step: 696019, loss: 0.05675872415304184, data time: 0.010699089835671818\n",
      "step: 696020, loss: 0.06112734228372574, data time: 0.010450996671404157\n",
      "step: 696021, loss: 0.07237695157527924, data time: 0.010216646724277072\n",
      "step: 696022, loss: 0.062167681753635406, data time: 0.009994674373317409\n",
      "step: 696023, loss: 0.0651296004652977, data time: 0.00978687562440571\n",
      "step: 696024, loss: 0.05997416377067566, data time: 0.00959172004308456\n",
      "step: 696025, loss: 0.07423660159111023, data time: 0.00940532684326172\n",
      "step: 696026, loss: 0.06350181996822357, data time: 0.3003511428833008\n",
      "step: 696027, loss: 0.056895431131124496, data time: 0.15092456340789795\n",
      "step: 696028, loss: 0.06382803618907928, data time: 0.10114455223083496\n",
      "step: 696029, loss: 0.060759287327528, data time: 0.07689952850341797\n",
      "step: 696030, loss: 0.06318511068820953, data time: 0.0618492603302002\n",
      "step: 696031, loss: 0.06197425723075867, data time: 0.05180990695953369\n",
      "step: 696032, loss: 0.06701652705669403, data time: 0.0446441514151437\n",
      "step: 696033, loss: 0.05671662092208862, data time: 0.03936782479286194\n",
      "step: 696034, loss: 0.06022416800260544, data time: 0.03517821100023058\n",
      "step: 696035, loss: 0.05668085068464279, data time: 0.03190102577209473\n",
      "step: 696036, loss: 0.06400296092033386, data time: 0.02922810207713734\n",
      "step: 696037, loss: 0.05840558931231499, data time: 0.02700916926066081\n",
      "step: 696038, loss: 0.06585374474525452, data time: 0.025121120306161735\n",
      "step: 696039, loss: 0.060802824795246124, data time: 0.023494277681623186\n",
      "step: 696040, loss: 0.060730576515197754, data time: 0.022085205713907877\n",
      "step: 696041, loss: 0.05537879094481468, data time: 0.02085314691066742\n",
      "step: 696042, loss: 0.05877043306827545, data time: 0.01976651303908404\n",
      "step: 696043, loss: 0.06449086964130402, data time: 0.018797384368048772\n",
      "step: 696044, loss: 0.0608709454536438, data time: 0.017932891845703125\n",
      "step: 696045, loss: 0.05919152498245239, data time: 0.017158973217010497\n",
      "step: 696046, loss: 0.05725696682929993, data time: 0.01646264394124349\n",
      "step: 696047, loss: 0.059344034641981125, data time: 0.01582645286213268\n",
      "step: 696048, loss: 0.060763582587242126, data time: 0.015244546143904976\n",
      "step: 696049, loss: 0.06450507044792175, data time: 0.01471206545829773\n",
      "step: 696050, loss: 0.06895250082015991, data time: 0.014218740463256836\n",
      "step: 696051, loss: 0.06017143651843071, data time: 0.013764298879183255\n",
      "step: 696052, loss: 0.061613909900188446, data time: 0.013339466518825956\n",
      "step: 696053, loss: 0.05794781446456909, data time: 0.012946818556104387\n",
      "step: 696054, loss: 0.05716779828071594, data time: 0.012586174340083682\n",
      "step: 696055, loss: 0.054981641471385956, data time: 0.012250868479410808\n",
      "step: 696056, loss: 0.06144148111343384, data time: 0.011936718417752174\n",
      "step: 696057, loss: 0.06160242483019829, data time: 0.011646054685115814\n",
      "step: 696058, loss: 0.06179986149072647, data time: 0.011354309139829693\n",
      "step: 696059, loss: 0.06304843723773956, data time: 0.011079893392675063\n",
      "step: 696060, loss: 0.06225524842739105, data time: 0.010819414683750698\n",
      "step: 696061, loss: 0.06080388277769089, data time: 0.010572923554314507\n",
      "step: 696062, loss: 0.0639447420835495, data time: 0.010340709944029112\n",
      "step: 696063, loss: 0.06092771515250206, data time: 0.010123717157464279\n",
      "step: 696064, loss: 0.0677291601896286, data time: 0.0099182373438126\n",
      "step: 696065, loss: 0.06653936952352524, data time: 0.009723377227783204\n",
      "step: 696066, loss: 0.05938800424337387, data time: 0.29280948638916016\n",
      "step: 696067, loss: 0.06045013293623924, data time: 0.14778566360473633\n",
      "step: 696068, loss: 0.0654265508055687, data time: 0.0990438461303711\n",
      "step: 696069, loss: 0.06504836678504944, data time: 0.07509487867355347\n",
      "step: 696070, loss: 0.06636258214712143, data time: 0.06037845611572266\n",
      "step: 696071, loss: 0.06053455173969269, data time: 0.0505685011545817\n",
      "step: 696072, loss: 0.06196530908346176, data time: 0.04353843416486468\n",
      "step: 696073, loss: 0.0613597109913826, data time: 0.03836604952812195\n",
      "step: 696074, loss: 0.060751378536224365, data time: 0.03425206078423394\n",
      "step: 696075, loss: 0.06578002870082855, data time: 0.031028008460998534\n",
      "step: 696076, loss: 0.06334152817726135, data time: 0.028402371840043503\n",
      "step: 696077, loss: 0.0670173317193985, data time: 0.026211321353912354\n",
      "step: 696078, loss: 0.062323957681655884, data time: 0.02438631424537072\n",
      "step: 696079, loss: 0.059382908046245575, data time: 0.022783858435494558\n",
      "step: 696080, loss: 0.05818407982587814, data time: 0.021400610605875652\n",
      "step: 696081, loss: 0.061281632632017136, data time: 0.020191147923469543\n",
      "step: 696082, loss: 0.05975852534174919, data time: 0.019122951170977426\n",
      "step: 696083, loss: 0.06070989370346069, data time: 0.018165932761298284\n",
      "step: 696084, loss: 0.06125256046652794, data time: 0.017315638692755448\n",
      "step: 696085, loss: 0.06687963753938675, data time: 0.016562128067016603\n",
      "step: 696086, loss: 0.06183413416147232, data time: 0.015874783198038738\n",
      "step: 696087, loss: 0.06058778613805771, data time: 0.015250075947154652\n",
      "step: 696088, loss: 0.06225677579641342, data time: 0.014672590338665506\n",
      "step: 696089, loss: 0.06016914173960686, data time: 0.014144539833068848\n",
      "step: 696090, loss: 0.06381396949291229, data time: 0.01365859031677246\n",
      "step: 696091, loss: 0.06336387246847153, data time: 0.01321116777566763\n",
      "step: 696092, loss: 0.06118343025445938, data time: 0.012792640262179904\n",
      "step: 696093, loss: 0.0648173987865448, data time: 0.012407234736851283\n",
      "step: 696094, loss: 0.06686755269765854, data time: 0.012053358143773573\n",
      "step: 696095, loss: 0.06380238384008408, data time: 0.01172311305999756\n",
      "step: 696096, loss: 0.06528544425964355, data time: 0.011414812457176947\n",
      "step: 696097, loss: 0.06356234848499298, data time: 0.011128813028335571\n",
      "step: 696098, loss: 0.0622854046523571, data time: 0.01084800200028853\n",
      "step: 696099, loss: 0.06603832542896271, data time: 0.010584158055922565\n",
      "step: 696100, loss: 0.05906181037425995, data time: 0.010336208343505859\n",
      "step: 696101, loss: 0.06153620406985283, data time: 0.010108715958065458\n",
      "step: 696102, loss: 0.061604417860507965, data time: 0.009885878176302524\n",
      "step: 696103, loss: 0.05828474462032318, data time: 0.00967897239484285\n",
      "step: 696104, loss: 0.06402099877595901, data time: 0.00948349023476625\n",
      "step: 696105, loss: 0.07021640241146088, data time: 0.00929727554321289\n",
      "step: 696106, loss: 0.06565345823764801, data time: 0.293060302734375\n",
      "step: 696107, loss: 0.06259318441152573, data time: 0.1483839750289917\n",
      "step: 696108, loss: 0.0594022274017334, data time: 0.09942626953125\n",
      "step: 696109, loss: 0.06183233857154846, data time: 0.0753365159034729\n",
      "step: 696110, loss: 0.067733034491539, data time: 0.060551691055297854\n",
      "step: 696111, loss: 0.06385186314582825, data time: 0.05069446563720703\n",
      "step: 696112, loss: 0.05942780524492264, data time: 0.04364923068455288\n",
      "step: 696113, loss: 0.06680013984441757, data time: 0.03846421837806702\n",
      "step: 696114, loss: 0.06143251061439514, data time: 0.03435254096984863\n",
      "step: 696115, loss: 0.06035923585295677, data time: 0.031115174293518066\n",
      "step: 696116, loss: 0.05981370061635971, data time: 0.02848740057511763\n",
      "step: 696117, loss: 0.062460750341415405, data time: 0.026291529337565105\n",
      "step: 696118, loss: 0.05718138813972473, data time: 0.024434328079223633\n",
      "step: 696119, loss: 0.06001610681414604, data time: 0.02283353464944022\n",
      "step: 696120, loss: 0.06630054861307144, data time: 0.021448882420857747\n",
      "step: 696121, loss: 0.05776287242770195, data time: 0.020232990384101868\n",
      "step: 696122, loss: 0.05623771995306015, data time: 0.019161827423993277\n",
      "step: 696123, loss: 0.06290635466575623, data time: 0.018209470642937556\n",
      "step: 696124, loss: 0.06914444267749786, data time: 0.017357023138748973\n",
      "step: 696125, loss: 0.05982647091150284, data time: 0.01659555435180664\n",
      "step: 696126, loss: 0.059310492128133774, data time: 0.015907560076032366\n",
      "step: 696127, loss: 0.0610012523829937, data time: 0.015281417153098366\n",
      "step: 696128, loss: 0.05964154005050659, data time: 0.014705346978229025\n",
      "step: 696129, loss: 0.06471162289381027, data time: 0.014177153507868448\n",
      "step: 696130, loss: 0.06390377879142761, data time: 0.013689174652099609\n",
      "step: 696131, loss: 0.0635165274143219, data time: 0.013242584008436937\n",
      "step: 696132, loss: 0.06480638682842255, data time: 0.012825515535142686\n",
      "step: 696133, loss: 0.05802693963050842, data time: 0.012440545218331473\n",
      "step: 696134, loss: 0.0623801052570343, data time: 0.012084903388187802\n",
      "step: 696135, loss: 0.0604342445731163, data time: 0.011751969655354818\n",
      "step: 696136, loss: 0.06149626895785332, data time: 0.01144268435816611\n",
      "step: 696137, loss: 0.0580337792634964, data time: 0.011159472167491913\n",
      "step: 696138, loss: 0.06330259889364243, data time: 0.010880903764204546\n",
      "step: 696139, loss: 0.06667999923229218, data time: 0.01061701073366053\n",
      "step: 696140, loss: 0.06367334723472595, data time: 0.010367604664393834\n",
      "step: 696141, loss: 0.06518890708684921, data time: 0.010130670335557725\n",
      "step: 696142, loss: 0.053789108991622925, data time: 0.009907799798089105\n",
      "step: 696143, loss: 0.06156902015209198, data time: 0.009699978326496324\n",
      "step: 696144, loss: 0.0570104718208313, data time: 0.009502612627469577\n",
      "step: 696145, loss: 0.06456486135721207, data time: 0.009320580959320068\n",
      "step: 696146, loss: 0.05817781388759613, data time: 0.30167412757873535\n",
      "step: 696147, loss: 0.062624491751194, data time: 0.15158140659332275\n",
      "step: 696148, loss: 0.06461040675640106, data time: 0.10160557428995769\n",
      "step: 696149, loss: 0.06118655204772949, data time: 0.07695937156677246\n",
      "step: 696150, loss: 0.06454813480377197, data time: 0.06184682846069336\n",
      "step: 696151, loss: 0.0651557594537735, data time: 0.05177032947540283\n",
      "step: 696152, loss: 0.05091261863708496, data time: 0.04457449913024902\n",
      "step: 696153, loss: 0.06272133439779282, data time: 0.03927963972091675\n",
      "step: 696154, loss: 0.06452547013759613, data time: 0.03506059116787381\n",
      "step: 696155, loss: 0.058724112808704376, data time: 0.031754040718078615\n",
      "step: 696156, loss: 0.058246295899152756, data time: 0.029062336141412907\n",
      "step: 696157, loss: 0.06864039599895477, data time: 0.02681589126586914\n",
      "step: 696158, loss: 0.06697432696819305, data time: 0.024916373766385592\n",
      "step: 696159, loss: 0.06758195161819458, data time: 0.023287091936383928\n",
      "step: 696160, loss: 0.062225889414548874, data time: 0.02187485694885254\n",
      "step: 696161, loss: 0.05478796362876892, data time: 0.020634949207305908\n",
      "step: 696162, loss: 0.062382787466049194, data time: 0.019539636724135456\n",
      "step: 696163, loss: 0.06086699664592743, data time: 0.018561151292588975\n",
      "step: 696164, loss: 0.060025982558727264, data time: 0.01769299256174188\n",
      "step: 696165, loss: 0.06630472838878632, data time: 0.01691986322402954\n",
      "step: 696166, loss: 0.055041711777448654, data time: 0.016216505141485305\n",
      "step: 696167, loss: 0.06699031591415405, data time: 0.01557569070295854\n",
      "step: 696168, loss: 0.058150820434093475, data time: 0.014988805936730427\n",
      "step: 696169, loss: 0.06712283939123154, data time: 0.014452079931894938\n",
      "step: 696170, loss: 0.06464634090662003, data time: 0.013955307006835938\n",
      "step: 696171, loss: 0.06534218788146973, data time: 0.013495206832885742\n",
      "step: 696172, loss: 0.06075916439294815, data time: 0.013072305255466037\n",
      "step: 696173, loss: 0.061065107583999634, data time: 0.012677507741110665\n",
      "step: 696174, loss: 0.06571079790592194, data time: 0.012314508701192921\n",
      "step: 696175, loss: 0.05872642993927002, data time: 0.011976496378580729\n",
      "step: 696176, loss: 0.06646101176738739, data time: 0.011659514519476121\n",
      "step: 696177, loss: 0.059986770153045654, data time: 0.011365510523319244\n",
      "step: 696178, loss: 0.06509159505367279, data time: 0.01107806870431611\n",
      "step: 696179, loss: 0.057972442358732224, data time: 0.010808327618767233\n",
      "step: 696180, loss: 0.056883811950683594, data time: 0.010554061617170062\n",
      "step: 696181, loss: 0.059219710528850555, data time: 0.010313133398691813\n",
      "step: 696182, loss: 0.05642568692564964, data time: 0.01008600157660407\n",
      "step: 696183, loss: 0.05799392983317375, data time: 0.00987259965193899\n",
      "step: 696184, loss: 0.06453194469213486, data time: 0.009671553587302184\n",
      "step: 696185, loss: 0.05002902075648308, data time: 0.009483259916305543\n",
      "step: 696186, loss: 0.0619027316570282, data time: 0.2918429374694824\n",
      "step: 696187, loss: 0.06425688415765762, data time: 0.1472611427307129\n",
      "step: 696188, loss: 0.05580292269587517, data time: 0.09906991322835286\n",
      "step: 696189, loss: 0.053322434425354004, data time: 0.07505780458450317\n",
      "step: 696190, loss: 0.059940457344055176, data time: 0.06032247543334961\n",
      "step: 696191, loss: 0.061066240072250366, data time: 0.050493200620015465\n",
      "step: 696192, loss: 0.06912344694137573, data time: 0.04349964005606515\n",
      "step: 696193, loss: 0.06743171811103821, data time: 0.038309454917907715\n",
      "step: 696194, loss: 0.06312911212444305, data time: 0.034201463063557945\n",
      "step: 696195, loss: 0.06430225074291229, data time: 0.030977606773376465\n",
      "step: 696196, loss: 0.06064628064632416, data time: 0.028351523659446022\n",
      "step: 696197, loss: 0.06629664450883865, data time: 0.026172518730163574\n",
      "step: 696198, loss: 0.06451103091239929, data time: 0.02433006580059345\n",
      "step: 696199, loss: 0.05601337552070618, data time: 0.022734880447387695\n",
      "step: 696200, loss: 0.06729483604431152, data time: 0.021352624893188475\n",
      "step: 696201, loss: 0.06018940359354019, data time: 0.020142093300819397\n",
      "step: 696202, loss: 0.05949937552213669, data time: 0.0190737107220818\n",
      "step: 696203, loss: 0.06271201372146606, data time: 0.018119917975531682\n",
      "step: 696204, loss: 0.058407243341207504, data time: 0.017272409639860455\n",
      "step: 696205, loss: 0.06484420597553253, data time: 0.016515278816223146\n",
      "step: 696206, loss: 0.06461642682552338, data time: 0.01583243551708403\n",
      "step: 696207, loss: 0.058308497071266174, data time: 0.015210628509521484\n",
      "step: 696208, loss: 0.06323325634002686, data time: 0.014634588490361753\n",
      "step: 696209, loss: 0.060043662786483765, data time: 0.014110823472340902\n",
      "step: 696210, loss: 0.06320364773273468, data time: 0.013626384735107421\n",
      "step: 696211, loss: 0.0600351020693779, data time: 0.013179458104647122\n",
      "step: 696212, loss: 0.06248163431882858, data time: 0.012762166835643627\n",
      "step: 696213, loss: 0.065737783908844, data time: 0.012379322733197893\n",
      "step: 696214, loss: 0.06366290152072906, data time: 0.012025084988824252\n",
      "step: 696215, loss: 0.06532403826713562, data time: 0.011697657903035482\n",
      "step: 696216, loss: 0.058266058564186096, data time: 0.011388824832054877\n",
      "step: 696217, loss: 0.057985059916973114, data time: 0.011105135083198547\n",
      "step: 696218, loss: 0.06300289928913116, data time: 0.01082581462282123\n",
      "step: 696219, loss: 0.06246675178408623, data time: 0.01056652910569135\n",
      "step: 696220, loss: 0.060756172984838486, data time: 0.010323544910975865\n",
      "step: 696221, loss: 0.061086103320121765, data time: 0.010090483559502495\n",
      "step: 696222, loss: 0.0588008388876915, data time: 0.009872307648529878\n",
      "step: 696223, loss: 0.06574534624814987, data time: 0.009669648973565353\n",
      "step: 696224, loss: 0.06418310850858688, data time: 0.009477896568102714\n",
      "step: 696225, loss: 0.04954567179083824, data time: 0.009295809268951415\n",
      "step: 696226, loss: 0.058936476707458496, data time: 0.31219053268432617\n",
      "step: 696227, loss: 0.060735072940588, data time: 0.15685880184173584\n",
      "step: 696228, loss: 0.06720122694969177, data time: 0.10511501630147298\n",
      "step: 696229, loss: 0.06699865311384201, data time: 0.07963728904724121\n",
      "step: 696230, loss: 0.07050371170043945, data time: 0.0639955997467041\n",
      "step: 696231, loss: 0.05723174661397934, data time: 0.05356756846110026\n",
      "step: 696232, loss: 0.060606710612773895, data time: 0.04611400195530483\n",
      "step: 696233, loss: 0.061616551131010056, data time: 0.04060894250869751\n",
      "step: 696234, loss: 0.0616065077483654, data time: 0.03624388906690809\n",
      "step: 696235, loss: 0.05273633822798729, data time: 0.03282365798950195\n",
      "step: 696236, loss: 0.06289568543434143, data time: 0.030037099664861507\n",
      "step: 696237, loss: 0.060460176318883896, data time: 0.027713656425476074\n",
      "step: 696238, loss: 0.05888381227850914, data time: 0.025746932396521933\n",
      "step: 696239, loss: 0.05597414821386337, data time: 0.024053198950631276\n",
      "step: 696240, loss: 0.06052481383085251, data time: 0.02258297602335612\n",
      "step: 696241, loss: 0.05945074185729027, data time: 0.021296516060829163\n",
      "step: 696242, loss: 0.056296687573194504, data time: 0.02016426535213695\n",
      "step: 696243, loss: 0.0562630295753479, data time: 0.019158773952060275\n",
      "step: 696244, loss: 0.05946197733283043, data time: 0.018258973171836453\n",
      "step: 696245, loss: 0.06830336153507233, data time: 0.017452406883239745\n",
      "step: 696246, loss: 0.06290489435195923, data time: 0.01672263372512091\n",
      "step: 696247, loss: 0.05844917893409729, data time: 0.01606256311590021\n",
      "step: 696248, loss: 0.06922591477632523, data time: 0.015456635019053583\n",
      "step: 696249, loss: 0.06032126396894455, data time: 0.0149020254611969\n",
      "step: 696250, loss: 0.06791561096906662, data time: 0.014392805099487305\n",
      "step: 696251, loss: 0.06805232167243958, data time: 0.01391587807581975\n",
      "step: 696252, loss: 0.05840872973203659, data time: 0.01347441143459744\n",
      "step: 696253, loss: 0.058225058019161224, data time: 0.013062272753034319\n",
      "step: 696254, loss: 0.0590367317199707, data time: 0.012685578444908405\n",
      "step: 696255, loss: 0.06152058765292168, data time: 0.01233502229054769\n",
      "step: 696256, loss: 0.06501808762550354, data time: 0.012008451646374117\n",
      "step: 696257, loss: 0.06365714967250824, data time: 0.01170707494020462\n",
      "step: 696258, loss: 0.0662069246172905, data time: 0.011409788420706085\n",
      "step: 696259, loss: 0.064407579600811, data time: 0.011129049693836886\n",
      "step: 696260, loss: 0.06227701157331467, data time: 0.010864700589861189\n",
      "step: 696261, loss: 0.060675498098134995, data time: 0.010614024268256294\n",
      "step: 696262, loss: 0.05902679264545441, data time: 0.010378167435929581\n",
      "step: 696263, loss: 0.06552816927433014, data time: 0.010158908994574296\n",
      "step: 696264, loss: 0.059020500630140305, data time: 0.009950203773302909\n",
      "step: 696265, loss: 0.072083979845047, data time: 0.009754347801208495\n",
      "step: 696266, loss: 0.0641787201166153, data time: 0.2929108142852783\n",
      "step: 696267, loss: 0.06482981890439987, data time: 0.14787018299102783\n",
      "step: 696268, loss: 0.06493882834911346, data time: 0.09963583946228027\n",
      "step: 696269, loss: 0.06365885585546494, data time: 0.07564651966094971\n",
      "step: 696270, loss: 0.06181105598807335, data time: 0.06084952354431152\n",
      "step: 696271, loss: 0.06157851964235306, data time: 0.0510024627049764\n",
      "step: 696272, loss: 0.05671944469213486, data time: 0.04396288735525949\n",
      "step: 696273, loss: 0.06097544729709625, data time: 0.038773566484451294\n",
      "step: 696274, loss: 0.06726415455341339, data time: 0.03465053770277235\n",
      "step: 696275, loss: 0.0626981109380722, data time: 0.03142387866973877\n",
      "step: 696276, loss: 0.059071362018585205, data time: 0.028797994960438122\n",
      "step: 696277, loss: 0.06280844658613205, data time: 0.026610950628916424\n",
      "step: 696278, loss: 0.06293710321187973, data time: 0.02475318541893592\n",
      "step: 696279, loss: 0.062422048300504684, data time: 0.023158890860421315\n",
      "step: 696280, loss: 0.05976305529475212, data time: 0.021771621704101563\n",
      "step: 696281, loss: 0.06093108281493187, data time: 0.020557940006256104\n",
      "step: 696282, loss: 0.06651240587234497, data time: 0.019490802989286536\n",
      "step: 696283, loss: 0.05884372442960739, data time: 0.018536925315856934\n",
      "step: 696284, loss: 0.058515679091215134, data time: 0.017684635363127057\n",
      "step: 696285, loss: 0.06733014434576035, data time: 0.016924464702606203\n",
      "step: 696286, loss: 0.06084130331873894, data time: 0.016237497329711914\n",
      "step: 696287, loss: 0.05867531895637512, data time: 0.015612038699063387\n",
      "step: 696288, loss: 0.06148076057434082, data time: 0.015039029328719429\n",
      "step: 696289, loss: 0.06390313804149628, data time: 0.014515539010365805\n",
      "step: 696290, loss: 0.06502886116504669, data time: 0.014033746719360352\n",
      "step: 696291, loss: 0.05894673615694046, data time: 0.013584531270540677\n",
      "step: 696292, loss: 0.061145514249801636, data time: 0.01316929746557165\n",
      "step: 696293, loss: 0.07057929784059525, data time: 0.012786260672977992\n",
      "step: 696294, loss: 0.06244537979364395, data time: 0.012431350247613314\n",
      "step: 696295, loss: 0.06488275527954102, data time: 0.012100489934285481\n",
      "step: 696296, loss: 0.059057384729385376, data time: 0.011792344431723318\n",
      "step: 696297, loss: 0.0631307065486908, data time: 0.011505164206027985\n",
      "step: 696298, loss: 0.0630519688129425, data time: 0.01121876456520774\n",
      "step: 696299, loss: 0.06218155473470688, data time: 0.01094979398390826\n",
      "step: 696300, loss: 0.05946784466505051, data time: 0.010695900235857283\n",
      "step: 696301, loss: 0.05959904193878174, data time: 0.010453237427605523\n",
      "step: 696302, loss: 0.06463582068681717, data time: 0.010224799852113466\n",
      "step: 696303, loss: 0.06522154062986374, data time: 0.01000914448185971\n",
      "step: 696304, loss: 0.062383778393268585, data time: 0.009803240115825947\n",
      "step: 696305, loss: 0.0427112840116024, data time: 0.009610414505004883\n",
      "step: 696306, loss: 0.06285803765058517, data time: 0.30281662940979004\n",
      "step: 696307, loss: 0.058376625180244446, data time: 0.15276598930358887\n",
      "step: 696308, loss: 0.06569162011146545, data time: 0.10276993115743001\n",
      "step: 696309, loss: 0.06492224335670471, data time: 0.0777444839477539\n",
      "step: 696310, loss: 0.06505792587995529, data time: 0.062478494644165036\n",
      "step: 696311, loss: 0.06304553896188736, data time: 0.0522987445195516\n",
      "step: 696312, loss: 0.06302642822265625, data time: 0.04512214660644531\n",
      "step: 696313, loss: 0.06731133908033371, data time: 0.0397317111492157\n",
      "step: 696314, loss: 0.062183111906051636, data time: 0.03546863132052951\n",
      "step: 696315, loss: 0.06254158914089203, data time: 0.032121014595031736\n",
      "step: 696316, loss: 0.06388095021247864, data time: 0.029396338896317917\n",
      "step: 696317, loss: 0.0669301375746727, data time: 0.027129530906677246\n",
      "step: 696318, loss: 0.06702040135860443, data time: 0.025201705785898063\n",
      "step: 696319, loss: 0.05790102109313011, data time: 0.02354616778237479\n",
      "step: 696320, loss: 0.05980750173330307, data time: 0.02211791674296061\n",
      "step: 696321, loss: 0.061833612620830536, data time: 0.02086314558982849\n",
      "step: 696322, loss: 0.061391234397888184, data time: 0.019752614638384652\n",
      "step: 696323, loss: 0.06208205968141556, data time: 0.018772244453430176\n",
      "step: 696324, loss: 0.061490148305892944, data time: 0.017894820163124485\n",
      "step: 696325, loss: 0.05830364674329758, data time: 0.01710538864135742\n",
      "step: 696326, loss: 0.06093444675207138, data time: 0.016393706912086123\n",
      "step: 696327, loss: 0.05755011737346649, data time: 0.01574563980102539\n",
      "step: 696328, loss: 0.06380867958068848, data time: 0.015148639678955078\n",
      "step: 696329, loss: 0.058970652520656586, data time: 0.01460482676823934\n",
      "step: 696330, loss: 0.05671749636530876, data time: 0.014118480682373046\n",
      "step: 696331, loss: 0.05555843561887741, data time: 0.013666712320767917\n",
      "step: 696332, loss: 0.06151769682765007, data time: 0.013249123537981952\n",
      "step: 696333, loss: 0.06142732501029968, data time: 0.012859591415950231\n",
      "step: 696334, loss: 0.058972835540771484, data time: 0.012503097797262257\n",
      "step: 696335, loss: 0.062203824520111084, data time: 0.012170370419820149\n",
      "step: 696336, loss: 0.05791708081960678, data time: 0.01185979381684334\n",
      "step: 696337, loss: 0.06274189800024033, data time: 0.01157042384147644\n",
      "step: 696338, loss: 0.05669565498828888, data time: 0.011280522202000473\n",
      "step: 696339, loss: 0.062108948826789856, data time: 0.011009090086993049\n",
      "step: 696340, loss: 0.061719976365566254, data time: 0.01075150626046317\n",
      "step: 696341, loss: 0.06377457082271576, data time: 0.010507643222808838\n",
      "step: 696342, loss: 0.06205526739358902, data time: 0.01027863734477275\n",
      "step: 696343, loss: 0.06304123252630234, data time: 0.010064934429369475\n",
      "step: 696344, loss: 0.0639316588640213, data time: 0.009862667474991236\n",
      "step: 696345, loss: 0.054765354841947556, data time: 0.009670090675354005\n",
      "step: 696346, loss: 0.06029055267572403, data time: 0.29090166091918945\n",
      "step: 696347, loss: 0.06597169488668442, data time: 0.14657139778137207\n",
      "step: 696348, loss: 0.06711864471435547, data time: 0.09860881169637044\n",
      "step: 696349, loss: 0.0578017383813858, data time: 0.07482540607452393\n",
      "step: 696350, loss: 0.06072785705327988, data time: 0.06014494895935059\n",
      "step: 696351, loss: 0.05883985012769699, data time: 0.050365567207336426\n",
      "step: 696352, loss: 0.060160309076309204, data time: 0.04336942945207868\n",
      "step: 696353, loss: 0.06732184439897537, data time: 0.03833451867103577\n",
      "step: 696354, loss: 0.0627969428896904, data time: 0.03426371680365668\n",
      "step: 696355, loss: 0.05523598939180374, data time: 0.03107428550720215\n",
      "step: 696356, loss: 0.06128045171499252, data time: 0.02848076820373535\n",
      "step: 696357, loss: 0.06381796300411224, data time: 0.026320815086364746\n",
      "step: 696358, loss: 0.06269659847021103, data time: 0.024489292731651895\n",
      "step: 696359, loss: 0.06799836456775665, data time: 0.022913285664149692\n",
      "step: 696360, loss: 0.06008732318878174, data time: 0.02152242660522461\n",
      "step: 696361, loss: 0.06114942952990532, data time: 0.020303308963775635\n",
      "step: 696362, loss: 0.06051131710410118, data time: 0.019231235279756433\n",
      "step: 696363, loss: 0.05981774255633354, data time: 0.018279446495903864\n",
      "step: 696364, loss: 0.061500340700149536, data time: 0.0174278083600496\n",
      "step: 696365, loss: 0.05914030969142914, data time: 0.01666524410247803\n",
      "step: 696366, loss: 0.06690157949924469, data time: 0.015972977592831568\n",
      "step: 696367, loss: 0.06572934985160828, data time: 0.015346592122858221\n",
      "step: 696368, loss: 0.06552106142044067, data time: 0.014769585236259129\n",
      "step: 696369, loss: 0.06053176522254944, data time: 0.01424702008565267\n",
      "step: 696370, loss: 0.06338311731815338, data time: 0.013758821487426758\n",
      "step: 696371, loss: 0.062412362545728683, data time: 0.013307424692007212\n",
      "step: 696372, loss: 0.06592810153961182, data time: 0.012889473526566117\n",
      "step: 696373, loss: 0.06120189279317856, data time: 0.012502167906079973\n",
      "step: 696374, loss: 0.06622334569692612, data time: 0.012149070871287379\n",
      "step: 696375, loss: 0.0571967177093029, data time: 0.011820777257283529\n",
      "step: 696376, loss: 0.06349100172519684, data time: 0.011507995666996125\n",
      "step: 696377, loss: 0.05997105687856674, data time: 0.011219486594200134\n",
      "step: 696378, loss: 0.06035302206873894, data time: 0.010940074920654297\n",
      "step: 696379, loss: 0.05719241127371788, data time: 0.010674623882069308\n",
      "step: 696380, loss: 0.059518616646528244, data time: 0.010423360552106584\n",
      "step: 696381, loss: 0.06421881914138794, data time: 0.010185341040293375\n",
      "step: 696382, loss: 0.05834421515464783, data time: 0.009962075465434306\n",
      "step: 696383, loss: 0.054253749549388885, data time: 0.009752775493421052\n",
      "step: 696384, loss: 0.06735481321811676, data time: 0.00955481406969902\n",
      "step: 696385, loss: 0.067390076816082, data time: 0.009366828203201293\n",
      "step: 696386, loss: 0.05645052343606949, data time: 0.297335147857666\n",
      "step: 696387, loss: 0.06385636329650879, data time: 0.15026140213012695\n",
      "step: 696388, loss: 0.06575262546539307, data time: 0.1008152961730957\n",
      "step: 696389, loss: 0.05549763888120651, data time: 0.07652944326400757\n",
      "step: 696390, loss: 0.06324800848960876, data time: 0.061557865142822264\n",
      "step: 696391, loss: 0.0706215351819992, data time: 0.05160105228424072\n",
      "step: 696392, loss: 0.06112256646156311, data time: 0.04446329389299665\n",
      "step: 696393, loss: 0.05630441755056381, data time: 0.03920269012451172\n",
      "step: 696394, loss: 0.06374293565750122, data time: 0.03502228524949816\n",
      "step: 696395, loss: 0.05699755996465683, data time: 0.031719970703125\n",
      "step: 696396, loss: 0.06819921731948853, data time: 0.0290359150279652\n",
      "step: 696397, loss: 0.06071990355849266, data time: 0.026790042718251545\n",
      "step: 696398, loss: 0.060133617371320724, data time: 0.024892311829787035\n",
      "step: 696399, loss: 0.06564880162477493, data time: 0.02326127461024693\n",
      "step: 696400, loss: 0.0558195635676384, data time: 0.0218475341796875\n",
      "step: 696401, loss: 0.06217507645487785, data time: 0.02060626447200775\n",
      "step: 696402, loss: 0.054528526961803436, data time: 0.019516411949606502\n",
      "step: 696403, loss: 0.06365424394607544, data time: 0.01856011814541287\n",
      "step: 696404, loss: 0.06297215819358826, data time: 0.01770867799457751\n",
      "step: 696405, loss: 0.06777705997228622, data time: 0.016946005821228027\n",
      "step: 696406, loss: 0.05770471319556236, data time: 0.01625855763753255\n",
      "step: 696407, loss: 0.0686117634177208, data time: 0.01563760367306796\n",
      "step: 696408, loss: 0.06362579017877579, data time: 0.01506178275398586\n",
      "step: 696409, loss: 0.06537701189517975, data time: 0.014533480008443197\n",
      "step: 696410, loss: 0.05938791483640671, data time: 0.014047079086303711\n",
      "step: 696411, loss: 0.056987494230270386, data time: 0.013597607612609863\n",
      "step: 696412, loss: 0.06094179302453995, data time: 0.013180803369592738\n",
      "step: 696413, loss: 0.060138147324323654, data time: 0.012794664927891322\n",
      "step: 696414, loss: 0.06259079277515411, data time: 0.012440821220134866\n",
      "step: 696415, loss: 0.06374245882034302, data time: 0.0121095339457194\n",
      "step: 696416, loss: 0.058332547545433044, data time: 0.011798574078467584\n",
      "step: 696417, loss: 0.06512043625116348, data time: 0.011512093245983124\n",
      "step: 696418, loss: 0.05850710719823837, data time: 0.011227282610806551\n",
      "step: 696419, loss: 0.06124696135520935, data time: 0.010957051725948559\n",
      "step: 696420, loss: 0.06538371741771698, data time: 0.01070173808506557\n",
      "step: 696421, loss: 0.06353561580181122, data time: 0.010458727677663168\n",
      "step: 696422, loss: 0.06137733906507492, data time: 0.010231166272550015\n",
      "step: 696423, loss: 0.05955466628074646, data time: 0.010018467903137207\n",
      "step: 696424, loss: 0.062332477420568466, data time: 0.00981602301964393\n",
      "step: 696425, loss: 0.05219188332557678, data time: 0.009624087810516357\n",
      "step: 696426, loss: 0.059241943061351776, data time: 0.3064992427825928\n",
      "step: 696427, loss: 0.070092111825943, data time: 0.15465092658996582\n",
      "step: 696428, loss: 0.06309185922145844, data time: 0.10414759318033855\n",
      "step: 696429, loss: 0.05365836247801781, data time: 0.0788004994392395\n",
      "step: 696430, loss: 0.059162430465221405, data time: 0.06331710815429688\n",
      "step: 696431, loss: 0.056449513882398605, data time: 0.05300486087799072\n",
      "step: 696432, loss: 0.06294350326061249, data time: 0.04563723291669573\n",
      "step: 696433, loss: 0.06020484119653702, data time: 0.04018044471740723\n",
      "step: 696434, loss: 0.06882557272911072, data time: 0.03586051199171278\n",
      "step: 696435, loss: 0.05593264847993851, data time: 0.032486486434936526\n",
      "step: 696436, loss: 0.058000050485134125, data time: 0.029727914116599342\n",
      "step: 696437, loss: 0.05953999608755112, data time: 0.027436554431915283\n",
      "step: 696438, loss: 0.06133049353957176, data time: 0.025502736751849834\n",
      "step: 696439, loss: 0.06206124648451805, data time: 0.023824708802359446\n",
      "step: 696440, loss: 0.061414800584316254, data time: 0.022374089558919272\n",
      "step: 696441, loss: 0.06729467958211899, data time: 0.021107390522956848\n",
      "step: 696442, loss: 0.062257491052150726, data time: 0.019986264845904184\n",
      "step: 696443, loss: 0.05307453125715256, data time: 0.01899045043521457\n",
      "step: 696444, loss: 0.06693803519010544, data time: 0.018099333110608552\n",
      "step: 696445, loss: 0.06684626638889313, data time: 0.017300069332122803\n",
      "step: 696446, loss: 0.07212802022695541, data time: 0.016578697022937593\n",
      "step: 696447, loss: 0.06376108527183533, data time: 0.01592563499103893\n",
      "step: 696448, loss: 0.06233775615692139, data time: 0.015320539474487305\n",
      "step: 696449, loss: 0.06807690858840942, data time: 0.01476698120435079\n",
      "step: 696450, loss: 0.06390328705310822, data time: 0.014260549545288086\n",
      "step: 696451, loss: 0.05404651165008545, data time: 0.013788580894470215\n",
      "step: 696452, loss: 0.06395269930362701, data time: 0.013350760495221173\n",
      "step: 696453, loss: 0.06197866052389145, data time: 0.012946844100952148\n",
      "step: 696454, loss: 0.06633909046649933, data time: 0.012576119653109846\n",
      "step: 696455, loss: 0.06271477043628693, data time: 0.012230531374613444\n",
      "step: 696456, loss: 0.06376858800649643, data time: 0.011905147183325982\n",
      "step: 696457, loss: 0.06140635535120964, data time: 0.011603198945522308\n",
      "step: 696458, loss: 0.06365326792001724, data time: 0.011310815811157227\n",
      "step: 696459, loss: 0.06258115917444229, data time: 0.011037532021017635\n",
      "step: 696460, loss: 0.06255287677049637, data time: 0.01077632222856794\n",
      "step: 696461, loss: 0.06342574954032898, data time: 0.01052811410692003\n",
      "step: 696462, loss: 0.06654083728790283, data time: 0.010295539288907437\n",
      "step: 696463, loss: 0.06407251209020615, data time: 0.010077275727924547\n",
      "step: 696464, loss: 0.060972146689891815, data time: 0.009870590307773689\n",
      "step: 696465, loss: 0.04594600200653076, data time: 0.009673970937728881\n",
      "step: 696466, loss: 0.05822104215621948, data time: 0.30581116676330566\n",
      "step: 696467, loss: 0.056047093123197556, data time: 0.15367531776428223\n",
      "step: 696468, loss: 0.06443788856267929, data time: 0.1029508113861084\n",
      "step: 696469, loss: 0.06293906271457672, data time: 0.07801586389541626\n",
      "step: 696470, loss: 0.06609272211790085, data time: 0.06269650459289551\n",
      "step: 696471, loss: 0.060536377131938934, data time: 0.05248661835988363\n",
      "step: 696472, loss: 0.054894156754016876, data time: 0.04518825667245047\n",
      "step: 696473, loss: 0.0675307884812355, data time: 0.03978922963142395\n",
      "step: 696474, loss: 0.06426729261875153, data time: 0.03551385137769911\n",
      "step: 696475, loss: 0.06410995125770569, data time: 0.03216574192047119\n",
      "step: 696476, loss: 0.059775955975055695, data time: 0.029440706426447086\n",
      "step: 696477, loss: 0.06371530145406723, data time: 0.027173598607381184\n",
      "step: 696478, loss: 0.06585092842578888, data time: 0.02524720705472506\n",
      "step: 696479, loss: 0.061343900859355927, data time: 0.023589934621538435\n",
      "step: 696480, loss: 0.060075514018535614, data time: 0.022187137603759767\n",
      "step: 696481, loss: 0.05806427821516991, data time: 0.02093130350112915\n",
      "step: 696482, loss: 0.05790281295776367, data time: 0.019822485306683707\n",
      "step: 696483, loss: 0.057057201862335205, data time: 0.018832736545138888\n",
      "step: 696484, loss: 0.05743352323770523, data time: 0.017947824377762645\n",
      "step: 696485, loss: 0.059336595237255096, data time: 0.01715569496154785\n",
      "step: 696486, loss: 0.05913115665316582, data time: 0.01643892696925572\n",
      "step: 696487, loss: 0.05721202492713928, data time: 0.015788013284856624\n",
      "step: 696488, loss: 0.06737562268972397, data time: 0.015192415403283161\n",
      "step: 696489, loss: 0.0625065416097641, data time: 0.014646818240483602\n",
      "step: 696490, loss: 0.06648917496204376, data time: 0.014147157669067384\n",
      "step: 696491, loss: 0.06180267035961151, data time: 0.01373121371636024\n",
      "step: 696492, loss: 0.06501349806785583, data time: 0.0133003658718533\n",
      "step: 696493, loss: 0.0625433549284935, data time: 0.012899833066122872\n",
      "step: 696494, loss: 0.05979682132601738, data time: 0.012530491269867996\n",
      "step: 696495, loss: 0.06037881597876549, data time: 0.012184890111287434\n",
      "step: 696496, loss: 0.0672936737537384, data time: 0.01186227798461914\n",
      "step: 696497, loss: 0.05689137428998947, data time: 0.011564232409000397\n",
      "step: 696498, loss: 0.06005638465285301, data time: 0.011271895784320253\n",
      "step: 696499, loss: 0.06714006513357162, data time: 0.010998767964980182\n",
      "step: 696500, loss: 0.06440798938274384, data time: 0.010742316927228655\n",
      "step: 696501, loss: 0.07019875943660736, data time: 0.010496980614132352\n",
      "step: 696502, loss: 0.06238503381609917, data time: 0.010265395448014542\n",
      "step: 696503, loss: 0.062482498586177826, data time: 0.010048527466623406\n",
      "step: 696504, loss: 0.06894030421972275, data time: 0.009842933752597907\n",
      "step: 696505, loss: 0.04982719570398331, data time: 0.009647560119628907\n",
      "step: 696506, loss: 0.05970356613397598, data time: 0.303342342376709\n",
      "step: 696507, loss: 0.0607755109667778, data time: 0.1524486541748047\n",
      "step: 696508, loss: 0.06594949215650558, data time: 0.1021588643391927\n",
      "step: 696509, loss: 0.06225600093603134, data time: 0.07747048139572144\n",
      "step: 696510, loss: 0.06301401555538177, data time: 0.062262678146362306\n",
      "step: 696511, loss: 0.06491650640964508, data time: 0.052129785219828285\n",
      "step: 696512, loss: 0.06355135142803192, data time: 0.04489626203264509\n",
      "step: 696513, loss: 0.060586780309677124, data time: 0.039533764123916626\n",
      "step: 696514, loss: 0.059995513409376144, data time: 0.03528557883368598\n",
      "step: 696515, loss: 0.06012408062815666, data time: 0.03197588920593262\n",
      "step: 696516, loss: 0.05780288577079773, data time: 0.029265100305730648\n",
      "step: 696517, loss: 0.056444063782691956, data time: 0.027005831400553387\n",
      "step: 696518, loss: 0.06257564574480057, data time: 0.025093683829674356\n",
      "step: 696519, loss: 0.06254549324512482, data time: 0.023445027215140208\n",
      "step: 696520, loss: 0.058549389243125916, data time: 0.022014919916788736\n",
      "step: 696521, loss: 0.06353576481342316, data time: 0.020768776535987854\n",
      "step: 696522, loss: 0.06378065049648285, data time: 0.019666671752929688\n",
      "step: 696523, loss: 0.06546445935964584, data time: 0.01868327458699544\n",
      "step: 696524, loss: 0.06476102769374847, data time: 0.017816342805561266\n",
      "step: 696525, loss: 0.06229991838335991, data time: 0.017034482955932618\n",
      "step: 696526, loss: 0.061949100345373154, data time: 0.01632645016624814\n",
      "step: 696527, loss: 0.06013328954577446, data time: 0.01568403027274392\n",
      "step: 696528, loss: 0.058808475732803345, data time: 0.015090361885402514\n",
      "step: 696529, loss: 0.060334764420986176, data time: 0.014547914266586304\n",
      "step: 696530, loss: 0.06421701610088348, data time: 0.014051599502563477\n",
      "step: 696531, loss: 0.06477116048336029, data time: 0.013587474822998047\n",
      "step: 696532, loss: 0.061771195381879807, data time: 0.013155734097516095\n",
      "step: 696533, loss: 0.0700322836637497, data time: 0.012756611619676863\n",
      "step: 696534, loss: 0.05925769731402397, data time: 0.01238831980475064\n",
      "step: 696535, loss: 0.06418640911579132, data time: 0.01204684575398763\n",
      "step: 696536, loss: 0.06451696902513504, data time: 0.011732224495180191\n",
      "step: 696537, loss: 0.05856848880648613, data time: 0.011436350643634796\n",
      "step: 696538, loss: 0.06517307460308075, data time: 0.011148886247114702\n",
      "step: 696539, loss: 0.06393739581108093, data time: 0.010879362330717198\n",
      "step: 696540, loss: 0.06384260952472687, data time: 0.010625253404889788\n",
      "step: 696541, loss: 0.056476861238479614, data time: 0.010382042990790473\n",
      "step: 696542, loss: 0.06079358607530594, data time: 0.010152707228789458\n",
      "step: 696543, loss: 0.06166457384824753, data time: 0.009939984271400854\n",
      "step: 696544, loss: 0.06412899494171143, data time: 0.009739301143548427\n",
      "step: 696545, loss: 0.04690907150506973, data time: 0.00954757332801819\n",
      "tensor([   80.0000,    63.8662,    50.4472,    39.3850,    30.3545,    23.0621,\n",
      "           17.2438,    12.6640,     9.1135,     6.4081,     4.3871,     2.9116,\n",
      "            1.8628,     1.1407,     0.6622,     0.3597,     0.1794,     0.0800,\n",
      "            0.0000], device='cuda:0')\n",
      "the sample time of 20 images takes 0.41252875328063965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 696546, loss: 0.0580788180232048, data time: 0.3124833106994629\n",
      "step: 696547, loss: 0.059462882578372955, data time: 0.1569538116455078\n",
      "step: 696548, loss: 0.06391163915395737, data time: 0.10514148076375325\n",
      "step: 696549, loss: 0.061911821365356445, data time: 0.07970798015594482\n",
      "step: 696550, loss: 0.06098051369190216, data time: 0.0640373706817627\n",
      "step: 696551, loss: 0.05530005693435669, data time: 0.05359288056691488\n",
      "step: 696552, loss: 0.05790770798921585, data time: 0.04614530290876116\n",
      "step: 696553, loss: 0.06622152030467987, data time: 0.040625154972076416\n",
      "step: 696554, loss: 0.05600729584693909, data time: 0.03625533315870497\n",
      "step: 696555, loss: 0.06318127363920212, data time: 0.03282959461212158\n",
      "step: 696556, loss: 0.05894196033477783, data time: 0.030043276873501865\n",
      "step: 696557, loss: 0.0612931065261364, data time: 0.027714133262634277\n",
      "step: 696558, loss: 0.06185663864016533, data time: 0.025747262514554538\n",
      "step: 696559, loss: 0.06454579532146454, data time: 0.024044292313711985\n",
      "step: 696560, loss: 0.060049399733543396, data time: 0.022572724024454753\n",
      "step: 696561, loss: 0.059156160801649094, data time: 0.021290212869644165\n",
      "step: 696562, loss: 0.05609240382909775, data time: 0.02015521947075339\n",
      "step: 696563, loss: 0.061571475118398666, data time: 0.019155343373616535\n",
      "step: 696564, loss: 0.06210358440876007, data time: 0.01824893449482165\n",
      "step: 696565, loss: 0.06433966010808945, data time: 0.017436885833740236\n",
      "step: 696566, loss: 0.06812188774347305, data time: 0.016706228256225586\n",
      "step: 696567, loss: 0.06660489737987518, data time: 0.016039913350885563\n",
      "step: 696568, loss: 0.06139475852251053, data time: 0.01543054373367973\n",
      "step: 696569, loss: 0.06334332376718521, data time: 0.014872143665949503\n",
      "step: 696570, loss: 0.06357653439044952, data time: 0.014359140396118164\n",
      "step: 696571, loss: 0.06575781106948853, data time: 0.013883269750154935\n",
      "step: 696572, loss: 0.0602298304438591, data time: 0.013442101301970306\n",
      "step: 696573, loss: 0.06134190410375595, data time: 0.013030273573739188\n",
      "step: 696574, loss: 0.06314089894294739, data time: 0.012650341823183257\n",
      "step: 696575, loss: 0.061690375208854675, data time: 0.012300936381022136\n",
      "step: 696576, loss: 0.06145729124546051, data time: 0.01197043541939028\n",
      "step: 696577, loss: 0.061520516872406006, data time: 0.011666178703308105\n",
      "step: 696578, loss: 0.06041121482849121, data time: 0.01136990026994185\n",
      "step: 696579, loss: 0.06357762217521667, data time: 0.011091463706072639\n",
      "step: 696580, loss: 0.05776665359735489, data time: 0.010828604016985212\n",
      "step: 696581, loss: 0.06368708610534668, data time: 0.010577963458167182\n",
      "step: 696582, loss: 0.06315921247005463, data time: 0.010341721612053949\n",
      "step: 696583, loss: 0.06020477041602135, data time: 0.010120291458932977\n",
      "step: 696584, loss: 0.058758169412612915, data time: 0.009911848948552059\n",
      "step: 696585, loss: 0.04753535985946655, data time: 0.009712541103363037\n",
      "step: 696586, loss: 0.06493213027715683, data time: 0.2975895404815674\n",
      "step: 696587, loss: 0.06245720386505127, data time: 0.15034592151641846\n",
      "step: 696588, loss: 0.05851710960268974, data time: 0.10074766476949056\n",
      "step: 696589, loss: 0.06042695790529251, data time: 0.07631301879882812\n",
      "step: 696590, loss: 0.0589737594127655, data time: 0.061318588256835935\n",
      "step: 696591, loss: 0.06003282219171524, data time: 0.05132746696472168\n",
      "step: 696592, loss: 0.05687347799539566, data time: 0.044191598892211914\n",
      "step: 696593, loss: 0.05841483920812607, data time: 0.038926899433135986\n",
      "step: 696594, loss: 0.06676696240901947, data time: 0.03474863370259603\n",
      "step: 696595, loss: 0.06877946853637695, data time: 0.03146884441375732\n",
      "step: 696596, loss: 0.057413529604673386, data time: 0.02879344333301891\n",
      "step: 696597, loss: 0.06574684381484985, data time: 0.026567618052164715\n",
      "step: 696598, loss: 0.06141999363899231, data time: 0.02469330567580003\n",
      "step: 696599, loss: 0.06353471428155899, data time: 0.023079957280840193\n",
      "step: 696600, loss: 0.05752001330256462, data time: 0.02167401313781738\n",
      "step: 696601, loss: 0.06025230512022972, data time: 0.020444020628929138\n",
      "step: 696602, loss: 0.060602616518735886, data time: 0.019364034428316003\n",
      "step: 696603, loss: 0.06387954950332642, data time: 0.018393728468153212\n",
      "step: 696604, loss: 0.06141888350248337, data time: 0.01752985151190507\n",
      "step: 696605, loss: 0.061401914805173874, data time: 0.01675896644592285\n",
      "step: 696606, loss: 0.05695214867591858, data time: 0.01606569971357073\n",
      "step: 696607, loss: 0.06408262252807617, data time: 0.015454519878734242\n",
      "step: 696608, loss: 0.061711929738521576, data time: 0.01486832162608271\n",
      "step: 696609, loss: 0.06546463072299957, data time: 0.014332751433054606\n",
      "step: 696610, loss: 0.058601342141628265, data time: 0.013846282958984374\n",
      "step: 696611, loss: 0.06699670106172562, data time: 0.013395988024198092\n",
      "step: 696612, loss: 0.06355752050876617, data time: 0.012971524839048032\n",
      "step: 696613, loss: 0.06813935190439224, data time: 0.012589940002986364\n",
      "step: 696614, loss: 0.06251734495162964, data time: 0.01222675422142292\n",
      "step: 696615, loss: 0.0654904842376709, data time: 0.01188955307006836\n",
      "step: 696616, loss: 0.06310340017080307, data time: 0.011573853031281502\n",
      "step: 696617, loss: 0.06685809046030045, data time: 0.011282563209533691\n",
      "step: 696618, loss: 0.062163081020116806, data time: 0.01099851637175589\n",
      "step: 696619, loss: 0.06309551000595093, data time: 0.010733450160307042\n",
      "step: 696620, loss: 0.058638256043195724, data time: 0.010481364386422293\n",
      "step: 696621, loss: 0.06413252651691437, data time: 0.010242058171166314\n",
      "step: 696622, loss: 0.06730609387159348, data time: 0.01001641557023332\n",
      "step: 696623, loss: 0.06020740792155266, data time: 0.009804487228393555\n",
      "step: 696624, loss: 0.058573395013809204, data time: 0.00960682599972456\n",
      "step: 696625, loss: 0.037441954016685486, data time: 0.009417831897735596\n",
      "step: 696626, loss: 0.061948806047439575, data time: 0.3043816089630127\n",
      "step: 696627, loss: 0.0630541443824768, data time: 0.15350747108459473\n",
      "step: 696628, loss: 0.061377789825201035, data time: 0.10329937934875488\n",
      "step: 696629, loss: 0.05425954610109329, data time: 0.07823258638381958\n",
      "step: 696630, loss: 0.05714283511042595, data time: 0.06284432411193848\n",
      "step: 696631, loss: 0.05222772806882858, data time: 0.05259589354197184\n",
      "step: 696632, loss: 0.06379605829715729, data time: 0.04528270448957171\n",
      "step: 696633, loss: 0.060346655547618866, data time: 0.039877235889434814\n",
      "step: 696634, loss: 0.0667165219783783, data time: 0.0355996290842692\n",
      "step: 696635, loss: 0.06046566367149353, data time: 0.032240438461303714\n",
      "step: 696636, loss: 0.05909591540694237, data time: 0.029497298327359287\n",
      "step: 696637, loss: 0.06296102702617645, data time: 0.02721989154815674\n",
      "step: 696638, loss: 0.05763405188918114, data time: 0.0252837951366718\n",
      "step: 696639, loss: 0.06505849957466125, data time: 0.02361576897757394\n",
      "step: 696640, loss: 0.06164199113845825, data time: 0.02218284606933594\n",
      "step: 696641, loss: 0.06345584243535995, data time: 0.020923495292663574\n",
      "step: 696642, loss: 0.061790697276592255, data time: 0.019811335731955135\n",
      "step: 696643, loss: 0.058984022587537766, data time: 0.01881555716196696\n",
      "step: 696644, loss: 0.06489451229572296, data time: 0.017929629275673313\n",
      "step: 696645, loss: 0.05717439576983452, data time: 0.01713900566101074\n",
      "step: 696646, loss: 0.060540370643138885, data time: 0.016427573703584216\n",
      "step: 696647, loss: 0.06386154145002365, data time: 0.01577656919305975\n",
      "step: 696648, loss: 0.06941936910152435, data time: 0.015173839486163595\n",
      "step: 696649, loss: 0.06218891963362694, data time: 0.014623920122782389\n",
      "step: 696650, loss: 0.06401318311691284, data time: 0.014117851257324218\n",
      "step: 696651, loss: 0.06365686655044556, data time: 0.01365193036886362\n",
      "step: 696652, loss: 0.05927412956953049, data time: 0.01321589505230939\n",
      "step: 696653, loss: 0.06516988575458527, data time: 0.01281874520438058\n",
      "step: 696654, loss: 0.05754861235618591, data time: 0.012453276535560345\n",
      "step: 696655, loss: 0.05812717601656914, data time: 0.012108977635701497\n",
      "step: 696656, loss: 0.05630296468734741, data time: 0.01178543798385128\n",
      "step: 696657, loss: 0.05589867755770683, data time: 0.011486515402793884\n",
      "step: 696658, loss: 0.05781622976064682, data time: 0.011195739110310873\n",
      "step: 696659, loss: 0.06773354113101959, data time: 0.010922018219442928\n",
      "step: 696660, loss: 0.060847412794828415, data time: 0.010664619718279157\n",
      "step: 696661, loss: 0.06229345127940178, data time: 0.010421401924557157\n",
      "step: 696662, loss: 0.06843018531799316, data time: 0.010191608119655299\n",
      "step: 696663, loss: 0.06359614431858063, data time: 0.009978695919639185\n",
      "step: 696664, loss: 0.06253872811794281, data time: 0.009776702293982873\n",
      "step: 696665, loss: 0.041594844311475754, data time: 0.009581565856933594\n",
      "step: 696666, loss: 0.06323173642158508, data time: 0.30580830574035645\n",
      "step: 696667, loss: 0.05776631832122803, data time: 0.15403485298156738\n",
      "step: 696668, loss: 0.06012692302465439, data time: 0.10376707712809245\n",
      "step: 696669, loss: 0.062034934759140015, data time: 0.07848501205444336\n",
      "step: 696670, loss: 0.0644453912973404, data time: 0.06307077407836914\n",
      "step: 696671, loss: 0.057655103504657745, data time: 0.052804152170817055\n",
      "step: 696672, loss: 0.06193001568317413, data time: 0.045460598809378486\n",
      "step: 696673, loss: 0.06464072316884995, data time: 0.04002824425697327\n",
      "step: 696674, loss: 0.06161050125956535, data time: 0.03584077623155382\n",
      "step: 696675, loss: 0.06333589553833008, data time: 0.032489562034606935\n",
      "step: 696676, loss: 0.052660297602415085, data time: 0.029763568531383167\n",
      "step: 696677, loss: 0.06175323575735092, data time: 0.027491013209025066\n",
      "step: 696678, loss: 0.06493043154478073, data time: 0.025563936967116136\n",
      "step: 696679, loss: 0.06102738901972771, data time: 0.023907218660627092\n",
      "step: 696680, loss: 0.05972011014819145, data time: 0.02247196833292643\n",
      "step: 696681, loss: 0.06334468722343445, data time: 0.02121661603450775\n",
      "step: 696682, loss: 0.06062968820333481, data time: 0.020113538293277517\n",
      "step: 696683, loss: 0.0624706856906414, data time: 0.019124070803324383\n",
      "step: 696684, loss: 0.06218801066279411, data time: 0.018238933462845653\n",
      "step: 696685, loss: 0.0551200769841671, data time: 0.017449474334716795\n",
      "step: 696686, loss: 0.05482807010412216, data time: 0.016740083694458008\n",
      "step: 696687, loss: 0.062342386692762375, data time: 0.01609083739194003\n",
      "step: 696688, loss: 0.06217999383807182, data time: 0.01549324781998344\n",
      "step: 696689, loss: 0.06160399317741394, data time: 0.01494427522023519\n",
      "step: 696690, loss: 0.06159445270895958, data time: 0.014441022872924805\n",
      "step: 696691, loss: 0.06334909796714783, data time: 0.013979306587806115\n",
      "step: 696692, loss: 0.0660671517252922, data time: 0.013548656746193214\n",
      "step: 696693, loss: 0.06380055844783783, data time: 0.013148673943110875\n",
      "step: 696694, loss: 0.06581797450780869, data time: 0.012779112519889042\n",
      "step: 696695, loss: 0.062150340527296066, data time: 0.012438098589579264\n",
      "step: 696696, loss: 0.06696536391973495, data time: 0.01211681673603673\n",
      "step: 696697, loss: 0.060807161033153534, data time: 0.011819548904895782\n",
      "step: 696698, loss: 0.06257262825965881, data time: 0.011523557431770094\n",
      "step: 696699, loss: 0.05885430797934532, data time: 0.011244598556967342\n",
      "step: 696700, loss: 0.06311234831809998, data time: 0.01098095348903111\n",
      "step: 696701, loss: 0.06707195937633514, data time: 0.010731107658810086\n",
      "step: 696702, loss: 0.06591439247131348, data time: 0.010493961540428368\n",
      "step: 696703, loss: 0.06252449750900269, data time: 0.010274002426549009\n",
      "step: 696704, loss: 0.058433353900909424, data time: 0.010065751198010568\n",
      "step: 696705, loss: 0.052032068371772766, data time: 0.00986790657043457\n",
      "step: 696706, loss: 0.06546612083911896, data time: 0.3022727966308594\n",
      "step: 696707, loss: 0.06278060376644135, data time: 0.15230178833007812\n",
      "step: 696708, loss: 0.06349318474531174, data time: 0.10254915555318196\n",
      "step: 696709, loss: 0.05951424688100815, data time: 0.07764756679534912\n",
      "step: 696710, loss: 0.0545533262193203, data time: 0.06238560676574707\n",
      "step: 696711, loss: 0.061739157885313034, data time: 0.05222471555074056\n",
      "step: 696712, loss: 0.06085082143545151, data time: 0.04495917047773089\n",
      "step: 696713, loss: 0.06358738988637924, data time: 0.03959128260612488\n",
      "step: 696714, loss: 0.05721045657992363, data time: 0.035335010952419706\n",
      "step: 696715, loss: 0.05685979127883911, data time: 0.03200986385345459\n",
      "step: 696716, loss: 0.0630754828453064, data time: 0.029292648488825016\n",
      "step: 696717, loss: 0.05921299010515213, data time: 0.02703118324279785\n",
      "step: 696718, loss: 0.0639636218547821, data time: 0.025130253571730394\n",
      "step: 696719, loss: 0.0665169358253479, data time: 0.023480790002005442\n",
      "step: 696720, loss: 0.0620194673538208, data time: 0.02207342783610026\n",
      "step: 696721, loss: 0.06135139986872673, data time: 0.020844146609306335\n",
      "step: 696722, loss: 0.057656656950712204, data time: 0.0197601739098044\n",
      "step: 696723, loss: 0.05865411087870598, data time: 0.01879494720035129\n",
      "step: 696724, loss: 0.06546324491500854, data time: 0.017931461334228516\n",
      "step: 696725, loss: 0.05830804631114006, data time: 0.01715805530548096\n",
      "step: 696726, loss: 0.06522223353385925, data time: 0.01646030516851516\n",
      "step: 696727, loss: 0.061172276735305786, data time: 0.015819636258212002\n",
      "step: 696728, loss: 0.06383856385946274, data time: 0.015232625214949898\n",
      "step: 696729, loss: 0.06253045052289963, data time: 0.014695505301157633\n",
      "step: 696730, loss: 0.06555399298667908, data time: 0.01420475959777832\n",
      "step: 696731, loss: 0.05953487753868103, data time: 0.013750571470994215\n",
      "step: 696732, loss: 0.06556206941604614, data time: 0.013327271850020797\n",
      "step: 696733, loss: 0.05691785737872124, data time: 0.012936464377811976\n",
      "step: 696734, loss: 0.060401808470487595, data time: 0.012589051805693528\n",
      "step: 696735, loss: 0.05860535427927971, data time: 0.012253022193908692\n",
      "step: 696736, loss: 0.05970718711614609, data time: 0.01193531866996519\n",
      "step: 696737, loss: 0.06392304599285126, data time: 0.011643141508102417\n",
      "step: 696738, loss: 0.06116896867752075, data time: 0.011352445140029446\n",
      "step: 696739, loss: 0.06249874085187912, data time: 0.011078617152045755\n",
      "step: 696740, loss: 0.05938131734728813, data time: 0.010820668084280831\n",
      "step: 696741, loss: 0.06843870878219604, data time: 0.01057389047410753\n",
      "step: 696742, loss: 0.06627512723207474, data time: 0.010343235892218512\n",
      "step: 696743, loss: 0.06347629427909851, data time: 0.010126640922144839\n",
      "step: 696744, loss: 0.06871499866247177, data time: 0.009920511490259415\n",
      "step: 696745, loss: 0.05145072937011719, data time: 0.009724682569503785\n",
      "step: 696746, loss: 0.06128939986228943, data time: 0.3054006099700928\n",
      "step: 696747, loss: 0.06137465685606003, data time: 0.15391135215759277\n",
      "step: 696748, loss: 0.05703841149806976, data time: 0.10366590817769368\n",
      "step: 696749, loss: 0.060819949954748154, data time: 0.07850676774978638\n",
      "step: 696750, loss: 0.054543282836675644, data time: 0.06308469772338868\n",
      "step: 696751, loss: 0.05354952812194824, data time: 0.052816311518351235\n",
      "step: 696752, loss: 0.05688760429620743, data time: 0.045469965253557475\n",
      "step: 696753, loss: 0.05896952375769615, data time: 0.04003703594207764\n",
      "step: 696754, loss: 0.05876753851771355, data time: 0.03573547469245063\n",
      "step: 696755, loss: 0.05970406532287598, data time: 0.032359647750854495\n",
      "step: 696756, loss: 0.06390281021595001, data time: 0.02961999719793146\n",
      "step: 696757, loss: 0.06553999334573746, data time: 0.027332226435343426\n",
      "step: 696758, loss: 0.060588158667087555, data time: 0.025391908792349007\n",
      "step: 696759, loss: 0.07006755471229553, data time: 0.023724828447614397\n",
      "step: 696760, loss: 0.0623771958053112, data time: 0.022277307510375977\n",
      "step: 696761, loss: 0.05484219640493393, data time: 0.021008417010307312\n",
      "step: 696762, loss: 0.06253866851329803, data time: 0.01989307123072007\n",
      "step: 696763, loss: 0.06447167694568634, data time: 0.01889857980940077\n",
      "step: 696764, loss: 0.05921831727027893, data time: 0.018008257213391755\n",
      "step: 696765, loss: 0.059588029980659485, data time: 0.017213261127471922\n",
      "step: 696766, loss: 0.05851226672530174, data time: 0.01649437631879534\n",
      "step: 696767, loss: 0.06465186923742294, data time: 0.015857176347212357\n",
      "step: 696768, loss: 0.05999106168746948, data time: 0.015254932901133661\n",
      "step: 696769, loss: 0.0640869066119194, data time: 0.014702181021372477\n",
      "step: 696770, loss: 0.06006976217031479, data time: 0.014193696975708008\n",
      "step: 696771, loss: 0.06390134245157242, data time: 0.013726408664996807\n",
      "step: 696772, loss: 0.05767403170466423, data time: 0.013290369952166523\n",
      "step: 696773, loss: 0.061956942081451416, data time: 0.012887188366481237\n",
      "step: 696774, loss: 0.05917361378669739, data time: 0.012518849866143588\n",
      "step: 696775, loss: 0.0604519359767437, data time: 0.012174375851949056\n",
      "step: 696776, loss: 0.06719189882278442, data time: 0.01185018785538212\n",
      "step: 696777, loss: 0.06217943876981735, data time: 0.011553525924682617\n",
      "step: 696778, loss: 0.06149567291140556, data time: 0.011261614886197176\n",
      "step: 696779, loss: 0.06102422997355461, data time: 0.010986594592823702\n",
      "step: 696780, loss: 0.05866310000419617, data time: 0.01072812761579241\n",
      "step: 696781, loss: 0.06286497414112091, data time: 0.010482152303059896\n",
      "step: 696782, loss: 0.05601298063993454, data time: 0.010249182984635636\n",
      "step: 696783, loss: 0.06547082960605621, data time: 0.0100323150032445\n",
      "step: 696784, loss: 0.06314488500356674, data time: 0.009825883767543694\n",
      "step: 696785, loss: 0.0659884661436081, data time: 0.009630548954010009\n",
      "step: 696786, loss: 0.053963810205459595, data time: 0.29949307441711426\n",
      "step: 696787, loss: 0.05965562164783478, data time: 0.1513211727142334\n",
      "step: 696788, loss: 0.05940043553709984, data time: 0.10137979189554851\n",
      "step: 696789, loss: 0.06270191073417664, data time: 0.07679837942123413\n",
      "step: 696790, loss: 0.0638522207736969, data time: 0.06172938346862793\n",
      "step: 696791, loss: 0.06452669948339462, data time: 0.05167504151662191\n",
      "step: 696792, loss: 0.06743347644805908, data time: 0.0444918019430978\n",
      "step: 696793, loss: 0.0595351979136467, data time: 0.03918537497520447\n",
      "step: 696794, loss: 0.05883229896426201, data time: 0.03497624397277832\n",
      "step: 696795, loss: 0.06642551720142365, data time: 0.031696319580078125\n",
      "step: 696796, loss: 0.06079918146133423, data time: 0.02901437065818093\n",
      "step: 696797, loss: 0.06456662714481354, data time: 0.0267755389213562\n",
      "step: 696798, loss: 0.06291550397872925, data time: 0.02488257334782527\n",
      "step: 696799, loss: 0.0676458328962326, data time: 0.023247582571847097\n",
      "step: 696800, loss: 0.060002222657203674, data time: 0.021831973393758138\n",
      "step: 696801, loss: 0.058055710047483444, data time: 0.020591139793395996\n",
      "step: 696802, loss: 0.06170107424259186, data time: 0.01949966655058019\n",
      "step: 696803, loss: 0.06252013146877289, data time: 0.018523494402567547\n",
      "step: 696804, loss: 0.06087131425738335, data time: 0.01766155895433928\n",
      "step: 696805, loss: 0.06657891720533371, data time: 0.016884636878967286\n",
      "step: 696806, loss: 0.058071963489055634, data time: 0.01618338766552153\n",
      "step: 696807, loss: 0.06517568230628967, data time: 0.015544013543562456\n",
      "step: 696808, loss: 0.06265273690223694, data time: 0.014955157819001572\n",
      "step: 696809, loss: 0.06478913128376007, data time: 0.014417638381322226\n",
      "step: 696810, loss: 0.058604925870895386, data time: 0.013921937942504882\n",
      "step: 696811, loss: 0.06461339443922043, data time: 0.013470099522517277\n",
      "step: 696812, loss: 0.06306924670934677, data time: 0.013048666494864004\n",
      "step: 696813, loss: 0.06284154206514359, data time: 0.01265334231512887\n",
      "step: 696814, loss: 0.05842690169811249, data time: 0.012289598070341965\n",
      "step: 696815, loss: 0.06040053814649582, data time: 0.011951351165771484\n",
      "step: 696816, loss: 0.06285201013088226, data time: 0.011639956505067887\n",
      "step: 696817, loss: 0.05655132234096527, data time: 0.011346802115440369\n",
      "step: 696818, loss: 0.06499145925045013, data time: 0.011064861759994968\n",
      "step: 696819, loss: 0.06305079907178879, data time: 0.010799309786628275\n",
      "step: 696820, loss: 0.07100017368793488, data time: 0.010548019409179687\n",
      "step: 696821, loss: 0.05782494693994522, data time: 0.010323239697350396\n",
      "step: 696822, loss: 0.06304347515106201, data time: 0.01009908882347313\n",
      "step: 696823, loss: 0.060189198702573776, data time: 0.009889119549801475\n",
      "step: 696824, loss: 0.06405012309551239, data time: 0.009689881251408504\n",
      "step: 696825, loss: 0.043494753539562225, data time: 0.009501636028289795\n",
      "step: 696826, loss: 0.06181354448199272, data time: 0.29955077171325684\n",
      "step: 696827, loss: 0.06180886551737785, data time: 0.1515566110610962\n",
      "step: 696828, loss: 0.05817756801843643, data time: 0.10154310862223308\n",
      "step: 696829, loss: 0.061798255890607834, data time: 0.07693696022033691\n",
      "step: 696830, loss: 0.06242551654577255, data time: 0.061823129653930664\n",
      "step: 696831, loss: 0.05879915878176689, data time: 0.05175320307413737\n",
      "step: 696832, loss: 0.06538589298725128, data time: 0.04455137252807617\n",
      "step: 696833, loss: 0.06251826882362366, data time: 0.03923904895782471\n",
      "step: 696834, loss: 0.06798458099365234, data time: 0.035028033786349826\n",
      "step: 696835, loss: 0.05923228710889816, data time: 0.03173222541809082\n",
      "step: 696836, loss: 0.05778662487864494, data time: 0.029039296236905186\n",
      "step: 696837, loss: 0.0629052147269249, data time: 0.026793301105499268\n",
      "step: 696838, loss: 0.06348863244056702, data time: 0.02489465933579665\n",
      "step: 696839, loss: 0.06361015886068344, data time: 0.023267013686043874\n",
      "step: 696840, loss: 0.06066809222102165, data time: 0.021852397918701173\n",
      "step: 696841, loss: 0.06386636942625046, data time: 0.020626187324523926\n",
      "step: 696842, loss: 0.057361066341400146, data time: 0.0195312920738669\n",
      "step: 696843, loss: 0.05727756768465042, data time: 0.018552700678507488\n",
      "step: 696844, loss: 0.05850180611014366, data time: 0.01768009286177786\n",
      "step: 696845, loss: 0.07174797356128693, data time: 0.016911029815673828\n",
      "step: 696846, loss: 0.06027175113558769, data time: 0.01621088527497791\n",
      "step: 696847, loss: 0.05835537239909172, data time: 0.01557148586619984\n",
      "step: 696848, loss: 0.05763017386198044, data time: 0.014982306438943615\n",
      "step: 696849, loss: 0.0630505234003067, data time: 0.014445573091506958\n",
      "step: 696850, loss: 0.06383803486824036, data time: 0.013948135375976563\n",
      "step: 696851, loss: 0.059092484414577484, data time: 0.013489448107205905\n",
      "step: 696852, loss: 0.06517168879508972, data time: 0.013062115068788882\n",
      "step: 696853, loss: 0.06436912715435028, data time: 0.01267235619681222\n",
      "step: 696854, loss: 0.058281511068344116, data time: 0.01230881954061574\n",
      "step: 696855, loss: 0.06718061864376068, data time: 0.011972268422444662\n",
      "step: 696856, loss: 0.05788533017039299, data time: 0.011654561565768334\n",
      "step: 696857, loss: 0.06033360958099365, data time: 0.011365294456481934\n",
      "step: 696858, loss: 0.06386370211839676, data time: 0.011078538316668886\n",
      "step: 696859, loss: 0.06940804421901703, data time: 0.010812724337858312\n",
      "step: 696860, loss: 0.06337937712669373, data time: 0.010558605194091797\n",
      "step: 696861, loss: 0.05898096784949303, data time: 0.010326981544494629\n",
      "step: 696862, loss: 0.056460462510585785, data time: 0.010099005054783177\n",
      "step: 696863, loss: 0.06638815253973007, data time: 0.009888843486183569\n",
      "step: 696864, loss: 0.06260204315185547, data time: 0.009687442045945387\n",
      "step: 696865, loss: 0.05981551110744476, data time: 0.009497809410095214\n",
      "step: 696866, loss: 0.05895078182220459, data time: 0.2991197109222412\n",
      "step: 696867, loss: 0.065546914935112, data time: 0.15030980110168457\n",
      "step: 696868, loss: 0.0636015385389328, data time: 0.10111331939697266\n",
      "step: 696869, loss: 0.063524529337883, data time: 0.0766211748123169\n",
      "step: 696870, loss: 0.060584720224142075, data time: 0.06158132553100586\n",
      "step: 696871, loss: 0.06580635160207748, data time: 0.051559011141459145\n",
      "step: 696872, loss: 0.05955148860812187, data time: 0.04439599173409598\n",
      "step: 696873, loss: 0.06539340317249298, data time: 0.03910273313522339\n",
      "step: 696874, loss: 0.06702010333538055, data time: 0.03490355279710558\n",
      "step: 696875, loss: 0.06634360551834106, data time: 0.03161103725433349\n",
      "step: 696876, loss: 0.06413203477859497, data time: 0.028932137922807175\n",
      "step: 696877, loss: 0.0689426064491272, data time: 0.02670075496037801\n",
      "step: 696878, loss: 0.061792295426130295, data time: 0.0248110294342041\n",
      "step: 696879, loss: 0.06331022828817368, data time: 0.023187450000217984\n",
      "step: 696880, loss: 0.0526038333773613, data time: 0.021776851018269858\n",
      "step: 696881, loss: 0.06388945877552032, data time: 0.020542845129966736\n",
      "step: 696882, loss: 0.0644490048289299, data time: 0.019457760979147518\n",
      "step: 696883, loss: 0.06291964650154114, data time: 0.018487837579515245\n",
      "step: 696884, loss: 0.0655655711889267, data time: 0.017628694835462068\n",
      "step: 696885, loss: 0.057142164558172226, data time: 0.016858839988708497\n",
      "step: 696886, loss: 0.05979312211275101, data time: 0.016158274241856167\n",
      "step: 696887, loss: 0.0582674965262413, data time: 0.015518654476512562\n",
      "step: 696888, loss: 0.05981377884745598, data time: 0.014930486679077148\n",
      "step: 696889, loss: 0.05920761823654175, data time: 0.014394839604695639\n",
      "step: 696890, loss: 0.0624382309615612, data time: 0.013901119232177734\n",
      "step: 696891, loss: 0.06272044777870178, data time: 0.013448045803950382\n",
      "step: 696892, loss: 0.05989662930369377, data time: 0.01302196361400463\n",
      "step: 696893, loss: 0.0608043298125267, data time: 0.012634141104561942\n",
      "step: 696894, loss: 0.0633242055773735, data time: 0.012271026085162985\n",
      "step: 696895, loss: 0.06162872910499573, data time: 0.011933231353759765\n",
      "step: 696896, loss: 0.06214657425880432, data time: 0.011615714719218592\n",
      "step: 696897, loss: 0.06273840367794037, data time: 0.011323228478431702\n",
      "step: 696898, loss: 0.06156156584620476, data time: 0.011038744088375208\n",
      "step: 696899, loss: 0.06294216215610504, data time: 0.010769970276776482\n",
      "step: 696900, loss: 0.062231820076704025, data time: 0.010516854694911412\n",
      "step: 696901, loss: 0.06333908438682556, data time: 0.010276065932379829\n",
      "step: 696902, loss: 0.06053142994642258, data time: 0.010049388215348527\n",
      "step: 696903, loss: 0.06217215582728386, data time: 0.009838236005682694\n",
      "step: 696904, loss: 0.06798768788576126, data time: 0.009637704262366662\n",
      "step: 696905, loss: 0.07525959610939026, data time: 0.009446072578430175\n",
      "step: 696906, loss: 0.05501289293169975, data time: 0.29526472091674805\n",
      "step: 696907, loss: 0.06155601888895035, data time: 0.14880359172821045\n",
      "step: 696908, loss: 0.05424460023641586, data time: 0.09971475601196289\n",
      "step: 696909, loss: 0.059486113488674164, data time: 0.07573962211608887\n",
      "step: 696910, loss: 0.06171279400587082, data time: 0.06087789535522461\n",
      "step: 696911, loss: 0.05905570089817047, data time: 0.05096137523651123\n",
      "step: 696912, loss: 0.06187456473708153, data time: 0.043938636779785156\n",
      "step: 696913, loss: 0.05939023941755295, data time: 0.03869935870170593\n",
      "step: 696914, loss: 0.06648232042789459, data time: 0.03455731603834364\n",
      "step: 696915, loss: 0.0589066743850708, data time: 0.03130052089691162\n",
      "step: 696916, loss: 0.06355288624763489, data time: 0.02864924344149503\n",
      "step: 696917, loss: 0.061142273247241974, data time: 0.026439766089121502\n",
      "step: 696918, loss: 0.06812246143817902, data time: 0.024574885001549356\n",
      "step: 696919, loss: 0.055424340069293976, data time: 0.022969824927193776\n",
      "step: 696920, loss: 0.05910707637667656, data time: 0.02157290776570638\n",
      "step: 696921, loss: 0.06036325916647911, data time: 0.020351171493530273\n",
      "step: 696922, loss: 0.05578294396400452, data time: 0.019273828057681814\n",
      "step: 696923, loss: 0.061359867453575134, data time: 0.01831140783098009\n",
      "step: 696924, loss: 0.059495698660612106, data time: 0.01745520139995374\n",
      "step: 696925, loss: 0.06224268302321434, data time: 0.016694891452789306\n",
      "step: 696926, loss: 0.0678248479962349, data time: 0.016011658168974378\n",
      "step: 696927, loss: 0.06079491972923279, data time: 0.015380035747181286\n",
      "step: 696928, loss: 0.06505756825208664, data time: 0.014798827793287195\n",
      "step: 696929, loss: 0.06375136226415634, data time: 0.014266719420750936\n",
      "step: 696930, loss: 0.06213071197271347, data time: 0.013782463073730468\n",
      "step: 696931, loss: 0.06410254538059235, data time: 0.0133284330368042\n",
      "step: 696932, loss: 0.06747149676084518, data time: 0.012905633008038556\n",
      "step: 696933, loss: 0.06591620296239853, data time: 0.012516447475978307\n",
      "step: 696934, loss: 0.06504403054714203, data time: 0.012157678604125977\n",
      "step: 696935, loss: 0.06354789435863495, data time: 0.011824639638264973\n",
      "step: 696936, loss: 0.06036774814128876, data time: 0.011512394874326645\n",
      "step: 696937, loss: 0.05827287957072258, data time: 0.011226296424865723\n",
      "step: 696938, loss: 0.06584316492080688, data time: 0.010944467602354107\n",
      "step: 696939, loss: 0.07166217267513275, data time: 0.010677898631376378\n",
      "step: 696940, loss: 0.06407082080841064, data time: 0.010426889147077288\n",
      "step: 696941, loss: 0.05887710303068161, data time: 0.010187831189897325\n",
      "step: 696942, loss: 0.0627366453409195, data time: 0.009963776614214923\n",
      "step: 696943, loss: 0.059103816747665405, data time: 0.009754519713552375\n",
      "step: 696944, loss: 0.06010165438055992, data time: 0.009556097862048026\n",
      "step: 696945, loss: 0.05958294868469238, data time: 0.009370124340057373\n",
      "step: 696946, loss: 0.06873083859682083, data time: 0.3075132369995117\n",
      "step: 696947, loss: 0.059093303978443146, data time: 0.15486359596252441\n"
     ]
    }
   ],
   "source": [
    "runner = Diffusion(args, config, config.device)\n",
    "# runner.sample()\n",
    "# runner.test()\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "lcc_arn": "arn:aws:sagemaker:eu-central-1:268765770896:studio-lifecycle-config/juma-gend35-dev-kernel-eu-central-1-adeb109009e3143877e389ec1f2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
